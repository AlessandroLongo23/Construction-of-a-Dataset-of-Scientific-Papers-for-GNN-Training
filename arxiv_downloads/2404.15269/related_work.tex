\section{Related Work}

We describe related work in this area grouped by main themes in this work.


\paragraph{Learning from Feedback.} Besides pair-wise comparison feedback from annotators used in Reinforcement Learning from Human Feedback (RLHF) research~\citep[inter alia]{Ziegler2019FineTuningLM,Stiennon2020LearningTS,Nakano2021WebGPTBQ,Ouyang2022TrainingLM}, prior work has also studied free-form text feedback provided by annotators ~\citep{Fernandes2023BridgingTG}, such as on the task of dialog~\citep{Weston2016DialogbasedLL,Li2016DialogueLW,Hancock2019LearningFD,Xu2022LearningNS,Petrak2023LearningFF}, question answering~\citep{Li2022UsingIF,Malaviya2023PachinkoPI}, summarization~\citep{Saunders2022SelfcritiquingMF}, and general decision making~\citep{cheng2023llf}. This feedback, tailored to each example, is often utilized to rank candidate outputs, thereby improving task performance. Some work studies learning from text feedback to generate outputs directly~\citep{Scheurer2023TrainingLM,Bai2022ConstitutionalAH,Shi2022WhenLG}, by generating multiple refinements of the original output based on the feedback and fine-tuning the original model to maximize the likelihood of the best refinement. In grounded settings such as instruction-based navigation, one line of work has also used hindsight feedback that explicitly provides a text instruction for the generated trajectory, to train policies~\citep{nguyen2021interactive,misra2024provable}. Moving beyond the conventional focus on text feedback that explicitly articulates human intent, we investigate feedback in the form of direct edits on the original model output. Such revisions by users occur naturally during model deployment in practice.  Additionally, we examine the learning of user preferences through historical interactions, aiming to surpass the constraints of example-specific feedback.

\paragraph{Language Agents and Personalization.} LLMs have enabled the development of language agents for a variety of tasks from writing assistants~\citep{Lee2024ADS}, coding assistants~\citep{githubcopilot}, and customer service assistants~\citep{brynjolfsson2023generative}. Since these LLM-based assistants are often used by individuals, a natural question has arisen on how to personalize these agents for each user. Straightforward approaches for fine-tuning LLMs includes supervised learning, online DPO~\citep{guo2024direct}, learning-to-search~\citep{chang2023learning}, and reinforcement learning~\citep{ouyang2022training}. These approaches can be directly applied to our setting. For example, one can use $(y_t, y'_t)$ in~\pref{proto:learning_from_edits} as the preference data where $y'_t$ is preferred over $y_t$, or use $y'_t$ as the ground truth for supervised learning. However, fine-tuning is expensive and hard to scale with the number of users. Therefore, a line of work has explored improving the alignment of frozen LLMs by \emph{prompt engineering}, such as learning a personalized retrieval model~\citep{mysore2023pearl}, learning a prompt policy given a reward function~\citep{deng2022rlprompt}, or more generally, learning to rewrite the entire prompt~\citep{li2023automatic}. We focus on learning a prompt policy by learning from user edits, and specifically, using them to extract textural descriptions of user preference.\looseness=-1

\paragraph{Edits and Revisions.} Many prior work on editing model output focuses on error correction, such as fixing source code~\citep{Yin2018LearningTR,Chen2018SequenceRSL,Reid2023DiffusERDV} and improving the factual consistency of model summaries~\citep{Cao2020FactualEC,Liu2022OnIS,Balachandran2022CorrectingDF}. A line of work has explored understanding human edits based on edit history of Wikipedia~\citep{Botha2018LearningTS,Faltings2020TextEB,Rajagopal2022OneDM,Reid2022LearningTM,Laban2023SWiPEAD}, or revisions of academic writings~\citep{Mita2022TowardsAD,Du2022UnderstandingIR,DArcy2023ARIESAC}. Prior work explores predicting text revisions with edit intents~\citep{Brody2020ASM,Kim2022ImprovingIT,Chong2023LeveragingPT}, and modeling edits with various approaches, including latent vectors~\citep{Guu2017GeneratingSB,MarreseTaylor2020VariationalIF,MarreseTaylor2023EditAR}, structured trees~\citep{Yao2021LearningSE}, discrete diffusion process~\citep{Reid2023DiffusERDV}, or a series of singular edit operations~\citep{Stahlberg2020Seq2EditsST,Mallinson2020FELIXFT,Agrawal2022AnIL,Zhang2022CoditT5PF,Liu2023SecondTA}. However, these methodologies predominantly target generic improvements in model performance, overlooking the intricacies of individual user satisfaction and preference. Our research takes a distinct direction, focusing on understanding edits across a variety of examples to study user-level preferences, with a practical goal of aligning the agent to individual preferences.
