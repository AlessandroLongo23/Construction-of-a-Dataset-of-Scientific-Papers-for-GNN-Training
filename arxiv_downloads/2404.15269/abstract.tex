\begin{abstract}
We study interactive learning of language agents based on user edits made to the agent's output. 
In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their \emph{latent} preference, in addition to improving the correctness. The edit feedback is \emph{naturally generated}, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time.
We propose a learning framework, \textbf{\framework}, to conduct \textbf{PRE}ference \textbf{L}earning from \textbf{U}ser's \textbf{D}irect \textbf{E}dits by inferring a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference  can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named \textbf{\algname} (\textbf{C}onsolidates \textbf{I}nduced \textbf{P}references based on \textbf{H}istorical \textbf{E}dits with \textbf{R}etrieval). \algname~leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, \algname~retrieves inferred preferences from the $k$-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. %
We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks,~\algname~outperforms baselines by achieving the lowest edit distance cost. Meanwhile,~\algname~has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits. Our further analysis reports that the user preference learned by~\algname~shows significant similarity to the ground truth latent preference.\footnote{Our code and data are publicly available at \url{https://github.com/gao-g/prelude}.}\looseness=-1






\end{abstract}
