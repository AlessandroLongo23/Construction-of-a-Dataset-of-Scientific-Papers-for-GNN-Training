% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}
@inproceedings{goldstein-carbonell-1998-summarization,
    title = "Summarization: (1) Using {MMR} for Diversity- Based Reranking and (2) Evaluating Summaries",
    author = "Goldstein, Jade  and
      Carbonell, Jaime",
    booktitle = "TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",
    month = oct,
    year = "1998",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/X98-1025",
    doi = "10.3115/1119089.1119120",
    pages = "181--195",
}

@inproceedings{shin_few-shot_2022,
	address = {Seattle, United States},
	title = {Few-{Shot} {Semantic} {Parsing} with {Language} {Models} {Trained} on {Code}},
	url = {https://aclanthology.org/2022.naacl-main.396},
	abstract = {Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.},
	urldate = {2022-07-10},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Shin, Richard and Van Durme, Benjamin},
	month = jul,
	year = {2022},
	keywords = {code\_plms, few\_shot, semantic\_parsing},
	pages = {5417--5425},
	file = {Shin_Van Durme_2022_Few-Shot Semantic Parsing with Language Models Trained on Code.pdf:/Users/bking/Zotero/storage/AS29J6SM/Shin_Van Durme_2022_Few-Shot Semantic Parsing with Language Models Trained on Code.pdf:application/pdf},
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{mehri_dialoglue_2020,
	title = {{DialoGLUE}: {A} {Natural} {Language} {Understanding} {Benchmark} for {Task}-{Oriented} {Dialogue}},
	shorttitle = {{DialoGLUE}},
	url = {http://arxiv.org/abs/2009.13570},
	abstract = {A long-standing goal of task-oriented dialogue research is the ability to flexibly adapt dialogue models to new domains. To progress research in this direction, we introduce DialoGLUE (Dialogue Language Understanding Evaluation), a public benchmark consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks, designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. We release several strong baseline models, demonstrating performance improvements over a vanilla BERT architecture and state-of-the-art results on 5 out of 7 tasks, by pre-training on a large open-domain dialogue corpus and task-adaptive self-supervised training. Through the DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we hope to facilitate progress towards the goal of developing more general task-oriented dialogue models.},
	urldate = {2022-01-06},
	journal = {arXiv:2009.13570 [cs]},
	author = {Mehri, Shikib and Eric, Mihail and Hakkani-Tur, Dilek},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13570},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Benchmark hosted on: https://evalai.cloudcv.org/web/challenges/challenge-page/708/},
	file = {arXiv Fulltext PDF:/Users/bking/Zotero/storage/P33QDC3A/Mehri et al. - 2020 - DialoGLUE A Natural Language Understanding Benchm.pdf:application/pdf;arXiv.org Snapshot:/Users/bking/Zotero/storage/IYHKYNAG/2009.html:text/html},
}

@misc{hu_-context_2022,
	title = {In-{Context} {Learning} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
	url = {http://arxiv.org/abs/2203.08568},
	abstract = {Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin on MultiWOZ.},
	urldate = {2022-06-26},
	publisher = {arXiv},
	author = {Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A. and Ostendorf, Mari},
	month = may,
	year = {2022},
	note = {Number: arXiv:2203.08568
arXiv:2203.08568 [cs]},
	keywords = {Computer Science - Computation and Language, cites:schema\_driven\_prompting\_DST},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/4WGYCCTI/2203.html:text/html;Hu et al_2022_In-Context Learning for Few-Shot Dialogue State Tracking.pdf:/Users/bking/Zotero/storage/ZIXI2QCB/Hu et al_2022_In-Context Learning for Few-Shot Dialogue State Tracking.pdf:application/pdf},
}
@inproceedings{ye_multiwoz_2022,
	address = {Edinburgh, UK},
	title = {{MultiWOZ} 2.4: {A} {Multi}-{Domain} {Task}-{Oriented} {Dialogue} {Dataset} with {Essential} {Annotation} {Corrections} to {Improve} {State} {Tracking} {Evaluation}},
	shorttitle = {{MultiWOZ} 2.4},
	url = {https://aclanthology.org/2022.sigdial-1.34},
	abstract = {The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented dialogue systems. However, its state annotations contain substantial noise, which hinders a proper evaluation of model performance. To address this issue, massive efforts were devoted to correcting the annotations. Three improved versions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there are still plenty of incorrect and inconsistent annotations. This work introduces MultiWOZ 2.4, which refines the annotations in the validation set and test set of MultiWOZ 2.1. The annotations in the training set remain unchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model training. We benchmark eight state-of-the-art dialogue state tracking models on MultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ 2.1.},
	urldate = {2022-12-28},
	booktitle = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Ye, Fanghua and Manotumruksa, Jarana and Yilmaz, Emine},
	month = sep,
	year = {2022},
	pages = {351--360},
	file = {Ye et al_2022_MultiWOZ 2.pdf:/Users/bking/Zotero/storage/L6NMMCZS/Ye et al_2022_MultiWOZ 2.pdf:application/pdf},
}

@inproceedings{eric_multiwoz_2020,
	address = {Marseille, France},
	title = {{MultiWOZ} 2.1: {A} {Consolidated} {Multi}-{Domain} {Dialogue} {Dataset} with {State} {Corrections} and {State} {Tracking} {Baselines}},
	isbn = {979-10-95546-34-4},
	shorttitle = {{MultiWOZ} 2.1},
	url = {https://aclanthology.org/2020.lrec-1.53},
	abstract = {MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there are substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32\% of state annotations across 40\% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.},
	language = {English},
	urldate = {2022-12-29},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Eric, Mihail and Goel, Rahul and Paul, Shachi and Sethi, Abhishek and Agarwal, Sanchit and Gao, Shuyang and Kumar, Adarsh and Goyal, Anuj and Ku, Peter and Hakkani-Tur, Dilek},
	month = may,
	year = {2020},
	pages = {422--428},
	file = {Eric et al_2020_MultiWOZ 2.pdf:/Users/bking/Zotero/storage/WN4G56HF/Eric et al_2020_MultiWOZ 2.pdf:application/pdf},
}
@inproceedings{henderson-etal-2014-word,
    title = "Word-Based Dialog State Tracking with Recurrent Neural Networks",
    author = "Henderson, Matthew  and
      Thomson, Blaise  and
      Young, Steve",
    booktitle = "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})",
    month = jun,
    year = "2014",
    address = "Philadelphia, PA, U.S.A.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4340",
    doi = "10.3115/v1/W14-4340",
    pages = "292--299",
}
@inproceedings{mrksic-etal-2017-neural,
    title = "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
    author = "Mrk{\v{s}}i{\'c}, Nikola  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Wen, Tsung-Hsien  and
      Thomson, Blaise  and
      Young, Steve",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1163",
    doi = "10.18653/v1/P17-1163",
    pages = "1777--1788",
    abstract = "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user{'}s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users{'} language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",
}

@inproceedings{wu_improving_2020,
	address = {Online},
	title = {Improving {Limited} {Labeled} {Dialogue} {State} {Tracking} with {Self}-{Supervision}},
	url = {https://aclanthology.org/2020.findings-emnlp.400},
	doi = {10.18653/v1/2020.findings-emnlp.400},
	abstract = {Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives: preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve joint goal accuracy by 8.95\% when only 1\% labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76\% improvement if some unlabeled data is jointly trained as semi-supervised learning. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.},
	urldate = {2022-12-29},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Chien-Sheng and Hoi, Steven C.H. and Xiong, Caiming},
	month = nov,
	year = {2020},
	pages = {4462--4472},
	file = {Wu et al_2020_Improving Limited Labeled Dialogue State Tracking with Self-Supervision.pdf:/Users/bking/Zotero/storage/WY6FXYCR/Wu et al_2020_Improving Limited Labeled Dialogue State Tracking with Self-Supervision.pdf:application/pdf},
}
@inproceedings{ min2022rethinking,
    title={ Rethinking the Role of Demonstrations: What makes In-context Learning Work? },
    author={ Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke },
    booktitle={ EMNLP },
    year={ 2022 }
}
@article{chen_schema-guided_2020,
	title = {Schema-{Guided} {Multi}-{Domain} {Dialogue} {State} {Tracking} with {Graph} {Attention} {Neural} {Networks}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6250},
	doi = {10.1609/aaai.v34i05.6250},
	abstract = {Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is also a major obstacle due to the increased number of state candidates. Existing approaches generally predict the value for each slot independently and do not consider slot relations, which may aggravate the data sparsity problem. In this paper, we propose a Schema-guided multi-domain dialogue State Tracker with graph attention networks (SST) that predicts dialogue states from dialogue utterances and schema graphs which contain slot relations in edges. We also introduce a graph attention matching network to fuse information from utterances and graphs, and a recurrent graph attention network to control state updating. Experiment results show that our approach obtains new state-of-the-art performance on both MultiWOZ 2.0 and MultiWOZ 2.1 benchmarks.},
	language = {en},
	number = {05},
	urldate = {2023-01-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Lu and Lv, Boer and Wang, Chi and Zhu, Su and Tan, Bowen and Yu, Kai},
	month = apr,
	year = {2020},
	note = {Number: 05},
	pages = {7521--7528},
	file = {Chen et al_2020_Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural.pdf:/Users/bking/Zotero/storage/89RD8R9X/Chen et al_2020_Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2022-04-24},
	journal = {arXiv:1910.10683 [cs, stat]},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv: 1910.10683},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Final version as published in JMLR},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/MSCF3HVV/1910.html:text/html;Raffel et al_2020_Exploring the Limits of Transfer Learning with a Unified Text-to-Text.pdf:/Users/bking/Zotero/storage/LLJNJFMW/Raffel et al_2020_Exploring the Limits of Transfer Learning with a Unified Text-to-Text.pdf:application/pdf},
}

@inproceedings{lewis_bart_2020,
	address = {Online},
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://aclanthology.org/2020.acl-main.703},
	doi = {10.18653/v1/2020.acl-main.703},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
	urldate = {2021-11-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	month = jul,
	year = {2020},
	pages = {7871--7880},
	file = {Lewis et al_2020_BART.pdf:/Users/bking/Zotero/storage/2F3AE7K8/Lewis et al_2020_BART.pdf:application/pdf},
}

@misc{venkateswaran_district_2022,
	title = {{DiSTRICT}: {Dialogue} {State} {Tracking} with {Retriever} {Driven} {In}-{Context} {Tuning}},
	shorttitle = {{DiSTRICT}},
	url = {http://arxiv.org/abs/2212.02851},
	abstract = {Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often have limited resource availability.},
	urldate = {2023-01-15},
	publisher = {arXiv},
	author = {Venkateswaran, Praveen and Duesterwald, Evelyn and Isahagian, Vatche},
	month = dec,
	year = {2022},
	note = {arXiv:2212.02851 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Venkateswaran et al_2022_DiSTRICT.pdf:/Users/bking/Zotero/storage/V73TGJQF/Venkateswaran et al_2022_DiSTRICT.pdf:application/pdf},
}
@inproceedings{Daille1994ApprocheMP,
  title={Approche mixte pour l'extraction de terminologie : statistique lexicale et filtres linguistiques},
  author={B{\'e}atrice Daille},
  year={1994}
}

@inproceedings{eric_multiwoz_2020,
	address = {Marseille, France},
	title = {{MultiWOZ} 2.1: {A} {Consolidated} {Multi}-{Domain} {Dialogue} {Dataset} with {State} {Corrections} and {State} {Tracking} {Baselines}},
	isbn = {979-10-95546-34-4},
	shorttitle = {{MultiWOZ} 2.1},
	url = {https://aclanthology.org/2020.lrec-1.53},
	abstract = {MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there are substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32\% of state annotations across 40\% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future.},
	language = {English},
	urldate = {2022-12-29},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Eric, Mihail and Goel, Rahul and Paul, Shachi and Sethi, Abhishek and Agarwal, Sanchit and Gao, Shuyang and Kumar, Adarsh and Goyal, Anuj and Ku, Peter and Hakkani-Tur, Dilek},
	month = may,
	year = {2020},
	pages = {422--428},
	file = {Eric et al_2020_MultiWOZ 2.pdf:/Users/bking/Zotero/storage/WN4G56HF/Eric et al_2020_MultiWOZ 2.pdf:application/pdf},
}

@inproceedings{zang_multiwoz_2020,
	address = {Online},
	title = {{MultiWOZ} 2.2 : {A} {Dialogue} {Dataset} with {Additional} {Annotation} {Corrections} and {State} {Tracking} {Baselines}},
	shorttitle = {{MultiWOZ} 2.2},
	url = {https://aclanthology.org/2020.nlp4convai-1.13},
	doi = {10.18653/v1/2020.nlp4convai-1.13},
	abstract = {MultiWOZ is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. MultiWOZ 2.1 identified and fixed many of these erroneous annotations and user utterances, resulting in an improved version of this dataset. This work introduces MultiWOZ 2.2, which is a yet another improved version of this dataset. Firstly, we identify and fix dialogue state annotation errors across 17.3\% of the utterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors.},
	urldate = {2021-12-30},
	booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
	month = jul,
	year = {2020},
	pages = {109--117},
	file = {Zang et al_2020_MultiWOZ 2.pdf:/Users/bking/Zotero/storage/WNJDQQEG/Zang et al_2020_MultiWOZ 2.pdf:application/pdf},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	doi = {10.48550/arXiv.1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published in ICLR 2020},
}

@misc{song_mpnet_2020,
	title = {{MPNet}: {Masked} and {Permuted} {Pre}-training for {Language} {Understanding}},
	shorttitle = {{MPNet}},
	url = {http://arxiv.org/abs/2004.09297},
	doi = {10.48550/arXiv.2004.09297},
	abstract = {BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. The code and the pre-trained models are available at: https://github.com/microsoft/MPNet.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	month = nov,
	year = {2020},
	note = {arXiv:2004.09297 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bking/Zotero/storage/9JAUVRFD/Song et al_2020_MPNet.pdf:application/pdf},
}

@article{han_multiwoz_2021,
	title = {{MultiWOZ} 2.3: {A} multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation},
	shorttitle = {{MultiWOZ} 2.3},
	url = {http://arxiv.org/abs/2010.05594},
	abstract = {Task-oriented dialogue systems have made unprecedented progress with multiple state-of-the-art (SOTA) models underpinned by a number of publicly available MultiWOZ datasets. Dialogue state annotations are error-prone, leading to sub-optimal performance. Various efforts have been put in rectifying the annotation errors presented in the original MultiWOZ dataset. In this paper, we introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in dialogue acts from dialogue states, identifying a lack of co-reference when publishing the updated dataset. To ensure consistency between dialogue acts and dialogue states, we implement co-reference features and unify annotations of dialogue acts and dialogue states. We update the state of the art performance of natural language understanding and dialogue state tracking on MultiWOZ 2.3, where the results show significant improvements than on previous versions of MultiWOZ datasets (2.0-2.2).},
	urldate = {2022-01-06},
	journal = {arXiv:2010.05594 [cs]},
	author = {Han, Ting and Liu, Ximing and Takanobu, Ryuichi and Lian, Yixin and Huang, Chongxuan and Wan, Dazhen and Peng, Wei and Huang, Minlie},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.05594
version: 3},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/bking/Zotero/storage/48TICES8/Han et al. - 2021 - MultiWOZ 2.3 A multi-domain task-oriented dialogu.pdf:application/pdf;arXiv.org Snapshot:/Users/bking/Zotero/storage/EJTKVSMJ/2010.html:text/html},
}

@misc{ye_multiwoz_2022,
	title = {{MultiWOZ} 2.4: {A} {Multi}-{Domain} {Task}-{Oriented} {Dialogue} {Dataset} with {Essential} {Annotation} {Corrections} to {Improve} {State} {Tracking} {Evaluation}},
	shorttitle = {{MultiWOZ} 2.4},
	url = {http://arxiv.org/abs/2104.00773},
	doi = {10.48550/arXiv.2104.00773},
	abstract = {The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented dialogue systems. However, its state annotations contain substantial noise, which hinders a proper evaluation of model performance. To address this issue, massive efforts were devoted to correcting the annotations. Three improved versions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there are still plenty of incorrect and inconsistent annotations. This work introduces MultiWOZ 2.4, which refines the annotations in the validation set and test set of MultiWOZ 2.1. The annotations in the training set remain unchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model training. We benchmark eight state-of-the-art dialogue state tracking models on MultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ 2.1.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Ye, Fanghua and Manotumruksa, Jarana and Yilmaz, Emine},
	month = jul,
	year = {2022},
	note = {arXiv:2104.00773 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to SIGDIAL 2022 (https://2022.sigdial.org/)},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/FAPGQYAW/2104.html:text/html;Ye et al_2022_MultiWOZ 2.pdf:/Users/bking/Zotero/storage/KP8AG8NY/Ye et al_2022_MultiWOZ 2.pdf:application/pdf},
}
@article{Rajkumar2022EvaluatingTT,
  title={Evaluating the Text-to-SQL Capabilities of Large Language Models},
  author={Nitarshan Rajkumar and Raymond Li and Dzmitry Bahdanau},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.00498}
}
@misc{nijkamp_codegen_2022,
	title = {{CodeGen}: {An} {Open} {Large} {Language} {Model} for {Code} with {Multi}-{Turn} {Program} {Synthesis}},
	shorttitle = {{CodeGen}},
	url = {http://arxiv.org/abs/2203.13474},
	doi = {10.48550/arXiv.2203.13474},
	abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	month = sep,
	year = {2022},
	note = {arXiv:2203.13474 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {Nijkamp et al_2022_CodeGen.pdf:/Users/bking/Zotero/storage/DE8HWHFN/Nijkamp et al_2022_CodeGen.pdf:application/pdf},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: corrected typos, added references, added authors, added acknowledgements},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/ZUFLYQEA/2107.html:text/html;Chen et al_2021_Evaluating Large Language Models Trained on Code.pdf:/Users/bking/Zotero/storage/B6EM6TRG/Chen et al_2021_Evaluating Large Language Models Trained on Code.pdf:application/pdf},
}

@article{li_task_2024,
	title = {Task {Contamination}: {Language} {Models} {May} {Not} {Be} {Few}-{Shot} {Anymore}},
	volume = {38},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29808},
	doi = {10.1609/aaai.v38i16.29808},
	number = {16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Changmao and Flanigan, Jeffrey},
	month = mar,
	year = {2024},
	pages = {18471--18480},
}

@article{roemmele_choice_nodate,
	title = {Choice of {Plausible} {Alternatives}: {An} {Evaluation} of {Commonsense} {Causal} {Reasoning}},
	abstract = {Research in open-domain commonsense reasoning has been hindered by the lack of evaluation metrics for judging progress and comparing alternative approaches. Taking inspiration from large-scale question sets used in natural language processing research, we authored one thousand English-language questions that directly assess commonsense causal reasoning, called the Choice Of Plausible Alternatives (COPA) evaluation. Using a forcedchoice format, each question gives a premise and two plausible causes or effects, where the correct choice is the alternative that is more plausible than the other. This paper describes the authoring methodology that we used to develop a validated question set with sufficient breadth to advance open-domain commonsense reasoning research. We discuss the design decisions made during the authoring process, and explain how these decisions will affect the design of high-scoring systems. We also present the performance of multiple baseline approaches that use statistical natural language processing techniques, establishing initial benchmarks for future systems.},
	language = {en},
	author = {Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
	file = {Roemmele et al. - Choice of Plausible Alternatives An Evaluation of.pdf:/Users/bking/Zotero/storage/6K5W46RT/Roemmele et al. - Choice of Plausible Alternatives An Evaluation of.pdf:application/pdf},
}
@inproceedings{roemmele2011choice,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI spring symposium: logical formalizations of commonsense reasoning},
  pages={90--95},
  year={2011}
}
@INPROCEEDINGS{hadsell_et_all_2006,
  author={Hadsell, R. and Chopra, S. and LeCun, Y.},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
  title={Dimensionality Reduction by Learning an Invariant Mapping}, 
  year={2006},
  volume={2},
  number={},
  pages={1735-1742},
  doi={10.1109/CVPR.2006.100}}
@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}
@inproceedings{liu-etal-2022-makes,
    title = "What Makes Good In-Context Examples for {GPT}-3?",
    author = "Liu, Jiachang  and
      Shen, Dinghan  and
      Zhang, Yizhe  and
      Dolan, Bill  and
      Carin, Lawrence  and
      Chen, Weizhu",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.10",
    doi = "10.18653/v1/2022.deelio-1.10",
    pages = "100--114",
    abstract = "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3{'}s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3{'}s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3{\%} on the ToTTo dataset) and open-domain question answering (45.5{\%} on the NQ dataset).",
}
@inproceedings{hosseini-asl_simple_2020,
	title = {A {Simple} {Language} {Model} for {Task}-{Oriented} {Dialogue}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/e946209592563be0f01c844ab2170f0c-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hosseini-Asl, Ehsan and McCann, Bryan and Wu, Chien-Sheng and Yavuz, Semih and Socher, Richard},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {20179--20191},
}
@article{li2022controllable,
  title={Controllable Dialogue Simulation with In-Context Learning},
  author={Li, Zekun and Chen, Wenhu and Li, Shiyang and Wang, Hong and Qian, Jing and Yan, Xifeng},
  journal={arXiv preprint arXiv:2210.04185},
  year={2022}
}

@misc{levy_diverse_2022,
	title = {Diverse {Demonstrations} {Improve} {In}-context {Compositional} {Generalization}},
	url = {http://arxiv.org/abs/2212.06800},
	abstract = {In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input question. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning.},
	urldate = {2023-01-21},
	publisher = {arXiv},
	author = {Levy, Itay and Bogin, Ben and Berant, Jonathan},
	month = dec,
	year = {2022},
	note = {arXiv:2212.06800 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/N4GX3ALK/2212.html:text/html;Levy et al_2022_Diverse Demonstrations Improve In-context Compositional Generalization.pdf:/Users/bking/Zotero/storage/6GJWMC9Z/Levy et al_2022_Diverse Demonstrations Improve In-context Compositional Generalization.pdf:application/pdf},
}

@article{liu_unsupervised_2023,
	title = {Unsupervised {Dialogue} {State} {Tracking} for {End}-to-{End} {Task}-{Oriented} {Dialogue} with a {Multi}-{Span} {Prediction} {Network}},
	volume = {38},
	issn = {1860-4749},
	url = {https://doi.org/10.1007/s11390-021-1064-y},
	doi = {10.1007/s11390-021-1064-y},
	abstract = {This paper focuses on end-to-end task-oriented dialogue systems, which jointly handle dialogue state tracking (DST) and response generation. Traditional methods usually adopt a supervised paradigm to learn DST from a manually labeled corpus. However, the annotation of the corpus is costly, time-consuming, and cannot cover a wide range of domains in the real world. To solve this problem, we propose a multi-span prediction network (MSPN) that performs unsupervised DST for end-to-end task-oriented dialogue. Specifically, MSPN contains a novel split-merge copy mechanism that captures long-term dependencies in dialogues to automatically extract multiple text spans as keywords. Based on these keywords, MSPN uses a semantic distance based clustering approach to obtain the values of each slot. In addition, we propose an ontology-based reinforcement learning approach, which employs the values of each slot to train MSPN to generate relevant values. Experimental results on single-domain and multi-domain task-oriented dialogue datasets show that MSPN achieves state-of-the-art performance with significant improvements. Besides, we construct a new Chinese dialogue dataset MeDial in the low-resource medical domain, which further demonstrates the adaptability of MSPN.},
	number = {4},
	journal = {Journal of Computer Science and Technology},
	author = {Liu, Qing-Bin and He, Shi-Zhu and Liu, Cao and Liu, Kang and Zhao, Jun},
	month = jul,
	year = {2023},
	pages = {834--852},
}
@article{
li2023starcoder,
title={StarCoder: may the source be with you!},
author={Raymond Li and Loubna Ben allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia LI and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Joel Lamy-Poirier and Joao Monteiro and Nicolas Gontier and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Ben Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason T Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Urvashi Bhattacharyya and Wenhao Yu and Sasha Luccioni and Paulo Villegas and Fedor Zhdanov and Tony Lee and Nadav Timor and Jennifer Ding and Claire S Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro Von Werra and Harm de Vries},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=KoFOg41haE},
note={Reproducibility Certification}
}
@inproceedings{
feng2023fantastic,
title={Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems},
author={Yihao Feng and Shentao Yang and Shujian Zhang and Jianguo Zhang and Caiming Xiong and Mingyuan Zhou and Huan Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=086pmarAris}
}
@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{yu-etal-2023-krls,
    title = "{KRLS}: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
    author = "Yu, Xiao  and
      Wu, Qingyang  and
      Qian, Kun  and
      Yu, Zhou",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.759",
    doi = "10.18653/v1/2023.emnlp-main.759",
    pages = "12338--12358",
    abstract = "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an approach to create a more efficient RL-based algorithm to improve TOD performance in an offline setting. First, we use a faster generation procedure that samples from independent next-word distributions after training the language model (LM) with supervised learning. We then introduce a fine-grained reward function to help the model focus on learning key information in a dialog, by measuring the importance and semantic closeness of each generated token. Experiments on the MultiWoZ dataset show our new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), achieves state-of-the-art performance on the end-to-end response generation task, with a 15{\%} training time reduction compared to a standard RL algorithm using auto-regressive generation.",
}
@inproceedings{min-etal-2022-rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.759",
    doi = "10.18653/v1/2022.emnlp-main.759",
    pages = "11048--11064",
    abstract = "Large language models (LMs) are able to in-context learn{---}perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required{---}randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
}
@inproceedings{zhang-etal-2023-sgp,
    title = "{SGP}-{TOD}: Building Task Bots Effortlessly via Schema-Guided {LLM} Prompting",
    author = "Zhang, Xiaoying  and
      Peng, Baolin  and
      Li, Kun  and
      Zhou, Jingyan  and
      Meng, Helen",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.891",
    doi = "10.18653/v1/2023.findings-emnlp.891",
    pages = "13348--13369",
    abstract = "Building and maintaining end-to-end task bots using minimal human effort is a long-standing challenge in dialog research. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on large language models (LLMs). Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data. Specifically, SGP-TOD comprises three components: an LLM for interacting with users, a Dialog State Tracking (DST) Prompter to aid the LLM in tracking dialog states with the given belief instruction, and a Policy Prompter to direct the LLM to generate proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE, and STAR datasets show that our training-free strategy, SGP-TOD, yields state-of-the-art (SOTA) zero-shot performance, significantly surpassing the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available.",
}
@misc{fuzzywuzzy-2011,
  title = {fuzzywuzzy},
  year = {2011},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/seatgeek/fuzzywuzzy}}
}
@InProceedings{pmlr-v139-zhao21c,
  title = 	 {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author =       {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12697--12706},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract = 	 {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the models bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2s accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}

@article{zhu2022convlab3,
    title={ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format},
    author={Qi Zhu and Christian Geishauser and Hsien-chin Lin and Carel van Niekerk and Baolin Peng and Zheng Zhang and Michael Heck and Nurul Lubis and Dazhen Wan and Xiaochen Zhu and Jianfeng Gao and Milica Gai and Minlie Huang},
    journal={arXiv preprint arXiv:2211.17148},
    year={2022},
    url={http://arxiv.org/abs/2211.17148}
}

@article{holtzman_surface_2021,
	title = {Surface {Form} {Competition}: {Why} the {Highest} {Probability} {Answer} {Isn}'t {Always} {Right}},
	shorttitle = {Surface {Form} {Competition}},
	url = {http://arxiv.org/abs/2104.08315},
	abstract = {Large language models have shown promising results in zero-shot settings (Brown et al.,2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept, e.g. "computer" and "PC." Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.},
	urldate = {2021-10-26},
	journal = {arXiv:2104.08315 [cs]},
	author = {Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.08315},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/bking/Zotero/storage/QKN9EIFW/Holtzman et al. - 2021 - Surface Form Competition Why the Highest Probabil.pdf:application/pdf;arXiv.org Snapshot:/Users/bking/Zotero/storage/K9G2A3RY/2104.html:text/html},
}

@article{liu_pretraining_2021,
	title = {Pretraining the {Noisy} {Channel} {Model} for {Task}-{Oriented} {Dialogue}},
	volume = {9},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00390},
	doi = {10.1162/tacl_a_00390},
	abstract = {Direct decoding for task-oriented dialogue is known to suffer from the
explaining-away effect, manifested in models that prefer short and generic
responses. Here we argue for the use of Bayes theorem to factorize the
dialogue task into two models, the distribution of the context given the
response, and the prior for the response itself. This approach, an instantiation
of the noisy channel model, both mitigates the explaining-away effect and allows
the principled incorporation of large pretrained models for the response prior.
We present extensive experiments showing that a noisy channel model decodes
better responses compared to direct decoding and that a two-stage pretraining
strategy, employing both open-domain and task-oriented dialogue data, improves
over randomly initialized models.},
	urldate = {2021-12-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Liu, Qi and Yu, Lei and Rimell, Laura and Blunsom, Phil},
	month = jul,
	year = {2021},
	pages = {657--674},
	file = {Liu et al_2021_Pretraining the Noisy Channel Model for Task-Oriented Dialogue.pdf:/Users/bking/Zotero/storage/7XGFWGER/Liu et al_2021_Pretraining the Noisy Channel Model for Task-Oriented Dialogue.pdf:application/pdf;Snapshot:/Users/bking/Zotero/storage/CQ999F4N/Pretraining-the-Noisy-Channel-Model-for-Task.html:text/html},
}

@article{dempster_maximum_1977_fixed,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 urldate = {2024-04-16},
 volume = {39},
 year = {1977}
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} {Via} the {EM} {Algorithm}},
	volume = {39},
	copyright = { 1977 The Authors},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
	doi = {10.1111/j.2517-6161.1977.tb01600.x},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	language = {en},
	number = {1},
	urldate = {2024-02-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
	keywords = {em algorithm, incomplete data, maximum likelihood, posterior mode},
	pages = {1--22},
	file = {Snapshot:/Users/bking/Zotero/storage/893EQDBT/j.2517-6161.1977.tb01600.html:text/html},
}

@misc{ye_complementary_2022,
	title = {Complementary {Explanations} for {Effective} {In}-{Context} {Learning}},
	url = {http://arxiv.org/abs/2211.13892},
	doi = {10.48550/arXiv.2211.13892},
	abstract = {Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts. Yet, there has been limited understanding of what makes explanations effective for in-context learning. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on prompting performance when using explanations: the computation trace (the way the solution is decomposed) and the natural language of the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations, indicating that LLMs do faithfully follow the explanations to some extent. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set as they are able to fuse different reasoning specified by individual exemplars in prompts. Additionally, having relevant exemplars also contributes to more effective prompts. Therefore, we propose a maximal-marginal-relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.},
	urldate = {2023-01-21},
	publisher = {arXiv},
	author = {Ye, Xi and Iyer, Srinivasan and Celikyilmaz, Asli and Stoyanov, Ves and Durrett, Greg and Pasunuru, Ramakanth},
	month = nov,
	year = {2022},
	note = {arXiv:2211.13892 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/UAPJSCVW/2211.html:text/html;Ye et al_2022_Complementary Explanations for Effective In-Context Learning.pdf:/Users/bking/Zotero/storage/ZJJLWF3T/Ye et al_2022_Complementary Explanations for Effective In-Context Learning.pdf:application/pdf},
}
@inproceedings{budzianowski2018large,
    Author = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang  and Casanueva, I{\~n}igo and Ultes Stefan and Ramadan Osman and Ga{\v{s}}i\'c, Milica},
    title={MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling},
    booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2018}
}
@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2021-11-05},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/98MS7V9E/2005.html:text/html;Brown et al_2020_Language Models are Few-Shot Learners.pdf:/Users/bking/Zotero/storage/NAJ8YWNC/Brown et al_2020_Language Models are Few-Shot Learners.pdf:application/pdf},
}

% TODO: this is at Neurips
@article{li2023guiding-fix-arxiv,
  title={Guiding Large Language Models via Directional Stimulus Prompting},
  author={Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  journal={arXiv preprint arXiv:2302.11520},
  year={2023}
}
@inproceedings{paul19b_interspeech,
  author={Shachi Paul and Rahul Goel and Dilek Hakkani-Tr},
  title={{Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1453--1457},
  doi={10.21437/Interspeech.2019-1866}
}
@inproceedings{yu2017the,
title={The Neural Noisy Channel},
author={Lei Yu and Phil Blunsom and Chris Dyer and Edward Grefenstette and Tomas Kocisky},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJ25-B5eg}
}
@inproceedings{jin_explicit_2018,
	title = {Explicit {State} {Tracking} with {Semi}-{Supervision} for {Neural} {Dialogue} {Generation}},
	url = {http://arxiv.org/abs/1808.10596},
	doi = {10.1145/3269206.3271683},
	abstract = {The task of dialogue generation aims to automatically provide responses given previous utterances. Tracking dialogue states is an important ingredient in dialogue generation for estimating users' intention. However, the {\textbackslash}emph\{expensive nature of state labeling\} and the {\textbackslash}emph\{weak interpretability\} make the dialogue state tracking a challenging problem for both task-oriented and non-task-oriented dialogue generation: For generating responses in task-oriented dialogues, state tracking is usually learned from manually annotated corpora, where the human annotation is expensive for training; for generating responses in non-task-oriented dialogues, most of existing work neglects the explicit state tracking due to the unlimited number of dialogue states. In this paper, we propose the {\textbackslash}emph\{semi-supervised explicit dialogue state tracker\} (SEDST) for neural dialogue generation. To this end, our approach has two core ingredients: {\textbackslash}emph\{CopyFlowNet\} and {\textbackslash}emph\{posterior regularization\}. Specifically, we propose an encoder-decoder architecture, named {\textbackslash}emph\{CopyFlowNet\}, to represent an explicit dialogue state with a probabilistic distribution over the vocabulary space. To optimize the training procedure, we apply a posterior regularization strategy to integrate indirect supervision. Extensive experiments conducted on both task-oriented and non-task-oriented dialogue corpora demonstrate the effectiveness of our proposed model. Moreover, we find that our proposed semi-supervised dialogue state tracker achieves a comparable performance as state-of-the-art supervised learning baselines in state tracking procedure.},
	urldate = {2023-08-16},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Jin, Xisen and Lei, Wenqiang and Ren, Zhaochun and Chen, Hongshen and Liang, Shangsong and Zhao, Yihong and Yin, Dawei},
	month = oct,
	year = {2018},
	note = {arXiv:1808.10596 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1403--1412},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/M9A4FN5Z/1808.html:text/html;Jin et al_2018_Explicit State Tracking with Semi-Supervision for Neural Dialogue Generation.pdf:/Users/bking/Zotero/storage/CKWWJJH9/Jin et al_2018_Explicit State Tracking with Semi-Supervision for Neural Dialogue Generation.pdf:application/pdf},
}


@misc{liu_variational_2021,
	title = {Variational {Latent}-{State} {GPT} for {Semi}-{Supervised} {Task}-{Oriented} {Dialog} {Systems}},
	url = {http://arxiv.org/abs/2109.04314},
	abstract = {Recently, two approaches, fine-tuning large pre-trained language models and variational training, have attracted significant interests, separately, for semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper, we propose Variational Latent-State GPT model (VLS-GPT), which is the first to combine the strengths of the two approaches. Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner. Variational training of VLS-GPT is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models, which use turn-level first-order Markovian. The inference model in VLS-GPT is non-Markovian due to the use of the Transformer architecture. In this work, we establish Recursive Monte Carlo Approximation (RMCA) to the variational objective with non-Markovian inference model and prove its unbiasedness. Further, we develop the computational strategy of sampling-then-forward-computation to realize RMCA, which successfully overcomes the memory explosion issue of using GPT in variational learning and speeds up training. Semi-supervised TOD experiments are conducted on two benchmark multi-domain datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both supervised-only and semi-supervised self-training baselines.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian and Huang, Yi and Feng, Junlan},
	year = {2021},
	note = {arXiv:2109.04314 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted into IEEE/ACM Transactions on Audio, Speech and Language Processing},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/5IMJ6YR8/2109.html:text/html;Liu et al_2023_Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems.pdf:/Users/bking/Zotero/storage/X9IM9TEG/Liu et al_2023_Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems.pdf:application/pdf},
}

@inproceedings{yoo_variational_2020,
	address = {Online},
	title = {Variational {Hierarchical} {Dialog} {Autoencoder} for {Dialog} {State} {Tracking} {Data} {Augmentation}},
	url = {https://aclanthology.org/2020.emnlp-main.274},
	doi = {10.18653/v1/2020.emnlp-main.274},
	abstract = {Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers' robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs  dialog response generation and user simulation, where our model outperforms previous strong baselines.},
	urldate = {2023-08-16},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Yoo, Kang Min and Lee, Hanbit and Dernoncourt, Franck and Bui, Trung and Chang, Walter and Lee, Sang-goo},
	month = nov,
	year = {2020},
	pages = {3406--3425},
	file = {Full Text PDF:/Users/bking/Zotero/storage/QX8VWWSE/Yoo et al. - 2020 - Variational Hierarchical Dialog Autoencoder for Di.pdf:application/pdf},
}

@article{mishra_variational_2021,
	title = {Variational {Learning} for {Unsupervised} {Knowledge} {Grounded} {Dialogs}},
	url = {http://arxiv.org/abs/2112.00653},
	abstract = {Recent methods for knowledge grounded dialogs generate responses by incorporating information from an external textual document. These methods do not require the exact document to be known during training and rely on the use of a retrieval system to fetch relevant documents from a large index. The documents used to generate the responses are modeled as latent variables whose prior probabilities need to be estimated. Models such as RAG , marginalize the document probabilities over the documents retrieved from the index to define the log likelihood loss function which is optimized end-to-end. In this paper, we develop a variational approach to the above technique wherein, we instead maximize the Evidence Lower bound (ELBO). Using a collection of three publicly available open-conversation datasets, we demonstrate how the posterior distribution, that has information from the ground-truth response, allows for a better approximation of the objective function during training. To overcome the challenges associated with sampling over a large knowledge collection, we develop an efficient approach to approximate the ELBO. To the best of our knowledge we are the first to apply variational training for open-scale unsupervised knowledge grounded dialog systems.},
	urldate = {2022-01-07},
	journal = {arXiv:2112.00653 [cs]},
	author = {Mishra, Mayank and Madan, Dhiraj and Pandey, Gaurav and Contractor, Danish},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.00653},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bking/Zotero/storage/CML7ABHC/Mishra et al. - 2021 - Variational Learning for Unsupervised Knowledge Gr.pdf:application/pdf;arXiv.org Snapshot:/Users/bking/Zotero/storage/G3WAUIJS/2112.html:text/html},
}
@article{pan_preliminary_2023,
	title = {A {Preliminary} {Evaluation} of {ChatGPT} for {Zero}-shot {Dialogue} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2304.04256},
	doi = {10.48550/ARXIV.2304.04256},
	abstract = {Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs).},
	urldate = {2024-02-05},
	author = {Pan, Wenbo and Chen, Qiguang and Xu, Xiao and Che, Wanxiang and Qin, Libo},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), e2e:must\_cite, FOS: Computer and information sciences},
	annote = {Other
Technical Report},
	file = {Pan et al_2023_A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding.pdf:/Users/bking/Zotero/storage/P9X9FXEY/Pan et al_2023_A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding.pdf:application/pdf},
}


@inproceedings{min_dialogue_2020,
	address = {Yokohama, Japan},
	title = {Dialogue {State} {Induction} {Using} {Neural} {Latent} {Variable} {Models}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/532},
	doi = {10.24963/ijcai.2020/532},
	abstract = {Dialogue state modules are a useful component in a task-oriented dialogue system. Traditional methods nd dialogue states by manually labeling training corpora, upon which neural models are trained. However, the labeling process can be costly, slow, error-prone, and more importantly, cannot cover the vast range of domains in real-world dialogues for customer service. We propose the task of dialogue state induction, building two neural latent variable models that mine dialogue states automatically from unlabeled customer service dialogue records. Results show that the models can effectively nd meaningful dialogue states. In addition, equipped with induced dialogue states, a state-ofthe-art dialogue system gives better performance compared with not using a dialogue state module.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Min, Qingkai and Qin, Libo and Teng, Zhiyang and Liu, Xiao and Zhang, Yue},
	month = jul,
	year = {2020},
	pages = {3845--3852},
	file = {Min et al. - 2020 - Dialogue State Induction Using Neural Latent Varia.pdf:/Users/bking/Zotero/storage/Z92PZ4A4/Min et al. - 2020 - Dialogue State Induction Using Neural Latent Varia.pdf:application/pdf},
}

@misc{liu_building_2022,
	title = {Building {Markovian} {Generative} {Architectures} over {Pretrained} {LM} {Backbones} for {Efficient} {Task}-{Oriented} {Dialog} {Systems}},
	url = {http://arxiv.org/abs/2204.06452},
	abstract = {Recently, Transformer based pretrained language models (PLMs), such as GPT2 and T5, have been leveraged to build generative task-oriented dialog (TOD) systems. A drawback of existing PLM-based models is their non-Markov architectures across turns, i.e., the whole history is used as the conditioning input at each turn. First, this brings inefficiencies in memory and computation. Furthermore, using the whole history increases model complexity and may hurt the training efficiency, especially when facing small amounts of labeled training data (the low-resource setting). In this paper, motivated by the observation that dialog states could be viewed as Markov states, we propose to build Markovian Generative Architectures (MGA) over PLM backbones for efficient TOD systems. Experiments on MultiWOZ2.1 show that in the rich-resource setting, the proposed Markov models reduce memory and time costs without performance degradation; in the low-resource setting, the training efficiency of the Markov models is more significant.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Liu, Hong and Cai, Yucheng and Ou, Zhijian and Huang, Yi and Feng, Junlan},
	month = oct,
	year = {2022},
	note = {arXiv:2204.06452 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, citations\_finished:081723},
	annote = {Comment: Accepted by SLT 2022},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/THQRXMCN/2204.html:text/html;Full Text PDF:/Users/bking/Zotero/storage/IFZ7KDBW/Liu et al. - 2022 - Building Markovian Generative Architectures over P.pdf:application/pdf},
}

@inproceedings{chen_dialogved_2022,
	address = {Dublin, Ireland},
	title = {{DialogVED}: {A} {Pre}-trained {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Dialog} {Response} {Generation}},
	shorttitle = {{DialogVED}},
	url = {https://aclanthology.org/2022.acl-long.333},
	doi = {10.18653/v1/2022.acl-long.333},
	abstract = {Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and diversity of responses. With the help of a large dialog corpus (Reddit), we pre-train the model using the following 4 tasks, used in training language models (LMs) and Variational Autoencoders (VAEs) literature: 1) masked language model; 2) response generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also add additional parameters to model the turn structure in dialogs to improve the performance of the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental results show that our model achieves the new state-of-the-art results on all these datasets.},
	urldate = {2023-08-16},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Wei and Gong, Yeyun and Wang, Song and Yao, Bolun and Qi, Weizhen and Wei, Zhongyu and Hu, Xiaowu and Zhou, Bartuer and Mao, Yi and Chen, Weizhu and Cheng, Biao and Duan, Nan},
	month = may,
	year = {2022},
	pages = {4852--4864},
	file = {Chen et al_2022_DialogVED.pdf:/Users/bking/Zotero/storage/GX8FGNIV/Chen et al_2022_DialogVED.pdf:application/pdf},
}

@inproceedings{zhang_css_2022,
	address = {Online only},
	title = {{CSS}: {Combining} {Self}-training and {Self}-supervised {Learning} for {Few}-shot {Dialogue} {State} {Tracking}},
	shorttitle = {{CSS}},
	url = {https://aclanthology.org/2022.aacl-short.37},
	abstract = {Few-shot dialogue state tracking (DST) is a realistic problem that trains the DST model with limited labeled data. Existing few-shot methods mainly transfer knowledge learned from external labeled dialogue data (e.g., from question answering, dialogue summarization, machine reading comprehension tasks, etc.) into DST, whereas collecting a large amount of external labeled data is laborious, and the external data may not effectively contribute to the DST-specific task. In this paper, we propose a few-shot DST framework called CSS, which Combines Self-training and Self-supervised learning methods. The unlabeled data of the DST task is incorporated into the self-training iterations, where the pseudo labels are predicted by a DST model trained on limited labeled data in advance. Besides, a contrastive self-supervised method is used to learn better representations, where the data is augmented by the dropout operation to train the model. Experimental results on the MultiWOZ dataset show that our proposed CSS achieves competitive performance in several few-shot scenarios.},
	urldate = {2023-04-10},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Haoning and Bao, Junwei and Sun, Haipeng and Luo, Huaishao and Li, Wenye and Cui, Shuguang},
	month = nov,
	year = {2022},
	pages = {302--310},
	file = {Zhang et al_2022_CSS.pdf:/Users/bking/Zotero/storage/9XI3PZHN/Zhang et al_2022_CSS.pdf:application/pdf},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/DQNZPEZR/1312.html:text/html;Kingma_Welling_2022_Auto-Encoding Variational Bayes.pdf:/Users/bking/Zotero/storage/X6AFCWMI/Kingma_Welling_2022_Auto-Encoding Variational Bayes.pdf:application/pdf},
}

@article{su2021multitask,
   author = {Yixuan Su and
             Lei Shu and
             Elman Mansimov and
             Arshit Gupta and
             Deng Cai and
             Yi{-}An Lai and
             Yi Zhang},
   title     = {Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System},
   booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)",
   publisher = "Association for Computational Linguistics",
   year      = {2022},
   url       = {https://arxiv.org/abs/2109.14739}
}
@inproceedings{klein-manning-2002-conditional,
    title = "Conditional Structure versus Conditional Estimation in {NLP} Models",
    author = "Klein, Dan  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002)",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-1002",
    doi = "10.3115/1118693.1118695",
    pages = "9--16",
}

@inproceedings{zhang_probabilistic_2020,
	address = {Online},
	title = {A {Probabilistic} {End}-{To}-{End} {Task}-{Oriented} {Dialog} {Model} with {Latent} {Belief} {States} towards {Semi}-{Supervised} {Learning}},
	url = {https://aclanthology.org/2020.emnlp-main.740},
	doi = {10.18653/v1/2020.emnlp-main.740},
	abstract = {Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50\% without performance loss on MultiWOZ.},
	urldate = {2023-07-27},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yichi and Ou, Zhijian and Hu, Min and Feng, Junlan},
	month = nov,
	year = {2020},
	keywords = {citations\_finished:081723},
	pages = {9207--9219},
	file = {Full Text PDF:/Users/bking/Zotero/storage/4PY2GCPN/Zhang et al. - 2020 - A Probabilistic End-To-End Task-Oriented Dialog Mo.pdf:application/pdf},
}
@article{rastogi_towards_2020,
	title = {Towards {Scalable} {Multi}-{Domain} {Conversational} {Agents}: {The} {Schema}-{Guided} {Dialogue} {Dataset}},
	volume = {34},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6394},
	doi = {10.1609/aaai.v34i05.6394},
	number = {05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
	month = apr,
	year = {2020},
	pages = {8689--8696},
}

@misc{chung_instructtods_2023,
	title = {{InstructTODS}: {Large} {Language} {Models} for {End}-to-{End} {Task}-{Oriented} {Dialogue} {Systems}},
	shorttitle = {{InstructTODS}},
	url = {http://arxiv.org/abs/2310.08885},
	abstract = {Large language models (LLMs) have been used for diverse tasks in natural language processing (NLP), yet remain under-explored for task-oriented dialogue systems (TODS), especially for end-to-end TODS. We present InstructTODS, a novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue systems that can adapt to diverse domains without fine-tuning. By leveraging LLMs, InstructTODS generates a proxy belief state that seamlessly translates user intentions into dynamic queries for efficient interaction with any KB. Our extensive experiments demonstrate that InstructTODS achieves comparable performance to fully fine-tuned TODS in guiding dialogues to successful completion without prior knowledge or task-specific data. Furthermore, a rigorous human evaluation of end-to-end TODS shows that InstructTODS produces dialogue responses that notably outperform both the gold responses and the state-of-the-art TODS in terms of helpfulness, informativeness, and humanness. Moreover, the effectiveness of LLMs in TODS is further supported by our comprehensive evaluations on TODS subtasks: dialogue state tracking, intent classification, and response generation. Code and implementations could be found here https://github.com/WillyHC22/InstructTODS/},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Chung, Willy and Cahyawijaya, Samuel and Wilie, Bryan and Lovenia, Holy and Fung, Pascale},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08885 [cs]},
	keywords = {Computer Science - Computation and Language, concept\_scoop},
	annote = {MyTLDR: they propose a true zero-shot system for our problem which 1) uses instruction tuned massive LLMs and 2) forgoes unlabelled data or any training. This should be a strong zero-shot baseline for our system. 
},
	file = {arXiv.org Snapshot:/Users/bking/Zotero/storage/PR7FQYTW/2310.html:text/html;Chung et al_2023_InstructTODS.pdf:/Users/bking/Zotero/storage/NY4XE9QZ/Chung et al_2023_InstructTODS.pdf:application/pdf},
}

@inproceedings{zhang-etal-2022-css,
    title = "{CSS}: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking",
    author = "Zhang, Haoning  and
      Bao, Junwei  and
      Sun, Haipeng  and
      Luo, Huaishao  and
      Li, Wenye  and
      Cui, Shuguang",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.37",
    pages = "302--310",
    abstract = "Few-shot dialogue state tracking (DST) is a realistic problem that trains the DST model with limited labeled data. Existing few-shot methods mainly transfer knowledge learned from external labeled dialogue data (e.g., from question answering, dialogue summarization, machine reading comprehension tasks, etc.) into DST, whereas collecting a large amount of external labeled data is laborious, and the external data may not effectively contribute to the DST-specific task. In this paper, we propose a few-shot DST framework called CSS, which Combines Self-training and Self-supervised learning methods. The unlabeled data of the DST task is incorporated into the self-training iterations, where the pseudo labels are predicted by a DST model trained on limited labeled data in advance. Besides, a contrastive self-supervised method is used to learn better representations, where the data is augmented by the dropout operation to train the model. Experimental results on the MultiWOZ dataset show that our proposed CSS achieves competitive performance in several few-shot scenarios.",
}
@inproceedings{qin-etal-2023-end,
    title = "End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions",
    author = "Qin, Libo  and
      Pan, Wenbo  and
      Chen, Qiguang  and
      Liao, Lizi  and
      Yu, Zhou  and
      Zhang, Yue  and
      Che, Wanxiang  and
      Li, Min",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.363",
    doi = "10.18653/v1/2023.emnlp-main.363",
    pages = "5925--5941",
    abstract = "End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step to present a thorough survey of this research field; (2) New taxonomy: we first introduce a unified perspective for EToD, including (i) Modularly EToD and (ii) Fully EToD; (3) New Frontiers: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) Abundant resources: we build a public website, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.",
}
