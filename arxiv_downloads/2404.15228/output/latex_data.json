{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "ReThinking Inverse Graphics With Large Language Models",
            "leftover": "ReThinking Inverse Graphics With Large Language Models",
            "matches": []
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{\\name Peter Kulits\\textsuperscript{*} \\email kulits@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Haiwen Feng\\textsuperscript{*} \\email hfeng@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Weiyang Liu \\email wl396@cam.ac.uk \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany, University of Cambridge \\AND \\name Victoria Abrevaya \\email vabrevaya@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Michael J.~Black \\email black@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}",
            "leftover": "{\\name Peter Kulits\\textsuperscript{*} \\email kulits@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Haiwen Feng\\textsuperscript{*} \\email hfeng@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Weiyang Liu \\email wl396@cam.ac.uk \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany, University of Cambridge \\AND \\name Victoria Abrevaya \\email vabrevaya@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}bingen, Germany \\AND \\name Michael J.~Black \\email black@tue.mpg.de \\addr Max Planck Institute for Intelligent Systems, T{''u}",
            "matches": []
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Inverse graphics  the task of inverting}an image into physical variables that, when rendered, enable reproduction of the observed scene  is a fundamental challenge in computer vision and graphics. Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment. This requirement limits the ability of existing carefully engineered approaches to generalize across domains. Inspired by the zeroshot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inversegraphics problems. To this end, we propose the InverseGraphics Large Language Model (\\mbox{IGLLM}), an inversegraphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3Dscene representation. We incorporate a frozen pretrained visual encoder and a continuous numeric head to enable endtoend training. Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through nexttoken prediction, without the use of imagespace supervision. Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs. We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at \\hbox{https://igllm.is.tue.mpg.de/}",
            "leftover": "Inverse graphics  the task of inverting}an image into physical variables that, when rendered, enable reproduction of the observed scene  is a fundamental challenge in computer vision and graphics. Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment. This requirement limits the ability of existing carefully engineered approaches to generalize across domains. Inspired by the zeroshot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inversegraphics problems. To this end, we propose the InverseGraphics Large Language Model (\\mbox{IGLLM}), an inversegraphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3Dscene representation. We incorporate a frozen pretrained visual encoder and a continuous numeric head to enable endtoend training. Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through nexttoken prediction, without the use of imagespace supervision. Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs. We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at \\hbox{https://igllm.is.tue.mpg.de/}",
            "matches": []
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 3,
                            "key": "doc/body/sec0/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "Introduction",
                            "matches": []
                        },
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec0/txl0",
                            "block type": "txl",
                            "content": "The formulation of vision as ''inverse graphics'' traces its roots back at least to (see also and ). While the term encompasses various ideas and approaches to vision problems, it is often equated with ''analysis by synthesis''. What is typically meant here, however, is more akin to model fitting. This generally presupposes that one has models of the world, knows roughly where they are, and then fits them to image evidence.",
                            "leftover": "The formulation of vision as ''inverse graphics'' traces its roots back at least to (see also and ). While the term encompasses various ideas and approaches to vision problems, it is often equated with ''analysis by synthesis''. What is typically meant here, however, is more akin to model fitting. This generally presupposes that one has models of the world, knows roughly where they are, and then fits them to image evidence.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec0/figure1",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec0/figure1/subfigure0",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 5,
                                            "key": "doc/body/sec0/figure1/subfigure0/txl0",
                                            "block type": "txl",
                                            "content": "{0.655} \\includegraphics[width=]{figures/diagram/pipeline.pdf}",
                                            "leftover": "{0.655} \\includegraphics[width=]{figures/diagram/pipeline.pdf}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec0/figure1/subfigure1",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 6,
                                            "key": "doc/body/sec0/figure1/subfigure1/txl0",
                                            "block type": "txl",
                                            "content": "{0.325} \\small : Compositional Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/clevr/input/teaser.jpg} \\hspace{0.025cm} \\raisebox{0.25in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/clevr/output/teaser.jpg} : ParameterSpace Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/2d/sparsecheckerboard.pdf} \\hspace{0.025cm} \\raisebox{0.4in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/2d/floatscatter.pdf} : VisualDomain Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/airplane/6dof/input/teaser.jpg} \\hspace{0.025cm} \\raisebox{0.4in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/airplane/6dof/output/teaser.jpg}",
                                            "leftover": "{0.325} \\small : Compositional Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/clevr/input/teaser.jpg} \\hspace{0.025cm} \\raisebox{0.25in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/clevr/output/teaser.jpg} : ParameterSpace Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/2d/sparsecheckerboard.pdf} \\hspace{0.025cm} \\raisebox{0.4in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/2d/floatscatter.pdf} : VisualDomain Generalization \\vspace{0.025cm} \\includegraphics[width=0.400]{figures/airplane/6dof/input/teaser.jpg} \\hspace{0.025cm} \\raisebox{0.4in}{⇒} \\hspace{0.025cm} \\includegraphics[width=0.400]{figures/airplane/6dof/output/teaser.jpg}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec0/figure1/cpt2",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 7,
                                            "key": "doc/body/sec0/figure1/cpt2/txl0",
                                            "block type": "txl",
                                            "content": "IGLLM. We present the InverseGraphics Large Language Model (IGLLM) framework, a general approach to solving inversegraphics problems. We instructiontune an LLM to decode a visual (CLIP) embedding into graphics code that can be used to reproduce the observed scene using a standard graphics engine. Leveraging the broad reasoning abilities of LLMs, we demonstrate that our framework exhibits natural generalization across a variety of distribution shifts without the use of special inductive biases.",
                                            "leftover": "IGLLM. We present the InverseGraphics Large Language Model (IGLLM) framework, a general approach to solving inversegraphics problems. We instructiontune an LLM to decode a visual (CLIP) embedding into graphics code that can be used to reproduce the observed scene using a standard graphics engine. Leveraging the broad reasoning abilities of LLMs, we demonstrate that our framework exhibits natural generalization across a variety of distribution shifts without the use of special inductive biases.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec0/txl2",
                            "block type": "txl",
                            "content": "A more strict interpretation of inverse graphics targets the creation of a graphics program: a structured representation that can be used by a rendering engine to approximately reproduce the 3D scene. These programs are compact and interpretable representations of visual primitives, thereby aiding scene comprehension. The objective extends beyond mere pixel or objectlevel interpretation of an image; it seeks to leverage the inherent spatial and physical relationships among objects that are essential for holistic scene understanding.",
                            "leftover": "A more strict interpretation of inverse graphics targets the creation of a graphics program: a structured representation that can be used by a rendering engine to approximately reproduce the 3D scene. These programs are compact and interpretable representations of visual primitives, thereby aiding scene comprehension. The objective extends beyond mere pixel or objectlevel interpretation of an image; it seeks to leverage the inherent spatial and physical relationships among objects that are essential for holistic scene understanding.",
                            "matches": []
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec0/txl3",
                            "block type": "txl",
                            "content": "Our goal is to generate such a graphics program from a single image capable of reproducing the 3D scene and its constituent objects using a traditional graphics engine. This approach, known as visual program induction, has garnered significant attention, encompassing works on a variety of problem domains. propose the concept of ''neural scene derendering,'' wherein custom markup code, translatable into rendererfriendly inputs, is inferred from images. While their method handles synthetic scenes featuring arbitrarily many objects, it grapples with generalization beyond its trainingdata distribution. Subsequent research explores the utility of graphics programs for the downstream task of visual question answering (VQA). However, their scenereconstruction method still struggles with generalization, particularly regarding objects with unseen attribute combinations (e.g., a known shape with a novel shapecolor combination).",
                            "leftover": "Our goal is to generate such a graphics program from a single image capable of reproducing the 3D scene and its constituent objects using a traditional graphics engine. This approach, known as visual program induction, has garnered significant attention, encompassing works on a variety of problem domains. propose the concept of ''neural scene derendering,'' wherein custom markup code, translatable into rendererfriendly inputs, is inferred from images. While their method handles synthetic scenes featuring arbitrarily many objects, it grapples with generalization beyond its trainingdata distribution. Subsequent research explores the utility of graphics programs for the downstream task of visual question answering (VQA). However, their scenereconstruction method still struggles with generalization, particularly regarding objects with unseen attribute combinations (e.g., a known shape with a novel shapecolor combination).",
                            "matches": []
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec0/txl4",
                            "block type": "txl",
                            "content": "To address the problem of generalization, a method must possess a deep understanding of the visual world and its physical properties. Here, we explore whether we can exploit the generalization abilities of large language models (LLMs) for this purpose. LLMs have demonstrated remarkable performance across a wide variety of visionlanguage tasks, ranging from producing detailed textual descriptions of images and generating realistic images from text, to tasks such as visual question answering, visual instruction following, and robot planning. Intriguingly, these models are designed with generic architectures and are initially trained with objectives that are not specifically tailored to a downstream task. The breadth of their training data endows them with the capacity for compositional reasoning about the world. However, their proficiency in conducting precise spatial reasoning within the 3D Euclidean world remains largely unexplored. This prompts the question: Can LLMs, originally used to address semanticlevel}queries, be applied to the precise}realm of inversegraphics tasks? And if so, how?",
                            "leftover": "To address the problem of generalization, a method must possess a deep understanding of the visual world and its physical properties. Here, we explore whether we can exploit the generalization abilities of large language models (LLMs) for this purpose. LLMs have demonstrated remarkable performance across a wide variety of visionlanguage tasks, ranging from producing detailed textual descriptions of images and generating realistic images from text, to tasks such as visual question answering, visual instruction following, and robot planning. Intriguingly, these models are designed with generic architectures and are initially trained with objectives that are not specifically tailored to a downstream task. The breadth of their training data endows them with the capacity for compositional reasoning about the world. However, their proficiency in conducting precise spatial reasoning within the 3D Euclidean world remains largely unexplored. This prompts the question: Can LLMs, originally used to address semanticlevel}queries, be applied to the precise}realm of inversegraphics tasks? And if so, how?",
                            "matches": []
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec0/txl5",
                            "block type": "txl",
                            "content": "To address these questions, we investigate the potential of LLMs to perform such tasks. We hypothesize that LLMs can be trained with simple demonstrations to perform precise inversegraphics reasoning. This idea draws inspiration from instruction tuning in the languageprocessing domain, where LLMs acquire instructionfollowing skills after being finetuned on a limited set of curated data. We anticipate that LLMs, endowed with broad knowledge about the physical world, can be taught to recover accurate graphics programs from images beyond the training distribution. This insight motivates a reevaluation of the conventional inversegraphics pipeline, leading to the proposal of a new LLMbased framework: the InverseGraphics Large Language Model (IGLLM). We finetune an LLM equipped with a pretrained textaligned vision encoder, using an instructionbased synthetic dataset, and explore the model's capacity to infer graphics programs with accurate estimates of object quantity, shape, size, color, material, location, and orientation, as illustrated in .",
                            "leftover": "To address these questions, we investigate the potential of LLMs to perform such tasks. We hypothesize that LLMs can be trained with simple demonstrations to perform precise inversegraphics reasoning. This idea draws inspiration from instruction tuning in the languageprocessing domain, where LLMs acquire instructionfollowing skills after being finetuned on a limited set of curated data. We anticipate that LLMs, endowed with broad knowledge about the physical world, can be taught to recover accurate graphics programs from images beyond the training distribution. This insight motivates a reevaluation of the conventional inversegraphics pipeline, leading to the proposal of a new LLMbased framework: the InverseGraphics Large Language Model (IGLLM). We finetune an LLM equipped with a pretrained textaligned vision encoder, using an instructionbased synthetic dataset, and explore the model's capacity to infer graphics programs with accurate estimates of object quantity, shape, size, color, material, location, and orientation, as illustrated in .",
                            "matches": []
                        },
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec0/txl6",
                            "block type": "txl",
                            "content": "However, a question arises regarding the suitability of LLMs and natural language for generating the precise measurements necessary for inverse graphics, given the discrete nature of their tokenbased output. This constraint poses challenges for reasoning within metric spaces such as Euclidean space. To address this, we explore the integration of a numeric head in the languagebased output (see ), where numbers are represented as continuous values rather than discretetoken sequences. We compare this approach and observe that our pipeline achieves improved precision and an expanded generalization capacity across evaluations.",
                            "leftover": "However, a question arises regarding the suitability of LLMs and natural language for generating the precise measurements necessary for inverse graphics, given the discrete nature of their tokenbased output. This constraint poses challenges for reasoning within metric spaces such as Euclidean space. To address this, we explore the integration of a numeric head in the languagebased output (see ), where numbers are represented as continuous values rather than discretetoken sequences. We compare this approach and observe that our pipeline achieves improved precision and an expanded generalization capacity across evaluations.",
                            "matches": []
                        },
                        {
                            "leaf id": 13,
                            "key": "doc/body/sec0/txl7",
                            "block type": "txl",
                            "content": "Our study is an examination of the adaptability of LLMs to novel domains and an attempt to understand how these powerful, semantically driven models can be repurposed and refined to gain a precise, metric understanding of the 3D world. While our investigation is preliminary, our work paves the way for further endeavors to capitalize on the rapid advancements in LLMs.",
                            "leftover": "Our study is an examination of the adaptability of LLMs to novel domains and an attempt to understand how these powerful, semantically driven models can be repurposed and refined to gain a precise, metric understanding of the 3D world. While our investigation is preliminary, our work paves the way for further endeavors to capitalize on the rapid advancements in LLMs.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 14,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Related Work",
                            "leftover": "Related Work",
                            "matches": []
                        },
                        {
                            "leaf id": 15,
                            "key": "doc/body/sec1/txl0",
                            "block type": "txl",
                            "content": "Visual Program Induction. Visual program induction is a subfield of program synthesis that is focused on recovering a graphics program from a given visual target. Graphics programs, also known as procedural or symbolic programs, offer a concise, structured, and interpretable representation for scenes and have garnered significant attention in the field  see for an indepth overview. Commonly employed program types include constructivesolid geometry (CSG), computeraided design (CAD), vector graphics (e.g., SVG), and Lsystems, as well as custom program domains. Program discovery can be achieved from simplified representations of the same modality, such as 2D hand drawings or synthetic patterns, as well as 3D meshes and voxels. There have also been efforts in recovering 3D scenes from natural 2D images using graphics programs. propose a probabilistic programming language for representing arbitrary 2D/3D scenes, demonstrating preliminary results for analysis of faces, bodies, and objects. infer customdesigned markup code from images that can be easily translated to rendererfriendly inputs. The work can handle scenes with a number of objects, but cannot generalize beyond the training data distribution. In followup work, investigate how graphics programs can be used for visual question answering (VQA). However, their scene reconstruction also struggles with generalization problems, particularly to unseen attribute combinations. present a new language for representing scenes, along with a hierarchical approach for inference. MetaSIM uses probabilistic scene grammars to recover synthetic scenes from natural images, which are then used to train a generative scenesynthesis model. Despite promising results, such methods require special training data and complex modular architectures, and are difficult to generalize beyond their training distribution.",
                            "leftover": "Visual Program Induction. Visual program induction is a subfield of program synthesis that is focused on recovering a graphics program from a given visual target. Graphics programs, also known as procedural or symbolic programs, offer a concise, structured, and interpretable representation for scenes and have garnered significant attention in the field  see for an indepth overview. Commonly employed program types include constructivesolid geometry (CSG), computeraided design (CAD), vector graphics (e.g., SVG), and Lsystems, as well as custom program domains. Program discovery can be achieved from simplified representations of the same modality, such as 2D hand drawings or synthetic patterns, as well as 3D meshes and voxels. There have also been efforts in recovering 3D scenes from natural 2D images using graphics programs. propose a probabilistic programming language for representing arbitrary 2D/3D scenes, demonstrating preliminary results for analysis of faces, bodies, and objects. infer customdesigned markup code from images that can be easily translated to rendererfriendly inputs. The work can handle scenes with a number of objects, but cannot generalize beyond the training data distribution. In followup work, investigate how graphics programs can be used for visual question answering (VQA). However, their scene reconstruction also struggles with generalization problems, particularly to unseen attribute combinations. present a new language for representing scenes, along with a hierarchical approach for inference. MetaSIM uses probabilistic scene grammars to recover synthetic scenes from natural images, which are then used to train a generative scenesynthesis model. Despite promising results, such methods require special training data and complex modular architectures, and are difficult to generalize beyond their training distribution.",
                            "matches": []
                        },
                        {
                            "leaf id": 16,
                            "key": "doc/body/sec1/txl1",
                            "block type": "txl",
                            "content": "Vision as Inverse Graphics. Dating back to Larry Roberts's BlocksWorld thesis, there has been a long history of work that treats computer vision as the inverse problem to computer graphics. Efforts have included estimating object pose and reconstructing shape from single images. Multiobject scenes have also been recovered via geometric approaches, but they often overlook the semantics and interrelation between objects, hindering further reasoning. Holistic 3Dscene understanding takes the approach a step further by reconstructing individual objects together with the scene layout. Early methods focus on estimating 3D boundingbox representations, whereas morerecent works emphasize the reconstruction of finer shapes along with instance segmentations. Closely related are methods that perform CAD or meshmodel retrieval followed by 6DOF pose estimation of individual objects or scenes. An alternative to detailed shape reconstruction is primitive reconstruction, where objects or scenes are explained by a limited set of geometric primitives, offering a higher level of abstraction. This direction has been studied extensively, and it is still actively researched. While these works typically produce accurate reconstructions, they involve complex pipelines with multiple modules and require special training data, limiting generalization under distribution shifts. In contrast, we explore the use of LLMs as a potentially simpler and moregeneralizable solution to the inversegraphics problem.",
                            "leftover": "Vision as Inverse Graphics. Dating back to Larry Roberts's BlocksWorld thesis, there has been a long history of work that treats computer vision as the inverse problem to computer graphics. Efforts have included estimating object pose and reconstructing shape from single images. Multiobject scenes have also been recovered via geometric approaches, but they often overlook the semantics and interrelation between objects, hindering further reasoning. Holistic 3Dscene understanding takes the approach a step further by reconstructing individual objects together with the scene layout. Early methods focus on estimating 3D boundingbox representations, whereas morerecent works emphasize the reconstruction of finer shapes along with instance segmentations. Closely related are methods that perform CAD or meshmodel retrieval followed by 6DOF pose estimation of individual objects or scenes. An alternative to detailed shape reconstruction is primitive reconstruction, where objects or scenes are explained by a limited set of geometric primitives, offering a higher level of abstraction. This direction has been studied extensively, and it is still actively researched. While these works typically produce accurate reconstructions, they involve complex pipelines with multiple modules and require special training data, limiting generalization under distribution shifts. In contrast, we explore the use of LLMs as a potentially simpler and moregeneralizable solution to the inversegraphics problem.",
                            "matches": []
                        },
                        {
                            "leaf id": 17,
                            "key": "doc/body/sec1/txl2",
                            "block type": "txl",
                            "content": "LLMs and 3D Understanding. Recent and concurrent efforts have explored the use of LLMs for 3Drelated tasks, including 3D question answering, navigation, textto3D, procedural model editing, and multimodal representation learning. To the best of our knowledge, our work is the first to investigate the application of LLMs to inversegraphics tasks.",
                            "leftover": "LLMs and 3D Understanding. Recent and concurrent efforts have explored the use of LLMs for 3Drelated tasks, including 3D question answering, navigation, textto3D, procedural model editing, and multimodal representation learning. To the best of our knowledge, our work is the first to investigate the application of LLMs to inversegraphics tasks.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 18,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Method",
                            "leftover": "Method",
                            "matches": []
                        },
                        {
                            "leaf id": 19,
                            "key": "doc/body/sec2/txl0",
                            "block type": "txl",
                            "content": "The goal of this work is to assess the efficacy of LLMs in inversegraphics tasks. We frame the problem as that of estimating a graphics program from a single image (see ) and finetune an LLM using a small, synthetic dataset. We begin by analyzing the advantages this approach offers over traditional approaches in . Subsequently, we delineate the details of our methodology in . Finally, we elaborate on the design and motivation of the numeric head to enable precise metric reasoning in .",
                            "leftover": "The goal of this work is to assess the efficacy of LLMs in inversegraphics tasks. We frame the problem as that of estimating a graphics program from a single image (see ) and finetune an LLM using a small, synthetic dataset. We begin by analyzing the advantages this approach offers over traditional approaches in . Subsequently, we delineate the details of our methodology in . Finally, we elaborate on the design and motivation of the numeric head to enable precise metric reasoning in .",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec2/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 20,
                                    "key": "doc/body/sec2/sub1/tit",
                                    "block type": "title",
                                    "content": "Traditional Neural Scene DeRendering",
                                    "leftover": "Traditional Neural Scene DeRendering",
                                    "matches": []
                                },
                                {
                                    "leaf id": 21,
                                    "key": "doc/body/sec2/sub1/txl0",
                                    "block type": "txl",
                                    "content": "Our approach builds upon the concept of neural scene derendering introduced by . In this framework, the goal is to develop a generalizable model capable of comprehensively understanding a scene by estimating a graphics program executable by a renderer. NSVQA is representative of this paradigm, where the task is addressed by decomposing the visual input using multiple modules with taskspecific visual inductive biases. The method includes several components: a regionproposal network for object detection, a segmentation network for isolating the objects from the background, an attribute network for classifying various discrete graphics attributes, and a localization network for predicting the object's spatial location. Each network is independently trained in a supervised manner with a specific objective, and their outputs are aggregated to produce a structured representation of the scene.",
                                    "leftover": "Our approach builds upon the concept of neural scene derendering introduced by . In this framework, the goal is to develop a generalizable model capable of comprehensively understanding a scene by estimating a graphics program executable by a renderer. NSVQA is representative of this paradigm, where the task is addressed by decomposing the visual input using multiple modules with taskspecific visual inductive biases. The method includes several components: a regionproposal network for object detection, a segmentation network for isolating the objects from the background, an attribute network for classifying various discrete graphics attributes, and a localization network for predicting the object's spatial location. Each network is independently trained in a supervised manner with a specific objective, and their outputs are aggregated to produce a structured representation of the scene.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 22,
                                    "key": "doc/body/sec2/sub2/tit",
                                    "block type": "title",
                                    "content": "What Do LLMs Offer?",
                                    "leftover": "What Do LLMs Offer?",
                                    "matches": []
                                },
                                {
                                    "leaf id": 23,
                                    "key": "doc/body/sec2/sub2/txl0",
                                    "block type": "txl",
                                    "content": "The broad success of LLMs can be largely attributed to their exceptional ability to generalize. Unlike models that rely on taskspecific inductive biases or wellcrafted training objectives, LLMs perform proficiently across a variety of language tasks with relatively minor design differences and a simple training approach. This success can be attributed to the scale of the models and the sets of inthewild data on which they are trained.",
                                    "leftover": "The broad success of LLMs can be largely attributed to their exceptional ability to generalize. Unlike models that rely on taskspecific inductive biases or wellcrafted training objectives, LLMs perform proficiently across a variety of language tasks with relatively minor design differences and a simple training approach. This success can be attributed to the scale of the models and the sets of inthewild data on which they are trained.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec2/sub2/txl1",
                                    "block type": "txl",
                                    "content": "A particularly intriguing development in recent years has been the adaptation of LLMs to downstream tasks through instructiontuning, where LLMs are finetuned on a small set of curated taskspecific training samples (e.g., 52K instructionfollowing examples in Stanford Alpaca). This suggests a paradigm shift from traditional approaches, where generalization is often attained by scaling the amount of taskspecific training data. LLMs are primarily trained via an unsupervised nexttokenprediction objective to perform languagecompletion tasks, thereby unifying various natural language processing (NLP) tasks within a generic framework. Our research aims to explore whether this generalized approach can be effectively extended to the task of scene derendering while preserving their strong generalization capabilities.",
                                    "leftover": "A particularly intriguing development in recent years has been the adaptation of LLMs to downstream tasks through instructiontuning, where LLMs are finetuned on a small set of curated taskspecific training samples (e.g., 52K instructionfollowing examples in Stanford Alpaca). This suggests a paradigm shift from traditional approaches, where generalization is often attained by scaling the amount of taskspecific training data. LLMs are primarily trained via an unsupervised nexttokenprediction objective to perform languagecompletion tasks, thereby unifying various natural language processing (NLP) tasks within a generic framework. Our research aims to explore whether this generalized approach can be effectively extended to the task of scene derendering while preserving their strong generalization capabilities.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 25,
                                    "key": "doc/body/sec2/sub3/tit",
                                    "block type": "title",
                                    "content": "Tuning LLMs for Inverse Graphics",
                                    "leftover": "Tuning LLMs for Inverse Graphics",
                                    "matches": []
                                },
                                {
                                    "leaf id": 26,
                                    "key": "doc/body/sec2/sub3/txl0",
                                    "block type": "txl",
                                    "content": "While LLMs are traditionally trained to complete languagetoken sequences, VQA works have demonstrated that large pretrained vision transformers can be efficiently adapted as visual tokenizers. Such works unify image and language understanding, interleaving visual embeddings with language tokens for the LLM. In line with this approach, we adopt a similar strategy, constructing an LLM capable of ''seeing'' the input image and returning a structured code representation of the input scene.",
                                    "leftover": "While LLMs are traditionally trained to complete languagetoken sequences, VQA works have demonstrated that large pretrained vision transformers can be efficiently adapted as visual tokenizers. Such works unify image and language understanding, interleaving visual embeddings with language tokens for the LLM. In line with this approach, we adopt a similar strategy, constructing an LLM capable of ''seeing'' the input image and returning a structured code representation of the input scene.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec2/sub3/txl1",
                                    "block type": "txl",
                                    "content": "In the subsequent paragraphs, we detail the base architecture, elucidate the process of visionlanguage alignment, and introduce our methodology for preparing synthetic data for visual instruction finetuning. A highlevel overview of our pipeline can be seen in .",
                                    "leftover": "In the subsequent paragraphs, we detail the base architecture, elucidate the process of visionlanguage alignment, and introduce our methodology for preparing synthetic data for visual instruction finetuning. A highlevel overview of our pipeline can be seen in .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 28,
                                    "key": "doc/body/sec2/sub3/txl2",
                                    "block type": "txl",
                                    "content": "Architecture. Our model is based on an instructiontuned variant of LLaMA1 7B in conjunction with a frozen CLIP vision encoder, serving as the visual tokenizer. We apply a learnable linear projection to link the vision embeddings with the wordembedding space of the LLM.",
                                    "leftover": "Architecture. Our model is based on an instructiontuned variant of LLaMA1 7B in conjunction with a frozen CLIP vision encoder, serving as the visual tokenizer. We apply a learnable linear projection to link the vision embeddings with the wordembedding space of the LLM.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec2/sub3/txl3",
                                    "block type": "txl",
                                    "content": "VisionLanguage Alignment. The linear visionencoder projection is initially trained using the featurealignment pretraining method from LLaVA. This training uses instruction sequences constructed from imagecaption pairs sourced from the Conceptual Captions dataset (CC3M)~\\hbox{}. The LLM receives the projected image embedding, followed by a randomly sampled directive tasking the model to describe the image and its corresponding caption. Throughout this stage, all model parameters remain fixed except for the learnable vision projector. To ensure the generality of our model, we refrain from additional instruction tuning following this initial feature alignment.",
                                    "leftover": "VisionLanguage Alignment. The linear visionencoder projection is initially trained using the featurealignment pretraining method from LLaVA. This training uses instruction sequences constructed from imagecaption pairs sourced from the Conceptual Captions dataset (CC3M)~\\hbox{}. The LLM receives the projected image embedding, followed by a randomly sampled directive tasking the model to describe the image and its corresponding caption. Throughout this stage, all model parameters remain fixed except for the learnable vision projector. To ensure the generality of our model, we refrain from additional instruction tuning following this initial feature alignment.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 30,
                                    "key": "doc/body/sec2/sub3/txl4",
                                    "block type": "txl",
                                    "content": "TrainingData Generation. CLEVR is a procedurally generated dataset of simple 3D objects on a plane. The primitives are assigned randomly sampled attributes such as shape (sphere, cube, and cylinder), size, color, material, and spatial pose. Shape, size, color, and material are discrete attributes, while pose is a continuous parameter specifying the object's location and orientation. The images from the sampled scenes are rendered using the modeling software from its Python scripting API. Our domainspecific language consists of add}functions, facilitating the insertion of objects with specified attributes into the scene using Blender. See for an example of the representation of a single object.",
                                    "leftover": "TrainingData Generation. CLEVR is a procedurally generated dataset of simple 3D objects on a plane. The primitives are assigned randomly sampled attributes such as shape (sphere, cube, and cylinder), size, color, material, and spatial pose. Shape, size, color, and material are discrete attributes, while pose is a continuous parameter specifying the object's location and orientation. The images from the sampled scenes are rendered using the modeling software from its Python scripting API. Our domainspecific language consists of add}functions, facilitating the insertion of objects with specified attributes into the scene using Blender. See for an example of the representation of a single object.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 31,
                                    "key": "doc/body/sec2/sub3/txl5",
                                    "block type": "txl",
                                    "content": "To train our model, we generate pairs of rendered images and their corresponding code, prompting the model with the rendered image followed by the question, ''What Python Blender code could be used to produce the scene?''. The model is then trained with a standard nexttoken prediction objective, aiming to maximize the conditional probability of the next token given the previous ones:",
                                    "leftover": "To train our model, we generate pairs of rendered images and their corresponding code, prompting the model with the rendered image followed by the question, ''What Python Blender code could be used to produce the scene?''. The model is then trained with a standard nexttoken prediction objective, aiming to maximize the conditional probability of the next token given the previous ones:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 32,
                                    "key": "doc/body/sec2/sub3/equation6",
                                    "block type": "equation",
                                    "content": "p(x) = ∏i=1^n p(si|s1,…si1),",
                                    "leftover": "p(x) = ∏i=1^n p(si|s1,…si1),",
                                    "matches": []
                                },
                                {
                                    "leaf id": 33,
                                    "key": "doc/body/sec2/sub3/txl7",
                                    "block type": "txl",
                                    "content": "where si represents the ith token. Numbers are rendered with three decimal places in the textbased (tokenized) training data. We order the add}statements fronttoback in the objective token sequence and shuffle the order of the object attributes. See for complete examples of each task.",
                                    "leftover": "where si represents the ith token. Numbers are rendered with three decimal places in the textbased (tokenized) training data. We order the add}statements fronttoback in the objective token sequence and shuffle the order of the object attributes. See for complete examples of each task.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 34,
                                    "key": "doc/body/sec2/sub3/txl8",
                                    "block type": "txl",
                                    "content": "Differences From Traditional Approaches. The framework presented in this study marks a departure from conventional approaches. Notably, the visualencoding process does not include graphicsspecific inductive biases in its design. It undergoes no training for intermediate vision tasks, such as object detection, segmentation, or attribute regression. The model operates solely on rendered images without access to 3D assets. Moreover, the supervised finetuning process employs a training objective not directly related to the physical representation of the scene.",
                                    "leftover": "Differences From Traditional Approaches. The framework presented in this study marks a departure from conventional approaches. Notably, the visualencoding process does not include graphicsspecific inductive biases in its design. It undergoes no training for intermediate vision tasks, such as object detection, segmentation, or attribute regression. The model operates solely on rendered images without access to 3D assets. Moreover, the supervised finetuning process employs a training objective not directly related to the physical representation of the scene.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 35,
                                    "key": "doc/body/sec2/sub3/txl9",
                                    "block type": "txl",
                                    "content": "We show in that these departures do not impede performance or generalization capabilities compared with traditional approaches; in fact, they enhance them. Our experiments demonstrate a compositionalgeneralization ability without the need for tailormade designs, surpassing the conventional approach by approximately 60% in OOD shaperecognition accuracy.",
                                    "leftover": "We show in that these departures do not impede performance or generalization capabilities compared with traditional approaches; in fact, they enhance them. Our experiments demonstrate a compositionalgeneralization ability without the need for tailormade designs, surpassing the conventional approach by approximately 60% in OOD shaperecognition accuracy.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 36,
                                    "key": "doc/body/sec2/sub4/tit",
                                    "block type": "title",
                                    "content": "Precise Numeric Reasoning in LLMs",
                                    "leftover": "Precise Numeric Reasoning in LLMs",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/sub4/figure0",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "key": "doc/body/sec2/sub4/figure0/subfigure0",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 37,
                                                    "key": "doc/body/sec2/sub4/figure0/subfigure0/txl0",
                                                    "block type": "txl",
                                                    "content": "{0.5} \\raisebox{1.05cm}{\\includegraphics[width=]{figures/diagram/notnumerichead.pdf}}",
                                                    "leftover": "{0.5} \\raisebox{1.05cm}{\\includegraphics[width=]{figures/diagram/notnumerichead.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 38,
                                                    "key": "doc/body/sec2/sub4/figure0/subfigure0/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Discretized numerics",
                                                    "leftover": "Discretized numerics",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/sub4/figure0/subfigure1",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 39,
                                                    "key": "doc/body/sec2/sub4/figure0/subfigure1/txl0",
                                                    "block type": "txl",
                                                    "content": "{0.2350} \\vspace{0.1cm} \\includegraphics[width=]{figures/diagram/numericheaddiagram.pdf}",
                                                    "leftover": "{0.2350} \\vspace{0.1cm} \\includegraphics[width=]{figures/diagram/numericheaddiagram.pdf}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 40,
                                                    "key": "doc/body/sec2/sub4/figure0/subfigure1/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Continuous numerics",
                                                    "leftover": "Continuous numerics",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/sub4/figure0/cpt2",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 41,
                                                    "key": "doc/body/sec2/sub4/figure0/cpt2/txl0",
                                                    "block type": "txl",
                                                    "content": "Numeric Head.",
                                                    "leftover": "Numeric Head.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 42,
                                            "key": "doc/body/sec2/sub4/figure0/txl3",
                                            "block type": "txl",
                                            "content": "() Rather than producing digits as discrete tokens (a), we train our model to generate a [NUM] token when a number should be produced. The [NUM] token is used as a mask to signal the embedding should instead be passed through the numeric head, preserving the gradient (b). }",
                                            "leftover": "() Rather than producing digits as discrete tokens (a), we train our model to generate a [NUM] token when a number should be produced. The [NUM] token is used as a mask to signal the embedding should instead be passed through the numeric head, preserving the gradient (b). }",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 43,
                                    "key": "doc/body/sec2/sub4/txl1",
                                    "block type": "txl",
                                    "content": "Graphics programming requires the precise estimation of continuous quantities such as location and orientation, along with a comprehension of Euclidean space. This requirement extends beyond the coarse, semanticlevel spatial reasoning (e.g.~''left,'' ''in front of'') for which LLMs are typically employed, in tasks such as VQA. Estimating continuous values through characterbased outputs essentially transforms the task into a discrete, combinatorial challenge. In this loss space, prediction errors do not reflect real metric distances  a ground truth value of '4' is considered as close to a prediction of '3' as it is to '8,' highlighting the inherent limitations of this approach for tasks demanding high numerical precision.",
                                    "leftover": "Graphics programming requires the precise estimation of continuous quantities such as location and orientation, along with a comprehension of Euclidean space. This requirement extends beyond the coarse, semanticlevel spatial reasoning (e.g.~''left,'' ''in front of'') for which LLMs are typically employed, in tasks such as VQA. Estimating continuous values through characterbased outputs essentially transforms the task into a discrete, combinatorial challenge. In this loss space, prediction errors do not reflect real metric distances  a ground truth value of '4' is considered as close to a prediction of '3' as it is to '8,' highlighting the inherent limitations of this approach for tasks demanding high numerical precision.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 44,
                                    "key": "doc/body/sec2/sub4/txl2",
                                    "block type": "txl",
                                    "content": "To address this challenge, we introduce a numeric head tailored to enable continuous parameter estimation. A visual representation of the numerichead integration is depicted in, contrasting with the discrete textbased alternative. The module is implemented as a fourlayer MLP that processes the final hiddenlayer output of the LLM and transforms it into a scalar value. To allow the LLM to discern between generating numerical values or textual information, we designate a special token in the vocabulary  [NUM]  which serves as a mask to indicate whether a number should be produced. During training, we apply an MSE loss on each number in addition to the nexttoken prediction loss () used on the [NUM] token itself.",
                                    "leftover": "To address this challenge, we introduce a numeric head tailored to enable continuous parameter estimation. A visual representation of the numerichead integration is depicted in, contrasting with the discrete textbased alternative. The module is implemented as a fourlayer MLP that processes the final hiddenlayer output of the LLM and transforms it into a scalar value. To allow the LLM to discern between generating numerical values or textual information, we designate a special token in the vocabulary  [NUM]  which serves as a mask to indicate whether a number should be produced. During training, we apply an MSE loss on each number in addition to the nexttoken prediction loss () used on the [NUM] token itself.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 45,
                                    "key": "doc/body/sec2/sub4/txl3",
                                    "block type": "txl",
                                    "content": "We systematically investigate the behavior of characterbased and numeric IGLLM variants for precise spatial reasoning in . Our empirical findings support our intuition regarding the limitations of the characterbased output and demonstrate that the numeric head enables strong generalization when the testing samples are OOD in parameter space. These differences are highlighted throughout our evaluation.",
                                    "leftover": "We systematically investigate the behavior of characterbased and numeric IGLLM variants for precise spatial reasoning in . Our empirical findings support our intuition regarding the limitations of the characterbased output and demonstrate that the numeric head enables strong generalization when the testing samples are OOD in parameter space. These differences are highlighted throughout our evaluation.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 46,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Evaluations",
                            "leftover": "Evaluations",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/figure0",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec3/figure0/subfigure0",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 47,
                                            "key": "doc/body/sec3/figure0/subfigure0/txl0",
                                            "block type": "txl",
                                            "content": "{.32915} \\includegraphics[width=]{figures/clevr/input/0.jpg} \\includegraphics[width=]{figures/clevr/input/1.jpg}",
                                            "leftover": "{.32915} \\includegraphics[width=]{figures/clevr/input/0.jpg} \\includegraphics[width=]{figures/clevr/input/1.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 48,
                                            "key": "doc/body/sec3/figure0/subfigure0/cpt1",
                                            "block type": "cpt",
                                            "content": "Input",
                                            "leftover": "Input",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/figure0/subfigure1",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 49,
                                            "key": "doc/body/sec3/figure0/subfigure1/txl0",
                                            "block type": "txl",
                                            "content": "{.32915} \\includegraphics[width=]{figures/clevr/nsvqa/0.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/1.jpg}",
                                            "leftover": "{.32915} \\includegraphics[width=]{figures/clevr/nsvqa/0.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/1.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 50,
                                            "key": "doc/body/sec3/figure0/subfigure1/cpt1",
                                            "block type": "cpt",
                                            "content": "NSVQA",
                                            "leftover": "NSVQA",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/figure0/subfigure2",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 51,
                                            "key": "doc/body/sec3/figure0/subfigure2/txl0",
                                            "block type": "txl",
                                            "content": "{.32915} \\includegraphics[width=]{figures/clevr/output/0.jpg} \\includegraphics[width=]{figures/clevr/output/1.jpg}",
                                            "leftover": "{.32915} \\includegraphics[width=]{figures/clevr/output/0.jpg} \\includegraphics[width=]{figures/clevr/output/1.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 52,
                                            "key": "doc/body/sec3/figure0/subfigure2/cpt1",
                                            "block type": "cpt",
                                            "content": "Ours",
                                            "leftover": "Ours",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/figure0/cpt3",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 53,
                                            "key": "doc/body/sec3/figure0/cpt3/txl0",
                                            "block type": "txl",
                                            "content": "OOD CLEVRCoGenT Samples.",
                                            "leftover": "OOD CLEVRCoGenT Samples.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec3/figure0/txl4",
                                    "block type": "txl",
                                    "content": "() NSVQA, with its modular design, fails to disentangle shape from color, while our framework is able to effectively generalize to OOD attribute combinations. See for additional samples. }",
                                    "leftover": "() NSVQA, with its modular design, fails to disentangle shape from color, while our framework is able to effectively generalize to OOD attribute combinations. See for additional samples. }",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 55,
                            "key": "doc/body/sec3/txl1",
                            "block type": "txl",
                            "content": "To evaluate the ability of our proposed framework to generalize across distribution shifts, we design a number of focused evaluation settings. We conduct experiments on synthetic data in order to quantitatively analyze model capability under controlled shifts.",
                            "leftover": "To evaluate the ability of our proposed framework to generalize across distribution shifts, we design a number of focused evaluation settings. We conduct experiments on synthetic data in order to quantitatively analyze model capability under controlled shifts.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 56,
                                    "key": "doc/body/sec3/sub2/tit",
                                    "block type": "title",
                                    "content": "Compositional Generalization on CLEVR",
                                    "leftover": "Compositional Generalization on CLEVR",
                                    "matches": []
                                },
                                {
                                    "leaf id": 57,
                                    "key": "doc/body/sec3/sub2/txl0",
                                    "block type": "txl",
                                    "content": "An extension to CLEVR, known as CLEVRCoGenT, serves as a benchmark for evaluating the compositionalgeneralization capabilities of VQA models. This benchmark assesses the model's ability to answer questions about scenes containing objects with unseen combinations of attributes. During training, the dataset is structured such that particular types of objects are only assigned specific combinations of attributes (e.g.~blue cubes and red cylinders), while the testing data includes objects with attribute combinations not seen during training (e.g.~red cubes and blue cylinders). We adapt this VQA dataset to our inversegraphics problem domain, employing it for three primary purposes: 1) demonstrating that LLMs can effectively perform inverse graphics by testing on indistribution (ID) data; 2) illustrating that LLMs exhibit robust compositional generalization to OOD data, while the baseline approach in NSVQA struggles in this setting; and 3) exploring the dataefficiency of our framework.",
                                    "leftover": "An extension to CLEVR, known as CLEVRCoGenT, serves as a benchmark for evaluating the compositionalgeneralization capabilities of VQA models. This benchmark assesses the model's ability to answer questions about scenes containing objects with unseen combinations of attributes. During training, the dataset is structured such that particular types of objects are only assigned specific combinations of attributes (e.g.~blue cubes and red cylinders), while the testing data includes objects with attribute combinations not seen during training (e.g.~red cubes and blue cylinders). We adapt this VQA dataset to our inversegraphics problem domain, employing it for three primary purposes: 1) demonstrating that LLMs can effectively perform inverse graphics by testing on indistribution (ID) data; 2) illustrating that LLMs exhibit robust compositional generalization to OOD data, while the baseline approach in NSVQA struggles in this setting; and 3) exploring the dataefficiency of our framework.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 58,
                                    "key": "doc/body/sec3/sub2/txl1",
                                    "block type": "txl",
                                    "content": "Setting. Following the setting of CLEVRCoGenT, our training set consists of images of scenes containing objects with only a subset of possible attribute combinations (shape, size, material, and color). In the ID condition, all cubes are rendered in gray, blue, brown, or yellow, and all cylinders are depicted in red, green, purple, or cyan. In contrast, in the OOD condition the color palettes of the shapes are swapped. Spheres are consistently depicted with all eight colors under both conditions. We train both our proposed framework and NSVQA, our neuralscene derendering baseline, on 4k images from the ID condition and evaluate them on 1k images from both the ID and OOD conditions. We follow CLEVR and randomly apply their set of synonyms on the categorical attributes.",
                                    "leftover": "Setting. Following the setting of CLEVRCoGenT, our training set consists of images of scenes containing objects with only a subset of possible attribute combinations (shape, size, material, and color). In the ID condition, all cubes are rendered in gray, blue, brown, or yellow, and all cylinders are depicted in red, green, purple, or cyan. In contrast, in the OOD condition the color palettes of the shapes are swapped. Spheres are consistently depicted with all eight colors under both conditions. We train both our proposed framework and NSVQA, our neuralscene derendering baseline, on 4k images from the ID condition and evaluate them on 1k images from both the ID and OOD conditions. We follow CLEVR and randomly apply their set of synonyms on the categorical attributes.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub2/table2",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub2/table2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 59,
                                                    "key": "doc/body/sec3/sub2/table2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "CLEVRCoGenT Results.",
                                                    "leftover": "CLEVRCoGenT Results.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 60,
                                            "key": "doc/body/sec3/sub2/table2/txl1",
                                            "block type": "txl",
                                            "content": "() While both our proposed framework and the baseline, NSVQA, and are able to achieve \\textgreater 99% accuracy on the ID condition, the baseline fails to generalize, with its shaperecognition accuracy dropping by 66.12%. Color, Mat., and Shape}represent respective accuracies and ↑ indicates greater is better. }",
                                            "leftover": "() While both our proposed framework and the baseline, NSVQA, and are able to achieve \\textgreater 99% accuracy on the ID condition, the baseline fails to generalize, with its shaperecognition accuracy dropping by 66.12%. Color, Mat., and Shape}represent respective accuracies and ↑ indicates greater is better. }",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 61,
                                            "key": "doc/body/sec3/sub2/table2/tabular2",
                                            "block type": "tabular",
                                            "content": "{lrrr|rrr} & \\multicolumn{3}cID & \\multicolumn{3}cOOD & Char & Float & NSVQA & Char & Float & NSVQA ↓L2 & 0.21 & 0.16 & 0.18 & 0.22 & 0.17 & 0.18 ↑Size & 99.71 & 99.77 & 100.00 & 99.74 & 99.80 & 100.00 ↑Color & 99.58 & 99.71 & 100.00 & 98.60 & 98.14 & 99.95 ↑Shape & 99.51 & 99.59 & 100.00 & 93.50 & 93.14 & \\fbox{33.88}",
                                            "leftover": "{lrrr|rrr} & \\multicolumn{3}cID & \\multicolumn{3}cOOD & Char & Float & NSVQA & Char & Float & NSVQA ↓L2 & 0.21 & 0.16 & 0.18 & 0.22 & 0.17 & 0.18 ↑Size & 99.71 & 99.77 & 100.00 & 99.74 & 99.80 & 100.00 ↑Color & 99.58 & 99.71 & 100.00 & 98.60 & 98.14 & 99.95 ↑Shape & 99.51 & 99.59 & 100.00 & 93.50 & 93.14 & \\fbox{33.88}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 62,
                                    "key": "doc/body/sec3/sub2/txl3",
                                    "block type": "txl",
                                    "content": "Evaluation Metrics. To evaluate attributerecognition accuracy, we employ linearsum assignment on pairwise Euclidean distances to match predicted and groundtruth objects. However, since attributerecognition accuracy does not account for missing or duplicated objects, we also evaluate the method's ability to produce accurate counts by computing the meanabsolute counting error between the predicted and groundtruth object sets across scenes (Count).",
                                    "leftover": "Evaluation Metrics. To evaluate attributerecognition accuracy, we employ linearsum assignment on pairwise Euclidean distances to match predicted and groundtruth objects. However, since attributerecognition accuracy does not account for missing or duplicated objects, we also evaluate the method's ability to produce accurate counts by computing the meanabsolute counting error between the predicted and groundtruth object sets across scenes (Count).",
                                    "matches": []
                                },
                                {
                                    "leaf id": 63,
                                    "key": "doc/body/sec3/sub2/txl4",
                                    "block type": "txl",
                                    "content": "R{0.45} \\includegraphics[width=]{figures/clevr/dataefficiencyplot.pdf}",
                                    "leftover": "R{0.45} \\includegraphics[width=]{figures/clevr/dataefficiencyplot.pdf}",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub2/cpt5",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 64,
                                            "key": "doc/body/sec3/sub2/cpt5/txl0",
                                            "block type": "txl",
                                            "content": "CLEVR Data Efficiency.",
                                            "leftover": "CLEVR Data Efficiency.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 65,
                                    "key": "doc/body/sec3/sub2/txl6",
                                    "block type": "txl",
                                    "content": "() Plot of the validation L2 positional error by the number of training samples. We observe that the floatbased model is consistently more dataefficient but that the difference between the models converges as the number of training samples reaches 4000. See for a full quantitative comparison. } \\vspace{0.5cm}",
                                    "leftover": "() Plot of the validation L2 positional error by the number of training samples. We observe that the floatbased model is consistently more dataefficient but that the difference between the models converges as the number of training samples reaches 4000. See for a full quantitative comparison. } \\vspace{0.5cm}",
                                    "matches": []
                                },
                                {
                                    "leaf id": 66,
                                    "key": "doc/body/sec3/sub2/txl7",
                                    "block type": "txl",
                                    "content": "Results. Both our proposed framework and NSVQA achieve \\textgreater 99% accuracy on the ID condition (), which underscores LLMs' ability to perform comparably with domainspecific modular designs. However, when evaluated on the OOD condition, the shaperecognition accuracy of the baseline method drops substantially by 66.12%, while the accuracy of our pipeline decreases by only 6.01%. Notably, when spheres, observed with all colors during training, are removed from the evaluation, the shape accuracy of NSVQA plummets further to 0.03%. Illustrative reconstructions from the OOD condition can be seen in .",
                                    "leftover": "Results. Both our proposed framework and NSVQA achieve \\textgreater 99% accuracy on the ID condition (), which underscores LLMs' ability to perform comparably with domainspecific modular designs. However, when evaluated on the OOD condition, the shaperecognition accuracy of the baseline method drops substantially by 66.12%, while the accuracy of our pipeline decreases by only 6.01%. Notably, when spheres, observed with all colors during training, are removed from the evaluation, the shape accuracy of NSVQA plummets further to 0.03%. Illustrative reconstructions from the OOD condition can be seen in .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 67,
                                    "key": "doc/body/sec3/sub2/txl8",
                                    "block type": "txl",
                                    "content": "In terms of data efficiency, we find the floatbased model to be much more efficient than the charbased model in estimating the positions of objects, but the difference diminishes as the number of samples reaches 4k (). Hypothesizing that the charbased model has learned to compositionally retrieve the exact positions of (individual) objects from the training set rather than learning to effectively interpolate between training values, we measure the likelihood of predicting arbitrary threedecimal values. Our findings reveal that the charbased model is 6.41 times more likely to predict a particular value if that discrete value was observed during training. We further explore floatestimation dynamics in .",
                                    "leftover": "In terms of data efficiency, we find the floatbased model to be much more efficient than the charbased model in estimating the positions of objects, but the difference diminishes as the number of samples reaches 4k (). Hypothesizing that the charbased model has learned to compositionally retrieve the exact positions of (individual) objects from the training set rather than learning to effectively interpolate between training values, we measure the likelihood of predicting arbitrary threedecimal values. Our findings reveal that the charbased model is 6.41 times more likely to predict a particular value if that discrete value was observed during training. We further explore floatestimation dynamics in .",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 68,
                                    "key": "doc/body/sec3/sub3/tit",
                                    "block type": "title",
                                    "content": "Numeric ParameterSpace Generalization",
                                    "leftover": "Numeric ParameterSpace Generalization",
                                    "matches": []
                                },
                                {
                                    "leaf id": 69,
                                    "key": "doc/body/sec3/sub3/txl0",
                                    "block type": "txl",
                                    "content": "In this section, we investigate the addition of a numeric head and the ability of our framework to generalize across parameter space.",
                                    "leftover": "In this section, we investigate the addition of a numeric head and the ability of our framework to generalize across parameter space.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub3/figure*1",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/subfigure0",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 70,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure0/txl0",
                                                    "block type": "txl",
                                                    "content": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/sparsecheckerboard.pdf}}",
                                                    "leftover": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/sparsecheckerboard.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 71,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure0/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Train Distribution",
                                                    "leftover": "Train Distribution",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/subfigure1",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 72,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure1/txl0",
                                                    "block type": "txl",
                                                    "content": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/charscatter.pdf}}",
                                                    "leftover": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/charscatter.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 73,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure1/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Char Model Pred.",
                                                    "leftover": "Char Model Pred.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/subfigure2",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 74,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure2/txl0",
                                                    "block type": "txl",
                                                    "content": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/floatscatter.pdf}}",
                                                    "leftover": "{.196} \\raisebox{0cm}{\\includegraphics[width=]{figures/2d/floatscatter.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 75,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure2/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Float Model Pred.",
                                                    "leftover": "Float Model Pred.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/subfigure3",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 76,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure3/txl0",
                                                    "block type": "txl",
                                                    "content": "{.196} \\raisebox{0.0cm}{\\includegraphics[width=]{figures/2d/bar.pdf}}",
                                                    "leftover": "{.196} \\raisebox{0.0cm}{\\includegraphics[width=]{figures/2d/bar.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 77,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure3/cpt1",
                                                    "block type": "cpt",
                                                    "content": "IDOOD",
                                                    "leftover": "IDOOD",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/subfigure4",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 78,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure4/txl0",
                                                    "block type": "txl",
                                                    "content": "{.196} \\includegraphics[width=]{figures/2d/dynamics.pdf}",
                                                    "leftover": "{.196} \\includegraphics[width=]{figures/2d/dynamics.pdf}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 79,
                                                    "key": "doc/body/sec3/sub3/figure*1/subfigure4/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Dynamics",
                                                    "leftover": "Dynamics",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/figure*1/cpt5",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 80,
                                                    "key": "doc/body/sec3/sub3/figure*1/cpt5/txl0",
                                                    "block type": "txl",
                                                    "content": "2D ParameterSpace Generalization.",
                                                    "leftover": "2D ParameterSpace Generalization.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 81,
                                            "key": "doc/body/sec3/sub3/figure*1/txl6",
                                            "block type": "txl",
                                            "content": "() (a) Training positions are sampled from the checkerboard. When evaluated on images with uniformly sampled positions, the charbased model fails to generalize outside the training distribution (b) while the floatbased model effectively interpolates samples (c). Randomlysampled testing locations are shown in red and the corresponding predictions in blue. (d) shows that while both methods wellestimate samples from the ID condition, the charbased model struggles to generalize. (e) shows a plot of the model's validation MSE as a function of the number of training steps. We observe that the training of the floatbased model is much smoother and converges quickly. }",
                                            "leftover": "() (a) Training positions are sampled from the checkerboard. When evaluated on images with uniformly sampled positions, the charbased model fails to generalize outside the training distribution (b) while the floatbased model effectively interpolates samples (c). Randomlysampled testing locations are shown in red and the corresponding predictions in blue. (d) shows that while both methods wellestimate samples from the ID condition, the charbased model struggles to generalize. (e) shows a plot of the model's validation MSE as a function of the number of training steps. We observe that the training of the floatbased model is much smoother and converges quickly. }",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/ssb2",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 82,
                                            "key": "doc/body/sec3/sub3/ssb2/tit",
                                            "block type": "title",
                                            "content": "2D Parameter Space",
                                            "leftover": "2D Parameter Space",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 83,
                                            "key": "doc/body/sec3/sub3/ssb2/txl0",
                                            "block type": "txl",
                                            "content": "We begin by scrutinizing the framework's ability to generalize in 2D parameter space across range gaps. To accomplish this, we create a dataset comprising 10k images, each featuring a red dot on a white background. During training, the model is shown images where the location of the dot is sampled from a sparse checkerboard grid, as shown in . During evaluation, the model is shown 1k images where the dot's location is uniformly sampled across the square; points lying outside the checkerboard are effectively OOD inputs.",
                                            "leftover": "We begin by scrutinizing the framework's ability to generalize in 2D parameter space across range gaps. To accomplish this, we create a dataset comprising 10k images, each featuring a red dot on a white background. During training, the model is shown images where the location of the dot is sampled from a sparse checkerboard grid, as shown in . During evaluation, the model is shown 1k images where the dot's location is uniformly sampled across the square; points lying outside the checkerboard are effectively OOD inputs.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 84,
                                            "key": "doc/body/sec3/sub3/ssb2/txl1",
                                            "block type": "txl",
                                            "content": "Results. As shown in, the charbased model exhibits significant overfitting to the training distribution, consistently predicting dot locations restricted to the checkerboard distribution observed during training. In contrast, the floatbased model is able to effectively generalize across parameter space, adapting to the uniformly sampled testing distribution during evaluation (). Although the floatbased model exhibits a slight positional bias toward predicting positions on the grid (as evidenced by the higher OOD error), the disparity in the IDOOD validationL2 performance gap of the charbased model is 14 times as high as that of the floatbased model (). Moreover, the validation MSE of the floatbased model converges quickly to near zero, while the error of the charbased model is much less stable over time (), suggesting that the floatbased model learns smooth, lowdimensional representations of the space while the charbased variant may not.",
                                            "leftover": "Results. As shown in, the charbased model exhibits significant overfitting to the training distribution, consistently predicting dot locations restricted to the checkerboard distribution observed during training. In contrast, the floatbased model is able to effectively generalize across parameter space, adapting to the uniformly sampled testing distribution during evaluation (). Although the floatbased model exhibits a slight positional bias toward predicting positions on the grid (as evidenced by the higher OOD error), the disparity in the IDOOD validationL2 performance gap of the charbased model is 14 times as high as that of the floatbased model (). Moreover, the validation MSE of the floatbased model converges quickly to near zero, while the error of the charbased model is much less stable over time (), suggesting that the floatbased model learns smooth, lowdimensional representations of the space while the charbased variant may not.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/ssb3",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 85,
                                            "key": "doc/body/sec3/sub3/ssb3/tit",
                                            "block type": "title",
                                            "content": "SO(3) Parameter Space",
                                            "leftover": "SO(3) Parameter Space",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 86,
                                            "key": "doc/body/sec3/sub3/ssb3/txl0",
                                            "block type": "txl",
                                            "content": "r{0.4}",
                                            "leftover": "r{0.4}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/ssb3/subfigure1",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 87,
                                                    "key": "doc/body/sec3/sub3/ssb3/subfigure1/txl0",
                                                    "block type": "txl",
                                                    "content": "{.46} \\raisebox{0.1cm}{\\includegraphics[width=]{figures/airplane/so3/airplane.pdf}}",
                                                    "leftover": "{.46} \\raisebox{0.1cm}{\\includegraphics[width=]{figures/airplane/so3/airplane.pdf}}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 88,
                                                    "key": "doc/body/sec3/sub3/ssb3/subfigure1/cpt1",
                                                    "block type": "cpt",
                                                    "content": "RangeGap Vis.",
                                                    "leftover": "RangeGap Vis.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/ssb3/subfigure2",
                                            "block_type": "subfigure",
                                            "children": [
                                                {
                                                    "leaf id": 89,
                                                    "key": "doc/body/sec3/sub3/ssb3/subfigure2/txl0",
                                                    "block type": "txl",
                                                    "content": "{.5} \\includegraphics[width=]{figures/airplane/so3/bar.pdf}",
                                                    "leftover": "{.5} \\includegraphics[width=]{figures/airplane/so3/bar.pdf}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 90,
                                                    "key": "doc/body/sec3/sub3/ssb3/subfigure2/cpt1",
                                                    "block type": "cpt",
                                                    "content": "IDOOD",
                                                    "leftover": "IDOOD",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub3/ssb3/cpt3",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 91,
                                                    "key": "doc/body/sec3/sub3/ssb3/cpt3/txl0",
                                                    "block type": "txl",
                                                    "content": "SO(3) Range Gap.",
                                                    "leftover": "SO(3) Range Gap.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 92,
                                            "key": "doc/body/sec3/sub3/ssb3/txl4",
                                            "block type": "txl",
                                            "content": "() (a) Visualization of the SO(3) rangegap sampling space. For training, Euler rotation components are sampled from the blue regions. In OOD, samples are exclusively sampled from the red ranges. (b) The floatbased model outperforms the charbased variant on validation data sampled from both the ID and OOD conditions. } \\vspace{0.5cm}",
                                            "leftover": "() (a) Visualization of the SO(3) rangegap sampling space. For training, Euler rotation components are sampled from the blue regions. In OOD, samples are exclusively sampled from the red ranges. (b) The floatbased model outperforms the charbased variant on validation data sampled from both the ID and OOD conditions. } \\vspace{0.5cm}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 93,
                                            "key": "doc/body/sec3/sub3/ssb3/txl5",
                                            "block type": "txl",
                                            "content": "We continue our parameterspace evaluation within the more complex task of SO(3)pose estimation of orientable objects. For this, we make use of five toyairplane assets sourced from SuperCLEVR. We construct a training dataset of 10k images of single planes at a fixed location and sampled attributes identical to those in CLEVR. Extending the rangegap setup used in, the airplanes are assigned random extrinsicEuler rotations, where the components are sampled from ranges containing inserted gaps (e.g., [π/20,π/20]). A visual depiction of this space is provided in, with the training values exclusively sampled from the blue ranges. During testing, we invert the gaps to assess OOD generalization. We evaluate performance across intrinsicEuler, extrinsicEuler, axisangle, and 6D representations.",
                                            "leftover": "We continue our parameterspace evaluation within the more complex task of SO(3)pose estimation of orientable objects. For this, we make use of five toyairplane assets sourced from SuperCLEVR. We construct a training dataset of 10k images of single planes at a fixed location and sampled attributes identical to those in CLEVR. Extending the rangegap setup used in, the airplanes are assigned random extrinsicEuler rotations, where the components are sampled from ranges containing inserted gaps (e.g., [π/20,π/20]). A visual depiction of this space is provided in, with the training values exclusively sampled from the blue ranges. During testing, we invert the gaps to assess OOD generalization. We evaluate performance across intrinsicEuler, extrinsicEuler, axisangle, and 6D representations.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 94,
                                            "key": "doc/body/sec3/sub3/ssb3/txl6",
                                            "block type": "txl",
                                            "content": "Results. We report full results in and here discuss only results from the bestperforming representation variants in each evaluation, being intrinsicEuler for char and 6D for float in ID, and axisangle for both in OOD. We do so to avoid biasing results with a representation that is better suited for one model variant or the other.",
                                            "leftover": "Results. We report full results in and here discuss only results from the bestperforming representation variants in each evaluation, being intrinsicEuler for char and 6D for float in ID, and axisangle for both in OOD. We do so to avoid biasing results with a representation that is better suited for one model variant or the other.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 95,
                                            "key": "doc/body/sec3/sub3/ssb3/txl7",
                                            "block type": "txl",
                                            "content": "As depicted in, the error of the charbased model is 2.64 times higher than that of the floatbased model when evaluated indistribution. Upon testing in the OOD condition, the disparity is nearly consistent at 2.52 times that observed in the ID scenario, with the IDOOD gap of the charbased model being 2.21 times that observed in the floatbased variant. We attribute the superiority of the floatbased model across both conditions to the increased data dimensionality. Additionally, the lesser performance decline observed when evaluating on the OOD training gaps further underscores the parameterspace efficiency of the floatbased model.",
                                            "leftover": "As depicted in, the error of the charbased model is 2.64 times higher than that of the floatbased model when evaluated indistribution. Upon testing in the OOD condition, the disparity is nearly consistent at 2.52 times that observed in the ID scenario, with the IDOOD gap of the charbased model being 2.21 times that observed in the floatbased variant. We attribute the superiority of the floatbased model across both conditions to the increased data dimensionality. Additionally, the lesser performance decline observed when evaluating on the OOD training gaps further underscores the parameterspace efficiency of the floatbased model.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 96,
                                    "key": "doc/body/sec3/sub4/tit",
                                    "block type": "title",
                                    "content": "6DoF Pose Estimation",
                                    "leftover": "6DoF Pose Estimation",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub4/figure0",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub4/figure0/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 97,
                                                    "key": "doc/body/sec3/sub4/figure0/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "OOD SingleObject 6DoF Samples.",
                                                    "leftover": "OOD SingleObject 6DoF Samples.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 98,
                                            "key": "doc/body/sec3/sub4/figure0/txl1",
                                            "block type": "txl",
                                            "content": "() A sample 6DoF reconstruction of realworld images. The model is finetuned with only Blender renderings of toy airplanes that have a white backdrop. See for additional samples. }",
                                            "leftover": "() A sample 6DoF reconstruction of realworld images. The model is finetuned with only Blender renderings of toy airplanes that have a white backdrop. See for additional samples. }",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 99,
                                    "key": "doc/body/sec3/sub4/txl1",
                                    "block type": "txl",
                                    "content": "We examine the ability of our framework to scale in tackling a more challenging inversegraphics task: that of 6DoFpose estimation. Our exploration begins with an evaluation on singleobject images, encompassing both quantitative and qualitative assessments, where we illustrate the framework's ability to generalize across visual domain shifts. We subsequently extend the setting to include morecomplex (albeit, synthetic) multiobject scenes, demonstrating promising results for scene estimation, handling larger collections (\\textgreater 100) of diverse assets.",
                                    "leftover": "We examine the ability of our framework to scale in tackling a more challenging inversegraphics task: that of 6DoFpose estimation. Our exploration begins with an evaluation on singleobject images, encompassing both quantitative and qualitative assessments, where we illustrate the framework's ability to generalize across visual domain shifts. We subsequently extend the setting to include morecomplex (albeit, synthetic) multiobject scenes, demonstrating promising results for scene estimation, handling larger collections (\\textgreater 100) of diverse assets.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub4/table2",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub4/table2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 100,
                                                    "key": "doc/body/sec3/sub4/table2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "SingleObject 6DoF Results.",
                                                    "leftover": "SingleObject 6DoF Results.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 101,
                                            "key": "doc/body/sec3/sub4/table2/txl1",
                                            "block type": "txl",
                                            "content": "() When evaluating on ID data in the onemillionsample singleobject 6DoF eval, we observe little difference between models; both wellcapture the distribution. Geod.~represents geodesic distance in degrees. }",
                                            "leftover": "() When evaluating on ID data in the onemillionsample singleobject 6DoF eval, we observe little difference between models; both wellcapture the distribution. Geod.~represents geodesic distance in degrees. }",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 102,
                                            "key": "doc/body/sec3/sub4/table2/tabular2",
                                            "block type": "tabular",
                                            "content": "lrrrrr & ↓L2 & ↓Geod.\\ & ↑Color & ↑Mat.\\ & ↑Shape Char & 0.02}& 5.03}& 79.10 & 99.00}& 99.80 Float & 0.04 & 6.18 & 81.90}& 99.00}& 100.00}",
                                            "leftover": "lrrrrr & ↓L2 & ↓Geod.\\ & ↑Color & ↑Mat.\\ & ↑Shape Char & 0.02}& 5.03}& 79.10 & 99.00}& 99.80 Float & 0.04 & 6.18 & 81.90}& 99.00}& 100.00}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/ssb3",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 103,
                                            "key": "doc/body/sec3/sub4/ssb3/tit",
                                            "block type": "title",
                                            "content": "SingleObject 6DoF",
                                            "leftover": "SingleObject 6DoF",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 104,
                                            "key": "doc/body/sec3/sub4/ssb3/txl0",
                                            "block type": "txl",
                                            "content": "We first evaluate our framework's ability to scale to singleobject 6DoF pose estimation. The float and charbased models are assessed quantitatively using rendered images.",
                                            "leftover": "We first evaluate our framework's ability to scale to singleobject 6DoF pose estimation. The float and charbased models are assessed quantitatively using rendered images.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 105,
                                            "key": "doc/body/sec3/sub4/ssb3/txl1",
                                            "block type": "txl",
                                            "content": "Setting. We extend the setting used in but unfreeze the previously fixed 3D position and assign it a randomly sampled value. We expand the number of colors used in the dataset to 133https://simple.wikipedia.org/wiki/ListofCrayolacrayoncolors to better emulate the diversity observed in the real world. Differing from the previous setup, we fix the size of the objects due to the relative depthscale ambiguity of the toy airplanes. To evaluate our framework's ability to scale beyond dataconstrained scenarios, we render a training dataset of onemillion images. Following the rotationrepresentation results of, we use the intrinsicEuler representation for the charbased model and the 6D representation for the floatbased model as their use led to the greatest ID performance.",
                                            "leftover": "Setting. We extend the setting used in but unfreeze the previously fixed 3D position and assign it a randomly sampled value. We expand the number of colors used in the dataset to 133https://simple.wikipedia.org/wiki/ListofCrayolacrayoncolors to better emulate the diversity observed in the real world. Differing from the previous setup, we fix the size of the objects due to the relative depthscale ambiguity of the toy airplanes. To evaluate our framework's ability to scale beyond dataconstrained scenarios, we render a training dataset of onemillion images. Following the rotationrepresentation results of, we use the intrinsicEuler representation for the charbased model and the 6D representation for the floatbased model as their use led to the greatest ID performance.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 106,
                                            "key": "doc/body/sec3/sub4/ssb3/txl2",
                                            "block type": "txl",
                                            "content": "Results. illustrates that, under this nondataconstrained scenario, both model variants effectively capture the dynamics of the task",
                                            "leftover": "Results. illustrates that, under this nondataconstrained scenario, both model variants effectively capture the dynamics of the task",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 107,
                                            "key": "doc/body/sec3/sub4/ssb3/txl3",
                                            "block type": "txl",
                                            "content": "The models both notably exhibit an order of magnitude lower positional error than in the CLEVR setting, despite the addition of 3D orientation and an additional positional dimension, and achieve rotational error 28% of that observed in the ID portion of the SO(3) rangegap evaluation. This reinforces the earlier observation from the CLEVR dataefficiency evaluation that, given sufficient data, the model variants exhibit a similar performance ceiling. Still, neither achieves the level of precision necessary to be directly constrained by the threedecimalplace discretization applied to numeric quantities throughout the evaluations nor the 16bit training precision in the case of the floatbased model. See for further training details.",
                                            "leftover": "The models both notably exhibit an order of magnitude lower positional error than in the CLEVR setting, despite the addition of 3D orientation and an additional positional dimension, and achieve rotational error 28% of that observed in the ID portion of the SO(3) rangegap evaluation. This reinforces the earlier observation from the CLEVR dataefficiency evaluation that, given sufficient data, the model variants exhibit a similar performance ceiling. Still, neither achieves the level of precision necessary to be directly constrained by the threedecimalplace discretization applied to numeric quantities throughout the evaluations nor the 16bit training precision in the case of the floatbased model. See for further training details.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 108,
                                            "key": "doc/body/sec3/sub4/ssb3/txl4",
                                            "block type": "txl",
                                            "content": "As part of our evaluation, we also qualitatively examine the ability of the model to transfer from the renders of toy planes with a solidwhite background, on which it was finetuned, to estimating the pose and attributes of planes in realworld images. We provide qualitative samples of our model's generalization to such images in . We observe encouraging generalization across the majority of images tested, despite the lack of augmentation or domainspecific inductive bias applied during the training process. However, it is difficult to quantitatively evaluate such model performance due to a lack of paired realworld data in line with our compositional task. As a proxy for such an evaluation, we introduce a synthetic setting in to quantitatively evaluate the ability of our framework to generalize across visual domains.",
                                            "leftover": "As part of our evaluation, we also qualitatively examine the ability of the model to transfer from the renders of toy planes with a solidwhite background, on which it was finetuned, to estimating the pose and attributes of planes in realworld images. We provide qualitative samples of our model's generalization to such images in . We observe encouraging generalization across the majority of images tested, despite the lack of augmentation or domainspecific inductive bias applied during the training process. However, it is difficult to quantitatively evaluate such model performance due to a lack of paired realworld data in line with our compositional task. As a proxy for such an evaluation, we introduce a synthetic setting in to quantitatively evaluate the ability of our framework to generalize across visual domains.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/ssb4",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 109,
                                            "key": "doc/body/sec3/sub4/ssb4/tit",
                                            "block type": "title",
                                            "content": "SceneLevel 6DoF",
                                            "leftover": "SceneLevel 6DoF",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/sub4/ssb4/table0",
                                            "block_type": "table",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec3/sub4/ssb4/table0/cpt0",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 110,
                                                            "key": "doc/body/sec3/sub4/ssb4/table0/cpt0/txl0",
                                                            "block type": "txl",
                                                            "content": "ShapeNet 6DoF Results.",
                                                            "leftover": "ShapeNet 6DoF Results.",
                                                            "matches": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 111,
                                                    "key": "doc/body/sec3/sub4/ssb4/table0/txl1",
                                                    "block type": "txl",
                                                    "content": "() The floatbased model outperforms the charbased variant across all evaluations. Chamf.~represents the Chamfer distance between the groundtruth and estimated scenes. Cat.~represents category accuracy (sofa, chair, table). }",
                                                    "leftover": "() The floatbased model outperforms the charbased variant across all evaluations. Chamf.~represents the Chamfer distance between the groundtruth and estimated scenes. Cat.~represents category accuracy (sofa, chair, table). }",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 112,
                                                    "key": "doc/body/sec3/sub4/ssb4/table0/tabular2",
                                                    "block type": "tabular",
                                                    "content": "{lrr|rr|rr} & \\multicolumn{2}cID & \\multicolumn{2}c{OODT} & \\multicolumn{2}c{OODT+S} & Char & Float & Char & Float & Char & Float ↓L2 & 0.22 & 0.18}& 0.31 & 0.26}& 0.52 & 0.40} ↓Geod. & 8.14 & 5.65}& 14.40 & 10.48}& 45.11 & 43.46} ↓Count & 0.01}& 0.01}& 0.08}& 0.08}& 0.09 & 0.08} ↑Color & 77.07 & 83.42}& N/A & N/A & N/A & N/A ↑Shape & 89.21 & 93.31}& 68.72 & 78.26}& N/A & N/A ↑Cat.\\ & 97.27 & 98.33}& 94.55 & 96.58}& 85.99 & 86.71} ↓Chamf. & 0.45 & 0.22}& 1.17 & 0.57}& 14.63 & 2.58}",
                                                    "leftover": "{lrr|rr|rr} & \\multicolumn{2}cID & \\multicolumn{2}c{OODT} & \\multicolumn{2}c{OODT+S} & Char & Float & Char & Float & Char & Float ↓L2 & 0.22 & 0.18}& 0.31 & 0.26}& 0.52 & 0.40} ↓Geod. & 8.14 & 5.65}& 14.40 & 10.48}& 45.11 & 43.46} ↓Count & 0.01}& 0.01}& 0.08}& 0.08}& 0.09 & 0.08} ↑Color & 77.07 & 83.42}& N/A & N/A & N/A & N/A ↑Shape & 89.21 & 93.31}& 68.72 & 78.26}& N/A & N/A ↑Cat.\\ & 97.27 & 98.33}& 94.55 & 96.58}& 85.99 & 86.71} ↓Chamf. & 0.45 & 0.22}& 1.17 & 0.57}& 14.63 & 2.58}",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/sub4/ssb4/figure1",
                                            "block_type": "figure",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec3/sub4/ssb4/figure1/cpt0",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 113,
                                                            "key": "doc/body/sec3/sub4/ssb4/figure1/cpt0/txl0",
                                                            "block type": "txl",
                                                            "content": "OOD ShapeNet 6DoF Samples.",
                                                            "leftover": "OOD ShapeNet 6DoF Samples.",
                                                            "matches": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 114,
                                                    "key": "doc/body/sec3/sub4/ssb4/figure1/txl1",
                                                    "block type": "txl",
                                                    "content": "() Two sample reconstructions from the OOD ShapeNet 6Dof poseestimation experiment. Left to right: input, output. We evaluate on assets not shown during training, with outofdistribution textures. See \\Cref{fig:scene6dofoodsamplesadditional} for additional samples. }",
                                                    "leftover": "() Two sample reconstructions from the OOD ShapeNet 6Dof poseestimation experiment. Left to right: input, output. We evaluate on assets not shown during training, with outofdistribution textures. See \\Cref{fig:scene6dofoodsamplesadditional} for additional samples. }",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 115,
                                            "key": "doc/body/sec3/sub4/ssb4/txl2",
                                            "block type": "txl",
                                            "content": "In this section, we explore the scalability of our framework to scenelevel 6DoFpose estimation, featuring 35 objects per scene and a muchexpanded array of assets. This experiment not only assesses performance under morechallenging conditions, but also enables a quantitative evaluation on the framework's ability to generalize to scenes with OOD visual appearance.",
                                            "leftover": "In this section, we explore the scalability of our framework to scenelevel 6DoFpose estimation, featuring 35 objects per scene and a muchexpanded array of assets. This experiment not only assesses performance under morechallenging conditions, but also enables a quantitative evaluation on the framework's ability to generalize to scenes with OOD visual appearance.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 116,
                                            "key": "doc/body/sec3/sub4/ssb4/txl3",
                                            "block type": "txl",
                                            "content": "Setting. We construct an expanded CLEVRlike imagescene dataset, incorporating objects sourced from ShapeNet. The dataset comprises 56 chair types, 35 sofa types, and 47 table types. We remove the size and material attributes used in CLEVR, but employ the expanded color set used in to randomly color the objects. After doing so, the total number of possible combinations of attributes is 191fold that used in the CLEVRCoGenT experiment. Differing from previous evaluations, we also vary the pitch of the camera and the radius of its arc, but maintain a fixed camera focal point. Returning from the millionimage singleobject 6DoF evaluation to a relatively dataconstrained setting, we render 10k training images and evaluate the framework on three conditions, each with 1K images: (1) ID, which matches the training distribution of scenes with solidcolored objects; (2) OOD texture (OODT), where the same object assets are used as in ID but the objects are rendered with original ShapeNet textures instead of the randomly assigned solid colors; and (3) OOD encompassing both unseen objects and original ShapeNet textures (OODT+S). We use this to emulate the distribution shift of modeling realworld scenes, while facilitating quantitative evaluation.",
                                            "leftover": "Setting. We construct an expanded CLEVRlike imagescene dataset, incorporating objects sourced from ShapeNet. The dataset comprises 56 chair types, 35 sofa types, and 47 table types. We remove the size and material attributes used in CLEVR, but employ the expanded color set used in to randomly color the objects. After doing so, the total number of possible combinations of attributes is 191fold that used in the CLEVRCoGenT experiment. Differing from previous evaluations, we also vary the pitch of the camera and the radius of its arc, but maintain a fixed camera focal point. Returning from the millionimage singleobject 6DoF evaluation to a relatively dataconstrained setting, we render 10k training images and evaluate the framework on three conditions, each with 1K images: (1) ID, which matches the training distribution of scenes with solidcolored objects; (2) OOD texture (OODT), where the same object assets are used as in ID but the objects are rendered with original ShapeNet textures instead of the randomly assigned solid colors; and (3) OOD encompassing both unseen objects and original ShapeNet textures (OODT+S). We use this to emulate the distribution shift of modeling realworld scenes, while facilitating quantitative evaluation.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 117,
                                            "key": "doc/body/sec3/sub4/ssb4/txl4",
                                            "block type": "txl",
                                            "content": "Results. We observe that both approaches scale to the task, though the floatbased model outperforms  or ties with  the charbased variant across evaluations (). This disparity is emphasized in the OODT+S setting where scenelevel chamfer distance of the charbased model jumps from being approximately twice that of the floatbased variant in the ID and OODT evaluations to being 5.67 times as much.",
                                            "leftover": "Results. We observe that both approaches scale to the task, though the floatbased model outperforms  or ties with  the charbased variant across evaluations (). This disparity is emphasized in the OODT+S setting where scenelevel chamfer distance of the charbased model jumps from being approximately twice that of the floatbased variant in the ID and OODT evaluations to being 5.67 times as much.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 118,
                                            "key": "doc/body/sec3/sub4/ssb4/txl5",
                                            "block type": "txl",
                                            "content": "There is a decrease in performance observed when stepping to the OODT setting, which is moststrongly observed in the count error (x8 for both) and the shaperecognition accuracy (20.49% in char and 15.05% in float). We empirically attribute this to the model occasionally explaining some multicolor textured objects using a composition of multiple, solidcolor assets. Quantitatively supporting this, the performance decrease is not as strongly reflected in scenelevel chamfer distance (x2.6 for both).",
                                            "leftover": "There is a decrease in performance observed when stepping to the OODT setting, which is moststrongly observed in the count error (x8 for both) and the shaperecognition accuracy (20.49% in char and 15.05% in float). We empirically attribute this to the model occasionally explaining some multicolor textured objects using a composition of multiple, solidcolor assets. Quantitatively supporting this, the performance decrease is not as strongly reflected in scenelevel chamfer distance (x2.6 for both).",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 119,
                                            "key": "doc/body/sec3/sub4/ssb4/txl6",
                                            "block type": "txl",
                                            "content": "See for samples reconstructions from OODT+S and for samples from the ID setting. We additionally test our model on realworld samples, but find that it fails to consistently generalize (). We attribute this failure partially to limitations in the cameraposition training distribution.",
                                            "leftover": "See for samples reconstructions from OODT+S and for samples from the ID setting. We additionally test our model on realworld samples, but find that it fails to consistently generalize (). We attribute this failure partially to limitations in the cameraposition training distribution.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 120,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Discussion and Limitations",
                            "leftover": "Discussion and Limitations",
                            "matches": []
                        },
                        {
                            "leaf id": 121,
                            "key": "doc/body/sec4/txl0",
                            "block type": "txl",
                            "content": "Through our investigation, we demonstrated the ability of LLMs to facilitate inversegraphics tasks across a variety of domain shifts, albeit within controlled settings. In designing targeted evaluations to analyze the model's generalization ability, our goal was to lay the groundwork necessary for future advancements. However, scaling up these models to metrically reconstruct complex realworld scenes will undoubtedly pose additional challenges.",
                            "leftover": "Through our investigation, we demonstrated the ability of LLMs to facilitate inversegraphics tasks across a variety of domain shifts, albeit within controlled settings. In designing targeted evaluations to analyze the model's generalization ability, our goal was to lay the groundwork necessary for future advancements. However, scaling up these models to metrically reconstruct complex realworld scenes will undoubtedly pose additional challenges.",
                            "matches": []
                        },
                        {
                            "leaf id": 122,
                            "key": "doc/body/sec4/txl1",
                            "block type": "txl",
                            "content": "The primary limitation of our approach lies in that its expressiveness is constrained by the expressiveness of the trainingdatageneration framework. We demonstrated its ability to learn to compositionally disentangle images of scenes into constituent elements, reconstructing scenes under distribution shifts. However, reproducing scenes as text, it can reconstruct scenes containing unknown objects in OOD configurations, but it does so in terms of the objects  and language  it is trained with. If it does not know the name of asset \\mbox{chairs0055}, it will not be able to use it. Even if the model produces the name of a new color or shape from outside of the training data, the graphics engine rendering the LLM output must have an understanding of it in order to apply it.",
                            "leftover": "The primary limitation of our approach lies in that its expressiveness is constrained by the expressiveness of the trainingdatageneration framework. We demonstrated its ability to learn to compositionally disentangle images of scenes into constituent elements, reconstructing scenes under distribution shifts. However, reproducing scenes as text, it can reconstruct scenes containing unknown objects in OOD configurations, but it does so in terms of the objects  and language  it is trained with. If it does not know the name of asset \\mbox{chairs0055}, it will not be able to use it. Even if the model produces the name of a new color or shape from outside of the training data, the graphics engine rendering the LLM output must have an understanding of it in order to apply it.",
                            "matches": []
                        },
                        {
                            "leaf id": 123,
                            "key": "doc/body/sec4/txl2",
                            "block type": "txl",
                            "content": "In contrast, the generality of our approach, which doesn't incorporate special taskspecific inductive biases, allows it to scale with the diversity of the training data or the expressivity of the code format. Future work may explore morescalable trainingdata generators or integrate selfsupervision techniques to enable learning from unlabeled images. While we employ a relatively straightforward objectcentric code representation across experiments for simplicity, moreexpressive scene representations should also be explored.",
                            "leftover": "In contrast, the generality of our approach, which doesn't incorporate special taskspecific inductive biases, allows it to scale with the diversity of the training data or the expressivity of the code format. Future work may explore morescalable trainingdata generators or integrate selfsupervision techniques to enable learning from unlabeled images. While we employ a relatively straightforward objectcentric code representation across experiments for simplicity, moreexpressive scene representations should also be explored.",
                            "matches": []
                        },
                        {
                            "leaf id": 124,
                            "key": "doc/body/sec4/txl3",
                            "block type": "txl",
                            "content": "Our evaluation scenes feature only minor object occlusions and are relatively simple. While a generic nexttoken objective paired with MSE float supervision sufficed for these scenarios, addressing hardertodisentangle scenes may require a tradeoff between generality and inductive bias, to incorporate additional supervision.",
                            "leftover": "Our evaluation scenes feature only minor object occlusions and are relatively simple. While a generic nexttoken objective paired with MSE float supervision sufficed for these scenarios, addressing hardertodisentangle scenes may require a tradeoff between generality and inductive bias, to incorporate additional supervision.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 125,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Conclusion",
                            "leftover": "Conclusion",
                            "matches": []
                        },
                        {
                            "leaf id": 126,
                            "key": "doc/body/sec5/txl0",
                            "block type": "txl",
                            "content": "In this work, we investigated the ability of LLMs to solve inversegraphics challenges. Introducing the InverseGraphics LargeLanguageModel (IGLLM) framework, we demonstrated that the broad generalization and reasoning capabilities of LLMs can be harnessed to facilitate inversegraphics tasks. Through extensive evaluation, we assessed the model's capacity to generalize outofdomain, revealing its ability to abstract scene elements compositionally. We additionally explored the integration of a numeric head to adapt LLMs for continuous metricvalue estimation, providing enhanced generalization and smoother training dynamics. Our quantitative analyses demonstrate its ability to generalize compositionally (), in parameter space (), and across visual domains (). Our investigation demonstrates the ability of IGLLM to leverage the general knowledge of LLMs in solving inversegraphics problems, opening a new avenue for research.",
                            "leftover": "In this work, we investigated the ability of LLMs to solve inversegraphics challenges. Introducing the InverseGraphics LargeLanguageModel (IGLLM) framework, we demonstrated that the broad generalization and reasoning capabilities of LLMs can be harnessed to facilitate inversegraphics tasks. Through extensive evaluation, we assessed the model's capacity to generalize outofdomain, revealing its ability to abstract scene elements compositionally. We additionally explored the integration of a numeric head to adapt LLMs for continuous metricvalue estimation, providing enhanced generalization and smoother training dynamics. Our quantitative analyses demonstrate its ability to generalize compositionally (), in parameter space (), and across visual domains (). Our investigation demonstrates the ability of IGLLM to leverage the general knowledge of LLMs in solving inversegraphics problems, opening a new avenue for research.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec5/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 127,
                                    "key": "doc/body/sec5/par1/txl0",
                                    "block type": "txl",
                                    "content": "Acknowledgements We thank Silvia Zuffi for useful discussions and Benjamin Pellkofer for IT support.",
                                    "leftover": "Acknowledgements We thank Silvia Zuffi for useful discussions and Benjamin Pellkofer for IT support.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/par2",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 128,
                                    "key": "doc/body/sec5/par2/txl0",
                                    "block type": "txl",
                                    "content": "Disclosure MJB has received research gift funds from Adobe, Intel, Nvidia, Meta, and Amazon. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While MJB is a consultant for Meshcapade, his research in this project was performed solely at, and funded solely by, the Max Planck Society. \\clearpage \\bibliographystyletmlr \\bibliographymain \\clearpage \\clearpage \\renewcommand{S.\\arabicfigure }{S.\\arabicfigure} \\renewcommand{S.\\arabictable }{S.\\arabictable} \\renewcommand{S.\\arabicequation }{S.\\arabicequation} \\setcounterfigure{0} \\setcountertable{0} \\setcounterequation{0}",
                                    "leftover": "Disclosure MJB has received research gift funds from Adobe, Intel, Nvidia, Meta, and Amazon. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While MJB is a consultant for Meshcapade, his research in this project was performed solely at, and funded solely by, the Max Planck Society. \\clearpage \\bibliographystyletmlr \\bibliographymain \\clearpage \\clearpage \\renewcommand{S.\\arabicfigure }{S.\\arabicfigure} \\renewcommand{S.\\arabictable }{S.\\arabictable} \\renewcommand{S.\\arabicequation }{S.\\arabicequation} \\setcounterfigure{0} \\setcountertable{0} \\setcounterequation{0}",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 129,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Further Training Details",
                            "leftover": "Further Training Details",
                            "matches": []
                        },
                        {
                            "leaf id": 130,
                            "key": "doc/body/sec6/txl0",
                            "block type": "txl",
                            "content": "We finetune the LLaMA 1based Vicuna 1.3 modelhttps://huggingface.co/lmsys/vicuna7bv1.3 with LoRA. We use the HuggingFace Transformers and PEFT libraries, along with DeepSpeed \\mbox{ZeRO2}. In all experiments, we use a \\mbox{lorar} of \\mbox{128}, a \\mbox{loraalpha} of \\mbox{256}, a LoRA learning rate of \\mbox{2e05}, a linear projector learning rate of \\mbox{2e05}, a numeric head learning rate of \\mbox{2e04}, and a cosine learningrate schedule. All models are trained with an effective batch size of \\mbox{32} with \\mbox{bfloat16} mixedprecision training. Both the crossentropy nexttokenprediction and meansquareerror losses are given a weight of 1.",
                            "leftover": "We finetune the LLaMA 1based Vicuna 1.3 modelhttps://huggingface.co/lmsys/vicuna7bv1.3 with LoRA. We use the HuggingFace Transformers and PEFT libraries, along with DeepSpeed \\mbox{ZeRO2}. In all experiments, we use a \\mbox{lorar} of \\mbox{128}, a \\mbox{loraalpha} of \\mbox{256}, a LoRA learning rate of \\mbox{2e05}, a linear projector learning rate of \\mbox{2e05}, a numeric head learning rate of \\mbox{2e04}, and a cosine learningrate schedule. All models are trained with an effective batch size of \\mbox{32} with \\mbox{bfloat16} mixedprecision training. Both the crossentropy nexttokenprediction and meansquareerror losses are given a weight of 1.",
                            "matches": []
                        },
                        {
                            "leaf id": 131,
                            "key": "doc/body/sec6/txl1",
                            "block type": "txl",
                            "content": "The models for the CLEVR and parameterspace generalization experiments are trained for \\mbox{40k} steps. The 6DoF poseestimation models are trained for \\mbox{200k} steps.",
                            "leftover": "The models for the CLEVR and parameterspace generalization experiments are trained for \\mbox{40k} steps. The 6DoF poseestimation models are trained for \\mbox{200k} steps.",
                            "matches": []
                        },
                        {
                            "leaf id": 132,
                            "key": "doc/body/sec6/txl2",
                            "block type": "txl",
                            "content": "We use the frozen CLIP visual tokenizer from https://huggingface.co/openai/clipvitlargepatch14336. This CLIP variant has an input size of 336x336 pixels. For the CLEVR evaluation, we render images at the original size of 480x320 to ensure compatibility with NSVQA, but pad and resize them for use with our model. For the remaining evaluations we directly render images at a resolution of 336x336.",
                            "leftover": "We use the frozen CLIP visual tokenizer from https://huggingface.co/openai/clipvitlargepatch14336. This CLIP variant has an input size of 336x336 pixels. For the CLEVR evaluation, we render images at the original size of 480x320 to ensure compatibility with NSVQA, but pad and resize them for use with our model. For the remaining evaluations we directly render images at a resolution of 336x336.",
                            "matches": []
                        },
                        {
                            "leaf id": 133,
                            "key": "doc/body/sec6/txl3",
                            "block type": "txl",
                            "content": "We employ greedy token sampling across evaluations.",
                            "leftover": "We employ greedy token sampling across evaluations.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec7",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 134,
                            "key": "doc/body/sec7/tit",
                            "block type": "title",
                            "content": "Further CLEVR DataGeneration Details",
                            "leftover": "Further CLEVR DataGeneration Details",
                            "matches": []
                        },
                        {
                            "leaf id": 135,
                            "key": "doc/body/sec7/txl0",
                            "block type": "txl",
                            "content": "The original CLEVR dataset is rendered with random positional jitter in both the lights and camera. This information is not recorded in the public dataset, so we rerender CLEVRCoGenT with a fixed camera position, but maintain the randomness in the lighting.",
                            "leftover": "The original CLEVR dataset is rendered with random positional jitter in both the lights and camera. This information is not recorded in the public dataset, so we rerender CLEVRCoGenT with a fixed camera position, but maintain the randomness in the lighting.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec8",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 136,
                            "key": "doc/body/sec8/tit",
                            "block type": "title",
                            "content": "Further NumericHead Details",
                            "leftover": "Further NumericHead Details",
                            "matches": []
                        },
                        {
                            "leaf id": 137,
                            "key": "doc/body/sec8/txl0",
                            "block type": "txl",
                            "content": "Our numeric head is composed of a tanh layer, followed by a linear layer, a GELU activation, and a final linear projection. The final LLaMA hidden state is passed through an RMS norm before it is shared between the token head and numeric head, which rescales but does not recenter the embedding.",
                            "leftover": "Our numeric head is composed of a tanh layer, followed by a linear layer, a GELU activation, and a final linear projection. The final LLaMA hidden state is passed through an RMS norm before it is shared between the token head and numeric head, which rescales but does not recenter the embedding.",
                            "matches": []
                        },
                        {
                            "leaf id": 138,
                            "key": "doc/body/sec8/txl1",
                            "block type": "txl",
                            "content": "During training, the locations of these tokens in the groundtruth sequence are known so they can be masked to apply the MSE loss. During sampling, the position of these tokens is not preknown and dependent on the generated sequence. We first generate the tokenonly sequence and then substitute the estimated numbers back in with a second pass.",
                            "leftover": "During training, the locations of these tokens in the groundtruth sequence are known so they can be masked to apply the MSE loss. During sampling, the position of these tokens is not preknown and dependent on the generated sequence. We first generate the tokenonly sequence and then substitute the estimated numbers back in with a second pass.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec8/figure2",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure2/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 139,
                                            "key": "doc/body/sec8/figure2/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "RealWorld ShapeNet 6DoF Samples.",
                                            "leftover": "RealWorld ShapeNet 6DoF Samples.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 140,
                                    "key": "doc/body/sec8/figure2/txl1",
                                    "block type": "txl",
                                    "content": "() Realworld sample reconstructions from the ShapeNet 6Dof poseestimation experiment. We observe that the model is sensitive to OOD camera configurations. During data generation, the camera is assigned a random pitch and radius, with its optical axis fixed passing through the global origin. As such, we find that the model learns the bias and is limited by the expressivity of the trainingdatageneration framework, and, while it effectively interpolates values, it struggles to extrapolate outside of the camera configurations on which it was trained on. We observe that the model is still, however, often able to identify the first few mostsalient objects in the scene and produce meaningful assets (the first two in each of these samples being the rightmost chair then the table) before attempting to explain background features. }",
                                    "leftover": "() Realworld sample reconstructions from the ShapeNet 6Dof poseestimation experiment. We observe that the model is sensitive to OOD camera configurations. During data generation, the camera is assigned a random pitch and radius, with its optical axis fixed passing through the global origin. As such, we find that the model learns the bias and is limited by the expressivity of the trainingdatageneration framework, and, while it effectively interpolates values, it struggles to extrapolate outside of the camera configurations on which it was trained on. We observe that the model is still, however, often able to identify the first few mostsalient objects in the scene and produce meaningful assets (the first two in each of these samples being the rightmost chair then the table) before attempting to explain background features. }",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/table3",
                            "block_type": "table",
                            "children": [
                                {
                                    "key": "doc/body/sec8/table3/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 141,
                                            "key": "doc/body/sec8/table3/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "Full CLEVR DataEfficiency Results.",
                                            "leftover": "Full CLEVR DataEfficiency Results.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 142,
                                    "key": "doc/body/sec8/table3/txl1",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec8/table3/subtable2",
                                    "block_type": "subtable",
                                    "children": [
                                        {
                                            "leaf id": 143,
                                            "key": "doc/body/sec8/table3/subtable2/txl0",
                                            "block type": "txl",
                                            "content": "{0.495}",
                                            "leftover": "{0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec8/table3/subtable2/cpt1",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 144,
                                                    "key": "doc/body/sec8/table3/subtable2/cpt1/txl0",
                                                    "block type": "txl",
                                                    "content": "ID",
                                                    "leftover": "ID",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 145,
                                            "key": "doc/body/sec8/table3/subtable2/txl2",
                                            "block type": "txl",
                                            "content": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "leftover": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 146,
                                            "key": "doc/body/sec8/table3/subtable2/tabular3",
                                            "block type": "tabular",
                                            "content": "lrrrrrr & ↓L2 & ↓Count & ↑Size & ↑Color & ↑Mat. & ↑Shape 500 \\cline{11} Char & 1.15 & 0.30}& 87.58 & 78.23 & 87.09 & 83.25 Float & 0.98}& 0.44 & 91.43}& 85.51}& 90.73}& 89.29} 1000 \\cline{11} Char & 0.73 & 0.18}& 97.14 & 94.54 & 96.50 & 95.98 Float & 0.39}& 0.18}& 98.96}& 98.69}& 98.53}& 98.40} 2000 \\cline{11} Char & 0.35 & 0.08}& 99.57}& 99.35}& 99.09}& 99.30} Float & 0.26}& 0.09 & 99.55 & 99.28 & 99.04 & 99.23 4000 \\cline{11} Char & 0.21 & 0.05}& 99.71 & 99.58 & 99.27 & 99.51 Float & 0.16}& 0.05}& 99.77}& 99.71}& 99.53}& 99.59}",
                                            "leftover": "lrrrrrr & ↓L2 & ↓Count & ↑Size & ↑Color & ↑Mat. & ↑Shape 500 \\cline{11} Char & 1.15 & 0.30}& 87.58 & 78.23 & 87.09 & 83.25 Float & 0.98}& 0.44 & 91.43}& 85.51}& 90.73}& 89.29} 1000 \\cline{11} Char & 0.73 & 0.18}& 97.14 & 94.54 & 96.50 & 95.98 Float & 0.39}& 0.18}& 98.96}& 98.69}& 98.53}& 98.40} 2000 \\cline{11} Char & 0.35 & 0.08}& 99.57}& 99.35}& 99.09}& 99.30} Float & 0.26}& 0.09 & 99.55 & 99.28 & 99.04 & 99.23 4000 \\cline{11} Char & 0.21 & 0.05}& 99.71 & 99.58 & 99.27 & 99.51 Float & 0.16}& 0.05}& 99.77}& 99.71}& 99.53}& 99.59}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 147,
                                            "key": "doc/body/sec8/table3/subtable2/txl4",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec8/table3/subtable3",
                                    "block_type": "subtable",
                                    "children": [
                                        {
                                            "leaf id": 148,
                                            "key": "doc/body/sec8/table3/subtable3/txl0",
                                            "block type": "txl",
                                            "content": "{0.495}",
                                            "leftover": "{0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec8/table3/subtable3/cpt1",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 149,
                                                    "key": "doc/body/sec8/table3/subtable3/cpt1/txl0",
                                                    "block type": "txl",
                                                    "content": "OOD",
                                                    "leftover": "OOD",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 150,
                                            "key": "doc/body/sec8/table3/subtable3/txl2",
                                            "block type": "txl",
                                            "content": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "leftover": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 151,
                                            "key": "doc/body/sec8/table3/subtable3/tabular3",
                                            "block type": "tabular",
                                            "content": "lrrrrrr & ↓L2 & ↓Count & ↑Size & ↑Color & ↑Mat. & ↑Shape 500 \\cline{11} Char & 1.13 & 0.36}& 87.21 & 75.51 & 85.57 & 79.50 Float & 1.01}& 0.53 & 90.71}& 79.45}& 89.24}& 84.42} 1000 \\cline{11} Char & 0.74 & 0.21}& 96.25 & 92.19 & 94.87 & 90.45 Float & 0.41}& 0.23 & 98.92}& 96.49}& 97.75}& 94.75} 2000 \\cline{11} Char & 0.36 & 0.11}& 99.52}& 97.56}& 98.66}& 92.26 Float & 0.28}& 0.12 & 99.29 & 97.33 & 98.66}& 94.76} 4000 \\cline{11} Char & 0.22 & 0.06 & 99.74 & 98.60}& 99.33}& 93.50} Float & 0.17}& 0.05}& 99.80}& 98.14 & 99.21 & 93.14",
                                            "leftover": "lrrrrrr & ↓L2 & ↓Count & ↑Size & ↑Color & ↑Mat. & ↑Shape 500 \\cline{11} Char & 1.13 & 0.36}& 87.21 & 75.51 & 85.57 & 79.50 Float & 1.01}& 0.53 & 90.71}& 79.45}& 89.24}& 84.42} 1000 \\cline{11} Char & 0.74 & 0.21}& 96.25 & 92.19 & 94.87 & 90.45 Float & 0.41}& 0.23 & 98.92}& 96.49}& 97.75}& 94.75} 2000 \\cline{11} Char & 0.36 & 0.11}& 99.52}& 97.56}& 98.66}& 92.26 Float & 0.28}& 0.12 & 99.29 & 97.33 & 98.66}& 94.76} 4000 \\cline{11} Char & 0.22 & 0.06 & 99.74 & 98.60}& 99.33}& 93.50} Float & 0.17}& 0.05}& 99.80}& 98.14 & 99.21 & 93.14",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 152,
                                            "key": "doc/body/sec8/table3/subtable3/txl4",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/table4",
                            "block_type": "table",
                            "children": [
                                {
                                    "key": "doc/body/sec8/table4/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 153,
                                            "key": "doc/body/sec8/table4/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "Full SO(3) RangeGap Results.",
                                            "leftover": "Full SO(3) RangeGap Results.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 154,
                                    "key": "doc/body/sec8/table4/txl1",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec8/table4/subtable2",
                                    "block_type": "subtable",
                                    "children": [
                                        {
                                            "leaf id": 155,
                                            "key": "doc/body/sec8/table4/subtable2/txl0",
                                            "block type": "txl",
                                            "content": "{0.495}",
                                            "leftover": "{0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec8/table4/subtable2/cpt1",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 156,
                                                    "key": "doc/body/sec8/table4/subtable2/cpt1/txl0",
                                                    "block type": "txl",
                                                    "content": "ID",
                                                    "leftover": "ID",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 157,
                                            "key": "doc/body/sec8/table4/subtable2/txl2",
                                            "block type": "txl",
                                            "content": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "leftover": "{ \\setlength{\\tabcolsep}{2pt}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 158,
                                            "key": "doc/body/sec8/table4/subtable2/tabular3",
                                            "block type": "tabular",
                                            "content": "lrrrrr & ↓Geod.\\ & ↑Size & ↑Color & ↑Mat. & ↑Shape Char \\cline{11} ExtEuler & 67.31 & 99.80 & 100.00 & 97.80 & 98.70 IntEuler & 46.86}& 99.90 & 100.00 & 97.10 & 98.30 AA & 53.74}& 100.00 & 100.00 & 97.40 & 98.30 6D & 77.69 & 100.00 & 99.90 & 97.40 & 98.60 Float \\cline{11} ExtEuler & 41.25 & 100.00 & 100.00 & 98.00 & 98.60 IntEuler & 27.05 & 99.90 & 100.00 & 97.70 & 99.30 AA & 26.58}& 100.00 & 100.00 & 97.50 & 99.00 6D & 17.76}& 100.00 & 100.00 & 97.40 & 99.30",
                                            "leftover": "lrrrrr & ↓Geod.\\ & ↑Size & ↑Color & ↑Mat. & ↑Shape Char \\cline{11} ExtEuler & 67.31 & 99.80 & 100.00 & 97.80 & 98.70 IntEuler & 46.86}& 99.90 & 100.00 & 97.10 & 98.30 AA & 53.74}& 100.00 & 100.00 & 97.40 & 98.30 6D & 77.69 & 100.00 & 99.90 & 97.40 & 98.60 Float \\cline{11} ExtEuler & 41.25 & 100.00 & 100.00 & 98.00 & 98.60 IntEuler & 27.05 & 99.90 & 100.00 & 97.70 & 99.30 AA & 26.58}& 100.00 & 100.00 & 97.50 & 99.00 6D & 17.76}& 100.00 & 100.00 & 97.40 & 99.30",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 159,
                                            "key": "doc/body/sec8/table4/subtable2/txl4",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec8/table4/subtable3",
                                    "block_type": "subtable",
                                    "children": [
                                        {
                                            "leaf id": 160,
                                            "key": "doc/body/sec8/table4/subtable3/txl0",
                                            "block type": "txl",
                                            "content": "{0.495}",
                                            "leftover": "{0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec8/table4/subtable3/cpt1",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 161,
                                                    "key": "doc/body/sec8/table4/subtable3/cpt1/txl0",
                                                    "block type": "txl",
                                                    "content": "OOD",
                                                    "leftover": "OOD",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 162,
                                            "key": "doc/body/sec8/table4/subtable3/txl2",
                                            "block type": "txl",
                                            "content": "{ \\setlength{\\tabcolsep}{2pt} \\setlength{\\tabcolsep}{2pt}",
                                            "leftover": "{ \\setlength{\\tabcolsep}{2pt} \\setlength{\\tabcolsep}{2pt}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 163,
                                            "key": "doc/body/sec8/table4/subtable3/tabular3",
                                            "block type": "tabular",
                                            "content": "lrrrrr & ↓Geod.\\ & ↑Size & ↑Color & ↑Mat. & ↑Shape Char \\cline{11} ExtEuler & 78.21 & 100.00 & 99.90 & 97.50 & 99.40 IntEuler & 68.50}& 100.00 & 100.00 & 97.90 & 99.30 AA & 62.80}& 100.00 & 100.00 & 98.10 & 99.30 6D & 104.53 & 100.00 & 99.90 & 97.20 & 99.00 Float \\cline{11} ExtEuler & 42.03 & 100.00 & 100.00 & 97.20 & 99.10 IntEuler & 43.49 & 100.00 & 100.00 & 97.30 & 99.40 AA & 24.96}& 100.00 & 100.00 & 97.80 & 99.40 6D & 27.12}& 100.00 & 100.00 & 98.10 & 99.50",
                                            "leftover": "lrrrrr & ↓Geod.\\ & ↑Size & ↑Color & ↑Mat. & ↑Shape Char \\cline{11} ExtEuler & 78.21 & 100.00 & 99.90 & 97.50 & 99.40 IntEuler & 68.50}& 100.00 & 100.00 & 97.90 & 99.30 AA & 62.80}& 100.00 & 100.00 & 98.10 & 99.30 6D & 104.53 & 100.00 & 99.90 & 97.20 & 99.00 Float \\cline{11} ExtEuler & 42.03 & 100.00 & 100.00 & 97.20 & 99.10 IntEuler & 43.49 & 100.00 & 100.00 & 97.30 & 99.40 AA & 24.96}& 100.00 & 100.00 & 97.80 & 99.40 6D & 27.12}& 100.00 & 100.00 & 98.10 & 99.50",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 164,
                                            "key": "doc/body/sec8/table4/subtable3/txl4",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure5",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure5/minted0",
                                    "block_type": "minted",
                                    "children": [
                                        {
                                            "leaf id": 165,
                                            "key": "doc/body/sec8/figure5/minted0/txl0",
                                            "block type": "txl",
                                            "content": "python add(color='green', size='tiny', material='shiny', shape='cylinder', loc=(2.163, 1.384, 0.350)) add(material='metal', rotation=0.126, shape='cube', loc=(0.033, 2.456, 0.700), color='blue', size='large') add(size='large', material='rubber', color='blue', loc=(1.352, 1.165, 0.700), shape='sphere') add(color='brown', material='matte', shape='cube', size='tiny', loc=(1.185, 2.816, 0.350), rotation=0.144)",
                                            "leftover": "python add(color='green', size='tiny', material='shiny', shape='cylinder', loc=(2.163, 1.384, 0.350)) add(material='metal', rotation=0.126, shape='cube', loc=(0.033, 2.456, 0.700), color='blue', size='large') add(size='large', material='rubber', color='blue', loc=(1.352, 1.165, 0.700), shape='sphere') add(color='brown', material='matte', shape='cube', size='tiny', loc=(1.185, 2.816, 0.350), rotation=0.144)",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 166,
                                    "key": "doc/body/sec8/figure5/cpt1",
                                    "block type": "cpt",
                                    "content": "CLEVRCoGenT Train Sample.",
                                    "leftover": "CLEVRCoGenT Train Sample.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 167,
                                    "key": "doc/body/sec8/figure5/txl2",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure6",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure6/minted0",
                                    "block_type": "minted",
                                    "children": [
                                        {
                                            "leaf id": 168,
                                            "key": "doc/body/sec8/figure6/minted0/txl0",
                                            "block type": "txl",
                                            "content": "python add(x=0.292, y=0.266)",
                                            "leftover": "python add(x=0.292, y=0.266)",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 169,
                                    "key": "doc/body/sec8/figure6/cpt1",
                                    "block type": "cpt",
                                    "content": "2D ParameterSpaceGeneralization Train Sample.",
                                    "leftover": "2D ParameterSpaceGeneralization Train Sample.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 170,
                                    "key": "doc/body/sec8/figure6/txl2",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure7",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure7/minted0",
                                    "block_type": "minted",
                                    "children": [
                                        {
                                            "leaf id": 171,
                                            "key": "doc/body/sec8/figure7/minted0/txl0",
                                            "block type": "txl",
                                            "content": "python add(shape='airliner', size='tiny', color='green', material='matte', rotation=(0.798, 0.124, 0.590, 0.562, 0.507, 0.654))",
                                            "leftover": "python add(shape='airliner', size='tiny', color='green', material='matte', rotation=(0.798, 0.124, 0.590, 0.562, 0.507, 0.654))",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 172,
                                    "key": "doc/body/sec8/figure7/cpt1",
                                    "block type": "cpt",
                                    "content": "SO(3) RangeGap Train Sample.",
                                    "leftover": "SO(3) RangeGap Train Sample.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 173,
                                    "key": "doc/body/sec8/figure7/txl2",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure8",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure8/minted0",
                                    "block_type": "minted",
                                    "children": [
                                        {
                                            "leaf id": 174,
                                            "key": "doc/body/sec8/figure8/minted0/txl0",
                                            "block type": "txl",
                                            "content": "python add(loc=(6.355, 4.600, 4.206), color='Mahogany', shape='jet', material='matte', rotation=(0.941, 0.337, 0.022, 0.303, 0.815, 0.493))",
                                            "leftover": "python add(loc=(6.355, 4.600, 4.206), color='Mahogany', shape='jet', material='matte', rotation=(0.941, 0.337, 0.022, 0.303, 0.815, 0.493))",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 175,
                                    "key": "doc/body/sec8/figure8/cpt1",
                                    "block type": "cpt",
                                    "content": "SingleObject 6DoF Train Sample.",
                                    "leftover": "SingleObject 6DoF Train Sample.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 176,
                                    "key": "doc/body/sec8/figure8/txl2",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure9",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure9/minted0",
                                    "block_type": "minted",
                                    "children": [
                                        {
                                            "leaf id": 177,
                                            "key": "doc/body/sec8/figure9/minted0/txl0",
                                            "block type": "txl",
                                            "content": "python add(rotation=(0.999, 0.024, 0.042, 0.024, 0.496, 0.868), color='Olive Green', loc=(2.228, 0.057, 10.362), shape='tables0010') add(rotation=(0.934, 0.177, 0.310, 0.163, 0.561, 0.812), loc=(0.032, 1.816, 12.639), color='Melon', shape='chairs0055') add(loc=(3.707, 1.009, 10.332), rotation=(0.235, 0.481, 0.845, 0.689, 0.696, 0.204), shape='chairs0008', color='Red Violet')",
                                            "leftover": "python add(rotation=(0.999, 0.024, 0.042, 0.024, 0.496, 0.868), color='Olive Green', loc=(2.228, 0.057, 10.362), shape='tables0010') add(rotation=(0.934, 0.177, 0.310, 0.163, 0.561, 0.812), loc=(0.032, 1.816, 12.639), color='Melon', shape='chairs0055') add(loc=(3.707, 1.009, 10.332), rotation=(0.235, 0.481, 0.845, 0.689, 0.696, 0.204), shape='chairs0008', color='Red Violet')",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 178,
                                    "key": "doc/body/sec8/figure9/cpt1",
                                    "block type": "cpt",
                                    "content": "ShapeNet 6DoF Train Sample.",
                                    "leftover": "ShapeNet 6DoF Train Sample.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 179,
                                    "key": "doc/body/sec8/figure9/txl2",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure10",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure10/subfigure0",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 180,
                                            "key": "doc/body/sec8/figure10/subfigure0/txl0",
                                            "block type": "txl",
                                            "content": "{.3225} \\includegraphics[width=]{figures/clevr/input/2.jpg} \\includegraphics[width=]{figures/clevr/input/3.jpg} \\includegraphics[width=]{figures/clevr/input/4.jpg} \\includegraphics[width=]{figures/clevr/input/5.jpg} \\includegraphics[width=]{figures/clevr/input/6.jpg} \\includegraphics[width=]{figures/clevr/input/7.jpg}",
                                            "leftover": "{.3225} \\includegraphics[width=]{figures/clevr/input/2.jpg} \\includegraphics[width=]{figures/clevr/input/3.jpg} \\includegraphics[width=]{figures/clevr/input/4.jpg} \\includegraphics[width=]{figures/clevr/input/5.jpg} \\includegraphics[width=]{figures/clevr/input/6.jpg} \\includegraphics[width=]{figures/clevr/input/7.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 181,
                                            "key": "doc/body/sec8/figure10/subfigure0/cpt1",
                                            "block type": "cpt",
                                            "content": "Input",
                                            "leftover": "Input",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec8/figure10/subfigure1",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 182,
                                            "key": "doc/body/sec8/figure10/subfigure1/txl0",
                                            "block type": "txl",
                                            "content": "{.3225} \\includegraphics[width=]{figures/clevr/nsvqa/2.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/3.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/4.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/5.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/6.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/7.jpg}",
                                            "leftover": "{.3225} \\includegraphics[width=]{figures/clevr/nsvqa/2.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/3.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/4.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/5.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/6.jpg} \\includegraphics[width=]{figures/clevr/nsvqa/7.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 183,
                                            "key": "doc/body/sec8/figure10/subfigure1/cpt1",
                                            "block type": "cpt",
                                            "content": "NSVQA",
                                            "leftover": "NSVQA",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec8/figure10/subfigure2",
                                    "block_type": "subfigure",
                                    "children": [
                                        {
                                            "leaf id": 184,
                                            "key": "doc/body/sec8/figure10/subfigure2/txl0",
                                            "block type": "txl",
                                            "content": "{.3225} \\includegraphics[width=]{figures/clevr/output/2.jpg} \\includegraphics[width=]{figures/clevr/output/3.jpg} \\includegraphics[width=]{figures/clevr/output/4.jpg} \\includegraphics[width=]{figures/clevr/output/5.jpg} \\includegraphics[width=]{figures/clevr/output/6.jpg} \\includegraphics[width=]{figures/clevr/output/7.jpg}",
                                            "leftover": "{.3225} \\includegraphics[width=]{figures/clevr/output/2.jpg} \\includegraphics[width=]{figures/clevr/output/3.jpg} \\includegraphics[width=]{figures/clevr/output/4.jpg} \\includegraphics[width=]{figures/clevr/output/5.jpg} \\includegraphics[width=]{figures/clevr/output/6.jpg} \\includegraphics[width=]{figures/clevr/output/7.jpg}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 185,
                                            "key": "doc/body/sec8/figure10/subfigure2/cpt1",
                                            "block type": "cpt",
                                            "content": "Ours",
                                            "leftover": "Ours",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 186,
                                    "key": "doc/body/sec8/figure10/cpt3",
                                    "block type": "cpt",
                                    "content": "Additional OOD CLEVRCoGenT Samples.",
                                    "leftover": "Additional OOD CLEVRCoGenT Samples.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 187,
                                    "key": "doc/body/sec8/figure10/txl4",
                                    "block type": "txl",
                                    "content": "()}",
                                    "leftover": "()}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure11",
                            "block_type": "figure",
                            "children": [
                                {
                                    "leaf id": 188,
                                    "key": "doc/body/sec8/figure11/cpt0",
                                    "block type": "cpt",
                                    "content": "ID ShapeNet 6DoF Samples.",
                                    "leftover": "ID ShapeNet 6DoF Samples.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 189,
                                    "key": "doc/body/sec8/figure11/txl1",
                                    "block type": "txl",
                                    "content": "() Inputoutput pairs are shown lefttoright.}",
                                    "leftover": "() Inputoutput pairs are shown lefttoright.}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure12",
                            "block_type": "figure",
                            "children": [
                                {
                                    "leaf id": 190,
                                    "key": "doc/body/sec8/figure12/cpt0",
                                    "block type": "cpt",
                                    "content": "Additional OOD ShapeNet 6DoF Samples.",
                                    "leftover": "Additional OOD ShapeNet 6DoF Samples.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 191,
                                    "key": "doc/body/sec8/figure12/txl1",
                                    "block type": "txl",
                                    "content": "() Inputoutput pairs are shown lefttoright.}",
                                    "leftover": "() Inputoutput pairs are shown lefttoright.}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/figure13",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec8/figure13/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 192,
                                            "key": "doc/body/sec8/figure13/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "Additional OOD SingleObject 6DoF Samples.",
                                            "leftover": "Additional OOD SingleObject 6DoF Samples.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 193,
                                    "key": "doc/body/sec8/figure13/txl1",
                                    "block type": "txl",
                                    "content": "() Inputoutput pairs are shown lefttoright. }",
                                    "leftover": "() Inputoutput pairs are shown lefttoright. }",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 194,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "JeanBaptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob~L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\\l~aj Bi\\'nkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\\'en Simonyan. Flamingo: A visual language model for fewshot learning. In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh (eds.), Advances in Neural Information Processing Systems, volume~35, pp.\\ 2371623736. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800PaperConference.pdf.",
            "leftover": "JeanBaptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob~L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\\l~aj Bi\\'nkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\\'en Simonyan. Flamingo: A visual language model for fewshot learning. In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh (eds.), Advances in Neural Information Processing Systems, volume~35, pp.\\ 2371623736. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800PaperConference.pdf.",
            "matches": []
        },
        {
            "leaf id": 195,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C.~Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.",
            "leftover": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C.~Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.",
            "matches": []
        },
        {
            "leaf id": 196,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Mathieu Aubry, Daniel Maturana, Alexei~A. Efros, Bryan~C. Russell, and Josef Sivic. Seeing {3D} chairs: Exemplar partbased {2D}{3D} alignment using a large dataset of CAD models. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 2328, 2014, pp.\\ 37623769, Los Alamitos, CA, USA, 2014. IEEE Computer Society. \\doi{10.1109/CVPR.2014.487}. URL https://doi.org/10.1109/CVPR.2014.487.",
            "leftover": "Mathieu Aubry, Daniel Maturana, Alexei~A. Efros, Bryan~C. Russell, and Josef Sivic. Seeing {3D} chairs: Exemplar partbased {2D}{3D} alignment using a large dataset of CAD models. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 2328, 2014, pp.\\ 37623769, Los Alamitos, CA, USA, 2014. IEEE Computer Society. \\doi{10.1109/CVPR.2014.487}. URL https://doi.org/10.1109/CVPR.2014.487.",
            "matches": []
        },
        {
            "leaf id": 197,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Aayush Bansal, Bryan~C. Russell, and Abhinav Gupta. Marr revisited: {2D}{3D} alignment via surface normal prediction. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 2730, 2016, pp.\\ 59655974, Los Alamitos, CA, USA, 2016. IEEE Computer Society. \\doi{10.1109/CVPR.2016.642}. URL https://doi.org/10.1109/CVPR.2016.642.",
            "leftover": "Aayush Bansal, Bryan~C. Russell, and Abhinav Gupta. Marr revisited: {2D}{3D} alignment via surface normal prediction. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 2730, 2016, pp.\\ 59655974, Los Alamitos, CA, USA, 2016. IEEE Computer Society. \\doi{10.1109/CVPR.2016.642}. URL https://doi.org/10.1109/CVPR.2016.642.",
            "matches": []
        },
        {
            "leaf id": 198,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Bruce~Guenther Baumgart. Geometric modeling for computer vision. Technical report, Stanford University CA Department of Computer Science, 1974.",
            "leftover": "Bruce~Guenther Baumgart. Geometric modeling for computer vision. Technical report, Stanford University CA Department of Computer Science, 1974.",
            "matches": []
        },
        {
            "leaf id": 199,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Yoshua Bengio, R\\'ejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T.~Leen, T.~Dietterich, and V.~Tresp (eds.), Advances in Neural Information Processing Systems, volume~13. MIT Press, 2000. URL https://proceedings.neurips.cc/paperfiles/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4cPaper.pdf.",
            "leftover": "Yoshua Bengio, R\\'ejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T.~Leen, T.~Dietterich, and V.~Tresp (eds.), Advances in Neural Information Processing Systems, volume~13. MIT Press, 2000. URL https://proceedings.neurips.cc/paperfiles/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4cPaper.pdf.",
            "matches": []
        },
        {
            "leaf id": 200,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Thomas Binford. Visual perception by computer. In Proceedings of the IEEE Conference on Systems and Control, 1975, Los Alamitos, CA, USA, 1975. IEEE Computer Society.",
            "leftover": "Thomas Binford. Visual perception by computer. In Proceedings of the IEEE Conference on Systems and Control, 1975, Los Alamitos, CA, USA, 1975. IEEE Computer Society.",
            "matches": []
        },
        {
            "leaf id": 201,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Blender. Blender  A {3D} modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org.",
            "leftover": "Blender. Blender  A {3D} modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org.",
            "matches": []
        },
        {
            "leaf id": 202,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Martin Bokeloh, Michael Wand, and HansPeter Seidel. A connection between partial symmetry and inverse procedural modeling. In ACM SIGGRAPH 2010 Papers, SIGGRAPH '10, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450302104. \\doi{10.1145/1833349.1778841}. URL https://doi.org/10.1145/1833349.1778841.",
            "leftover": "Martin Bokeloh, Michael Wand, and HansPeter Seidel. A connection between partial symmetry and inverse procedural modeling. In ACM SIGGRAPH 2010 Papers, SIGGRAPH '10, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450302104. \\doi{10.1145/1833349.1778841}. URL https://doi.org/10.1145/1833349.1778841.",
            "matches": []
        },
        {
            "leaf id": 203,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf.",
            "leftover": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf.",
            "matches": []
        },
        {
            "leaf id": 204,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Angel~X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li~Yi, and Fisher Yu. ShapeNet: An informationrich {3D} model repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University  Princeton University  Toyota Technological Institute at Chicago, 2015.",
            "leftover": "Angel~X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li~Yi, and Fisher Yu. ShapeNet: An informationrich {3D} model repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University  Princeton University  Toyota Technological Institute at Chicago, 2015.",
            "matches": []
        },
        {
            "leaf id": 205,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "WeiLin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing. Vicuna: An opensource chatbot impressing {GPT4} with 90%* ChatGPT quality, March 2023. URL https://lmsys.org/blog/20230330vicuna/.",
            "leftover": "WeiLin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing. Vicuna: An opensource chatbot impressing {GPT4} with 90%* ChatGPT quality, March 2023. URL https://lmsys.org/blog/20230330vicuna/.",
            "matches": []
        },
        {
            "leaf id": 206,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "Christopher~B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. {3DR2N2}: A unified approach for single and multiview {3D} object reconstruction. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016, pp.\\ 628644, Cham, 2016. Springer International Publishing. ISBN 9783319464848.",
            "leftover": "Christopher~B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. {3DR2N2}: A unified approach for single and multiview {3D} object reconstruction. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision  ECCV 2016, pp.\\ 628644, Cham, 2016. Springer International Publishing. ISBN 9783319464848.",
            "matches": []
        },
        {
            "leaf id": 207,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex CastroRos, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25\\penalty0 (70):\\penalty0 153, 2024. URL http://jmlr.org/papers/v25/230870.html.",
            "leftover": "Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex CastroRos, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25\\penalty0 (70):\\penalty0 153, 2024. URL http://jmlr.org/papers/v25/230870.html.",
            "matches": []
        },
        {
            "leaf id": 208,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Manuel Dahnert, Ji~Hou, Matthias Niessner, and Angela Dai. Panoptic {3D} scene reconstruction from a single RGB image. In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume~34, pp.\\ 82828293, Red Hook, NY, USA, 2021. Curran Associates, Inc. URL https://proceedings.neurips.cc/paperfiles/paper/2021/file/46031b3d04dc90994ca317a7c55c4289Paper.pdf.",
            "leftover": "Manuel Dahnert, Ji~Hou, Matthias Niessner, and Angela Dai. Panoptic {3D} scene reconstruction from a single RGB image. In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume~34, pp.\\ 82828293, Red Hook, NY, USA, 2021. Curran Associates, Inc. URL https://proceedings.neurips.cc/paperfiles/paper/2021/file/46031b3d04dc90994ca317a7c55c4289Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 209,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Saumitro Dasgupta, Kuan Fang, Kevin Chen, and Silvio Savarese. DeLay: Robust spatial layout estimation for cluttered indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, June 2016. IEEE Computer Society.",
            "leftover": "Saumitro Dasgupta, Kuan Fang, Kevin Chen, and Silvio Savarese. DeLay: Robust spatial layout estimation for cluttered indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, June 2016. IEEE Computer Society.",
            "matches": []
        },
        {
            "leaf id": 210,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. CvxNet: Learnable convex decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
            "leftover": "Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. CvxNet: Learnable convex decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
            "matches": []
        },
        {
            "leaf id": 211,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Boyang Deng, Sumith Kulal, Zhengyang Dong, Congyue Deng, Yonglong Tian, and Jiajun Wu. Unsupervised learning of shape programs with repeatable implicit parts. In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh (eds.), Advances in Neural Information Processing Systems, volume~35, pp.\\ 3783737850. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/f6adf61977467560f79b95485d1f3a79PaperConference.pdf.",
            "leftover": "Boyang Deng, Sumith Kulal, Zhengyang Dong, Congyue Deng, Yonglong Tian, and Jiajun Wu. Unsupervised learning of shape programs with repeatable implicit parts. In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh (eds.), Advances in Neural Information Processing Systems, volume~35, pp.\\ 3783737850. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paperfiles/paper/2022/file/f6adf61977467560f79b95485d1f3a79PaperConference.pdf.",
            "matches": []
        },
        {
            "leaf id": 212,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Maximilian Denninger and Rudolph Triebel. {3D} scene reconstruction from a single viewport. In European Conference on Computer Vision, pp.\\ 5167. Springer, 2020.",
            "leftover": "Maximilian Denninger and Rudolph Triebel. {3D} scene reconstruction from a single viewport. In European Conference on Computer Vision, pp.\\ 5167. Springer, 2020.",
            "matches": []
        },
        {
            "leaf id": 213,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. MetaSim2: Unsupervised learning of scene structure for synthetic data generation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVII 16, pp.\\ 715733. Springer, 2020.",
            "leftover": "Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. MetaSim2: Unsupervised learning of scene structure for synthetic data generation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVII 16, pp.\\ 715733. Springer, 2020.",
            "matches": []
        },
        {
            "leaf id": 214,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.",
            "leftover": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.",
            "matches": []
        },
        {
            "leaf id": 215,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Tao Du, Jeevana~Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus, Armando SolarLezama, and Wojciech Matusik. InverseCSG: Automatic conversion of {3D} models to CSG trees. ACM Trans. Graph., 37\\penalty0 (6), dec 2018. ISSN 07300301. \\doi{10.1145/3272127.3275006}. URL https://doi.org/10.1145/3272127.3275006.",
            "leftover": "Tao Du, Jeevana~Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela Rus, Armando SolarLezama, and Wojciech Matusik. InverseCSG: Automatic conversion of {3D} models to CSG trees. ACM Trans. Graph., 37\\penalty0 (6), dec 2018. ISSN 07300301. \\doi{10.1145/3272127.3275006}. URL https://doi.org/10.1145/3272127.3275006.",
            "matches": []
        },
        {
            "leaf id": 216,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Mohammed~Munzer Dwedari, Matthias Niessner, and Dave~Zhenyu Chen. Generating contextaware natural answers for questions in {3D} scenes. arXiv preprint arXiv:2310.19516, 2023.",
            "leftover": "Mohammed~Munzer Dwedari, Matthias Niessner, and Dave~Zhenyu Chen. Generating contextaware natural answers for questions in {3D} scenes. arXiv preprint arXiv:2310.19516, 2023.",
            "matches": []
        },
        {
            "leaf id": 217,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Kevin Ellis, Daniel Ritchie, Armando SolarLezama, and Josh Tenenbaum. Learning to infer graphics programs from handdrawn images. In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~CesaBianchi, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paperfiles/paper/2018/file/6788076842014c83cedadbe6b0ba0314Paper.pdf.",
            "leftover": "Kevin Ellis, Daniel Ritchie, Armando SolarLezama, and Josh Tenenbaum. Learning to infer graphics programs from handdrawn images. In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~CesaBianchi, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paperfiles/paper/2018/file/6788076842014c83cedadbe6b0ba0314Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 218,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando SolarLezama. Write, execute, assess: Program synthesis with a REPL. In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\\textquotesingle Alch\\'eBuc, E.~Fox, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paperfiles/paper/2019/file/50d2d2262762648589b1943078712aa6Paper.pdf.",
            "leftover": "Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando SolarLezama. Write, execute, assess: Program synthesis with a REPL. In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\\textquotesingle Alch\\'eBuc, E.~Fox, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paperfiles/paper/2019/file/50d2d2262762648589b1943078712aa6Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 219,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Francis Engelmann, Konstantinos Rematas, Bastian Leibe, and Vittorio Ferrari. From points to multiobject {3D} reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 45884597, June 2021.",
            "leftover": "Francis Engelmann, Konstantinos Rematas, Bastian Leibe, and Vittorio Ferrari. From points to multiobject {3D} reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 45884597, June 2021.",
            "matches": []
        },
        {
            "leaf id": 220,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Haoqiang Fan, Hao Su, and Leonidas~J. Guibas. A point set generation network for {3D} object reconstruction from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "leftover": "Haoqiang Fan, Hao Su, and Leonidas~J. Guibas. A point set generation network for {3D} object reconstruction from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "matches": []
        },
        {
            "leaf id": 221,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Aditya Ganeshan, R.~Kenny Jones, and Daniel Ritchie. Improving unsupervised visual program inference with code rewriting families. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.\\ 1579115801, October 2023.",
            "leftover": "Aditya Ganeshan, R.~Kenny Jones, and Daniel Ritchie. Improving unsupervised visual program inference with code rewriting families. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.\\ 1579115801, October 2023.",
            "matches": []
        },
        {
            "leaf id": 222,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M.~Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. In Proceedings of the 35th International Conference on Machine Learning, volume~80 of Proceedings of Machine Learning Research, pp.\\ 16661675. PMLR, 1015 Jul 2018.",
            "leftover": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M.~Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. In Proceedings of the 35th International Conference on Machine Learning, volume~80 of Proceedings of Machine Learning Research, pp.\\ 16661675. PMLR, 1015 Jul 2018.",
            "matches": []
        },
        {
            "leaf id": 223,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh {RCNN}. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.",
            "leftover": "Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh {RCNN}. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.",
            "matches": []
        },
        {
            "leaf id": 224,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Georgia Gkioxari, Nikhila Ravi, and Justin Johnson. Learning {3D} object shape and layout without {3D} supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 16951704, June 2022.",
            "leftover": "Georgia Gkioxari, Nikhila Ravi, and Justin Johnson. Learning {3D} object shape and layout without {3D} supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 16951704, June 2022.",
            "matches": []
        },
        {
            "leaf id": 225,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Nishad Gothoskar, Marco CusumanoTowner, Ben Zinberg, Matin Ghavamizadeh, Falk Pollok, Austin Garrett, Josh Tenenbaum, Dan Gutfreund, and Vikash Mansinghka. {3DP3}: {3D} scene perception via probabilistic programming. Advances in Neural Information Processing Systems, 34:\\penalty0 96009612, 2021.",
            "leftover": "Nishad Gothoskar, Marco CusumanoTowner, Ben Zinberg, Matin Ghavamizadeh, Falk Pollok, Austin Garrett, Josh Tenenbaum, Dan Gutfreund, and Vikash Mansinghka. {3DP3}: {3D} scene perception via probabilistic programming. Advances in Neural Information Processing Systems, 34:\\penalty0 96009612, 2021.",
            "matches": []
        },
        {
            "leaf id": 226,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Ulf Grenander. Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern Synthesis and Regular Structures. SpringerVerlag, HeidelbergNew York, 19761981.",
            "leftover": "Ulf Grenander. Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern Synthesis and Regular Structures. SpringerVerlag, HeidelbergNew York, 19761981.",
            "matches": []
        },
        {
            "leaf id": 227,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Thibault Groueix, Matthew Fisher, Vladimir~G. Kim, Bryan~C. Russell, and Mathieu Aubry. A papierm{\\^a}ch{\\'e} approach to learning {3D} surface generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 216224, 2018.",
            "leftover": "Thibault Groueix, Matthew Fisher, Vladimir~G. Kim, Bryan~C. Russell, and Mathieu Aubry. A papierm{\\^a}ch{\\'e} approach to learning {3D} surface generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 216224, 2018.",
            "matches": []
        },
        {
            "leaf id": 228,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and Trends{\\textregistered} in Programming Languages, 4\\penalty0 (12):\\penalty0 1119, 2017. ISSN 23251107. \\doi{10.1561/2500000010}. URL http://dx.doi.org/10.1561/2500000010.",
            "leftover": "Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and Trends{\\textregistered} in Programming Languages, 4\\penalty0 (12):\\penalty0 1119, 2017. ISSN 23251107. \\doi{10.1561/2500000010}. URL http://dx.doi.org/10.1561/2500000010.",
            "matches": []
        },
        {
            "leaf id": 229,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Can G''umeli, Angela Dai, and Matthias Nie{\\ss}ner. ROCA: Robust CAD model retrieval and alignment from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 40224031, June 2022.",
            "leftover": "Can G''umeli, Angela Dai, and Matthias Nie{\\ss}ner. ROCA: Robust CAD model retrieval and alignment from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 40224031, June 2022.",
            "matches": []
        },
        {
            "leaf id": 230,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Jianwei Guo, Haiyong Jiang, Bedrich Benes, Oliver Deussen, Xiaopeng Zhang, Dani Lischinski, and Hui Huang. Inverse procedural modeling of branching structures by inferring Lsystems. ACM Trans. Graph., 39\\penalty0 (5), jun 2020. ISSN 07300301. URL https://doi.org/10.1145/3394105.",
            "leftover": "Jianwei Guo, Haiyong Jiang, Bedrich Benes, Oliver Deussen, Xiaopeng Zhang, Dani Lischinski, and Hui Huang. Inverse procedural modeling of branching structures by inferring Lsystems. ACM Trans. Graph., 39\\penalty0 (5), jun 2020. ISSN 07300301. URL https://doi.org/10.1145/3394105.",
            "matches": []
        },
        {
            "leaf id": 231,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Abhinav Gupta, Alexei~A. Efros, and Martial Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In Computer VisionECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 511, 2010, Proceedings, Part IV 11, pp.\\ 482496. Springer, 2010{\\natexlaba}.",
            "leftover": "Abhinav Gupta, Alexei~A. Efros, and Martial Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In Computer VisionECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 511, 2010, Proceedings, Part IV 11, pp.\\ 482496. Springer, 2010{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 232,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Abhinav Gupta, Martial Hebert, Takeo Kanade, and David Blei. Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. Advances in Neural Information Processing Systems, 23, 2010{\\natexlabb}.",
            "leftover": "Abhinav Gupta, Martial Hebert, Takeo Kanade, and David Blei. Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. Advances in Neural Information Processing Systems, 23, 2010{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 233,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering the spatial layout of cluttered rooms. In 2009 IEEE 12th International Conference on Computer Vision, pp.\\ 18491856, 2009. \\doi{10.1109/ICCV.2009.5459411}.",
            "leftover": "Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering the spatial layout of cluttered rooms. In 2009 IEEE 12th International Conference on Computer Vision, pp.\\ 18491856, 2009. \\doi{10.1109/ICCV.2009.5459411}.",
            "matches": []
        },
        {
            "leaf id": 234,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. {3DLLM}: Injecting the {3D} world into large language models. arXiv preprint arXiv:2307.12981, 2023.",
            "leftover": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. {3DLLM}: Injecting the {3D} world into large language models. arXiv preprint arXiv:2307.12981, 2023.",
            "matches": []
        },
        {
            "leaf id": 235,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Edward~J. Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. LoRA: Lowrank adaptation of large language models. In International Conference on Learning Representations, 2022{\\natexlaba}. URL https://openreview.net/forum?id=nZeVKeeFYf9.",
            "leftover": "Edward~J. Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. LoRA: Lowrank adaptation of large language models. In International Conference on Learning Representations, 2022{\\natexlaba}. URL https://openreview.net/forum?id=nZeVKeeFYf9.",
            "matches": []
        },
        {
            "leaf id": 236,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Yiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey, and Holly Rushmeier. An inverse procedural modeling pipeline for SVBRDF maps. ACM Trans. Graph., 41\\penalty0 (2), 2022{\\natexlabb}.",
            "leftover": "Yiwei Hu, Chengan He, Valentin Deschaintre, Julie Dorsey, and Holly Rushmeier. An inverse procedural modeling pipeline for SVBRDF maps. ACM Trans. Graph., 41\\penalty0 (2), 2022{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 237,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and SongChun Zhu. Holistic {3D} scene parsing and reconstruction from a single RGB image. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.",
            "leftover": "Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and SongChun Zhu. Holistic {3D} scene parsing and reconstruction from a single RGB image. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.",
            "matches": []
        },
        {
            "leaf id": 238,
            "key": "doc/bib44",
            "block type": "bibliography",
            "content": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp.\\ 91189147. PMLR, 2022.",
            "leftover": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp.\\ 91189147. PMLR, 2022.",
            "matches": []
        },
        {
            "leaf id": 239,
            "key": "doc/bib45",
            "block type": "bibliography",
            "content": "Hamid Izadinia, Qi~Shan, and Steven~M. Seitz. {IM2CAD}. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "leftover": "Hamid Izadinia, Qi~Shan, and Steven~M. Seitz. {IM2CAD}. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "matches": []
        },
        {
            "leaf id": 240,
            "key": "doc/bib46",
            "block type": "bibliography",
            "content": "Justin Johnson, Bharath Hariharan, Laurens van~der Maaten, Li~FeiFei, C.~Lawrence~Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "leftover": "Justin Johnson, Bharath Hariharan, Laurens van~der Maaten, Li~FeiFei, C.~Lawrence~Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
            "matches": []
        },
        {
            "leaf id": 241,
            "key": "doc/bib47",
            "block type": "bibliography",
            "content": "R.~Kenny Jones, Homer Walke, and Daniel Ritchie. PLAD: Learning to infer shape programs with pseudolabels and approximate distributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 98719880, June 2022.",
            "leftover": "R.~Kenny Jones, Homer Walke, and Daniel Ritchie. PLAD: Learning to infer shape programs with pseudolabels and approximate distributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 98719880, June 2022.",
            "matches": []
        },
        {
            "leaf id": 242,
            "key": "doc/bib48",
            "block type": "bibliography",
            "content": "R.~Kenny Jones, Paul Guerrero, Niloy~J. Mitra, and Daniel Ritchie. ShapeCoder: Discovering abstractions for visual programs from unstructured primitives. ACM Trans. Graph., 42\\penalty0 (4), 2023.",
            "leftover": "R.~Kenny Jones, Paul Guerrero, Niloy~J. Mitra, and Daniel Ritchie. ShapeCoder: Discovering abstractions for visual programs from unstructured primitives. ACM Trans. Graph., 42\\penalty0 (4), 2023.",
            "matches": []
        },
        {
            "leaf id": 243,
            "key": "doc/bib49",
            "block type": "bibliography",
            "content": "Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz. {UCSGNET}Unsupervised discovering of constructive solid geometry tree. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 87768786. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/63d5fb54a858dd033fe90e6e4a74b0f0Paper.pdf.",
            "leftover": "Kacper Kania, Maciej Zieba, and Tomasz Kajdanowicz. {UCSGNET}Unsupervised discovering of constructive solid geometry tree. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 87768786. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/63d5fb54a858dd033fe90e6e4a74b0f0Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 244,
            "key": "doc/bib50",
            "block type": "bibliography",
            "content": "Amlan Kar, Aayush Prakash, MingYu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, and Sanja Fidler. MetaSim: Learning to generate synthetic datasets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 45514560, 2019.",
            "leftover": "Amlan Kar, Aayush Prakash, MingYu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, and Sanja Fidler. MetaSim: Learning to generate synthetic datasets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 45514560, 2019.",
            "matches": []
        },
        {
            "leaf id": 245,
            "key": "doc/bib51",
            "block type": "bibliography",
            "content": "D.~Knill~D. Kersten and A.~Yuille. Introduction: A Bayesian formulation of visual perception. Perception as Bayesian inference, pp.\\ 121, 1996.",
            "leftover": "D.~Knill~D. Kersten and A.~Yuille. Introduction: A Bayesian formulation of visual perception. Perception as Bayesian inference, pp.\\ 121, 1996.",
            "matches": []
        },
        {
            "leaf id": 246,
            "key": "doc/bib52",
            "block type": "bibliography",
            "content": "Florian Kluger, Hanno Ackermann, Eric Brachmann, Michael~Ying Yang, and Bodo Rosenhahn. Cuboids revisited: Learning robust {3D} shape fitting to single RGB images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1307013079, June 2021.",
            "leftover": "Florian Kluger, Hanno Ackermann, Eric Brachmann, Michael~Ying Yang, and Bodo Rosenhahn. Cuboids revisited: Learning robust {3D} shape fitting to single RGB images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1307013079, June 2021.",
            "matches": []
        },
        {
            "leaf id": 247,
            "key": "doc/bib53",
            "block type": "bibliography",
            "content": "Milin Kodnongbua, Benjamin~T. Jones, Maaz Bin~Safeer Ahmad, Vladimir~G. Kim, and Adriana Schulz. ReparamCAD: Zeroshot CAD reparameterization for interactive manipulation. SIGGRAPH Asia (Conference track), 2023.",
            "leftover": "Milin Kodnongbua, Benjamin~T. Jones, Maaz Bin~Safeer Ahmad, Vladimir~G. Kim, and Adriana Schulz. ReparamCAD: Zeroshot CAD reparameterization for interactive manipulation. SIGGRAPH Asia (Conference track), 2023.",
            "matches": []
        },
        {
            "leaf id": 248,
            "key": "doc/bib54",
            "block type": "bibliography",
            "content": "Jing~Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. NeurIPS, 2023.",
            "leftover": "Jing~Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. NeurIPS, 2023.",
            "matches": []
        },
        {
            "leaf id": 249,
            "key": "doc/bib55",
            "block type": "bibliography",
            "content": "Tejas~D. Kulkarni, Pushmeet Kohli, Joshua~B. Tenenbaum, and Vikash Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.",
            "leftover": "Tejas~D. Kulkarni, Pushmeet Kohli, Joshua~B. Tenenbaum, and Vikash Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.",
            "matches": []
        },
        {
            "leaf id": 250,
            "key": "doc/bib56",
            "block type": "bibliography",
            "content": "Abhijit Kundu, Yin Li, and James~M. Rehg. {3DRCNN}: Instancelevel {3D} object reconstruction via renderandcompare. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "leftover": "Abhijit Kundu, Yin Li, and James~M. Rehg. {3DRCNN}: Instancelevel {3D} object reconstruction via renderandcompare. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "matches": []
        },
        {
            "leaf id": 251,
            "key": "doc/bib57",
            "block type": "bibliography",
            "content": "Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas~J. Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural fields: A semantic objectaware neural scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 1287112881, 2022.",
            "leftover": "Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas~J. Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural fields: A semantic objectaware neural scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 1287112881, 2022.",
            "matches": []
        },
        {
            "leaf id": 252,
            "key": "doc/bib58",
            "block type": "bibliography",
            "content": "Weicheng Kuo, Anelia Angelova, TsungYi Lin, and Angela Dai. {Mask2CAD}: {3D} shape prediction by learning to segment and retrieve. In Computer Vision  ECCV 2020. Springer International Publishing, 2020.",
            "leftover": "Weicheng Kuo, Anelia Angelova, TsungYi Lin, and Angela Dai. {Mask2CAD}: {3D} shape prediction by learning to segment and retrieve. In Computer Vision  ECCV 2020. Springer International Publishing, 2020.",
            "matches": []
        },
        {
            "leaf id": 253,
            "key": "doc/bib59",
            "block type": "bibliography",
            "content": "Weicheng Kuo, Anelia Angelova, TsungYi Lin, and Angela Dai. {Patch2CAD}: Patchwise embedding learning for inthewild shape retrieval from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.\\ 1258912599, October 2021.",
            "leftover": "Weicheng Kuo, Anelia Angelova, TsungYi Lin, and Angela Dai. {Patch2CAD}: Patchwise embedding learning for inthewild shape retrieval from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.\\ 1258912599, October 2021.",
            "matches": []
        },
        {
            "leaf id": 254,
            "key": "doc/bib60",
            "block type": "bibliography",
            "content": "Yann Labb{\\'e}, Justin Carpentier, Mathieu Aubry, and Josef Sivic. CosyPose: Consistent multiview multiobject {6D} pose estimation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVII 16, pp.\\ 574591. Springer, 2020.",
            "leftover": "Yann Labb{\\'e}, Justin Carpentier, Mathieu Aubry, and Josef Sivic. CosyPose: Consistent multiview multiobject {6D} pose estimation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVII 16, pp.\\ 574591. Springer, 2020.",
            "matches": []
        },
        {
            "leaf id": 255,
            "key": "doc/bib61",
            "block type": "bibliography",
            "content": "David~C. Lee, Martial Hebert, and Takeo Kanade. Geometric reasoning for single image structure recovery. In 2009 IEEE conference on computer vision and pattern recognition, pp.\\ 21362143. IEEE, 2009.",
            "leftover": "David~C. Lee, Martial Hebert, and Takeo Kanade. Geometric reasoning for single image structure recovery. In 2009 IEEE conference on computer vision and pattern recognition, pp.\\ 21362143. IEEE, 2009.",
            "matches": []
        },
        {
            "leaf id": 256,
            "key": "doc/bib62",
            "block type": "bibliography",
            "content": "Vincent Lepetit, Francesc MorenoNoguer, and Pascal Fua. EPnP: An accurate {O(n)} solution to the PnP problem. International Journal of Computer Vision, 81\\penalty0 (2):\\penalty0 155166, Feb 2009. ISSN 15731405. \\doi{10.1007/s1126300801526}. URL https://doi.org/10.1007/s1126300801526.",
            "leftover": "Vincent Lepetit, Francesc MorenoNoguer, and Pascal Fua. EPnP: An accurate {O(n)} solution to the PnP problem. International Journal of Computer Vision, 81\\penalty0 (2):\\penalty0 155166, Feb 2009. ISSN 15731405. \\doi{10.1007/s1126300801526}. URL https://doi.org/10.1007/s1126300801526.",
            "matches": []
        },
        {
            "leaf id": 257,
            "key": "doc/bib63",
            "block type": "bibliography",
            "content": "Changjian Li, Hao Pan, Adrien Bousseau, and Niloy~J. Mitra. {Sketch2CAD}: Sequential CAD modeling by sketching in context. ACM Trans. Graph., 39\\penalty0 (6), nov 2020{\\natexlaba}. ISSN 07300301. \\doi{10.1145/3414685.3417807}. URL https://doi.org/10.1145/3414685.3417807.",
            "leftover": "Changjian Li, Hao Pan, Adrien Bousseau, and Niloy~J. Mitra. {Sketch2CAD}: Sequential CAD modeling by sketching in context. ACM Trans. Graph., 39\\penalty0 (6), nov 2020{\\natexlaba}. ISSN 07300301. \\doi{10.1145/3414685.3417807}. URL https://doi.org/10.1145/3414685.3417807.",
            "matches": []
        },
        {
            "leaf id": 258,
            "key": "doc/bib64",
            "block type": "bibliography",
            "content": "Changjian Li, Hao Pan, Adrien Bousseau, and Niloy~J. Mitra. {Free2CAD}: Parsing freehand drawings into CAD commands. ACM Trans. Graph., 41\\penalty0 (4), jul 2022{\\natexlaba}.",
            "leftover": "Changjian Li, Hao Pan, Adrien Bousseau, and Niloy~J. Mitra. {Free2CAD}: Parsing freehand drawings into CAD commands. ACM Trans. Graph., 41\\penalty0 (4), jul 2022{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 259,
            "key": "doc/bib65",
            "block type": "bibliography",
            "content": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping languageimage pretraining for unified visionlanguage understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.\\ 1288812900. PMLR, 1723 Jul 2022{\\natexlabb}. URL https://proceedings.mlr.press/v162/li22n.html.",
            "leftover": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping languageimage pretraining for unified visionlanguage understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.\\ 1288812900. PMLR, 1723 Jul 2022{\\natexlabb}. URL https://proceedings.mlr.press/v162/li22n.html.",
            "matches": []
        },
        {
            "leaf id": 260,
            "key": "doc/bib66",
            "block type": "bibliography",
            "content": "Yikai Li, Jiayuan Mao, Xiuming Zhang, Bill Freeman, Josh Tenenbaum, Noah Snavely, and Jiajun Wu. Multiplane program induction with {3D} box priors. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 74257436. Curran Associates, Inc., 2020{\\natexlabb}. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/5301c4d888f5204274439e6dcf5fdb54Paper.pdf.",
            "leftover": "Yikai Li, Jiayuan Mao, Xiuming Zhang, Bill Freeman, Josh Tenenbaum, Noah Snavely, and Jiajun Wu. Multiplane program induction with {3D} box priors. In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), Advances in Neural Information Processing Systems, volume~33, pp.\\ 74257436. Curran Associates, Inc., 2020{\\natexlabb}. URL https://proceedings.neurips.cc/paperfiles/paper/2020/file/5301c4d888f5204274439e6dcf5fdb54Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 261,
            "key": "doc/bib67",
            "block type": "bibliography",
            "content": "Zhuowan Li, Xingrui Wang, Elias StengelEskin, Adam Kortylewski, Wufei Ma, Benjamin Van~Durme, and Alan~L. Yuille. {SuperCLEVR}: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1496314973, June 2023.",
            "leftover": "Zhuowan Li, Xingrui Wang, Elias StengelEskin, Adam Kortylewski, Wufei Ma, Benjamin Van~Durme, and Alan~L. Yuille. {SuperCLEVR}: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1496314973, June 2023.",
            "matches": []
        },
        {
            "leaf id": 262,
            "key": "doc/bib68",
            "block type": "bibliography",
            "content": "Joseph~J. Lim, Aditya Khosla, and Antonio Torralba. FPM: Fine pose partsbased model with {3D} CAD models. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 612, 2014, Proceedings, Part VI 13, pp.\\ 478493. Springer, 2014.",
            "leftover": "Joseph~J. Lim, Aditya Khosla, and Antonio Torralba. FPM: Fine pose partsbased model with {3D} CAD models. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 612, 2014, Proceedings, Part VI 13, pp.\\ 478493. Springer, 2014.",
            "matches": []
        },
        {
            "leaf id": 263,
            "key": "doc/bib69",
            "block type": "bibliography",
            "content": "Haolin Liu, Yujian Zheng, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Towards highfidelity singleview holistic reconstruction of indoor scenes. In Computer Vision  ECCV 2022, pp.\\ 429446. Springer Nature Switzerland, 2022.",
            "leftover": "Haolin Liu, Yujian Zheng, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Towards highfidelity singleview holistic reconstruction of indoor scenes. In Computer Vision  ECCV 2022, pp.\\ 429446. Springer Nature Switzerland, 2022.",
            "matches": []
        },
        {
            "leaf id": 264,
            "key": "doc/bib70",
            "block type": "bibliography",
            "content": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. Visual instruction tuning. In A.~Oh, T.~Neumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine (eds.), Advances in Neural Information Processing Systems, volume~36, pp.\\ 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paperfiles/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0PaperConference.pdf.",
            "leftover": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. Visual instruction tuning. In A.~Oh, T.~Neumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine (eds.), Advances in Neural Information Processing Systems, volume~36, pp.\\ 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paperfiles/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0PaperConference.pdf.",
            "matches": []
        },
        {
            "leaf id": 265,
            "key": "doc/bib71",
            "block type": "bibliography",
            "content": "Yunchao Liu, Jiajun Wu, Zheng Wu, Daniel Ritchie, William~T. Freeman, and Joshua~B. Tenenbaum. Learning to describe scenes with programs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SyNPk2R9K7.",
            "leftover": "Yunchao Liu, Jiajun Wu, Zheng Wu, Daniel Ritchie, William~T. Freeman, and Joshua~B. Tenenbaum. Learning to describe scenes with programs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SyNPk2R9K7.",
            "matches": []
        },
        {
            "leaf id": 266,
            "key": "doc/bib72",
            "block type": "bibliography",
            "content": "Wufei Ma, Angtian Wang, Alan Yuille, and Adam Kortylewski. Robust categorylevel {6D} pose estimation with coarsetofine rendering of neural features. In European Conference on Computer Vision, pp.\\ 492508. Springer, 2022.",
            "leftover": "Wufei Ma, Angtian Wang, Alan Yuille, and Adam Kortylewski. Robust categorylevel {6D} pose estimation with coarsetofine rendering of neural features. In European Conference on Computer Vision, pp.\\ 492508. Springer, 2022.",
            "matches": []
        },
        {
            "leaf id": 267,
            "key": "doc/bib73",
            "block type": "bibliography",
            "content": "Arun Mallya and Svetlana Lazebnik. Learning informative edge maps for indoor scene layout prediction. In Proceedings of the IEEE international conference on computer vision, pp.\\ 936944, 2015.",
            "leftover": "Arun Mallya and Svetlana Lazebnik. Learning informative edge maps for indoor scene layout prediction. In Proceedings of the IEEE international conference on computer vision, pp.\\ 936944, 2015.",
            "matches": []
        },
        {
            "leaf id": 268,
            "key": "doc/bib74",
            "block type": "bibliography",
            "content": "Vikash~K. Mansinghka, Tejas~D. Kulkarni, Yura~N. Perov, and Josh Tenenbaum. Approximate Bayesian image interpretation using generative probabilistic graphics programs. In C.J. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume~26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paperfiles/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169Paper.pdf.",
            "leftover": "Vikash~K. Mansinghka, Tejas~D. Kulkarni, Yura~N. Perov, and Josh Tenenbaum. Approximate Bayesian image interpretation using generative probabilistic graphics programs. In C.J. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume~26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paperfiles/paper/2013/file/fa14d4fe2f19414de3ebd9f63d5c0169Paper.pdf.",
            "matches": []
        },
        {
            "leaf id": 269,
            "key": "doc/bib75",
            "block type": "bibliography",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua~B. Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJgMlhRctm.",
            "leftover": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua~B. Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJgMlhRctm.",
            "matches": []
        },
        {
            "leaf id": 270,
            "key": "doc/bib76",
            "block type": "bibliography",
            "content": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning {3D} reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\\ 44604470, 2019.",
            "leftover": "Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning {3D} reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\\ 44604470, 2019.",
            "matches": []
        },
        {
            "leaf id": 271,
            "key": "doc/bib77",
            "block type": "bibliography",
            "content": "Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei~A. Efros, and Mathieu Aubry. {Differentiable Blocks World: Qualitative {3D} Decomposition by Rendering Primitives}. In NeurIPS, 2023.",
            "leftover": "Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei~A. Efros, and Mathieu Aubry. {Differentiable Blocks World: Qualitative {3D} Decomposition by Rendering Primitives}. In NeurIPS, 2023.",
            "matches": []
        },
        {
            "leaf id": 272,
            "key": "doc/bib78",
            "block type": "bibliography",
            "content": "Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian~Jun Zhang. {Total3DUnderstanding}: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
            "leftover": "Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian~Jun Zhang. {Total3DUnderstanding}: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.",
            "matches": []
        },
        {
            "leaf id": 273,
            "key": "doc/bib79",
            "block type": "bibliography",
            "content": "Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias Nie{\\ss}ner. Learning {3D} scene priors with {2D} supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 792802, June 2023.",
            "leftover": "Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias Nie{\\ss}ner. Learning {3D} scene priors with {2D} supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 792802, June 2023.",
            "matches": []
        },
        {
            "leaf id": 274,
            "key": "doc/bib80",
            "block type": "bibliography",
            "content": "OpenAI. {GPT4} technical report. ArXiv preprint arXiv:2303.08774, 2023.",
            "leftover": "OpenAI. {GPT4} technical report. ArXiv preprint arXiv:2303.08774, 2023.",
            "matches": []
        },
        {
            "leaf id": 275,
            "key": "doc/bib81",
            "block type": "bibliography",
            "content": "Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:\\penalty0 2773027744, 2022.",
            "leftover": "Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:\\penalty0 2773027744, 2022.",
            "matches": []
        },
        {
            "leaf id": 276,
            "key": "doc/bib82",
            "block type": "bibliography",
            "content": "Jeong~Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "leftover": "Jeong~Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "matches": []
        },
        {
            "leaf id": 277,
            "key": "doc/bib83",
            "block type": "bibliography",
            "content": "Despoina Paschalidou, Ali~Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning {3D} shape parsing beyond cuboids. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "leftover": "Despoina Paschalidou, Ali~Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning {3D} shape parsing beyond cuboids. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "matches": []
        },
        {
            "leaf id": 278,
            "key": "doc/bib84",
            "block type": "bibliography",
            "content": "Despoina Paschalidou, Luc~Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of {3D} objects from a single RGB image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 10601070, 2020.",
            "leftover": "Despoina Paschalidou, Luc~Van Gool, and Andreas Geiger. Learning unsupervised hierarchical part decomposition of {3D} objects from a single RGB image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 10601070, 2020.",
            "matches": []
        },
        {
            "leaf id": 279,
            "key": "doc/bib85",
            "block type": "bibliography",
            "content": "Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning expressive {3D} shape abstractions with invertible neural networks. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2021.",
            "leftover": "Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning expressive {3D} shape abstractions with invertible neural networks. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2021.",
            "matches": []
        },
        {
            "leaf id": 280,
            "key": "doc/bib86",
            "block type": "bibliography",
            "content": "Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos~G. Derpanis, and Kostas Daniilidis. {6DOF} object pose from semantic keypoints. In 2017 IEEE international conference on robotics and automation (ICRA), pp.\\ 20112018. IEEE, 2017.",
            "leftover": "Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos~G. Derpanis, and Kostas Daniilidis. {6DOF} object pose from semantic keypoints. In 2017 IEEE international conference on robotics and automation (ICRA), pp.\\ 20112018. IEEE, 2017.",
            "matches": []
        },
        {
            "leaf id": 281,
            "key": "doc/bib87",
            "block type": "bibliography",
            "content": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with {GPT4}, 2023.",
            "leftover": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with {GPT4}, 2023.",
            "matches": []
        },
        {
            "leaf id": 282,
            "key": "doc/bib88",
            "block type": "bibliography",
            "content": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.",
            "leftover": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.",
            "matches": []
        },
        {
            "leaf id": 283,
            "key": "doc/bib89",
            "block type": "bibliography",
            "content": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.\\ 87488763, 1824 Jul 2021.",
            "leftover": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.\\ 87488763, 1824 Jul 2021.",
            "matches": []
        },
        {
            "leaf id": 284,
            "key": "doc/bib90",
            "block type": "bibliography",
            "content": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20. IEEE Press, 2020. ISBN 9781728199986.",
            "leftover": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20. IEEE Press, 2020. ISBN 9781728199986.",
            "matches": []
        },
        {
            "leaf id": 285,
            "key": "doc/bib91",
            "block type": "bibliography",
            "content": "Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy~J. Mitra. {Im2Vec}: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 73427351, June 2021{\\natexlaba}.",
            "leftover": "Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy~J. Mitra. {Im2Vec}: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 73427351, June 2021{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 286,
            "key": "doc/bib92",
            "block type": "bibliography",
            "content": "Pradyumna Reddy, Zhifei Zhang, Zhaowen Wang, Matthew Fisher, Hailin Jin, and Niloy Mitra. A multiimplicit neural representation for fonts. In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume~34, pp.\\ 1263712647. Curran Associates, Inc., 2021{\\natexlabb}. URL https://proceedings.neurips.cc/paperfiles/paper/2021/file/6948bd44c91acd2b54ecdd1b132f10fbPaper.pdf.",
            "leftover": "Pradyumna Reddy, Zhifei Zhang, Zhaowen Wang, Matthew Fisher, Hailin Jin, and Niloy Mitra. A multiimplicit neural representation for fonts. In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume~34, pp.\\ 1263712647. Curran Associates, Inc., 2021{\\natexlabb}. URL https://proceedings.neurips.cc/paperfiles/paper/2021/file/6948bd44c91acd2b54ecdd1b132f10fbPaper.pdf.",
            "matches": []
        },
        {
            "leaf id": 287,
            "key": "doc/bib93",
            "block type": "bibliography",
            "content": "Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, and Junzhe Zhang. ExtrudeNet: Unsupervised inverse sketchandextrude for shape parsing. In Computer Vision  ECCV 2022, pp.\\ 482498, Cham, 2022. Springer Nature Switzerland.",
            "leftover": "Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, and Junzhe Zhang. ExtrudeNet: Unsupervised inverse sketchandextrude for shape parsing. In Computer Vision  ECCV 2022, pp.\\ 482498, Cham, 2022. Springer Nature Switzerland.",
            "matches": []
        },
        {
            "leaf id": 288,
            "key": "doc/bib94",
            "block type": "bibliography",
            "content": "Yuzhuo Ren, Shangwen Li, Chen Chen, and CC~Jay Kuo. A coarsetofine indoor layout estimation (CFILE) method. In Computer VisionACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 2024, 2016, Revised Selected Papers, Part V 13, pp.\\ 3651. Springer, 2017.",
            "leftover": "Yuzhuo Ren, Shangwen Li, Chen Chen, and CC~Jay Kuo. A coarsetofine indoor layout estimation (CFILE) method. In Computer VisionACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 2024, 2016, Revised Selected Papers, Part V 13, pp.\\ 3651. Springer, 2017.",
            "matches": []
        },
        {
            "leaf id": 289,
            "key": "doc/bib95",
            "block type": "bibliography",
            "content": "Marzia Riso, Davide Sforza, and Fabio Pellacini. pOp: Parameter optimization of differentiable vector patterns. Computer Graphics Forum, 41\\penalty0 (4):\\penalty0 161168, 2022.",
            "leftover": "Marzia Riso, Davide Sforza, and Fabio Pellacini. pOp: Parameter optimization of differentiable vector patterns. Computer Graphics Forum, 41\\penalty0 (4):\\penalty0 161168, 2022.",
            "matches": []
        },
        {
            "leaf id": 290,
            "key": "doc/bib96",
            "block type": "bibliography",
            "content": "Daniel Ritchie, Paul Guerrero, R.~Kenny Jones, Niloy~J. Mitra, Adriana Schulz, Karl D.~D. Willis, and Jiajun Wu. Neurosymbolic models for computer graphics. Computer Graphics Forum, 42\\penalty0 (2):\\penalty0 545568, 2023. \\doi{https://doi.org/10.1111/cgf.14775}.",
            "leftover": "Daniel Ritchie, Paul Guerrero, R.~Kenny Jones, Niloy~J. Mitra, Adriana Schulz, Karl D.~D. Willis, and Jiajun Wu. Neurosymbolic models for computer graphics. Computer Graphics Forum, 42\\penalty0 (2):\\penalty0 545568, 2023. \\doi{https://doi.org/10.1111/cgf.14775}.",
            "matches": []
        },
        {
            "leaf id": 291,
            "key": "doc/bib97",
            "block type": "bibliography",
            "content": "Lawrence~G. Roberts. Machine perception of threedimensional solids. PhD thesis, Massachusetts Institute of Technology, 1963.",
            "leftover": "Lawrence~G. Roberts. Machine perception of threedimensional solids. PhD thesis, Massachusetts Institute of Technology, 1963.",
            "matches": []
        },
        {
            "leaf id": 292,
            "key": "doc/bib98",
            "block type": "bibliography",
            "content": "Renato~F. SalasMoreno, Richard~A. Newcombe, Hauke Strasdat, Paul~HJ Kelly, and Andrew~J. Davison. SLAM++: Simultaneous localisation and mapping at the level of objects. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\\ 13521359, 2013.",
            "leftover": "Renato~F. SalasMoreno, Richard~A. Newcombe, Hauke Strasdat, Paul~HJ Kelly, and Andrew~J. Davison. SLAM++: Simultaneous localisation and mapping at the level of objects. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\\ 13521359, 2013.",
            "matches": []
        },
        {
            "leaf id": 293,
            "key": "doc/bib99",
            "block type": "bibliography",
            "content": "Ari Seff, Wenda Zhou, Nick Richardson, and Ryan~P. Adams. Vitruvion: A generative model of parametric CAD sketches. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Ow1C7s3UcY.",
            "leftover": "Ari Seff, Wenda Zhou, Nick Richardson, and Ryan~P. Adams. Vitruvion: A generative model of parametric CAD sketches. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Ow1C7s3UcY.",
            "matches": []
        },
        {
            "leaf id": 294,
            "key": "doc/bib100",
            "block type": "bibliography",
            "content": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. CSGNet: Neural shape parser for constructive solid geometry. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018{\\natexlaba}.",
            "leftover": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. CSGNet: Neural shape parser for constructive solid geometry. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 295,
            "key": "doc/bib101",
            "block type": "bibliography",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. {Conceptual Captions}: A cleaned, hypernymed, image alttext dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\\ 25562565. Association for Computational Linguistics, July 2018{\\natexlabb}. \\doi{10.18653/v1/P181238}.",
            "leftover": "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. {Conceptual Captions}: A cleaned, hypernymed, image alttext dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\\ 25562565. Association for Computational Linguistics, July 2018{\\natexlabb}. \\doi{10.18653/v1/P181238}.",
            "matches": []
        },
        {
            "leaf id": 296,
            "key": "doc/bib102",
            "block type": "bibliography",
            "content": "Daeyun Shin, Zhile Ren, Erik~B. Sudderth, and Charless~C Fowlkes. {3D} scene reconstruction with multilayer depth and epipolar transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp.\\ 21722182, 2019.",
            "leftover": "Daeyun Shin, Zhile Ren, Erik~B. Sudderth, and Charless~C Fowlkes. {3D} scene reconstruction with multilayer depth and epipolar transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp.\\ 21722182, 2019.",
            "matches": []
        },
        {
            "leaf id": 297,
            "key": "doc/bib103",
            "block type": "bibliography",
            "content": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp.\\ 1152311530, 2023. \\doi{10.1109/ICRA48891.2023.10161317}.",
            "leftover": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp.\\ 1152311530, 2023. \\doi{10.1109/ICRA48891.2023.10161317}.",
            "matches": []
        },
        {
            "leaf id": 298,
            "key": "doc/bib104",
            "block type": "bibliography",
            "content": "Vincent Sitzmann, Michael Zollh{''o}fer, and Gordon Wetzstein. Scene representation networks: Continuous {3D}structureaware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.",
            "leftover": "Vincent Sitzmann, Michael Zollh{''o}fer, and Gordon Wetzstein. Scene representation networks: Continuous {3D}structureaware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.",
            "matches": []
        },
        {
            "leaf id": 299,
            "key": "doc/bib105",
            "block type": "bibliography",
            "content": "Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. {3DGPT}: Procedural {3D} modeling with large language models. arXiv preprint arXiv:2310.12945, 2023.",
            "leftover": "Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. {3DGPT}: Procedural {3D} modeling with large language models. arXiv preprint arXiv:2310.12945, 2023.",
            "matches": []
        },
        {
            "leaf id": 300,
            "key": "doc/bib106",
            "block type": "bibliography",
            "content": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. Stanford Alpaca: An instructionfollowing LLaMA model. https://github.com/tatsulab/stanfordalpaca, 2023.",
            "leftover": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. Stanford Alpaca: An instructionfollowing LLaMA model. https://github.com/tatsulab/stanfordalpaca, 2023.",
            "matches": []
        },
        {
            "leaf id": 301,
            "key": "doc/bib107",
            "block type": "bibliography",
            "content": "Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and TaeKyun Kim. Latentclass Hough forests for {3D} object detection and pose estimation. In Computer Vision  ECCV 2014, pp.\\ 462477. Springer International Publishing, 2014.",
            "leftover": "Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and TaeKyun Kim. Latentclass Hough forests for {3D} object detection and pose estimation. In Computer Vision  ECCV 2014, pp.\\ 462477. Springer International Publishing, 2014.",
            "matches": []
        },
        {
            "leaf id": 302,
            "key": "doc/bib108",
            "block type": "bibliography",
            "content": "Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William~T. Freeman, Joshua~B. Tenenbaum, and Jiajun Wu. Learning to infer and execute {3D} shape programs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rylNH20qFQ.",
            "leftover": "Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William~T. Freeman, Joshua~B. Tenenbaum, and Jiajun Wu. Learning to infer and execute {3D} shape programs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rylNH20qFQ.",
            "matches": []
        },
        {
            "leaf id": 303,
            "key": "doc/bib109",
            "block type": "bibliography",
            "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, MarieAnne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models, 2023.",
            "leftover": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, MarieAnne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models, 2023.",
            "matches": []
        },
        {
            "leaf id": 304,
            "key": "doc/bib110",
            "block type": "bibliography",
            "content": "Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 15101519, 2015.",
            "leftover": "Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 15101519, 2015.",
            "matches": []
        },
        {
            "leaf id": 305,
            "key": "doc/bib111",
            "block type": "bibliography",
            "content": "Shubham Tulsiani, Hao Su, Leonidas~J. Guibas, Alexei~A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 26352643, 2017.",
            "leftover": "Shubham Tulsiani, Hao Su, Leonidas~J. Guibas, Alexei~A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 26352643, 2017.",
            "matches": []
        },
        {
            "leaf id": 306,
            "key": "doc/bib112",
            "block type": "bibliography",
            "content": "Anton van~den Hengel, Chris Russell, Anthony Dick, John Bastian, Daniel Pooley, Lachlan Fleming, and Lourdes Agapito. Partbased modelling of compound scenes from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 878886, 2015.",
            "leftover": "Anton van~den Hengel, Chris Russell, Anthony Dick, John Bastian, Daniel Pooley, Lachlan Fleming, and Lourdes Agapito. Partbased modelling of compound scenes from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 878886, 2015.",
            "matches": []
        },
        {
            "leaf id": 307,
            "key": "doc/bib113",
            "block type": "bibliography",
            "content": "Vaibhav Vavilala and David Forsyth. Convex decomposition of indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 91769186, 2023.",
            "leftover": "Vaibhav Vavilala and David Forsyth. Convex decomposition of indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 91769186, 2023.",
            "matches": []
        },
        {
            "leaf id": 308,
            "key": "doc/bib114",
            "block type": "bibliography",
            "content": "O.~\\vSt'ava, B.~Bene{\\vs}, R.~M{\\ve}ch, D.~G. Aliaga, and P.~Kri{\\vs}tof. Inverse procedural modeling by automatic generation of Lsystems. Computer Graphics Forum, 29\\penalty0 (2):\\penalty0 665674, 2010.",
            "leftover": "O.~\\vSt'ava, B.~Bene{\\vs}, R.~M{\\ve}ch, D.~G. Aliaga, and P.~Kri{\\vs}tof. Inverse procedural modeling by automatic generation of Lsystems. Computer Graphics Forum, 29\\penalty0 (2):\\penalty0 665674, 2010.",
            "matches": []
        },
        {
            "leaf id": 309,
            "key": "doc/bib115",
            "block type": "bibliography",
            "content": "O.~\\vSt'ava, S.~Pirk, J.~Kratt, B.~Chen, R.~M\\vech, O.~Deussen, and B.~Benes. Inverse procedural modelling of trees. Computer Graphics Forum, 33\\penalty0 (6):\\penalty0 118131, 2014.",
            "leftover": "O.~\\vSt'ava, S.~Pirk, J.~Kratt, B.~Chen, R.~M\\vech, O.~Deussen, and B.~Benes. Inverse procedural modelling of trees. Computer Graphics Forum, 33\\penalty0 (6):\\penalty0 118131, 2014.",
            "matches": []
        },
        {
            "leaf id": 310,
            "key": "doc/bib116",
            "block type": "bibliography",
            "content": "Angtian Wang, Adam Kortylewski, and Alan~L. Yuille. NeMo: Neural mesh models of contrastive features for robust {3D} pose estimation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 37, 2021. OpenReview.net, 2021{\\natexlaba}. URL https://openreview.net/forum?id=pmj131uIL9H.",
            "leftover": "Angtian Wang, Adam Kortylewski, and Alan~L. Yuille. NeMo: Neural mesh models of contrastive features for robust {3D} pose estimation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 37, 2021. OpenReview.net, 2021{\\natexlaba}. URL https://openreview.net/forum?id=pmj131uIL9H.",
            "matches": []
        },
        {
            "leaf id": 311,
            "key": "doc/bib117",
            "block type": "bibliography",
            "content": "Gu~Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. {GDRNet}: Geometryguided direct regression network for monocular {6D} object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1661116621, June 2021{\\natexlabb}.",
            "leftover": "Gu~Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. {GDRNet}: Geometryguided direct regression network for monocular {6D} object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1661116621, June 2021{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 312,
            "key": "doc/bib118",
            "block type": "bibliography",
            "content": "He~Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas~J. Guibas. Normalized object coordinate space for categorylevel {6D} object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 26422651, 2019.",
            "leftover": "He~Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas~J. Guibas. Normalized object coordinate space for categorylevel {6D} object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 26422651, 2019.",
            "matches": []
        },
        {
            "leaf id": 313,
            "key": "doc/bib119",
            "block type": "bibliography",
            "content": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and YuGang Jiang. {Pixel2Mesh}: Generating {3D} mesh models from single RGB images. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.",
            "leftover": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and YuGang Jiang. {Pixel2Mesh}: Generating {3D} mesh models from single RGB images. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.",
            "matches": []
        },
        {
            "leaf id": 314,
            "key": "doc/bib120",
            "block type": "bibliography",
            "content": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Selfinstruct: Aligning language models with selfgenerated instructions. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\\ 1348413508, Toronto, Canada, July 2023. Association for Computational Linguistics. \\doi{10.18653/v1/2023.acllong.754}. URL https://aclanthology.org/2023.acllong.754.",
            "leftover": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Selfinstruct: Aligning language models with selfgenerated instructions. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\\ 1348413508, Toronto, Canada, July 2023. Association for Computational Linguistics. \\doi{10.18653/v1/2023.acllong.754}. URL https://aclanthology.org/2023.acllong.754.",
            "matches": []
        },
        {
            "leaf id": 315,
            "key": "doc/bib121",
            "block type": "bibliography",
            "content": "Karl~DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph~G. Lambourne, Armando SolarLezama, and Wojciech Matusik. Fusion 360 gallery: A dataset and environment for programmatic CAD construction from human design sequences. ACM Transactions on Graphics (TOG), 40\\penalty0 (4):\\penalty0 124, 2021.",
            "leftover": "Karl~DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph~G. Lambourne, Armando SolarLezama, and Wojciech Matusik. Fusion 360 gallery: A dataset and environment for programmatic CAD construction from human design sequences. ACM Transactions on Graphics (TOG), 40\\penalty0 (4):\\penalty0 124, 2021.",
            "matches": []
        },
        {
            "leaf id": 316,
            "key": "doc/bib122",
            "block type": "bibliography",
            "content": "Jiajun Wu, Joshua~B. Tenenbaum, and Pushmeet Kohli. Neural scene derendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 699707, 2017.",
            "leftover": "Jiajun Wu, Joshua~B. Tenenbaum, and Pushmeet Kohli. Neural scene derendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\\ 699707, 2017.",
            "matches": []
        },
        {
            "leaf id": 317,
            "key": "doc/bib123",
            "block type": "bibliography",
            "content": "Yu~Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. PoseCNN: A convolutional neural network for {6D} object pose estimation in cluttered scenes, 2018.",
            "leftover": "Yu~Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. PoseCNN: A convolutional neural network for {6D} object pose estimation in cluttered scenes, 2018.",
            "matches": []
        },
        {
            "leaf id": 318,
            "key": "doc/bib124",
            "block type": "bibliography",
            "content": "Xianghao Xu, Wenzhe Peng, ChinYi Cheng, Karl~D.D. Willis, and Daniel Ritchie. Inferring CAD modeling sequences using zone graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 60626070, June 2021.",
            "leftover": "Xianghao Xu, Wenzhe Peng, ChinYi Cheng, Karl~D.D. Willis, and Daniel Ritchie. Inferring CAD modeling sequences using zone graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 60626070, June 2021.",
            "matches": []
        },
        {
            "leaf id": 319,
            "key": "doc/bib125",
            "block type": "bibliography",
            "content": "Le~Xue, Mingfei Gao, Chen Xing, Roberto Mart{\\'\\i}nMart{\\'\\i}n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese. ULIP: Learning a unified representation of language, images, and point clouds for {3D} understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 11791189, 2023.",
            "leftover": "Le~Xue, Mingfei Gao, Chen Xing, Roberto Mart{\\'\\i}nMart{\\'\\i}n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese. ULIP: Learning a unified representation of language, images, and point clouds for {3D} understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\ 11791189, 2023.",
            "matches": []
        },
        {
            "leaf id": 320,
            "key": "doc/bib126",
            "block type": "bibliography",
            "content": "Shunyu Yao, Tzu~Ming Hsu, JunYan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, and Josh Tenenbaum. {3D}aware scene manipulation via inverse graphics. Advances in Neural Information Processing Systems, 31, 2018.",
            "leftover": "Shunyu Yao, Tzu~Ming Hsu, JunYan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, and Josh Tenenbaum. {3D}aware scene manipulation via inverse graphics. Advances in Neural Information Processing Systems, 31, 2018.",
            "matches": []
        },
        {
            "leaf id": 321,
            "key": "doc/bib127",
            "block type": "bibliography",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neuralsymbolic VQA: Disentangling reasoning from vision and language understanding. In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~CesaBianchi, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paperfiles/paper/2018/file/5e388103a391daabe3de1d76a6739ccdPaper.pdf.",
            "leftover": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neuralsymbolic VQA: Disentangling reasoning from vision and language understanding. In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~CesaBianchi, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paperfiles/paper/2018/file/5e388103a391daabe3de1d76a6739ccdPaper.pdf.",
            "matches": []
        },
        {
            "leaf id": 322,
            "key": "doc/bib128",
            "block type": "bibliography",
            "content": "Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali MahdaviAmiri, and Hao Zhang. {CAPRINet}: Learning compact CAD shapes with adaptive primitive assembly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1176811778, June 2022.",
            "leftover": "Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali MahdaviAmiri, and Hao Zhang. {CAPRINet}: Learning compact CAD shapes with adaptive primitive assembly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 1176811778, June 2022.",
            "matches": []
        },
        {
            "leaf id": 323,
            "key": "doc/bib129",
            "block type": "bibliography",
            "content": "Alan Yuille and Daniel Kersten. Vision as Bayesian inference: Analysis by synthesis? Trends in cognitive sciences, 10\\penalty0 (7):\\penalty0 301308, 2006.",
            "leftover": "Alan Yuille and Daniel Kersten. Vision as Bayesian inference: Analysis by synthesis? Trends in cognitive sciences, 10\\penalty0 (7):\\penalty0 301308, 2006.",
            "matches": []
        },
        {
            "leaf id": 324,
            "key": "doc/bib130",
            "block type": "bibliography",
            "content": "Biao Zhang and Rico Sennrich. Root mean square layer normalization. In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\\textquotesingle Alch\\'eBuc, E.~Fox, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paperfiles/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53bPaper.pdf.",
            "leftover": "Biao Zhang and Rico Sennrich. Root mean square layer normalization. In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\\textquotesingle Alch\\'eBuc, E.~Fox, and R.~Garnett (eds.), Advances in Neural Information Processing Systems, volume~32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paperfiles/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53bPaper.pdf.",
            "matches": []
        },
        {
            "leaf id": 325,
            "key": "doc/bib131",
            "block type": "bibliography",
            "content": "Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, and Shuaicheng Liu. Holistic {3D} scene understanding from a single image with implicit representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 88338842, June 2021.",
            "leftover": "Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, and Shuaicheng Liu. Holistic {3D} scene understanding from a single image with implicit representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\\ 88338842, June 2021.",
            "matches": []
        },
        {
            "leaf id": 326,
            "key": "doc/bib132",
            "block type": "bibliography",
            "content": "Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua~B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023{\\natexlaba}.",
            "leftover": "Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua~B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 327,
            "key": "doc/bib133",
            "block type": "bibliography",
            "content": "Xiang Zhang, Zeyuan Chen, Fangyin Wei, and Zhuowen Tu. {Uni3D}: A universal model for panoptic {3D} scene reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 92569266, 2023{\\natexlabb}.",
            "leftover": "Xiang Zhang, Zeyuan Chen, Fangyin Wei, and Zhuowen Tu. {Uni3D}: A universal model for panoptic {3D} scene reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\\ 92569266, 2023{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 328,
            "key": "doc/bib134",
            "block type": "bibliography",
            "content": "Yi~Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "leftover": "Yi~Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "matches": []
        },
        {
            "leaf id": 329,
            "key": "doc/bib135",
            "block type": "bibliography",
            "content": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. {MiniGPT4}: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.",
            "leftover": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. {MiniGPT4}: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.",
            "matches": []
        }
    ]
}