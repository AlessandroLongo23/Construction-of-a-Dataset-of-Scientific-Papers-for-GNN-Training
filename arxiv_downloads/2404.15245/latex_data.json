{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "Mining Invariance from Nonlinear Multi-Environment Data: Binary Classification"
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "\\IEEEauthorblockN{Austin Goddard, Kang Du, Yu Xiang \\IEEEauthorblockA{\\textit{Department of Electrical and Computer Engineering} \\\\ \\textit{University of Utah}\\\\  \\{austin.goddard,\\,kang.du,\\,yu.xiang\\}@utah.edu} }"
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "  Making predictions in an unseen environment given data from multiple training environments is a challenging task. We approach this problem from an invariance perspective, focusing on binary classification to shed light on general nonlinear data generation mechanisms. We identify a unique form of invariance that exists solely in a binary setting that allows us to train models invariant over environments. We provide sufficient conditions for such invariance and show it is robust even when environmental conditions vary greatly. Our formulation admits a causal interpretation, allowing us to compare it with various frameworks. Finally, we propose a heuristic prediction method and conduct experiments using real and synthetic datasets.  "
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "leaf id": 3,
                    "key": "doc/body/sec0/tit",
                    "block type": "section",
                    "content": "Introduction"
                },
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec0/par0",
                            "block type": "par",
                            "content": "It is common practice to collect observations of a set of features X=(X_1,\u2026, X_m) and response Y from different environments to train a model. The prediction of the response in an unseen environment is often referred to as multi-environment domain adaptation, with practical applications in various fields (e.g., genetics~\\cite{meinshausen2016methods} and healthcare~\\cite{goddard2022invariance}). A common assumption in such problems is the principle of invariance, modularity, or autonomy~\\cite{haavelmo1944probability, aldrich1989autonomy,hoover1990logic,scholkopf2012causal,dawid2010identifying,pearl2009causality}. This invariance assumption states that the conditional distribution of Y given X is invariant with respect to different environment."
                        },
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec0/par1",
                            "block type": "par",
                            "content": "The invariant causal prediction (\\textsf{ICP}) framework~\\cite{peters2016causal}, along with its various extensions~\\cite{heinze2018invariant,pfister2019invariant}, employ the invariance principle to identify invariant predictors across environments. Following this framework, various domain adaptation approaches have been developed~\\cite{rojas2018invariant,rothenhausler2021anchor,pfister2021stabilizing}. Specifically, the stabilized regression (\\textsf{SR})~\\cite{pfister2021stabilizing} approach relies on a weaker form of invariance dependent on expectation as opposed to probability. The common assumption for the approaches mentioned is that the assignment of Y does not change over environments. In a causal sense, from which much of the literature in this area stems, this is referred to as an intervention on Y~\\cite{pearl2009causality}. When Y is intervened, the invariance principle, as well as the frameworks mentioned above,  fail. In a series of recent works\\cite{du2023learning,du2023generalized}, an alternative approach called the invariant matching property (\\textsf{IMP}) has been developed to detect linear invariant models in a regression setting even when the assignment of Y is altered over environment."
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec0/par2",
                            "block type": "par",
                            "content": "In this work, we extend general principles developed in~\\cite{du2023learning,du2023generalized} to the binary classification setting as an attempt to generalize to nonlinear settings. The proposed approach works even when data-generating models change over environments (e.g., Y can be generated using a probit model for one environment and a logistic model in another). Additionally, the approach is not constrained by the data type, meaning it can be useful on continuous, discrete, or categorical variables."
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec0/sec3/tit",
                            "block type": "section",
                            "content": "Problem Formulation"
                        },
                        {
                            "key": "doc/body/sec0/sec3",
                            "block_type": "sec",
                            "children": [
                                {
                                    "leaf id": 8,
                                    "key": "doc/body/sec0/sec3/par0",
                                    "block type": "par",
                                    "content": "Consider the following setting. For different environmental conditions indexed by the set \u2130, we have a random vector X=(X_1,\u2026, X_m) and a binary random variable Y whose elements form a joint distribution _e := _e^X,Y dependent on e\u2208\u2130. Denote X and Y as X^e and Y^e for a specific e\u2208\u2130, respectively. The supports of X and Y are \ud835\udcb3=\u211d^m and \ud835\udcb4={0,1}, respectively. Let X_S be a random vector containing the elements in X indexed by the set S\u2286{1,\u2026,m}, and let _S be its support. To simplify notation, let X_0^e:=Y^e. For each e\u2208\u2130, we keep the distribution _e general, with the exception that there exists an X^e_i generated according to the form"
                                },
                                {
                                    "leaf id": 9,
                                    "key": "doc/body/sec0/sec3/equation1",
                                    "block type": "equation",
                                    "content": "\\label{equ::Xi_def}    X^e_i = g(X^e_{S_i}) + \\epsilon^e,  for some i\\in\\{1,\\dots,m\\},"
                                },
                                {
                                    "leaf id": 10,
                                    "key": "doc/body/sec0/sec3/par2",
                                    "block type": "par",
                                    "content": "where X^e_S_i, for S_i\u2286{0,\u2026,m}\\ i, represents the variables that directly effect X^e_i,  and \u03f5^e is an independent, zero mean, noise variable. We assume the output of the function g is not constant with regards to any of its inputs; g is a constant function when S_i= \u2205."
                                },
                                {
                                    "leaf id": 11,
                                    "key": "doc/body/sec0/sec3/par3",
                                    "block type": "par",
                                    "content": "Additionally, while the function g does not change over environment (i.e., does not depend on e), the distribution of \u03f5^e can change arbitrarily as long as the mean of the distribution remains zero. Aside from a binary Y and the form of X_i^e in~\\eqref{equ::Xi_def}, we make no assumptions on the distribution or functional form of any variable. As such, this formulation applies to any set of features, be it continuous, discrete, or a mixture of the two."
                                },
                                {
                                    "leaf id": 12,
                                    "key": "doc/body/sec0/sec3/par4",
                                    "block type": "par",
                                    "content": "We assume only a subset of all environments are observed and denote this set by _obs\u2286\u2130. Where _obs = _train\u222a{e^test}, and Y^test:=Y^e^test, our goal is to make predictions on Y^test, given a set of training environments _train.  As such, we aim to find a function \u03d5_e:\ud835\udcb3\ud835\udcb2 such that, the probability of Y given \u03d5_e(X) does not vary over any environment. Specifically, for all w\u2208\ud835\udcb2 and e,h\u2208_obs,"
                                },
                                {
                                    "leaf id": 13,
                                    "key": "doc/body/sec0/sec3/equation5",
                                    "block type": "equation",
                                    "content": "\\label{equ::invar}     \\Pc_e(Y|\\phi_e(X)=w) = \\Pc_h(Y|\\phi_h(X)=w)."
                                },
                                {
                                    "leaf id": 14,
                                    "key": "doc/body/sec0/sec3/par6",
                                    "block type": "par",
                                    "content": "As Y is binary, it is equivalent to write~\\eqref{equ::invar} in the form: __e[Y|\u03d5_e(X)=w] = __h[Y|\u03d5_h(X)=w],  for all  w\u2208\ud835\udcb2 and e,h\u2208_obs. It is well-known that~\\eqref{equ::invar} is satisfied if \u03d5_e(X) = X_S_Y and for S_Y\u2286{1,\u2026,m},"
                                },
                                {
                                    "leaf id": 15,
                                    "key": "doc/body/sec0/sec3/equation7",
                                    "block type": "equation",
                                    "content": "\\label{equ::parentY}     Y^e = f(X_{S_Y}^e) + \\epsilon_Y,"
                                },
                                {
                                    "leaf id": 16,
                                    "key": "doc/body/sec0/sec3/par8",
                                    "block type": "par",
                                    "content": "where \u03f5_Y is an independent noise that does not vary over environment~\\cite{peters2016causal}. However, we are interested in a more general setting where the function f and distribution of the noise can vary over environment. From a causal perspective, this would indicate that Y^e had been intervened (see Section~\\ref{sec::causal}). In such a setting, \u03d5_e(X) = X_S_Y is no longer useful and other approaches must be considered. We now consider one such alternative, starting with a motivating example."
                                },
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec0/sec3/sec9/tit",
                                    "block type": "section",
                                    "content": "Motivating Example"
                                },
                                {
                                    "key": "doc/body/sec0/sec3/sec9",
                                    "block_type": "sec",
                                    "children": [
                                        {
                                            "leaf id": 18,
                                            "key": "doc/body/sec0/sec3/sec9/par0",
                                            "block type": "par",
                                            "content": "Consider the following setting with X^e = (X^e_1,X^e_2,X^e_3).  Let X^e_1 and X^e_2 be independent and follow X^e_1 \u223c\ud835\udca9(\u03bc_1^e,\u03c3^2_1) and X^e_2 \u223c\ud835\udca9(\u03bc_2^e,\u03c3^2_2). The variable Y^e is generated such that Y^e|X^e_1,X^e_2 forms a probit model. Specifically,"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec3/sec9/equation*1",
                                            "block_type": "equation*",
                                            "children": [
                                                {
                                                    "leaf id": 19,
                                                    "key": "doc/body/sec0/sec3/sec9/equation*1/cases0",
                                                    "block type": "cases",
                                                    "content": "1, & if \u03b2_1^eX^e_1 + \u03b2_2 X^e_2 + \u03f5_Y > 0,\\\\        0, & otherwise."
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 20,
                                            "key": "doc/body/sec0/sec3/sec9/par2",
                                            "block type": "par",
                                            "content": "Following a similar form as~\\eqref{equ::Xi_def}, X^e_3 is linear given Y^e so that"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec3/sec9/equation*3",
                                            "block_type": "equation*",
                                            "children": [
                                                {
                                                    "leaf id": 21,
                                                    "key": "doc/body/sec0/sec3/sec9/equation*3/cases0",
                                                    "block type": "cases",
                                                    "content": "\\gamma_1X^e_1  + \\epsilon_3, & if Y^e = 1,\\\\        \\gamma_0X^e_1 + \\epsilon_3, & if Y^e = 0."
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 22,
                                            "key": "doc/body/sec0/sec3/sec9/par4",
                                            "block type": "par",
                                            "content": "The noise variables \u03f5_Y and \u03f5_3 are \\iid \ud835\udca9(0, \u03c3^2). Suppose we wish to predict Y^e given only X^e_1.  Predicting Y^e for a particular e\u2208\u2130 becomes difficult as \u03b2_1^e and \u03bc^e_2 vary with environment. Specifically,"
                                        },
                                        {
                                            "leaf id": 23,
                                            "key": "doc/body/sec0/sec3/sec9/equation5",
                                            "block type": "equation",
                                            "content": "\\label{equ::pygxx}     \\E_{\\Pc_e}[Y|X_1=x_1] = \\Phi\\left(\\frac{\\beta_1^ex_1 + \\beta_2\\mu^e_2}{\\sqrt{(\\beta_2\\sigma_2)^2 + \\sigma^2}} \\right),"
                                        },
                                        {
                                            "leaf id": 24,
                                            "key": "doc/body/sec0/sec3/sec9/par6",
                                            "block type": "par",
                                            "content": "where \u03a6 is the cumulative distribution function of a standard normal random variable. As~\\eqref{equ::pygxx} varies over environment, it is not practical to use __e[Y|X_1] to estimate Y^e on different environments. Even while conditioning on both X^e_1 and X^e_2 (the variables that directly affect Y^e), the variance (w.r.t. environment) still remains through \u03b2_1^e."
                                        },
                                        {
                                            "leaf id": 25,
                                            "key": "doc/body/sec0/sec3/sec9/par7",
                                            "block type": "par",
                                            "content": "We can, however, decompose~\\eqref{equ::pygxx} into various variant and invariant components such that __e[Y|X_1=x_1] becomes the following (see the proof of Proposition~\\ref{prop::suff} for a general case),"
                                        },
                                        {
                                            "leaf id": 26,
                                            "key": "doc/body/sec0/sec3/sec9/align*8",
                                            "block type": "align*",
                                            "content": "\\frac{\\E_{\\Pc_e}[X_3|X_1=x_1] - \\E_{\\Pc_e}[X_3|X_1=x_1,Y=0]}{\\E_{\\Pc_e}[X_3|X_1=x_1,Y=1] - \\E_{\\Pc_e}[X_3|X_1=x_1,Y=0]}, \\numberthis\\label{equ::examp_matching}"
                                        },
                                        {
                                            "leaf id": 27,
                                            "key": "doc/body/sec0/sec3/sec9/par9",
                                            "block type": "par",
                                            "content": "where __e[X_3|X_1=x_1] is"
                                        },
                                        {
                                            "leaf id": 28,
                                            "key": "doc/body/sec0/sec3/sec9/align*10",
                                            "block type": "align*",
                                            "content": "\\Phi\\left(\\frac{\\beta_1^ex_1 + \\beta_2\\mu^e_2}{\\sqrt{(\\beta_2\\sigma_2)^2 + \\sigma^2}} \\right) (\\gamma_1 - \\gamma_0)x_1 + \\gamma_0x_1, \\numberthis"
                                        },
                                        {
                                            "leaf id": 29,
                                            "key": "doc/body/sec0/sec3/sec9/par11",
                                            "block type": "par",
                                            "content": "and __e[X_3|X_1=x_1,Y=y] is \u03b3_1x_1 if y=1 and \u03b3_0x_1 if y=0. We note that the variance (w.r.t environment) contributed by \u03b2_1^e and \u03bc_2^e is completely accounted for in the term __e[X_3|X_1] and that __e[X_3|X_1,Y] is invariant over environment. Thus,~\\eqref{equ::invar} holds for the function \u03d5_e(X) = (X_1,__e[X_3|X_1]). In addition to this, we also note that conditioning on both X_1 and X_2 leads to a similar invariance; we only condition on X_1 in this example for simplicity."
                                        },
                                        {
                                            "leaf id": 30,
                                            "key": "doc/body/sec0/sec3/sec9/par12",
                                            "block type": "par",
                                            "content": "This invariance does not hold if we replace X^e_3 with any other variable. For example, suppose we were to estimate Y^e, replacing X^e_3 with X^e_2. We can still decompose~\\eqref{equ::pygxx} similarly to~\\eqref{equ::examp_matching} by replacing X^e_3 with X^e_2. As __e[X_2|X_1] = \u03bc^e_2 does not contain \u03b2_1^e, the portion of __e[Y|X_1] that contains \u03b2_1^e must reside in __e[X_2|X_1,Y]. i.e., __e[X_2|X_1,Y] is not invariant over environments as is __e[X_3|X_1,Y]. Thus, the function \u03d5_e(X) = (X_1,__e[X_2|X_1]) will no longer  satisfy~\\eqref{equ::invar}."
                                        },
                                        {
                                            "leaf id": 31,
                                            "key": "doc/body/sec0/sec3/sec9/par13",
                                            "block type": "par",
                                            "content": "To further illustrate the difference in selecting X_3^e over X_2^e, suppose we wish to estimate on a new environment e^test. While we have access to X^test, we can easily construct __test[X_i|X_1] for either i\u2208{2,3}. We cannot, however, use Y^test to construct our estimate, and __test[X_i|X_1,Y] must be obtained by leveraging invariances over environment. Thus, for either i\u2208{2,3}, we construct the estimate"
                                        },
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec0/sec3/sec9/equation14",
                                            "block type": "equation",
                                            "content": "\\hat{Y}^{test}_i =: \\frac{\\E_{\\Pc_{test}}[X_i|X_1]-\\E_{\\Pc_e}[X_i|X_1,Y=0]}{\\E_{\\Pc_e}[X_i|X_1,Y=1]-\\E_{\\Pc_e}[X_i|X_1,Y=0]},"
                                        },
                                        {
                                            "leaf id": 33,
                                            "key": "doc/body/sec0/sec3/sec9/par15",
                                            "block type": "par",
                                            "content": "where e\u2208_train. As __e[X_3|X_1,Y] is invariant and __e[X_2|X_1,Y] is not invariant as discussed above, \u0176^test_3 will provide a good estimate of Y^test, while \u0176^test_2 will not."
                                        },
                                        {
                                            "leaf id": 34,
                                            "key": "doc/body/sec0/sec3/sec9/par16",
                                            "block type": "par",
                                            "content": "In Fig.~\\ref{fig:example} we compare  \u0176^test_3 and  \u0176^test_2 by simulating (x^test,y^test) pairs for a set of specific parameters. The estimate \u0176^test_2 does not fit the data as many x_1^test corresponding to y^test=0 will be incorrectly classified to one. However, this is not the case when \u0176^test_3 is used, and the fit is greatly improved (Fig.~\\ref{fig:example}). The poor fit on \u0176^test_2 is a result of __e[X_2|X_1,Y] varying across environments."
                                        },
                                        {
                                            "key": "doc/body/sec0/sec3/sec9/center17",
                                            "block_type": "center",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec0/sec3/sec9/center17/figure0",
                                                    "block_type": "figure",
                                                    "children": [
                                                        {
                                                            "leaf id": 35,
                                                            "key": "doc/body/sec0/sec3/sec9/center17/figure0/par0",
                                                            "block type": "par",
                                                            "content": "[h]   \\begin{subfigure}[b]{0.45\\columnwidth}"
                                                        },
                                                        {
                                                            "leaf id": 36,
                                                            "key": "doc/body/sec0/sec3/sec9/center17/figure0/subfigure1",
                                                            "block type": "subfigure",
                                                            "content": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 37,
                                            "key": "doc/body/sec0/sec3/sec9/sec18/tit",
                                            "block type": "section",
                                            "content": "The Binary Invariant Matching Property"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec3/sec9/sec18",
                                            "block_type": "sec",
                                            "children": [
                                                {
                                                    "leaf id": 38,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/par0",
                                                    "block type": "par",
                                                    "content": "A deterministic relationship such as the one in~\\eqref{equ::examp_matching} has been previously referred to as matching~\\cite{du2023learning}, and can be generalized to the formulation outlined in Section~\\ref{sec::formulation}."
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/definition1",
                                                    "block_type": "definition",
                                                    "children": [
                                                        {
                                                            "leaf id": 39,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/definition1/equation0",
                                                            "block type": "equation",
                                                            "content": "\\label{equ::EjgXs}     \\E_{\\Pc_e}[Y|X_S] = \\frac{\\E_{\\Pc_e}[X_k|X_S] - h(X_S,0)}{h(X_S,1) - h(X_S,0)},"
                                                        },
                                                        {
                                                            "leaf id": 40,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/definition1/par1",
                                                            "block type": "par",
                                                            "content": "For k\u2208{1,\u2026,m}, S \u2286{1,\u2026,m}\\ k, and h(X_S,Y) := __e[X_k|X_S,Y], the pair (k,S) satisfies the binary invariant matching property (bIMP)\\footnote{There are degenerate cases when h(X_S,0) = h(X_S,1), for which the tower property implies __e[X_k|X_S] = __e[h(X_S,Y)|X_S] = h(X_S,0), and the ratio in~\\eqref{equ::EjgXs} reduces to 0 divided by 0.} if,"
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 41,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/par2",
                                                    "block type": "par",
                                                    "content": "As seen in the example, there are a variety of choices for k and S, not all of which lead to invariant representations. We now detail the sufficient conditions for which a pair (k,S) satisfies the bIMP (see Appendix for the proof)."
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/proposition3",
                                                    "block_type": "proposition",
                                                    "children": [
                                                        {
                                                            "leaf id": 42,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/proposition3/enumerate0",
                                                            "block type": "enumerate",
                                                            "content": "\\item X^e_k = g(X^e_R,Y^e) + \u03f5^e as in~\\eqref{equ::Xi_def}\\ ,         \\item  X^e_Q  X^e_k  |  (X^e_R, Y^e)\\ ."
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 43,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/par4",
                                                    "block type": "par",
                                                    "content": "What remains is to show that the bIMP can be used to satisfy the invariance principle in~\\eqref{equ::invar}, and thus, can be beneficial in predicting on unknown environments, as shown below."
                                                },
                                                {
                                                    "leaf id": 44,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/theorem5",
                                                    "block type": "theorem",
                                                    "content": "\\label{the:1}  Let k \u2208{1,\u2026,m} and S = R \u222a Q where R,Q \u2286{1,\u2026,m}\u2216 k and R \u2229 Q = \u2205. When \u03d5_e(X) = (X_R,X_Q,__e[X_k|X_R,X_Q]),~\\eqref{equ::invar} holds if the pair (k,S) satisfies the bIMP."
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/proof6",
                                                    "block_type": "proof",
                                                    "children": [
                                                        {
                                                            "leaf id": 45,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/proof6/par0",
                                                            "block type": "par",
                                                            "content": "Let \u2113^e(X_R,X_Q):=__e[X_k|X_R,X_Q] and \u03d5_e(X) = (X_R,X_Q,\u2113^e(X_R,X_Q)). Since (k,S) satisfies the bIMP and \u2113^e(X_R,X_Q) is a function of X_R and X_Q,"
                                                        },
                                                        {
                                                            "leaf id": 46,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/proof6/align*1",
                                                            "block type": "align*",
                                                            "content": "\\E_{\\Pc_e}&[Y|\\phi_e(X) = (x_Q,x_R,z)] \\\\       &=\\E_{\\Pc_e}[Y|X_R=x_R,X_Q=x_q,\\ell^e(X_R,X_Q)=z] \\\\       &= \\frac{z - g(x_R,0)}{g(x_R,1) - g(x_R,0)}. \\numberthis\\label{equ::E_final}"
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 47,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/remark7",
                                                    "block type": "remark",
                                                    "content": "In this work, we focus specifically on settings where Y^e is binary. However, there does exist a corresponding matching property with sufficient conditions similar to those in Proposition~\\ref{prop::suff} for cases when Y^e is multi-class or continuous. We leave the analysis for the long version of this work."
                                                },
                                                {
                                                    "leaf id": 48,
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/tit",
                                                    "block type": "subsection",
                                                    "content": "A Causal Perspective"
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8",
                                                    "block_type": "sub",
                                                    "children": [
                                                        {
                                                            "leaf id": 49,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par0",
                                                            "block type": "par",
                                                            "content": "While the sufficient conditions in Theorem~\\ref{the:1} may seem abstract, we now show that, in fact, they have a specific meaning in a causal sense. To do so, we introduce the structural causal model (SCM)~\\cite{pearl2009causality}. Here, X^e and Y^e are part of an SCM \ud835\udcae^e that varies over environment such that"
                                                        },
                                                        {
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/equation1",
                                                            "block_type": "equation",
                                                            "children": [
                                                                {
                                                                    "leaf id": 50,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/equation1/cases0",
                                                                    "block type": "cases",
                                                                    "content": "Y^e := f_Y^e(X^e_{PA(Y^e)}\\, ,\\ \\epsilon^e_Y),\\\\     X_1^e := f_1^e(X^e_{PA(X_1^e)}\\, ,\\ \\epsilon^e_1),\\\\     \\qquad \\vdots \\\\     X_m^e := f_m^e(X^e_{PA(X_m^e)}\\, ,\\ \\epsilon^e_m)."
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "leaf id": 51,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par2",
                                                            "block type": "par",
                                                            "content": "where \u03f5_1^e\u2026\u03f5_m^e,\u03f5_Y^e are independent noise variables. To simplify notation, let X_0^e:=Y^e. Thus, PA(X_i^e)\u2286{0,\u2026,1} denotes the set indexed by the direct causal parents of X_i^e for all i\u2208{0,\u2026,m}."
                                                        },
                                                        {
                                                            "leaf id": 52,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par3",
                                                            "block type": "par",
                                                            "content": "As in Section~\\ref{sec::formulation}, Y^e is binary. Additionally, at least one structural assignment (i.e., f_i^e(\u00b7)) in \ud835\udcae^e is an additive noise function that does not vary over environment. Specifically, for some i\u2208{0,\u2026,m},  let f_i^e(X_PA(X_i^e)^e,\u03f5^e_i) = g(X_PA(X_i^e)^e) + \u03f5^e_i, where \u03f5^e_i has zero mean. An intervention on a variable from {X_1^e,\u2026,X_m^e,Y^e} occurs if the structural assignment changes for some e\u2208\u2130. Relating the SCM to the formation in Section~\\ref{sec::formulation} gives insight into the types of interventions that may occur.  While many methods~\\cite{peters2016causal,pfister2021stabilizing,du2023learning} make various assumptions on the types of interventions (e.g., shifts in the mean or variance), the setting in~\\eqref{equ:SCM} allows for very general interventions, including general interventions on Y^e, which many other approaches do not allow."
                                                        },
                                                        {
                                                            "leaf id": 53,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par4",
                                                            "block type": "par",
                                                            "content": "Given  \ud835\udcae^e for all e\u2208_obs, we can express the conditions of Proposition~\\ref{prop::suff} in the language of SCMs, detailed below."
                                                        },
                                                        {
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/corollary5",
                                                            "block_type": "corollary",
                                                            "children": [
                                                                {
                                                                    "leaf id": 54,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/corollary5/par0",
                                                                    "block type": "par",
                                                                    "content": "Let k \u2208{1,\u2026,m} and S = R \u222a Q where R,Q \u2286{1,\u2026,m}\u2216 k and R \u2229 Q = \u2205. For the SCM \ud835\udcae^e, the pair (k,S) satisfies the bIMP for all e\u2208_obs if the following cases hold."
                                                                },
                                                                {
                                                                    "leaf id": 55,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/corollary5/enumerate1",
                                                                    "block type": "enumerate",
                                                                    "content": "\\setlength\\itemsep{0em}     \\item X_k^e = g(X^e_PA(X_k^e)) + \u03f5^e_k\\ ,     \\item X_R^e and Y^e constitute the parents of X_k^e\\ ,     \\item The variables in X_Q^e can be any non-descendants of X_k^e."
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "leaf id": 56,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par6",
                                                            "block type": "par",
                                                            "content": "The first condition in Proposition~\\ref{prop::suff} is analogous to the first and second condition above as PA(X_k^e) = (X_S,Y). Additionally, in an SCM, any variable conditioned on its parents is independent of any non-descendant. As such, the set X_Q^e can be any non-descendant of X_k^e, bridging the final conditions in Proposition~\\ref{prop::suff} and Corollary~\\ref{cor:causal1}."
                                                        },
                                                        {
                                                            "leaf id": 57,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/par7",
                                                            "block type": "par",
                                                            "content": "In many cases, the set Q can be quite inclusive despite what may seem like a strong independence condition in Proposition~\\ref{prop::suff}. In Corollary~\\ref{cor:causal1}, we learn that, in a causal sense, X_Q^e can be any non-descendant of X_k^e. For example, if half of the predictors in an SCM are ancestors of Y^e, while the other half are descendants, then the set Q indexes at least half of all predictors (and potentially many more)."
                                                        },
                                                        {
                                                            "leaf id": 58,
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/tit",
                                                            "block type": "section",
                                                            "content": "Proposed Method"
                                                        },
                                                        {
                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8",
                                                            "block_type": "sec",
                                                            "children": [
                                                                {
                                                                    "leaf id": 59,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par0",
                                                                    "block type": "par",
                                                                    "content": "For each e\u2208_train, we have n^e samples, represented as a matrix X^e\u2208\u211d^n_e\u00d7 m, and a vector Y^e\u2208{0,1}^n_e (see~\\cite{goddard2023error} for a discussion on the impact of different environments). Additionally, we have n_test samples in the test environment, and we denote X^test\u2208\u211d^n_test\u00d7 m and Y^test as the predictor matrix and target vector for the environment e^test, respectively. We denote X as the pooled predictor matrix over all e\u2208_train, and X_Y=y as the matrix comprising the rows of X in which Y=y, for y\u2208{0,1}. Let X^-e be the matrix of samples indexed only by those samples not in e\u2208_train."
                                                                },
                                                                {
                                                                    "leaf id": 60,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par1",
                                                                    "block type": "par",
                                                                    "content": "We now leverage insights gained from Theorem~\\ref{the:1} and the bIMP to develop a practical method for estimation in unknown environments. At test time, we do not have access to Y^test. As such, one cannot say with definitive assurance that~\\eqref{equ::invar} holds for all e\u2208_obs. Thus, the best that can be done in such settings is to identify a \u03d5_e such that~\\eqref{equ::invar} holds for all e\u2208_train, implying that _train must have at least two environments."
                                                                },
                                                                {
                                                                    "leaf id": 61,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par2",
                                                                    "block type": "par",
                                                                    "content": "Thus, our goal in a practical setting is to identify (k,S) pairs that may satisfy the bIMP overall e\u2208_train. Simply put, we test whether __e[X_k|X_S,Y] is invariant. To do so, we consider a special form of the model in~\\eqref{equ::Xi_def} where X_k^e = g(X^e_S,Y^e) + \u03f5^e with \u03f5^e\u223c\ud835\udca9(0, (\u03c3^e)^2) is assigned a different nonlinear additive noise function for each value of Y^e. Specifically,"
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/equation3",
                                                                    "block_type": "equation",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 62,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/equation3/cases0",
                                                                            "block type": "cases",
                                                                            "content": "g_1(X_S^e), & if Y^e = 1,\\\\             g_0(X_S^e), & if Y^e = 0."
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "leaf id": 63,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par4",
                                                                    "block type": "par",
                                                                    "content": "As X_k^e can be split into two models, one for each value of Y^e, we can perform an invariance test on each model. If both are found to be invariant, we can consider __e[X_k|X_S,Y] as a whole to be invariant. Invariance tests on additive noise models have been widely studied: Various tests have been proposed for linear~\\cite{peters2016causal} and nonlinear~\\cite{heinze2018invariant} models. We adopt one such approximate test from~\\cite{heinze2018invariant} known as the residual distribution test for our setting, as further detailed in Algorithm~\\ref{alg:var_test}."
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/algorithm5",
                                                                    "block_type": "algorithm",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 64,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/algorithm5/algorithmic0",
                                                                            "block type": "algorithmic",
                                                                            "content": "\\State Regress X_k,Y=i on X_S,Y=i to get \u011d_i, for i\u2208{0,1}  \\For{each e\u2208_train and i\u2208{0,1}} \\State R^e_i = X^e_k,Y=i - \u011d_i(X^e_S,Y=i) \\State R^-e_i = X^-e_k,Y=i - \u011d_i(X^-e_S,Y=i) \\State pval_i^e = t-test(R^e_i,R^-e_i) \\EndFor \\State Combine p-values in pval_1^e and pval_0^e via Bonferroni correction  \\If{min_e\u2208_trainpval_1^e > \u03b1  and  min_e\u2208_trainpval_2^e > \u03b1}  \\State return accepted \\Else  return rejected \\EndIf"
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "leaf id": 65,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par6",
                                                                    "block type": "par",
                                                                    "content": "We use Algorithm~\\ref{alg:var_test} as an approximate test for whether __e[X_k|X_S,Y] is invariant over environments. We now employ this test to develop a practical method for estimating Y^test which we refer to as \\textsf{bIMP}. We adopt a similar approach to that of~\\cite{pfister2021stabilizing} and~\\cite{du2023learning} in which we test the invariance of __e[X_k|X_S,Y] for all possible pairs (k,S). We then train models using the X_k^e and X^e_S which are accepted according to Algorithm~\\ref{alg:var_test}. Our bIMP models are a combination of two separate models trained to estimate both __e[X_k|X_S,Y] and __e[X_k|X_S]. Given both of these estimates, we compute an estimate of Y^test using~\\eqref{equ::EjgXs}. As it is likely that more than one pair is accepted, the final estimate of Y^test is the average estimate over all accepted pairs."
                                                                },
                                                                {
                                                                    "leaf id": 66,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par7",
                                                                    "block type": "par",
                                                                    "content": "While we can guarantee invariance via the bIMP, there is no guarantee that the estimation will predict well on e^test. As such, in addition to filtering pairs based on invariance, \\textsf{bIMP} also filters based on a prediction score. Invariant pairs _inv computed using~\\eqref{equ::EjgXs} are filtered using the mean squared prediction error. The threshold by which the pairs are filtered is identical to the procedure proposed in~\\cite{pfister2021stabilizing}."
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/algorithm8",
                                                                    "block_type": "algorithm",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 67,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/algorithm8/algorithmic0",
                                                                            "block type": "algorithmic",
                                                                            "content": "\\State   Identify the set of all invariant pairs _inv using Algorithm~\\ref{alg:var_test} \\State  Filter pairs from _inv based on prediction score   \\For{each (k,S) in _inv} \\State Estimate __e[X_k|X_S,Y] by regressing X_k on (X_S,Y) \\State Estimate __test[X_k|X_S] by regressing X_k^test on X^test_S  \\State Using~\\eqref{equ::EjgXs}, compute \u0176_k,S^test for the pair (k,S) \\EndFor \\State \u0176^test = 1/|_inv|\u2211_(k,S)\u2208_inv\u0176_k,S^test"
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "leaf id": 68,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/par9",
                                                                    "block type": "par",
                                                                    "content": "The \\textsf{bIMP} method proposed gives freedom to the user to select the underlying models with which to estimate __e[X_k|X_S] and __e[X_k|X_S,Y]. In the case of __e[X_k|X_S], we have complete freedom to select whichever model suits the data, be it linear or nonlinear.  For __e[X_k|X_S,Y], we are restricted by the additive noise of~\\eqref{equ::Xi_def}. In addition, we have chosen to model X_k using two sub-models, one for each value of Y as in~\\eqref{equ::model_Z}. This, however, is not the only option and depends on the invariance test used. When estimating each model, ordinary least squares (OLS) could be used for linear models, and a generalized additive model (GAM) or Gaussian process regression could be used for nonlinear models. In practice, we found estimating each model using OLS to be the most efficient, as fitting two nonlinear models for all possible (k,S) pairs can be computationally expensive."
                                                                },
                                                                {
                                                                    "leaf id": 69,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/remark10",
                                                                    "block type": "remark",
                                                                    "content": "There are several challenges with this approach that we leave for future work. We observe that nonlinear implementations of the invariance test (Algorithm~\\ref{alg:var_test}) may lead to erroneously accepted invariant pairs. In addition to this, the complexity of training a nonlinear model for all possible (k,S) pairs can be high. Finally, the effects of model misspecification can be challenging to analyze."
                                                                },
                                                                {
                                                                    "leaf id": 70,
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/tit",
                                                                    "block type": "section",
                                                                    "content": "Experiments"
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11",
                                                                    "block_type": "sec",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 71,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/par0",
                                                                            "block type": "par",
                                                                            "content": "We provide one synthetic and two real datasets to test the effectiveness of \\textsf{bIMP} and compare with the following two baselines: (1) a binary adaptation of Method~II from~\\cite{peters2016causal} (\\textsf{ICP}), and (2) logistic regression (\\textsf{LR}). While we do not expect \\textsf{LR} to perform well on unknown environments, it serves as a natural baseline. While \\textsf{ICP} can handle the binary response setting via logistic regression, \\textsf{SR} is specific to regression settings and thus not reported. In all experiments, we set \u03b1 = 0.1."
                                                                        },
                                                                        {
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/center1",
                                                                            "block_type": "center",
                                                                            "children": [
                                                                                {
                                                                                    "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/center1/figure0",
                                                                                    "block_type": "figure",
                                                                                    "children": [
                                                                                        {
                                                                                            "leaf id": 72,
                                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/center1/figure0/par0",
                                                                                            "block type": "par",
                                                                                            "content": "[h!]"
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        },
                                                                        {
                                                                            "leaf id": 73,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/par2",
                                                                            "block type": "par",
                                                                            "content": "As there is some degree of freedom in selecting how the sub-models in \\textsf{bIMP} are trained, we explore two variants of \\textsf{bIMP}: \\textsf{bIMP} (linear) and \\textsf{bIMP} (GAM). For both variants, we follow the invariance test in Algorithm~\\ref{alg:var_test} and estimate g_1 and g_0 using OLS. We estimate __e[X_k|X_S] using OLS for \\textsf{bIMP} (linear), and a GAM for \\textsf{bIMP} (GAM)."
                                                                        },
                                                                        {
                                                                            "leaf id": 74,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/par3",
                                                                            "block type": "par",
                                                                            "content": "Simulation results on both accuracy and mean squared error (MSE) indicate that \\textsf{bIMP} can generalize to the test environment while \\textsf{LR} and \\textsf{ICP} are not (Fig~\\ref{fig:sim}). In addition, \\textsf{bIMP} (linear) slightly outperforms \\textsf{bIMP} (GAM). While we expect \\textsf{LR} to behave poorly, \\textsf{ICP} also performs poorly as all parents of Y are intervened in every simulation."
                                                                        },
                                                                        {
                                                                            "leaf id": 75,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/center4",
                                                                            "block type": "center",
                                                                            "content": "\\begin{table}[h!]  {\\small \\begin{tabular}{ |c||c|c|c|c|  } \\hline    & \\textsf{bIMP} (linear) & \\textsf{bIMP} (GAM)  & \\textsf{LR}\\\\   \\hline  \\hline  Environment & \\multicolumn{3}{|c|}{Accuracy} \\\\  \\hline  born in US & 85.0 & 84.9 & 78.2 \\\\  overtime   & 68.4 & 59.1 & 77.0 \\\\  caucasian  & 85.0 & 85.2 & 78.1 \\\\  \\hline               \\end{tabular} }  census: performance and training environments.\\label{tbl::census}"
                                                                        },
                                                                        {
                                                                            "leaf id": 76,
                                                                            "key": "doc/body/sec0/sec3/sec9/sec18/sub8/sec8/sec11/center5",
                                                                            "block type": "center",
                                                                            "content": ""
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 77,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "N.~Meinshausen, A.~Hauser, J.~M. Mooij, J.~Peters, P.~Versteeg, and P.~B{\\\"u}hlmann, ``Methods for causal inference from gene perturbation experiments and validation,'' Proceedings of the National Academy of Sciences, vol. 113, no.~27, pp. 7361--7368, 2016."
        },
        {
            "leaf id": 78,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "A.~V. Goddard, Y.~Xiang, and C.~J. Bryan, ``Invariance-based causal prediction to identify the direct causes of suicidal behavior,'' Frontiers in psychiatry, p. 2598, 2022."
        },
        {
            "leaf id": 79,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "T.~Haavelmo, ``The probability approach in econometrics,'' Econometrica: Journal of the Econometric Society, vol.~12, pp. 1--115, 1944."
        },
        {
            "leaf id": 80,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "J.~Aldrich, ``Autonomy,'' Oxford Economic Papers, vol.~41, no.~1, pp. 15--34, 1989."
        },
        {
            "leaf id": 81,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "K.~D. Hoover, ``The logic of causal inference: Econometrics and the conditional analysis of causation,'' Economics \\& Philosophy, vol.~6, no.~2, pp. 207--234, 1990."
        },
        {
            "leaf id": 82,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "B.~Sch{\\\"o}lkopf, D.~Janzing, J.~Peters, E.~Sgouritsa, K.~Zhang, and J.~Mooij, ``On causal and anticausal learning,'' arXiv preprint arXiv:1206.6471, 2012."
        },
        {
            "leaf id": 83,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "A.~P. Dawid and V.~Didelez, ``Identifying the consequences of dynamic treatment strategies: A decision-theoretic overview,'' Statistics Surveys, vol.~4, pp. 184--231, 2010."
        },
        {
            "leaf id": 84,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "J.~Pearl, Causality.\\hskip 1em plus 0.5em minus 0.4em\\relax Cambridge university press, 2009."
        },
        {
            "leaf id": 85,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "J.~Peters, P.~B{\\\"u}hlmann, and N.~Meinshausen, ``Causal inference by using invariant prediction: identification and confidence intervals,'' Journal of the Royal Statistical Society. Series B (Statistical Methodology), pp. 947--1012, 2016."
        },
        {
            "leaf id": 86,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "C.~Heinze-Deml, J.~Peters, and N.~Meinshausen, ``Invariant causal prediction for nonlinear models,'' Journal of Causal Inference, vol.~6, no.~2, 2018."
        },
        {
            "leaf id": 87,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "N.~Pfister, P.~B{\\\"u}hlmann, and J.~Peters, ``Invariant causal prediction for sequential data,'' Journal of the American Statistical Association, vol. 114, no. 527, pp. 1264--1276, 2019."
        },
        {
            "leaf id": 88,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "M.~Rojas-Carulla, B.~Sch{\\\"o}lkopf, R.~Turner, and J.~Peters, ``Invariant models for causal transfer learning,'' The Journal of Machine Learning Research, vol.~19, no.~1, pp. 1309--1342, 2018."
        },
        {
            "leaf id": 89,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "D.~Rothenh{\\\"a}usler, N.~Meinshausen, P.~B{\\\"u}hlmann, and J.~Peters, ``Anchor regression: Heterogeneous data meet causality,'' Journal of the Royal Statistical Society Series B: Statistical Methodology, vol.~83, no.~2, pp. 215--246, 2021."
        },
        {
            "leaf id": 90,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "N.~Pfister, E.~G. Williams, J.~Peters, R.~Aebersold, and P.~B{\\\"u}hlmann, ``Stabilizing variable selection and regression,'' The Annals of Applied Statistics, vol.~15, no.~3, pp. 1220--1246, 2021."
        },
        {
            "leaf id": 91,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "K.~Du and Y.~Xiang, ``Learning invariant representations under general interventions on the response,'' IEEE Journal on Selected Areas in Information Theory, 2023."
        },
        {
            "leaf id": 92,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "------, ``Generalized invariant matching property via lasso,'' in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE, 2023."
        },
        {
            "leaf id": 93,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "A.~Goddard, Y.~Xiang, and I.~Soloveychik, ``Error probability bounds for invariant causal prediction via multiple access channels,'' Asilomar Conference on Signals, Systems, and Computers, 2023."
        },
        {
            "leaf id": 94,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "B.~Becker and R.~Kohavi, ``{Adult},'' UCI Machine Learning Repository, 1996, {DOI}: https://doi.org/10.24432/C5XW20."
        },
        {
            "leaf id": 95,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "``{Mushroom},'' UCI Machine Learning Repository, 1987, {DOI}: https://doi.org/10.24432/C5959T."
        }
    ]
}