{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "Efficient Transformer Encoders for Mask2Former-style models"
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "Manyi Yao\\inst{2 \\and \t\tAbhishek Aich\\inst{1} \\and \t\tYumin Suh\\inst{1} \\and Amit Roy-Chowdhury\\inst{2} \\and \\\\Christian Shelton\\inst{2}\\and Manmohan Chandraker\\inst{1,3} } \t \t"
        },
        {
            "leaf id": 2,
            "key": "doc/aut1",
            "block type": "author",
            "content": "\\authorrunning{M.~Yao et al.} \t \t\\institute{NEC Laboratories, America, San Jose CA 95110, USA  \t\t\\and \t\tUniversity of California, Riverside, CA 92521, USA \t\t\\and \t\tUniversity of California, San Diego, CA 92093, USA\\\\ \t\tCorresponding author: aaich@nec-labs.com"
        },
        {
            "leaf id": 3,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "     Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices.  One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current one-size-fits-all approach. To this end, we introduce \\ours or EffiCient TransfOrmer Encoders for Mask2Former-style models. Noting that the encoder module of M2F-style models incur high resource-intensive computations, \\ours provides a strategy to self-select the number of hidden layers in the encoder, conditioned on the input image. To enable this self-selection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The \\textit{first} step is to train the parent architecture to enable early exiting from the encoder. The \\textit{second} step is to create an derived dataset of the ideal number of encoder layers required for each training example. The \\textit{third} step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on input image. Additionally, to change the computational-accuracy trade-off, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.  "
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "leaf id": 4,
                    "key": "doc/body/sec0/tit",
                    "block type": "section",
                    "content": "Introduction"
                },
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec0/par0",
                            "block type": "par",
                            "content": "With the advent of powerful \\textit{universal} image segmentation architectures \\cite{cheng2021per, cheng2021mask2former, jain2023oneformer, gu2024dataseg}, it is highly desirable to prioritize the computational efficiency of these architectures for their enhanced scalability, \\eg, use on resource-limited edge devices. These architectures are extremely useful in tackling instance \\cite{he2017mask}, semantic \\cite{tu2008auto}, and panoptic \\cite{kirillov2019panoptic} segmentation tasks using one generalized architecture, owing to the transformer-based \\cite{vaswani2017attention} modules. These universal architectures leverage DEtection TRansformers or DETR-style \\cite{carion2020end} modules and represent both \\textit{stuff} and \\textit{things} categories \\cite{kirillov2019panoptic} using general feature tokens. This is an incredible advantage over preceding segmentation methods \\cite{sun2023remax, hu2023you, xu2024rap} in literature that require careful considerations in design specifications. Hence, these segmentation architectures reduce the need for task-specific choices which favor performance of one task over the other \\cite{cheng2021mask2former}."
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec0/par1",
                            "block type": "par",
                            "content": "State-of-the-art models for universal segmentation like Mask2former (M2F) \\cite{cheng2021mask2former} are built on the key idea inspired from DETR: ``mask'' classification is versatile enough to address both semantic- and instance-level segmentation tasks. However, the problem of efficient M2F-style architectures have been under-explored. With backbone architectures (\\eg, Resnet-50 \\cite{he2016deep}, SWIN-Tiny \\cite{liu2021swin}), \\cite{li2023lite} showed that DETR-style models incur the highest computations from the transformer encoder due to maintaining full length token representations from multi-scale backbone features. While existing works like \\cite{li2023lite, lv2023detrs} primarily focus on scaling the input token to improve efficiency, this approach often neglects other aspects of model optimization and leads to a ``one-size-fits-all'' solution (Figure~\\ref{fig:teaser_a}). This limitation leaves significant room for further efficiency improvements."
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec0/par2",
                            "block type": "par",
                            "content": "Given this growing importance of M2F-style architectures and indispensable need for efficiency for real-world deployment, we introduce \\ours or `EffiCient TransfOrmer Encoders' for M2F-style architectures. Our key idea  comes from our observation made on the training set of COCO \\cite{lin2014microsoft} and Cityscapes \\cite{cordts2016cityscapes} dataset demonstrated in \\Figref{fig:teaser_b}. We plot a histogram of the number of hidden encoder layers that produces the best panoptic segmentation quality \\cite{kirillov2019panoptic} for each image. It can be seen that not all images require the use of all  hidden layers of the transformer encoder in order to achieve the maximum panoptic segmentation quality \\cite{kirillov2019panoptic}. With this insight, we propose to create a dynamic transformer encoder that economically uses the hidden layers, guided by a gating network that can select different depths for different images."
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec0/par3",
                            "block type": "par",
                            "content": "To achieve the aforementioned ability, \\ours leverages the well-studied early exiting strategy \\cite{tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive} to create stochastic depths for the transformer encoder to improve inference efficiency. Previous exit mechanisms have primarily relied on confidence scores or uncertainty scores, typically applied in classification tasks. However, implementing such mechanisms in our context would necessitate the inclusion of a decoder and a prediction head to generate a reliable confidence score. This additional complexity introduces a significant number of FLOPs, rendering it impractical for our purposes. By contrast, \\ours provides a three-step training recipe that can be used to customize the transformer encoder on the fly given the input image. Step \\stepA involves training the parent model to \\textit{be dynamic} by allowing stochastic depths at the transformer encoder. Using the fact that the transformer encoder maintains the token length of the input throughout the hidden layers  constant,  Step \\stepB involves creating a \\textit{Derived} dataset from the training dataset whose each sample contains a pair of image and layer number that provides the highest segmentation quality. Finally, Step \\stepC involves training a \\textit{Gating Network} using the derived dataset, whose function is to decide the number of layers to be used given the input image."
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec0/par4",
                            "block type": "par",
                            "content": "The key contributions of \\ours are multifold. \\textit{First}, given a trained M2F-style architecture, \\ours fine-tunes the model into one that allows the ability to randomly exit from the encoder by leveraging the fact that the token length remains constant in the hidden layers. \\textit{Second}, it introduces an accessory to the parent architecture \\via the Gating network that provides the ability to smartly use the encoder layers. Using this module, \\ours enables the parent architecture to  decide the optimal amount of layers for the given input without any performance degradation as well as any confidence threshold (unlike prior early exiting strategies). \\textit{Third}, as a result of our Gating network module's training strategy, \\ours can adapt the parent architecture to varying computational budgets using \\textit{only} Step \\stepC. On COCO \\cite{lin2014microsoft} dataset, the computational cost of Step \\stepB and \\stepC are only \u223c6\\% and \u223c2.5\\% of Step \\stepA cost, respectively. \\textit{Finally}, \\ours can also incorporate recent advances \\cite{li2023lite} in making transformer encoder efficient using token length scaling, bringing best of the both methods in pushing the limits of the efficiency. To summarize, we make the following contributions:"
                        },
                        {
                            "key": "doc/body/sec0/itemize5",
                            "block_type": "itemize",
                            "children": [
                                {
                                    "leaf id": 10,
                                    "key": "doc/body/sec0/itemize5/par0",
                                    "block type": "par",
                                    "content": "[topsep=0.0em,leftmargin=*]"
                                }
                            ]
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec0/sec6/tit",
                            "block type": "section",
                            "content": "Related Works"
                        },
                        {
                            "key": "doc/body/sec0/sec6",
                            "block_type": "sec",
                            "children": [
                                {
                                    "leaf id": 12,
                                    "key": "doc/body/sec0/sec6/sec0/tit",
                                    "block type": "section",
                                    "content": "Proposed Methodology: \\ours"
                                },
                                {
                                    "key": "doc/body/sec0/sec6/sec0",
                                    "block_type": "sec",
                                    "children": [
                                        {
                                            "leaf id": 13,
                                            "key": "doc/body/sec0/sec6/sec0/itemize0",
                                            "block type": "itemize",
                                            "content": "\\item a backbone (\u00b7) which takes the -th image  as input to generate multi-scale feature maps (), represented as _1, _2, _3, _4. These multi-scale feature maps correspond to spatial resolutions typically set at 1/32, 1/16, 1/8, and 1/4 of the original image size, respectively.     \\item a transformer encoder (called the ``pixel decoder'' \\cite{cheng2021mask2former}), which is composed of multiple layers of transformer encoders. The function of this module is to generate rich token representation from {_1, _2, _3} and generate per-pixel embeddings from _4. Each layer in the transformer encoder, denoted as _(\u00b7) (where \u2208{1,2,\u2026,}) is successively applied to (), with _(\u00b7) being the last layer in the transformer encoder.      \\item a transformer decoder (along with a segmentation head) that takes two inputs: the output of the transformer encoder and the object queries. The object queries are decoded to output a binary mask along with the corresponding class label."
                                        },
                                        {
                                            "leaf id": 14,
                                            "key": "doc/body/sec0/sec6/sec0/par1",
                                            "block type": "par",
                                            "content": "For brevity, we collectively refer to the operations in the transformer decoder and segmentation head together as (\u00b7). Thus, the output of the meta-architecture with K encoder layers (a predicted mask _ and corresponding label \u2113\u0303_) can be written as"
                                        },
                                        {
                                            "leaf id": 15,
                                            "key": "doc/body/sec0/sec6/sec0/equation2",
                                            "block type": "equation",
                                            "content": "\\{\\outMF_\\NGlayer\\superIdx, \\tilde{\\ell}_\\NGlayer\\superIdx\\} = h\\circ f_\\NGlayer\\circ\\cdots\\circ f_2\\circ f_1\\circ b(\\inImg\\superIdx)\\,."
                                        },
                                        {
                                            "leaf id": 16,
                                            "key": "doc/body/sec0/sec6/sec0/par3",
                                            "block type": "par",
                                            "content": "Here, the operation \u2218 represents function composition, \\eg, g\u2218 f(x) = g(f(x)) and subscript denotes output predicted using K encoder layers. With {, \u2113} as the pair of ground truth segmentation map and corresponding label of image , the final loss \\cite{cheng2021mask2former} is computed as"
                                        },
                                        {
                                            "leaf id": 17,
                                            "key": "doc/body/sec0/sec6/sec0/align4",
                                            "block type": "align",
                                            "content": "\\loss_\\NGlayer = \\lambda_{mask}\\loss_{mask}(\\outMF_\\NGlayer\\superIdx, \\gtseg\\superIdx) + \\lambda_{class}\\loss_{class}(\\tilde{\\ell}_\\NGlayer\\superIdx, \\ell\\superIdx )\\,,     \\label{eq:orig_loss}"
                                        },
                                        {
                                            "leaf id": 18,
                                            "key": "doc/body/sec0/sec6/sec0/par5",
                                            "block type": "par",
                                            "content": "where _mask(\u00b7,\u00b7) is a binary mask loss and _class(\u00b7,\u00b7) is the corresponding classification loss. \u03bb_mask and \u03bb_class represent the associated loss weights."
                                        },
                                        {
                                            "leaf id": 19,
                                            "key": "doc/body/sec0/sec6/sec0/enumerate6",
                                            "block type": "enumerate",
                                            "content": "\\item Model suitability for early exiting. Traditional early exiting techniques \\cite{tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive} often face challenges in maintaining satisfactory performance levels at potential exit points throughout the neural network. We recognize the importance of a model architecture that not only allows for early exiting but also ensures that the performance remains consistently high. Therefore, we aim to develop a model that not only permits early exits but also for which the accuracy steadily improves as the network delves deeper into its architecture. By prioritizing this aspect, we seek to establish a framework where early exiting does not compromise the overall performance of the model.          \\item Efficient and effective gating network for optimal exit decision making. The efficacy of an early exiting strategy heavily depends on the ability to make informed exit decisions. A gating network must strike a delicate balance, minimizing computational overhead while effectively identifying components that can be bypassed without compromising accuracy. Our objective is to design a lightweight yet powerful gating mechanism capable of discerning optimal exit points within the model architecture.          \\item Dynamic control mechanism for cost-performance trade-off. We require a mechanism with the ability to adaptively regulate the balance between computational cost and performance according to user-defined priorities. Such a mechanism empowers the model to exit at the optimal layer based on specific needs and desired outcomes, ensuring efficient resource allocation and maximizing utility in various application scenarios, particularly in resource-constrained environments like edge computing or real-time applications."
                                        },
                                        {
                                            "leaf id": 20,
                                            "key": "doc/body/sec0/sec6/sec0/par7",
                                            "block type": "par",
                                            "content": "Driven by these considerations, {\\ours} offers a novel training process that enables an adaptive early exiting mechanism designed to bolster computational efficiency while preserving satisfactory model accuracy. For better understanding, we'll begin with a general overview of model training and inference before diving into the specific details of our training process."
                                        },
                                        {
                                            "key": "doc/body/sec0/sec6/sec0/enumerate8",
                                            "block_type": "enumerate",
                                            "children": [
                                                {
                                                    "leaf id": 21,
                                                    "key": "doc/body/sec0/sec6/sec0/enumerate8/par0",
                                                    "block type": "par",
                                                    "content": "[label={\u2219}, leftmargin=*]     \\item Step \\stepA: Train parent model for early exit via the transformer encoder."
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 22,
                                            "key": "doc/body/sec0/sec6/sec0/par9",
                                            "block type": "par",
                                            "content": "We refer to Step \\stepA and \\stepB together as \\textit{model pre-processing} and Step \\stepC as \\textit{model adaptation}. The former is required only once, whereas the latter is repeated as per user requirements. All these steps use the training data subset."
                                        },
                                        {
                                            "leaf id": 23,
                                            "key": "doc/body/sec0/sec6/sec0/par10",
                                            "block type": "par",
                                            "content": "During inference, the gating network guides the parent model by selecting the optimal exit point based on features extracted from the backbone with just one forward pass for final predictions."
                                        },
                                        {
                                            "leaf id": 24,
                                            "key": "doc/body/sec0/sec6/sec0/sub11/tit",
                                            "block type": "subsection",
                                            "content": "Step \\stepA: Training the Model with Weighted Stochastic Depth"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec6/sec0/sub11",
                                            "block_type": "sub",
                                            "children": [
                                                {
                                                    "leaf id": 25,
                                                    "key": "doc/body/sec0/sec6/sec0/sub11/par0",
                                                    "block type": "par",
                                                    "content": "In this step, we enable the model to allow exiting at the encoder. To maintain consistently high performance at each exit point, we input each stochastic depth's output to a shared transformer decoder. We then apply \\Eqref{eq:orig_loss} to compute the loss _ for each exit point . However, we observe that direct training in this fashion does not encourage the model to use fewer layers to extract and prioritize informative representations, as shown in \\Tabref{tab:loss}. To address this, we introduce a set of coefficients _ to emphasize the quality of representations at later layers more, enabling earlier layers to also concentrate on producing effective intermediate representations. As the layer depth increases, the corresponding coefficient _ grows, ensuring a progressively stricter standard for feature quality. The new loss function is then expressed as"
                                                },
                                                {
                                                    "leaf id": 26,
                                                    "key": "doc/body/sec0/sec6/sec0/sub11/equation1",
                                                    "block type": "equation",
                                                    "content": "\\label{eq:loss}     \\loss_{total} = \\frac{1}N\\sum_\\idx^N\\sum_{\\Glayer}^\\NGlayer \\lossCoef_\\Glayer\\loss_\\Glayer,\\,\\,{where}\\ \\forall \\Glayer<\\Glayer', \\lossCoef_\\Glayer<\\lossCoef_{\\Glayer'}\\,,"
                                                },
                                                {
                                                    "leaf id": 27,
                                                    "key": "doc/body/sec0/sec6/sec0/sub11/par2",
                                                    "block type": "par",
                                                    "content": "where N is the number of images in the training set, and _ is from \\Eqref{eq:orig_loss}."
                                                },
                                                {
                                                    "leaf id": 28,
                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/tit",
                                                    "block type": "subsection",
                                                    "content": "Step \\stepB: Deriving the Gating Network Training Dataset"
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3",
                                                    "block_type": "sub",
                                                    "children": [
                                                        {
                                                            "leaf id": 29,
                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/par0",
                                                            "block type": "par",
                                                            "content": "To facilitate informed exit decisions during inference, our approach is to train a gating network to learn optimal exit strategies. In this step, we facilitate this gating network training by first deriving an intermediate dataset."
                                                        },
                                                        {
                                                            "leaf id": 30,
                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/par1",
                                                            "block type": "par",
                                                            "content": "To this end, we record the performance of the pre-trained stochastic depth model (obtained from Step \\stepA) at all potential exit points for each image within the training dataset and create a \\textit{Derived} dataset . Specifically, we associate the -th input image  with a vector  of length . Each element _ of  represents the predicted panoptic quality \\cite{kirillov2019panoptic} upon exiting at the encoder layer . Hence, each sample of  can be represented as (, ) \u2208."
                                                        },
                                                        {
                                                            "leaf id": 31,
                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/tit",
                                                            "block type": "subsection",
                                                            "content": "Step \\stepC: Training for Gating Network"
                                                        },
                                                        {
                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2",
                                                            "block_type": "sub",
                                                            "children": [
                                                                {
                                                                    "leaf id": 32,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/par0",
                                                                    "block type": "par",
                                                                    "content": "In this step, we train the gating network on dataset  (obtained from Step \\stepB) to self-select the number of encoder layers based on the input image. Ideally, this module should allow exiting at the encoder layer which would result in the highest quality segmentation map. With this in mind, we first establish the target exit for the gating network. Note that the panoptic quality generally increases with increasing encoder layers (see \\Figref{fig:PQ_vs_layers}). However, we would like the gating network to prioritize increasing the panoptic quality while also reducing the number of layers (to reduce the overall computations).   Consequently, we introduce a utility function expressed as the linear combination of segmentation quality and the depth of the network. This function is formulated as"
                                                                },
                                                                {
                                                                    "leaf id": 33,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/equation1",
                                                                    "block type": "equation",
                                                                    "content": "\\uF(\\Glayer) = q_\\Glayer\\superIdx - \\GPRatio\\Glayer\\,,     \\label{eq:utility_func}"
                                                                },
                                                                {
                                                                    "leaf id": 34,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/par2",
                                                                    "block type": "par",
                                                                    "content": "where  serves as an \\textit{adaptation factor} governing the trade-off between segmentation quality and computational cost. Clearly, a higher value of  signifies a greater emphasis on efficiency over segmentation quality. Using \\Eqref{eq:utility_func}, we determine a target exit point  for each image  using"
                                                                },
                                                                {
                                                                    "leaf id": 35,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/equation3",
                                                                    "block type": "equation",
                                                                    "content": "\\label{eq:target}     \\tgtIdx = \\argmax_\\Glayer (\\uF(\\Glayer))\\,."
                                                                },
                                                                {
                                                                    "leaf id": 36,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/par4",
                                                                    "block type": "par",
                                                                    "content": "With a target designated for each image using \\Eqref{eq:target}, the gating decision can be approached as a straightforward classification problem. The gating architecture consists of a pooling operation (\u00b7) on the token length dimension followed by a linear layer with weights . Its output logits can be represented as"
                                                                },
                                                                {
                                                                    "leaf id": 37,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/align5",
                                                                    "block type": "align",
                                                                    "content": "\\label{eq:gating}     \\Goutput\\superIdx &= \\mW \\vz(\\feature_1\\superIdx)\\,."
                                                                },
                                                                {
                                                                    "leaf id": 38,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/par6",
                                                                    "block type": "par",
                                                                    "content": "In consideration of having minimal impact on the computations due to the gating network, we use the output of the lowest resolution feature map _1 as input to the pooling operation. To optimize the gating network, we use the standard cross-entropy loss between the output logits  and the one-hot version of target exit  as our training objective. During inference, the gating network identifies the layer with the highest predicted logits, \\ie, _(g_), as the optimal exit layer for image . Note that while there can be more complex choices for the gating network, our simple linear layer in \\Eqref{eq:gating} works well in our experiments."
                                                                },
                                                                {
                                                                    "leaf id": 39,
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/tit",
                                                                    "block type": "subsection",
                                                                    "content": "Inference"
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7",
                                                                    "block_type": "sub",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 40,
                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/par0",
                                                                            "block type": "par",
                                                                            "content": "In the inference phase, the gating network guides the parent model toward an optimal exit point tailored to each input image. Similar to the training phase, the gating mechanism receives low-resolution features from the backbone and produces a vector of length  for each image. The value of  remains consistent with that determined in Step \\stepC. Subsequently, the gating network identifies the layer with the highest predicted logits as the optimal exit layer for each image. The parent model adheres to this decision, exiting at the determined layer, and subsequently progresses through the subsequent components to make the final prediction. This dynamic process ensures that the model adaptively selects the most optimal layer for exit during inference, enhancing its efficiency in handling diverse input data."
                                                                        },
                                                                        {
                                                                            "leaf id": 41,
                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/tit",
                                                                            "block type": "section",
                                                                            "content": "Experiments"
                                                                        },
                                                                        {
                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1",
                                                                            "block_type": "sec",
                                                                            "children": [
                                                                                {
                                                                                    "leaf id": 42,
                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/par0",
                                                                                    "block type": "par",
                                                                                    "content": "We follow the evaluation setting of \\cite{cheng2021mask2former} for evaluation of ``universal'' segmentation, \\ie, we train the model solely with panoptic segmentation annotations but evaluate it for panoptic, semantic, and instance segmentation tasks. We use the standard PQ (Panoptic Quality \\cite{kirillov2019panoptic}) metric to evaluate panoptic segmentation performance. We report AP_p (Average Precision \\cite{lin2014microsoft}) computed across all categories for instance segmentation, and mIOU_p(mean Intersection over Union \\cite{everingham2015pascal}) for semantic segmentation by merging instance masks from the same category. The subscript p denotes that these metrics are computed for the model trained solely with panoptic segmentation annotations. In terms of computational cost, we use GFLOPs calculated as the average GFLOPs across all validation images. All models are trained on the \\textit{train} split and evaluated on the \\textit{validation} split."
                                                                                },
                                                                                {
                                                                                    "leaf id": 43,
                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/par1",
                                                                                    "block type": "par",
                                                                                    "content": "We use Detectron2 \\cite{wu2019detectron2} and PyTorch\\cite{PyTorch} for our implementation. For the stochastic depth training phase (Step \\stepA), we initialize weights as provided by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations for Cityscapes, with a batch size of 16. For the training of the gating network (Step \\stepC), we perform 2 epochs of training on the COCO dataset and 20k iterations on the Cityscapes dataset, employing the Adam optimizer \\cite{kingma2017adam}. The adaptation factor  in the utility function, as discussed in \\Secref{sec:gating}, is set to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed training is performed using 8 A6000 GPUs. On the COCO dataset, the training time of Step \\stepA is 280 GPU hours, Step \\stepB is 17 GPU hours, and Step \\stepC 7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step \\stepA is 45 GPU hours, Step \\stepB is 1 GPU hours, and Step \\stepC is 7.2 GPU hours. In Step \\stepA, we use identical settings as M2F for the loss between the predicted segment and ground truth segment, \\ie, _. The weight \u03bb_mask is fixed at 5.0, while \u03bb_class is set to 2.0 for all classes, except 0.1 for the ``no object'' class."
                                                                                },
                                                                                {
                                                                                    "leaf id": 44,
                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/tit",
                                                                                    "block type": "subsection",
                                                                                    "content": "Main Results"
                                                                                },
                                                                                {
                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2",
                                                                                    "block_type": "sub",
                                                                                    "children": [
                                                                                        {
                                                                                            "leaf id": 45,
                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/par0",
                                                                                            "block type": "par",
                                                                                            "content": "In \\tabref{tab:result_coco} and \\tabref{tab:result_cityscapes}, we compare {\\ours} with our baseline prior works on the validation set of COCO and Cityscapes dataset, respectively. In \\tabref{tab:result_coco}, we observe that {\\ours} effectively reduces computational costs while upholding performance levels in comparison to M2F \\cite{cheng2021mask2former} using both SWIN-T \\cite{liu2021swin} and Res50 \\cite{he2016deep} backbones. Additionally, {\\ours} can be seamlessly integrated into efficient encoder designs, such as Lite-M2F \\cite{cheng2021mask2former}\\cite{li2023lite}, further reducing GLOPs by approximately 12.6\\%. With Res50 as the backbone, MF \\cite{cheng2021per}, YOSO \\cite{hu2023you}, and RAP-SAM \\cite{xu2024rap} exhibit inferior performance compared to {\\ours}. Although ReMax \\cite{sun2023remax} demonstrates competitive accuracy, its focus on specialized panoptic segmentation models limits its applicability. Our work, however, aims for a broader impact by creating efficient segmentation architectures that can be used for various segmentation tasks. We make similar observations on the Cityscapes dataset as presented in \\tabref{tab:result_cityscapes}."
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 46,
                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/tit",
                                                                                            "block type": "subsection",
                                                                                            "content": "Ablation Studies"
                                                                                        },
                                                                                        {
                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1",
                                                                                            "block_type": "sub",
                                                                                            "children": [
                                                                                                {
                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/itemize0",
                                                                                                    "block_type": "itemize",
                                                                                                    "children": [
                                                                                                        {
                                                                                                            "leaf id": 47,
                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/itemize0/par0",
                                                                                                            "block type": "par",
                                                                                                            "content": "[topsep=0.0em,leftmargin=*]"
                                                                                                        }
                                                                                                    ]
                                                                                                },
                                                                                                {
                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/itemize1",
                                                                                                    "block_type": "itemize",
                                                                                                    "children": [
                                                                                                        {
                                                                                                            "leaf id": 48,
                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/itemize1/par0",
                                                                                                            "block type": "par",
                                                                                                            "content": "[topsep=0.0em,leftmargin=*]"
                                                                                                        }
                                                                                                    ]
                                                                                                },
                                                                                                {
                                                                                                    "leaf id": 49,
                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/tit",
                                                                                                    "block type": "subsection",
                                                                                                    "content": "Qualitative Comparisons"
                                                                                                },
                                                                                                {
                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2",
                                                                                                    "block_type": "sub",
                                                                                                    "children": [
                                                                                                        {
                                                                                                            "leaf id": 50,
                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/par0",
                                                                                                            "block type": "par",
                                                                                                            "content": "We present a few examples of predicted segmentation maps in \\Figref{fig:qual_result} with SWIN-T \\cite{liu2021swin} backbone. Compared to the parent architecture, \\ours consistently shows strong performance while self-selecting the encoder layers based on the input examples, both in everyday scenes (on COCO dataset) as well as intricate traffic scenes (on Cityscapes dataset)."
                                                                                                        },
                                                                                                        {
                                                                                                            "leaf id": 51,
                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/tit",
                                                                                                            "block type": "section",
                                                                                                            "content": "Conclusions"
                                                                                                        },
                                                                                                        {
                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1",
                                                                                                            "block_type": "sec",
                                                                                                            "children": [
                                                                                                                {
                                                                                                                    "leaf id": 52,
                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/par0",
                                                                                                                    "block type": "par",
                                                                                                                    "content": "In this paper, we propose an efficient transformer encoder design {\\ours} for the Mask2Former-style frameworks. \\ours provides a three-step training recipe that can be used to customize the transformer encoder on the fly given the input image. The first step involves training the parent model to \\textit{be dynamic} by allowing stochastic depths at the transformer encoder. The second step involves creating a derived dataset from the training dataset which contains a pair of image and layer number that provides the highest segmentation quality. Finally, the third step involves training a gating network, whose function is to decide the number of layers to be used given the input image. Extensive experiments demonstrate that {\\ours} achieves significantly reduced computational complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\\ours}'s ability to dynamically trade-off between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks."
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 53,
                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/center1",
                                                                                                                    "block type": "center",
                                                                                                                    "content": "\\Large{Efficient Transformer Encoders for Mask2Former-style models \\\\ (Supplementary Material)} \t\t\\vspace*{2em}"
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 54,
                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/tit",
                                                                                                                    "block type": "section",
                                                                                                                    "content": "Additional Experiments"
                                                                                                                },
                                                                                                                {
                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2",
                                                                                                                    "block_type": "sec",
                                                                                                                    "children": [
                                                                                                                        {
                                                                                                                            "leaf id": 55,
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/par0",
                                                                                                                            "block type": "par",
                                                                                                                            "content": "First, we consider using cross-entropy loss between the output of the utility function (\u00b7) and the predicted logit passed through a softmax function (referred to as ``u-CE''), \\ie,"
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "leaf id": 56,
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/par1",
                                                                                                                            "block type": "par",
                                                                                                                            "content": "Second, we apply a softmax function to the utility function u() and use cross-entropy as the loss function (referred to as ``soft-CE''), \\ie,"
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "leaf id": 57,
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/par2",
                                                                                                                            "block type": "par",
                                                                                                                            "content": "Third, we apply a softmax function to the utility function, but use mean squared error (MSE) loss instead (referred to as ``soft-MSE''), \\ie,"
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "leaf id": 58,
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/par3",
                                                                                                                            "block type": "par",
                                                                                                                            "content": "The analysis in \\Tabref{tab:tgt_loss} is conducted using the SWIN-T \\cite{liu2021swin} backbone on the COCO dataset. We observe that ``hard-CE'' yields the most favorable results. As a result, we use this approach consistently in the main paper."
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4",
                                                                                                                            "block_type": "table",
                                                                                                                            "children": [
                                                                                                                                {
                                                                                                                                    "leaf id": 59,
                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/par0",
                                                                                                                                    "block type": "par",
                                                                                                                                    "content": "[!ht]"
                                                                                                                                },
                                                                                                                                {
                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1",
                                                                                                                                    "block_type": "minipage",
                                                                                                                                    "children": [
                                                                                                                                        {
                                                                                                                                            "leaf id": 60,
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/par0",
                                                                                                                                            "block type": "par",
                                                                                                                                            "content": "[!hb]{0.495\\textwidth}"
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1",
                                                                                                                                            "block_type": "tabular",
                                                                                                                                            "children": [
                                                                                                                                                {
                                                                                                                                                    "leaf id": 61,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par0",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "{llcccccccc}"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 62,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par1",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "& Lite-M2F \\cite{li2023lite} & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 \\\\ & Lite-{\\ours} & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\\\"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 63,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par2",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "& Lite-M2F \\cite{li2023lite} & 63.54 & 79.74 & 39.12 && 615.15 & 171.99 \\\\ & Lite-{\\ours} & 63.32 & 80.21 & 37.91 && 588.82 & 145.66 \\\\"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 64,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par3",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "& Lite-M2F \\cite{li2023lite} & 64.48 & 82.34 & 39.21 && 942.05 & 174.01 \\\\ & Lite-{\\ours} & 64.66 & 81.40 & 39.52 && 921.15 & 153.11 \\\\"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 65,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par4",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "& Lite-M2F \\cite{li2023lite} & 52.70 & 63.08 & 41.10 && 193.79 & 79.78 \\\\ & Lite-{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\\\"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 66,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/tabular1/par5",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "& Lite-M2F \\cite{li2023lite} & 54.30 & 64.81 & 43.94 && 269.26 & 74.45 \\\\   & Lite-{\\ours} & 54.47 & 64.14  & 43.55 && 258.00 & 63.96 \\\\"
                                                                                                                                                }
                                                                                                                                            ]
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "leaf id": 67,
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage1/par2",
                                                                                                                                            "block type": "par",
                                                                                                                                            "content": "}"
                                                                                                                                        }
                                                                                                                                    ]
                                                                                                                                },
                                                                                                                                {
                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2",
                                                                                                                                    "block_type": "minipage",
                                                                                                                                    "children": [
                                                                                                                                        {
                                                                                                                                            "leaf id": 68,
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2/par0",
                                                                                                                                            "block type": "par",
                                                                                                                                            "content": "[!hb]{0.495\\textwidth}"
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2/tabular1",
                                                                                                                                            "block_type": "tabular",
                                                                                                                                            "children": [
                                                                                                                                                {
                                                                                                                                                    "leaf id": 69,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2/tabular1/par0",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "{lcccccccc}"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 70,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2/tabular1/par1",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "hard-CE & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\\\ u-CE & 52.16 & 62.58 & 41.57 && 207.49 & 94.06 \\\\ soft-CE & 51.64 & 62.75 & 40.88 && 202.08 & 87.85 \\\\ soft-MSE & 51.54 & 62.73 & 40.91 && 198.46 & 84.53 \\\\"
                                                                                                                                                }
                                                                                                                                            ]
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "leaf id": 71,
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/table4/minipage2/par2",
                                                                                                                                            "block type": "par",
                                                                                                                                            "content": "}"
                                                                                                                                        }
                                                                                                                                    ]
                                                                                                                                }
                                                                                                                            ]
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "leaf id": 72,
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/tit",
                                                                                                                            "block type": "section",
                                                                                                                            "content": "Additional Qualitative Results"
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5",
                                                                                                                            "block_type": "sec",
                                                                                                                            "children": [
                                                                                                                                {
                                                                                                                                    "leaf id": 73,
                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/par0",
                                                                                                                                    "block type": "par",
                                                                                                                                    "content": "We provide additional examples of predicted segmentation maps in \\Figref{fig:supp_qual_result}."
                                                                                                                                },
                                                                                                                                {
                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/figure1",
                                                                                                                                    "block_type": "figure",
                                                                                                                                    "children": [
                                                                                                                                        {
                                                                                                                                            "leaf id": 74,
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/figure1/par0",
                                                                                                                                            "block type": "par",
                                                                                                                                            "content": "[!ht]"
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/figure1/tcbraster1",
                                                                                                                                            "block_type": "tcbraster",
                                                                                                                                            "children": [
                                                                                                                                                {
                                                                                                                                                    "leaf id": 75,
                                                                                                                                                    "key": "doc/body/sec0/sec6/sec0/sub11/sub3/sub2/sub7/sec1/sub2/sub1/sub2/sec1/sec2/sec5/figure1/tcbraster1/par0",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "[enhanced,     blank,      raster columns=3,     raster equal height,     ]     \\tcbincludegraphics[]{figures/qual_result_images/coco/exp9/000000176232.jpg}     \\tcbincludegraphics[]{figures/qual_result_images/coco/exp9/000000176232_m2f.jpg}     \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qual_result_images/coco/exp9/000000176232_ours.jpg}     \\vspace*{-2.5\\baselineskip}"
                                                                                                                                                }
                                                                                                                                            ]
                                                                                                                                        }
                                                                                                                                    ]
                                                                                                                                }
                                                                                                                            ]
                                                                                                                        }
                                                                                                                    ]
                                                                                                                }
                                                                                                            ]
                                                                                                        }
                                                                                                    ]
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 76,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Ammar, A., Khalil, M.I., Salama, C.: Rt-yoso: Revisiting yoso for real-time   panoptic segmentation. In: 2023 5th Novel Intelligent and Leading Emerging   Sciences Conference (NILES). pp. 306--311 (2023).   \\doi{10.1109/NILES59815.2023.10296714}"
        },
        {
            "leaf id": 77,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.:   End-to-end object detection with transformers. In: European conference on   computer vision. pp. 213--229. Springer (2020)"
        },
        {
            "leaf id": 78,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Caron, M., Touvron, H., Misra, I., J\\'egou, H., Mairal, J., Bojanowski, P.,   Joulin, A.: Emerging properties in self-supervised vision transformers. In:   Proceedings of the International Conference on Computer Vision (ICCV) (2021)"
        },
        {
            "leaf id": 79,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:   Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic   segmentation. In: Proceedings of the IEEE/CVF conference on computer vision   and pattern recognition. pp. 12475--12485 (2020)"
        },
        {
            "leaf id": 80,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.:   Masked-attention mask transformer for universal image segmentation. In: CVPR   (2022)"
        },
        {
            "leaf id": 81,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Cheng, B., Schwing, A., Kirillov, A.: Per-pixel classification is not all you   need for semantic segmentation. Advances in Neural Information Processing   Systems  34,  17864--17875 (2021)"
        },
        {
            "leaf id": 82,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,   Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban   scene understanding. In: Proceedings of the IEEE conference on computer   vision and pattern recognition. pp. 3213--3223 (2016)"
        },
        {
            "leaf id": 83,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A   large-scale hierarchical image database. In: 2009 IEEE conference on computer   vision and pattern recognition. pp. 248--255. Ieee (2009)"
        },
        {
            "leaf id": 84,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Everingham, M., Eslami, S.A., Van~Gool, L., Williams, C.K., Winn, J.,   Zisserman, A.: The pascal visual object classes challenge: A retrospective.   International journal of computer vision  111,  98--136 (2015)"
        },
        {
            "leaf id": 85,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking   bisenet for real-time semantic segmentation. In: Proceedings of the IEEE/CVF   conference on computer vision and pattern recognition. pp. 9716--9725 (2021)"
        },
        {
            "leaf id": 86,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Gu, X., Cui, Y., Huang, J., Rashwan, A., Yang, X., Zhou, X., Ghiasi, G., Kuo,   W., Chen, H., Chen, L.C., et~al.: Dataseg: Taming a universal multi-dataset   multi-task segmentation model. Advances in Neural Information Processing   Systems  36 (2024)"
        },
        {
            "leaf id": 87,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "He, K., Gkioxari, G., Doll{\\'a}r, P., Girshick, R.: Mask r-cnn. In: Proceedings   of the IEEE international conference on computer vision. pp. 2961--2969   (2017)"
        },
        {
            "leaf id": 88,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image   recognition. In: Proceedings of the IEEE conference on computer vision and   pattern recognition. pp. 770--778 (2016)"
        },
        {
            "leaf id": 89,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Hou, R., Li, J., Bhargava, A., Raventos, A., Guizilini, V., Fang, C., Lynch,   J., Gaidon, A.: Real-time panoptic segmentation from dense detections. In:   Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   Recognition. pp. 8523--8532 (2020)"
        },
        {
            "leaf id": 90,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Hu, J., Huang, L., Ren, T., Zhang, S., Ji, R., Cao, L.: You only segment once:   Towards real-time panoptic segmentation. In: Proceedings of the IEEE/CVF   Conference on Computer Vision and Pattern Recognition. pp. 17819--17829   (2023)"
        },
        {
            "leaf id": 91,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One   transformer to rule universal image segmentation. In: Proceedings of the   IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.   2989--2998 (2023)"
        },
        {
            "leaf id": 92,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jiang, Z., Gong, Z., Xu, Y., Wang, J.: Multi-exit vision transformer with   custom fine-tuning for fine-grained image recognition. In: 2023 IEEE   International Conference on Image Processing (ICIP). pp. 5233--5237 (2023)"
        },
        {
            "leaf id": 93,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)"
        },
        {
            "leaf id": 94,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Kirillov, A., He, K., Girshick, R., Rother, C., Doll\u00e1r, P.: Panoptic   segmentation (2019)"
        },
        {
            "leaf id": 95,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr:   An interleaved multi-scale encoder for efficient detr. In: Proceedings of the   IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.   18558--18567 (2023)"
        },
        {
            "leaf id": 96,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,   Doll{\\'a}r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:   Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland,   September 6-12, 2014, Proceedings, Part V 13. pp. 740--755. Springer (2014)"
        },
        {
            "leaf id": 97,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin   transformer: Hierarchical vision transformer using shifted windows. In:   Proceedings of the IEEE/CVF international conference on computer vision. pp.   10012--10022 (2021)"
        },
        {
            "leaf id": 98,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Liu, Z., Sun, Y., Li, Y., Zhou, Z., Hu, J., Li, F.: Multi-exit vision   transformer for dynamic inference. In: Proceedings of the IEEE/CVF Conference   on Computer Vision and Pattern Recognition (CVPR). pp. 5214--5223 (2021)"
        },
        {
            "leaf id": 99,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu,   Y.: Detrs beat yolos on real-time object detection. arXiv preprint   arXiv:2304.08069  (2023)"
        },
        {
            "leaf id": 100,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,   T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,   DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,   Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep   learning library. In: Advances in Neural Information Processing Systems 32,   pp. 8024--8035. Curran Associates, Inc. (2019)"
        },
        {
            "leaf id": 101,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Sun, S., Wang, W., Yu, Q., Howard, A., Torr, P., Chen, L.C.: Remax: Relaxing   for better training on efficient panoptic segmentation. arXiv preprint   arXiv:2306.17319  (2023)"
        },
        {
            "leaf id": 102,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Tang, J., Liu, Z., Li, Y., Sun, Y., Zhou, Z., Hu, J., Li, F.: You need multiple   exiting: Dynamic early exiting for accelerating unified vision language   model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and   Pattern Recognition (CVPR). pp. 13504--13513 (2023)"
        },
        {
            "leaf id": 103,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Tang, S., Wang, Y., Kong, Z., Zhang, T., Li, Y., Ding, C., Wang, Y., Liang, Y.,   Xu, D.: You need multiple exiting: Dynamic early exiting for accelerating   unified vision language model. In: Proceedings of the IEEE/CVF Conference on   Computer Vision and Pattern Recognition. pp. 10781--10791 (2023)"
        },
        {
            "leaf id": 104,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Tu, Z.: Auto-context and its application to high-level vision tasks. In: 2008   IEEE Conference on Computer Vision and Pattern Recognition. pp.~1--8. IEEE   (2008)"
        },
        {
            "leaf id": 105,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Valade, F., Hebiri, M., Gay, P.: Eero: Early exit with reject option for   efficient classification with limited budget (2024)"
        },
        {
            "leaf id": 106,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,   Kaiser, {\\L}., Polosukhin, I.: Attention is all you need. Advances in neural   information processing systems  30 (2017)"
        },
        {
            "leaf id": 107,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y.,   Zhang, Q., Chowdhury, M., et~al.: Efficient large language models: A survey.   arXiv preprint arXiv:2312.03863  1 (2023)"
        },
        {
            "leaf id": 108,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Wang, X., Zhou, W., He, X., Peng, X., Wei, F., Guo, Y.: Single-layer vision   transformers for more accurate early exits with less overhead. Pattern   Recognition  136,  102243 (2022)"
        },
        {
            "leaf id": 109,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2.   \\url{https://github.com/facebookresearch/detectron2} (2019)"
        },
        {
            "leaf id": 110,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised   learning with swin transformers. arXiv preprint arXiv:2105.04553  (2021)"
        },
        {
            "leaf id": 111,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised   learning with swin transformers. arXiv preprint arXiv:2105.04553  (2021)"
        },
        {
            "leaf id": 112,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Xu, F., Zhang, X., Ma, Z., Wang, J., Hu, J., Sun, J.: Lgvit: Dynamic early   exiting for accelerating vision transformer. In: Proceedings of the 32nd ACM   International Conference on Multimedia. pp. 1958--1966 (2023)"
        },
        {
            "leaf id": 113,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Xu, J., Xiong, Z., Bhattacharyya, S.P.: Pidnet: A real-time semantic   segmentation network inspired by pid controllers. In: Proceedings of the   IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.   19529--19539 (2023)"
        },
        {
            "leaf id": 114,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, C.,   Wang, S., et~al.: A survey of resource-efficient llm and multimodal   foundation models. arXiv preprint arXiv:2401.08092  (2024)"
        },
        {
            "leaf id": 115,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Xu, S., Yuan, H., Shi, Q., Qi, L., Wang, J., Yang, Y., Li, Y., Chen, K., Tong,   Y., Ghanem, B., et~al.: Rap-sam: Towards real-time all-purpose segment   anything. arXiv preprint arXiv:2401.10228  (2024)"
        },
        {
            "leaf id": 116,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Yang, J., Zhang, X., Zhang, X., Tang, J., Li, X.: Exploiting face   recognizability with early exit vision transformers. In: 2023 IEEE   International Conference on Image Processing (ICIP). pp. 6341--6345 (2023)"
        },
        {
            "leaf id": 117,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Yu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N.: Bisenet v2: Bilateral   network with guided aggregation for real-time semantic segmentation.   International Journal of Computer Vision  129,  3051--3068 (2021)"
        },
        {
            "leaf id": 118,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral   segmentation network for real-time semantic segmentation. In: Proceedings of   the European conference on computer vision (ECCV). pp. 325--341 (2018)"
        },
        {
            "leaf id": 119,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Zhang, T., He, X., Qin, Z., Sun, J.: Adaptive deep neural network inference   optimization with eenet. In: Proceedings of the 37th International Conference   on Machine Learning. pp. 15983--15993 (2023)"
        }
    ]
}