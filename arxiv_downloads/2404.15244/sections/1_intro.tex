\section{Introduction}
\label{sec:intro}
\input{figures/teaser}

% what is the task: universal segmentation
With the advent of powerful \textit{universal} image segmentation architectures \cite{cheng2021per, cheng2021mask2former, jain2023oneformer, gu2024dataseg}, it is highly desirable to prioritize the computational efficiency of these architectures for their enhanced scalability, \eg, use on resource-limited edge devices. These architectures are extremely useful in tackling instance \cite{he2017mask}, semantic \cite{tu2008auto}, and panoptic \cite{kirillov2019panoptic} segmentation tasks using one generalized architecture, owing to the transformer-based \cite{vaswani2017attention} modules. These universal architectures leverage DEtection TRansformers or DETR-style \cite{carion2020end} modules and represent both \textit{stuff} and \textit{things} categories \cite{kirillov2019panoptic} using general feature tokens. This is an incredible advantage over preceding segmentation methods \cite{sun2023remax, hu2023you, xu2024rap} in literature that require careful considerations in design specifications. Hence, these segmentation architectures reduce the need for task-specific choices which favor performance of one task over the other \cite{cheng2021mask2former}. 

% What are the existing methods and where do they lack?
State-of-the-art models for universal segmentation like Mask2former (M2F) \cite{cheng2021mask2former} are built on the key idea inspired from DETR: ``mask'' classification is versatile enough to address both semantic- and instance-level segmentation tasks. However, the problem of efficient M2F-style architectures have been under-explored. With backbone architectures (\eg, Resnet-50 \cite{he2016deep}, SWIN-Tiny \cite{liu2021swin}), \cite{li2023lite} showed that DETR-style models incur the highest computations from the transformer encoder due to maintaining full length token representations from multi-scale backbone features. While existing works like \cite{li2023lite, lv2023detrs} primarily focus on scaling the input token to improve efficiency, this approach often neglects other aspects of model optimization and leads to a ``one-size-fits-all'' solution (Figure~\ref{fig:teaser_a}). This limitation leaves significant room for further efficiency improvements.

% What is our method and what are the components?
Given this growing importance of M2F-style architectures and indispensable need for efficiency for real-world deployment, we introduce \ours or `\textbf{E}ffi\textbf{C}ient Transf\textbf{O}rmer Encoders' for M2F-style architectures. Our key idea  comes from our observation made on the training set of COCO \cite{lin2014microsoft} and Cityscapes \cite{cordts2016cityscapes} dataset demonstrated in \Figref{fig:teaser_b}. We plot a histogram of the number of hidden encoder layers that produces the best panoptic segmentation quality \cite{kirillov2019panoptic} for each image. It can be seen that not all images require the use of all $\NGlayer$ hidden layers of the transformer encoder in order to achieve the maximum panoptic segmentation quality \cite{kirillov2019panoptic}. With this insight, we propose to create a dynamic transformer encoder that economically uses the hidden layers, guided by a gating network that can select different depths for different images.

To achieve the aforementioned ability, \ours leverages the well-studied early exiting strategy \cite{tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive} to create stochastic depths for the transformer encoder to improve inference efficiency. Previous exit mechanisms have primarily relied on confidence scores or uncertainty scores, typically applied in classification tasks. However, implementing such mechanisms in our context would necessitate the inclusion of a decoder and a prediction head to generate a reliable confidence score. This additional complexity introduces a significant number of FLOPs, rendering it impractical for our purposes. By contrast, \ours provides a three-step training recipe that can be used to customize the transformer encoder on the fly given the input image. Step \stepA involves training the parent model to \textit{be dynamic} by allowing stochastic depths at the transformer encoder. Using the fact that the transformer encoder maintains the token length of the input throughout the hidden layers  constant,  Step \stepB involves creating a \textit{Derived} dataset from the training dataset whose each sample contains a pair of image and layer number that provides the highest segmentation quality. Finally, Step \stepC involves training a \textit{Gating Network} using the derived dataset, whose function is to decide the number of layers to be used given the input image. 

% What are the key advantages of our method?
The key contributions of \ours are multifold. \textit{First}, given a trained M2F-style architecture, \ours fine-tunes the model into one that allows the ability to randomly exit from the encoder by leveraging the fact that the token length remains constant in the hidden layers. \textit{Second}, it introduces an accessory to the parent architecture \via the Gating network that provides the ability to smartly use the encoder layers. Using this module, \ours enables the parent architecture to  decide the optimal amount of layers for the given input without any performance degradation as well as any confidence threshold (unlike prior early exiting strategies). \textit{Third}, as a result of our Gating network module's training strategy, \ours can adapt the parent architecture to varying computational budgets using \textit{only} Step \stepC. On COCO \cite{lin2014microsoft} dataset, the computational cost of Step \stepB and \stepC are only $\sim$6\% and $\sim$2.5\% of Step \stepA cost, respectively. \textit{Finally}, \ours can also incorporate recent advances \cite{li2023lite} in making transformer encoder efficient using token length scaling, bringing best of the both methods in pushing the limits of the efficiency. To summarize, we make the following contributions: 
% Summarize the contributions
\begin{itemize}[topsep=0.0em,leftmargin=*]
\setlength\itemsep{0.0em}
    \item We present a dynamic transformer encoder {\ours} for M2F-style universal segmentation that maintains performance but reduces the computational load.
    % 
    \item {\ours} consists of a novel training recipe that leverages input image based early exiting (in Step \stepA), creating a derived dataset (based on training set segmentation performance in Step \stepB), which in turn is used to train a gating function (using Step \stepC) that allows adapting the number of hidden layers to reduce computations.
    %
    \item Extensive experiments show that {\ours} improves the overall performance-efficiency trade-off, and adaptable to diverse architecture settings and can be extended beyond segmentation to the detection task.
\end{itemize}