\section{Related Works}
\label{sec:related_works}
%
\paragraph{Efficient image segmentation.} With the rise of transformers \cite{vaswani2017attention}, researchers are increasingly interested in creating image segmentation models that work effectively in various settings, without requiring segmentation type specific modifications to the model itself. Building on DETR \cite{carion2020end}, multiple universal segmentation architectures were proposed \cite{cheng2021per,cheng2021mask2former, jain2023oneformer, gu2024dataseg} that use transformer decoder to predict masks for each entity in the input image. However, despite the significant progress in overall performance across various tasks, these models still face challenges in deployment on resource-constrained devices. Current emphasis \cite{cheng2020panoptic, fan2021rethinking, hou2020real, hu2023you, 10296714, xu2023pidnet, yu2018bisenet, yu2021bisenet} for efficiency for image segmentation has mostly been on specialized architectures tailored to a single segmentation task. Unlike these preceding works, \ours makes no such assumption on the segmentation task and addresses the limitation of inefficiency in M2F-style universal architectures that are task-agnostic.
%
\paragraph{Early-exiting in vision transformers.} Recent works on early exiting \cite{wan2023efficient, xu2024survey, tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive} aim to boost inference efficiency for large transformers. Some works \cite{xu2023lgvit, liu2021mevt, tang2023you} used early exiting for classification tasks along with manually chosen confidence threshold in vision transformers. For example, \cite{xu2023lgvit} proposed an early exiting framework for classification task ViTs combining heterogeneous task heads. Similarly, \cite{tang2023you} proposed an early exiting strategy for vision-language models by measuring layer-wise similarities by checking multiple times to exit early.  Applying early exiting solely to the encoder (like \cite{xu2023lgvit}) is infeasible due to the dependency on separate decoders, leading to an unacceptable optimization load. In contrast, methods like \cite{tang2023you} suffer from redundant computations for exit decisions at all possible choices, hindering efficient resource allocation. In contrast, \ours only trains one decoder for all possible exit routes, as well as uses a gating module to decide the number of encoder layers required for the model depending on the input image. 