\section{Conclusions}
In this paper, we propose an efficient transformer encoder design {\ours} for the Mask2Former-style frameworks. \ours provides a three-step training recipe that can be used to customize the transformer encoder on the fly given the input image. The first step involves training the parent model to \textit{be dynamic} by allowing stochastic depths at the transformer encoder. The second step involves creating a derived dataset from the training dataset which contains a pair of image and layer number that provides the highest segmentation quality. Finally, the third step involves training a gating network, whose function is to decide the number of layers to be used given the input image. Extensive experiments demonstrate that {\ours} achieves significantly reduced computational complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\ours}'s ability to dynamically trade-off between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks.
%
\paragraph{Limitations.} While {\ours} offers dynamic trade-offs between performance and efficiency according to specific needs, the adaptation factor $\GPRatio$ is a  hyperparameter that needs separate tuning for each use case. This is because it relies on the model configuration and dataset characteristics. 