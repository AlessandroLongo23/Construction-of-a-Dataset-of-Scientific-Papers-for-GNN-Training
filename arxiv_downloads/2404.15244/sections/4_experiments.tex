\section{Experiments}
\label{sec:exp}
\paragraph{Datasets.} Our study illustrates the adaptability of {\ours} in dynamically managing the trade-off between computation and performance based on M2F \cite{cheng2021mask2former} meta-architecture. We do this on two widely used image segmentation datasets: COCO \cite{lin2014microsoft} and Cityscapes \cite{cordts2016cityscapes}. COCO comprises 80 ``things'' and 53 ``stuff'' categories, with 118k training images and 5k validation images. Cityscapes consists of 8 ``things'' and 11 ``stuff'' categories, with approximately 3k training images and 500 validation images. The evaluation is conducted over the union of ``things'' and ``stuff'' categories.

\paragraph{Evaluation metrics.}
We follow the evaluation setting of \cite{cheng2021mask2former} for evaluation of ``universal'' segmentation, \ie, we train the model solely with panoptic segmentation annotations but evaluate it for panoptic, semantic, and instance segmentation tasks. We use the standard \textbf{PQ} (Panoptic Quality \cite{kirillov2019panoptic}) metric to evaluate panoptic segmentation performance. We report \textbf{AP}$_p$ (Average Precision \cite{lin2014microsoft}) computed across all categories for instance segmentation, and \textbf{mIOU}$_p$(mean Intersection over Union \cite{everingham2015pascal}) for semantic segmentation by merging instance masks from the same category. The subscript $p$ denotes that these metrics are computed for the model trained solely with panoptic segmentation annotations. In terms of computational cost, we use GFLOPs calculated as the average GFLOPs across all validation images. All models are trained on the \textit{train} split and evaluated on the \textit{validation} split.

\paragraph{Baseline models.} We compare \ours with two sets of efficient segmentation methods. \textit{First}, we compare with our baseline  universal segmentation architecture M2F \cite{cheng2021mask2former}. Further, we also integrate recently proposed transformer encoder designs (Lite-DETR \cite{li2023lite} and RT-DETR \cite{lv2023detrs}) for efficient object detection into M2F and named them Lite-M2F and RT-M2F, respectively. \textit{Second}, we include comparisons with recent efficient architectures that proposed task-specific components, namely YOSO \cite{hu2023you}, RAP-SAM \cite{xu2024rap}, and ReMax \cite{sun2023remax}.

\paragraph{Architecture details.} We focus on standard backbones Res50 \cite{he2016deep} and SWIN-Tiny \cite{liu2021swin} pre-trained on ImageNet-1K \cite{deng2009imagenet}, unless specified otherwise. We set the total number of encoder layers to be 6 following \cite{cheng2021mask2former}. We consider layers 2 to 6 as potential exit points, unless stated otherwise. In our gating network, we use a straightforward 1D adaptive average pooling operation as our pooling function. 

\paragraph{Training settings.} The experimental setup closely mirrors that of M2F \cite{cheng2021mask2former}, with all model configurations and training specifics following identical procedures. 
We use Detectron2 \cite{wu2019detectron2} and PyTorch\cite{PyTorch} for our implementation.
For the stochastic depth training phase (Step \stepA), we initialize weights as provided by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations for Cityscapes, with a batch size of 16.
For the training of the gating network (Step \stepC), we perform 2 epochs of training on the COCO dataset and 20k iterations on the Cityscapes dataset, employing the Adam optimizer \cite{kingma2017adam}. The adaptation factor $\GPRatio$ in the utility function, as discussed in \Secref{sec:gating}, is set to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed training is performed using 8 A6000 GPUs. On the COCO dataset, the training time of Step \stepA is 280 GPU hours, Step \stepB is 17 GPU hours, and Step \stepC 7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step \stepA is 45 GPU hours, Step \stepB is 1 GPU hours, and Step \stepC is 7.2 GPU hours. In Step \stepA, we use identical settings as M2F for the loss between the predicted segment and ground truth segment, \ie, $\loss_\Glayer$. The weight $\lambda_{\text{mask}}$ is fixed at 5.0, while $\lambda_{\text{class}}$ is set to 2.0 for all classes, except 0.1 for the ``no object'' class. 

\subsection{Main Results}
\input{tables/SOTA}
In \tabref{tab:result_coco} and \tabref{tab:result_cityscapes}, we compare {\ours} with our baseline prior works on the validation set of COCO and Cityscapes dataset, respectively. In \tabref{tab:result_coco}, we observe that {\ours} effectively reduces computational costs while upholding performance levels in comparison to M2F \cite{cheng2021mask2former} using both SWIN-T \cite{liu2021swin} and Res50 \cite{he2016deep} backbones. Additionally, {\ours} can be seamlessly integrated into efficient encoder designs, such as Lite-M2F \cite{cheng2021mask2former}\cite{li2023lite}, further reducing GLOPs by approximately 12.6\%. With Res50 as the backbone, MF \cite{cheng2021per}, YOSO \cite{hu2023you}, and RAP-SAM \cite{xu2024rap} exhibit inferior performance compared to {\ours}. Although ReMax \cite{sun2023remax} demonstrates competitive accuracy, its focus on specialized panoptic segmentation models limits its applicability. Our work, however, aims for a broader impact by creating efficient segmentation architectures that can be used for various segmentation tasks. We make similar observations on the Cityscapes dataset as presented in \tabref{tab:result_cityscapes}. 

\subsection{Ablation Studies}
%
\paragraph{Balancing computational cost and performance.} Within \ours, the parameter $\GPRatio$ serves as an adaptation factor that governs the trade-off between computational cost and performance. Its value, however, is contingent upon the backbone and dataset characteristics. This dependency arises due to the disparate ranges of GFLOPs and segmentation quality (represented by PQ) which are a function of the architecture components and the training data distribution. \Figref{fig:perato} illustrates {\ours} with different values of $\GPRatio$ in Step \stepC using the exact same weights for the parent model from Step \stepA during inference. In comparison to the M2F$_i$ model (where each is trained standalone with $i$ layers), adjusting the value of $\GPRatio$ provides a trade-off between GFLOPs and PQ.
\input{tables/loss_and_smaller_models}
\begin{itemize}[topsep=0.0em,leftmargin=*]
\setlength\itemsep{0.0em}
    \item \textit{Impact of adaptation factor $\GPRatio$}. We analyze the impact of $\GPRatio$ on \ours and present our analysis in \tabref{tab:result_beta}. As expected, a smaller $\GPRatio$ prioritizes segmentation quality over computations resulting in superior performance. Conversely, a larger $\GPRatio$ signifies a greater emphasis on GFLOPs. This results in a slight sacrifice in PQ leading to a significant reduction in GFLOPs. 
    %
    \item \textit{Impact of total encoder layers $\NGlayer$ in the parent architecture.} In situations where computational resources are limited, we present an approach using {\ours} to create a scaled-down version of the parent model. As illustrated in \Tabref{tab:result_smaller_models}, we analyze the impact of $\NGlayer$ set to 5 and 4 (in place of the default value of 6). We set $\GPRatio=0.0005$ for all the models. It can be observed that {\ours} not only reduces the computational load but also preserves the performance of the parent model. Importantly, we \textit{only} execute Step \stepC for each configuration which involves training the gating network.
\end{itemize}
%
\input{tables/beta_bb_and_bb_impact}
\paragraph{Impact of backbone features.} We analyze the impact of backbone features on \ours in \Tabref{tab:result_backbones} and \Tabref{tab:result_weights}, since our gating network 
 mechanism takes these features as input. 
\begin{itemize}[topsep=0.0em,leftmargin=*]
\setlength\itemsep{0.0em}
    \item \textit{Size variations.} In \tabref{tab:result_backbones}, we evaluate {\ours} with backbones of different sizes. Specifically, we observe the performance with SWIN-Tiny (T), SWIN-Small (S), and SWIN-Base (B) architectures \cite{liu2021swin}. We can observe the adaptability of {\ours} in delivering robust performance efficiency across a range of these backbone sizes. Furthermore, this versatility of {\ours} extends to Lite-M2F meta-architecture as well (see supplementary material).
    %
    \item \textit{Pre-trained weight variations.} We explore how different pre-training strategies for the backbones affect the performance of {\ours} in \tabref{tab:result_weights}. We initialize the backbone weights using both \emph{supervised learning} (SL) and \emph{self-supervised learning} (SSL) techniques on the ImageNet-1K dataset \cite{deng2009imagenet}. For SSL pre-training on the SWIN-T \cite{liu2021swin} backbone, we utilize MoBY \cite{xie2021self}. For SSL pre-training on the Res50 \cite{he2016deep} backbone, we use DINO \cite{caron2021emerging}. With MoBY weights on the SWIN-T backbone, {\ours} maintains 99.7\% of the PQ of M2F while reducing GFLOPs in the transformer encoder by 26.34\%. With DINO weights on the Res50 backbone, we observe a reduction of 29.79\% in GFLOPs in the transformer encoder, alongside a slight improvement in PQ.
\end{itemize}
%
\input{tables/weights_and_detr}
\paragraph{Performance in object detection.} To demonstrate the applicability of our method beyond segmentation, we extend our proposed three step recipe for object detection task using DETR \cite{carion2020end} for object detection task and name it \oursdetr. In particular, we vary the values of the adaptation factor $\GPRatio$ to analyze performance-computation trade-offs of \oursdetr in \tabref{tab:result_detr}. Clearly, the resultant architecture maintains the performance but helps in reducing the computations of the encoder (\eg, it achieves a 35.38\% reduction in GFLOPs in the transformer encoder without significantly impacting performance).

\subsection{Qualitative Comparisons}
\input{figures/qual_results}
We present a few examples of predicted segmentation maps in \Figref{fig:qual_result} with SWIN-T \cite{liu2021swin} backbone. Compared to the parent architecture, \ours consistently shows strong performance while self-selecting the encoder layers based on the input examples, both in everyday scenes (on COCO dataset) as well as intricate traffic scenes (on Cityscapes dataset).