{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "Efficient Transformer Encoders for Mask2Formerstyle models",
            "leftover": "Efficient Transformer Encoders for Mask2Formerstyle models",
            "matches": []
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{Manyi Yao2, Abhishek Aich1, Yumin Suh1, Amit RoyChowdhury2, Christian Shelton2, Manmohan Chandraker1,3 } \\authorrunning{M.~Yao et al.} \\institute{NEC Laboratories, America, San Jose CA 95110, USA, University of California, Riverside, CA 92521, USA, University of California, San Diego, CA 92093, USA Corresponding author: aaich@neclabs.com",
            "leftover": "{Manyi Yao2, Abhishek Aich1, Yumin Suh1, Amit RoyChowdhury2, Christian Shelton2, Manmohan Chandraker1,3 } \\authorrunning{M.~Yao et al.} \\institute{NEC Laboratories, America, San Jose CA 95110, USA, University of California, Riverside, CA 92521, USA, University of California, San Diego, CA 92093, USA Corresponding author: aaich@neclabs.com",
            "matches": []
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices. One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current onesizefitsall approach. To this end, we introduce \\ours or EffiCient TransfOrmer Encoders for Mask2Formerstyle models. Noting that the encoder module of M2Fstyle models incur high resourceintensive computations, \\ours provides a strategy to selfselect the number of hidden layers in the encoder, conditioned on the input image. To enable this selfselection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The first}step is to train the parent architecture to enable early exiting from the encoder. The second}step is to create an derived dataset of the ideal number of encoder layers required for each training example. The third}step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on input image. Additionally, to change the computationalaccuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.",
            "leftover": "Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices. One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current onesizefitsall approach. To this end, we introduce \\ours or EffiCient TransfOrmer Encoders for Mask2Formerstyle models. Noting that the encoder module of M2Fstyle models incur high resourceintensive computations, \\ours provides a strategy to selfselect the number of hidden layers in the encoder, conditioned on the input image. To enable this selfselection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The first}step is to train the parent architecture to enable early exiting from the encoder. The second}step is to create an derived dataset of the ideal number of encoder layers required for each training example. The third}step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on input image. Additionally, to change the computationalaccuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.",
            "matches": []
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 3,
                            "key": "doc/body/sec0/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "Introduction",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec0/figure0",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec0/figure0/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 4,
                                            "key": "doc/body/sec0/figure0/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "Comparison to prior works.",
                                            "leftover": "Comparison to prior works.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 5,
                                    "key": "doc/body/sec0/figure0/cpt1",
                                    "block type": "cpt",
                                    "content": "Histogram of images achieving best panoptic segmentation by number of encoder layers.",
                                    "leftover": "Histogram of images achieving best panoptic segmentation by number of encoder layers.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec0/txl1",
                            "block type": "txl",
                            "content": "With the advent of powerful universal}image segmentation architectures, it is highly desirable to prioritize the computational efficiency of these architectures for their enhanced scalability, \\eg, use on resourcelimited edge devices. These architectures are extremely useful in tackling instance, semantic, and panoptic segmentation tasks using one generalized architecture, owing to the transformerbased modules. These universal architectures leverage DEtection TRansformers or DETRstyle modules and represent both stuff}and things}categories using general feature tokens. This is an incredible advantage over preceding segmentation methods in literature that require careful considerations in design specifications. Hence, these segmentation architectures reduce the need for taskspecific choices which favor performance of one task over the other .",
                            "leftover": "With the advent of powerful universal}image segmentation architectures, it is highly desirable to prioritize the computational efficiency of these architectures for their enhanced scalability, \\eg, use on resourcelimited edge devices. These architectures are extremely useful in tackling instance, semantic, and panoptic segmentation tasks using one generalized architecture, owing to the transformerbased modules. These universal architectures leverage DEtection TRansformers or DETRstyle modules and represent both stuff}and things}categories using general feature tokens. This is an incredible advantage over preceding segmentation methods in literature that require careful considerations in design specifications. Hence, these segmentation architectures reduce the need for taskspecific choices which favor performance of one task over the other .",
                            "matches": []
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec0/txl2",
                            "block type": "txl",
                            "content": "Stateoftheart models for universal segmentation like Mask2former (M2F) are built on the key idea inspired from DETR: ''mask'' classification is versatile enough to address both semantic and instancelevel segmentation tasks. However, the problem of efficient M2Fstyle architectures have been underexplored. With backbone architectures (\\eg, Resnet50, SWINTiny ), showed that DETRstyle models incur the highest computations from the transformer encoder due to maintaining full length token representations from multiscale backbone features. While existing works like primarily focus on scaling the input token to improve efficiency, this approach often neglects other aspects of model optimization and leads to a ''onesizefitsall'' solution (Figure). This limitation leaves significant room for further efficiency improvements.",
                            "leftover": "Stateoftheart models for universal segmentation like Mask2former (M2F) are built on the key idea inspired from DETR: ''mask'' classification is versatile enough to address both semantic and instancelevel segmentation tasks. However, the problem of efficient M2Fstyle architectures have been underexplored. With backbone architectures (\\eg, Resnet50, SWINTiny ), showed that DETRstyle models incur the highest computations from the transformer encoder due to maintaining full length token representations from multiscale backbone features. While existing works like primarily focus on scaling the input token to improve efficiency, this approach often neglects other aspects of model optimization and leads to a ''onesizefitsall'' solution (Figure). This limitation leaves significant room for further efficiency improvements.",
                            "matches": []
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec0/txl3",
                            "block type": "txl",
                            "content": "Given this growing importance of M2Fstyle architectures and indispensable need for efficiency for realworld deployment, we introduce \\ours or 'EffiCient TransfOrmer Encoders' for M2Fstyle architectures. Our key idea comes from our observation made on the training set of COCO and Cityscapes dataset demonstrated in \\Figref{fig:teaserb}. We plot a histogram of the number of hidden encoder layers that produces the best panoptic segmentation quality for each image. It can be seen that not all images require the use of all hidden layers of the transformer encoder in order to achieve the maximum panoptic segmentation quality . With this insight, we propose to create a dynamic transformer encoder that economically uses the hidden layers, guided by a gating network that can select different depths for different images.",
                            "leftover": "Given this growing importance of M2Fstyle architectures and indispensable need for efficiency for realworld deployment, we introduce \\ours or 'EffiCient TransfOrmer Encoders' for M2Fstyle architectures. Our key idea comes from our observation made on the training set of COCO and Cityscapes dataset demonstrated in \\Figref{fig:teaserb}. We plot a histogram of the number of hidden encoder layers that produces the best panoptic segmentation quality for each image. It can be seen that not all images require the use of all hidden layers of the transformer encoder in order to achieve the maximum panoptic segmentation quality . With this insight, we propose to create a dynamic transformer encoder that economically uses the hidden layers, guided by a gating network that can select different depths for different images.",
                            "matches": []
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec0/txl4",
                            "block type": "txl",
                            "content": "To achieve the aforementioned ability, \\ours leverages the wellstudied early exiting strategy to create stochastic depths for the transformer encoder to improve inference efficiency. Previous exit mechanisms have primarily relied on confidence scores or uncertainty scores, typically applied in classification tasks. However, implementing such mechanisms in our context would necessitate the inclusion of a decoder and a prediction head to generate a reliable confidence score. This additional complexity introduces a significant number of FLOPs, rendering it impractical for our purposes. By contrast, \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. Step \\stepA involves training the parent model to be dynamic}by allowing stochastic depths at the transformer encoder. Using the fact that the transformer encoder maintains the token length of the input throughout the hidden layers constant, Step \\stepB involves creating a Derived}dataset from the training dataset whose each sample contains a pair of image and layer number that provides the highest segmentation quality. Finally, Step \\stepC involves training a Gating Network}using the derived dataset, whose function is to decide the number of layers to be used given the input image.",
                            "leftover": "To achieve the aforementioned ability, \\ours leverages the wellstudied early exiting strategy to create stochastic depths for the transformer encoder to improve inference efficiency. Previous exit mechanisms have primarily relied on confidence scores or uncertainty scores, typically applied in classification tasks. However, implementing such mechanisms in our context would necessitate the inclusion of a decoder and a prediction head to generate a reliable confidence score. This additional complexity introduces a significant number of FLOPs, rendering it impractical for our purposes. By contrast, \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. Step \\stepA involves training the parent model to be dynamic}by allowing stochastic depths at the transformer encoder. Using the fact that the transformer encoder maintains the token length of the input throughout the hidden layers constant, Step \\stepB involves creating a Derived}dataset from the training dataset whose each sample contains a pair of image and layer number that provides the highest segmentation quality. Finally, Step \\stepC involves training a Gating Network}using the derived dataset, whose function is to decide the number of layers to be used given the input image.",
                            "matches": []
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec0/txl5",
                            "block type": "txl",
                            "content": "The key contributions of \\ours are multifold. First, given a trained M2Fstyle architecture, \\ours finetunes the model into one that allows the ability to randomly exit from the encoder by leveraging the fact that the token length remains constant in the hidden layers. Second, it introduces an accessory to the parent architecture \\via the Gating network that provides the ability to smartly use the encoder layers. Using this module, \\ours enables the parent architecture to decide the optimal amount of layers for the given input without any performance degradation as well as any confidence threshold (unlike prior early exiting strategies). Third, as a result of our Gating network module's training strategy, \\ours can adapt the parent architecture to varying computational budgets using only}Step \\stepC. On COCO dataset, the computational cost of Step \\stepB and \\stepC are only ∼6% and ∼2.5% of Step \\stepA cost, respectively. Finally, \\ours can also incorporate recent advances in making transformer encoder efficient using token length scaling, bringing best of the both methods in pushing the limits of the efficiency. To summarize, we make the following contributions:",
                            "leftover": "The key contributions of \\ours are multifold. First, given a trained M2Fstyle architecture, \\ours finetunes the model into one that allows the ability to randomly exit from the encoder by leveraging the fact that the token length remains constant in the hidden layers. Second, it introduces an accessory to the parent architecture \\via the Gating network that provides the ability to smartly use the encoder layers. Using this module, \\ours enables the parent architecture to decide the optimal amount of layers for the given input without any performance degradation as well as any confidence threshold (unlike prior early exiting strategies). Third, as a result of our Gating network module's training strategy, \\ours can adapt the parent architecture to varying computational budgets using only}Step \\stepC. On COCO dataset, the computational cost of Step \\stepB and \\stepC are only ∼6% and ∼2.5% of Step \\stepA cost, respectively. Finally, \\ours can also incorporate recent advances in making transformer encoder efficient using token length scaling, bringing best of the both methods in pushing the limits of the efficiency. To summarize, we make the following contributions:",
                            "matches": []
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec0/itemize6",
                            "block type": "itemize",
                            "content": "\\setlengthsep{0.0em} We present a dynamic transformer encoder {\\ours} for M2Fstyle universal segmentation that maintains performance but reduces the computational load. {\\ours} consists of a novel training recipe that leverages input image based early exiting (in Step \\stepA), creating a derived dataset (based on training set segmentation performance in Step \\stepB), which in turn is used to train a gating function (using Step \\stepC) that allows adapting the number of hidden layers to reduce computations. Extensive experiments show that {\\ours} improves the overall performanceefficiency tradeoff, and adaptable to diverse architecture settings and can be extended beyond segmentation to the detection task.",
                            "leftover": "\\setlengthsep{0.0em} We present a dynamic transformer encoder {\\ours} for M2Fstyle universal segmentation that maintains performance but reduces the computational load. {\\ours} consists of a novel training recipe that leverages input image based early exiting (in Step \\stepA), creating a derived dataset (based on training set segmentation performance in Step \\stepB), which in turn is used to train a gating function (using Step \\stepC) that allows adapting the number of hidden layers to reduce computations. Extensive experiments show that {\\ours} improves the overall performanceefficiency tradeoff, and adaptable to diverse architecture settings and can be extended beyond segmentation to the detection task.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Related Works",
                            "leftover": "Related Works",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec1/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 13,
                                    "key": "doc/body/sec1/par0/txl0",
                                    "block type": "txl",
                                    "content": "Efficient image segmentation. With the rise of transformers, researchers are increasingly interested in creating image segmentation models that work effectively in various settings, without requiring segmentation type specific modifications to the model itself. Building on DETR, multiple universal segmentation architectures were proposed that use transformer decoder to predict masks for each entity in the input image. However, despite the significant progress in overall performance across various tasks, these models still face challenges in deployment on resourceconstrained devices. Current emphasis for efficiency for image segmentation has mostly been on specialized architectures tailored to a single segmentation task. Unlike these preceding works, \\ours makes no such assumption on the segmentation task and addresses the limitation of inefficiency in M2Fstyle universal architectures that are taskagnostic.",
                                    "leftover": "Efficient image segmentation. With the rise of transformers, researchers are increasingly interested in creating image segmentation models that work effectively in various settings, without requiring segmentation type specific modifications to the model itself. Building on DETR, multiple universal segmentation architectures were proposed that use transformer decoder to predict masks for each entity in the input image. However, despite the significant progress in overall performance across various tasks, these models still face challenges in deployment on resourceconstrained devices. Current emphasis for efficiency for image segmentation has mostly been on specialized architectures tailored to a single segmentation task. Unlike these preceding works, \\ours makes no such assumption on the segmentation task and addresses the limitation of inefficiency in M2Fstyle universal architectures that are taskagnostic.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 14,
                                    "key": "doc/body/sec1/par1/txl0",
                                    "block type": "txl",
                                    "content": "Earlyexiting in vision transformers. Recent works on early exiting aim to boost inference efficiency for large transformers. Some works used early exiting for classification tasks along with manually chosen confidence threshold in vision transformers. For example, proposed an early exiting framework for classification task ViTs combining heterogeneous task heads. Similarly, proposed an early exiting strategy for visionlanguage models by measuring layerwise similarities by checking multiple times to exit early. Applying early exiting solely to the encoder (like ) is infeasible due to the dependency on separate decoders, leading to an unacceptable optimization load. In contrast, methods like suffer from redundant computations for exit decisions at all possible choices, hindering efficient resource allocation. In contrast, \\ours only trains one decoder for all possible exit routes, as well as uses a gating module to decide the number of encoder layers required for the model depending on the input image.",
                                    "leftover": "Earlyexiting in vision transformers. Recent works on early exiting aim to boost inference efficiency for large transformers. Some works used early exiting for classification tasks along with manually chosen confidence threshold in vision transformers. For example, proposed an early exiting framework for classification task ViTs combining heterogeneous task heads. Similarly, proposed an early exiting strategy for visionlanguage models by measuring layerwise similarities by checking multiple times to exit early. Applying early exiting solely to the encoder (like ) is infeasible due to the dependency on separate decoders, leading to an unacceptable optimization load. In contrast, methods like suffer from redundant computations for exit decisions at all possible choices, hindering efficient resource allocation. In contrast, \\ours only trains one decoder for all possible exit routes, as well as uses a gating module to decide the number of encoder layers required for the model depending on the input image.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 15,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Proposed Methodology: \\ours",
                            "leftover": "Proposed Methodology: \\ours",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec2/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 16,
                                    "key": "doc/body/sec2/par0/txl0",
                                    "block type": "txl",
                                    "content": "Model preliminaries. We first review the metaarchitecture of M2F upon which {\\ours} is based, along with the notation. This class of models contains",
                                    "leftover": "Model preliminaries. We first review the metaarchitecture of M2F upon which {\\ours} is based, along with the notation. This class of models contains",
                                    "matches": []
                                },
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec2/par0/itemize1",
                                    "block type": "itemize",
                                    "content": "a backbone (·) which takes the th image as input to generate multiscale feature maps (), represented as 1, 2, 3, 4. These multiscale feature maps correspond to spatial resolutions typically set at 1/32, 1/16, 1/8, and 1/4 of the original image size, respectively. a transformer encoder (called the ''pixel decoder'' ), which is composed of multiple layers of transformer encoders. The function of this module is to generate rich token representation from 1, 2, 3 and generate perpixel embeddings from 4. Each layer in the transformer encoder, denoted as (·) (where ∈1,2,…,) is successively applied to (), with (·) being the last layer in the transformer encoder. a transformer decoder (along with a segmentation head) that takes two inputs: the output of the transformer encoder and the object queries. The object queries are decoded to output a binary mask along with the corresponding class label.",
                                    "leftover": "a backbone (·) which takes the th image as input to generate multiscale feature maps (), represented as 1, 2, 3, 4. These multiscale feature maps correspond to spatial resolutions typically set at 1/32, 1/16, 1/8, and 1/4 of the original image size, respectively. a transformer encoder (called the ''pixel decoder'' ), which is composed of multiple layers of transformer encoders. The function of this module is to generate rich token representation from 1, 2, 3 and generate perpixel embeddings from 4. Each layer in the transformer encoder, denoted as (·) (where ∈1,2,…,) is successively applied to (), with (·) being the last layer in the transformer encoder. a transformer decoder (along with a segmentation head) that takes two inputs: the output of the transformer encoder and the object queries. The object queries are decoded to output a binary mask along with the corresponding class label.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 18,
                                    "key": "doc/body/sec2/par0/txl2",
                                    "block type": "txl",
                                    "content": "For brevity, we collectively refer to the operations in the transformer decoder and segmentation head together as (·). Thus, the output of the metaarchitecture with K encoder layers (a predicted mask  and corresponding label ℓ̃) can be written as",
                                    "leftover": "For brevity, we collectively refer to the operations in the transformer decoder and segmentation head together as (·). Thus, the output of the metaarchitecture with K encoder layers (a predicted mask  and corresponding label ℓ̃) can be written as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 19,
                                    "key": "doc/body/sec2/par0/equation3",
                                    "block type": "equation",
                                    "content": ", ℓ̃ = h∘f∘⋯∘f2∘f1∘b() .",
                                    "leftover": ", ℓ̃ = h∘f∘⋯∘f2∘f1∘b() .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 20,
                                    "key": "doc/body/sec2/par0/txl4",
                                    "block type": "txl",
                                    "content": "Here, the operation ∘ represents function composition, \\eg, g∘ f(x) = g(f(x)) and subscript denotes output predicted using K encoder layers. With, ℓ as the pair of ground truth segmentation map and corresponding label of image, the final loss is computed as",
                                    "leftover": "Here, the operation ∘ represents function composition, \\eg, g∘ f(x) = g(f(x)) and subscript denotes output predicted using K encoder layers. With, ℓ as the pair of ground truth segmentation map and corresponding label of image, the final loss is computed as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 21,
                                    "key": "doc/body/sec2/par0/align5",
                                    "block type": "align",
                                    "content": "= λmaskmask(, ) + λclassclass(ℓ̃, ℓ),",
                                    "leftover": "= λmaskmask(, ) + λclassclass(ℓ̃, ℓ),",
                                    "matches": []
                                },
                                {
                                    "leaf id": 22,
                                    "key": "doc/body/sec2/par0/txl6",
                                    "block type": "txl",
                                    "content": "where mask(·,·) is a binary mask loss and class(·,·) is the corresponding classification loss. λmask and λclass represent the associated loss weights.",
                                    "leftover": "where mask(·,·) is a binary mask loss and class(·,·) is the corresponding classification loss. λmask and λclass represent the associated loss weights.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 23,
                                    "key": "doc/body/sec2/par1/txl0",
                                    "block type": "txl",
                                    "content": "Method motivation. Our motivation stems from the observation that layers within the transformer encoder of M2F exhibit nonuniform contributions to Panoptic Quality (PQ), as discussed in \\Secref{sec:intro}. This prompts us to question the necessity of all K=6 layers for every image and target minimizing layer usage according to the user's computational constraints while ensuring that overall performance remains within acceptable bounds. Hence, we adopt an adaptive early exiting approach driven by three critical components:",
                                    "leftover": "Method motivation. Our motivation stems from the observation that layers within the transformer encoder of M2F exhibit nonuniform contributions to Panoptic Quality (PQ), as discussed in \\Secref{sec:intro}. This prompts us to question the necessity of all K=6 layers for every image and target minimizing layer usage according to the user's computational constraints while ensuring that overall performance remains within acceptable bounds. Hence, we adopt an adaptive early exiting approach driven by three critical components:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec2/par1/enumerate1",
                                    "block type": "enumerate",
                                    "content": "Model suitability for early exiting. Traditional early exiting techniques often face challenges in maintaining satisfactory performance levels at potential exit points throughout the neural network. We recognize the importance of a model architecture that not only allows for early exiting but also ensures that the performance remains consistently high. Therefore, we aim to develop a model that not only permits early exits but also for which the accuracy steadily improves as the network delves deeper into its architecture. By prioritizing this aspect, we seek to establish a framework where early exiting does not compromise the overall performance of the model. Efficient and effective gating network for optimal exit decision making. The efficacy of an early exiting strategy heavily depends on the ability to make informed exit decisions. A gating network must strike a delicate balance, minimizing computational overhead while effectively identifying components that can be bypassed without compromising accuracy. Our objective is to design a lightweight yet powerful gating mechanism capable of discerning optimal exit points within the model architecture. Dynamic control mechanism for costperformance tradeoff. We require a mechanism with the ability to adaptively regulate the balance between computational cost and performance according to userdefined priorities. Such a mechanism empowers the model to exit at the optimal layer based on specific needs and desired outcomes, ensuring efficient resource allocation and maximizing utility in various application scenarios, particularly in resourceconstrained environments like edge computing or realtime applications.",
                                    "leftover": "Model suitability for early exiting. Traditional early exiting techniques often face challenges in maintaining satisfactory performance levels at potential exit points throughout the neural network. We recognize the importance of a model architecture that not only allows for early exiting but also ensures that the performance remains consistently high. Therefore, we aim to develop a model that not only permits early exits but also for which the accuracy steadily improves as the network delves deeper into its architecture. By prioritizing this aspect, we seek to establish a framework where early exiting does not compromise the overall performance of the model. Efficient and effective gating network for optimal exit decision making. The efficacy of an early exiting strategy heavily depends on the ability to make informed exit decisions. A gating network must strike a delicate balance, minimizing computational overhead while effectively identifying components that can be bypassed without compromising accuracy. Our objective is to design a lightweight yet powerful gating mechanism capable of discerning optimal exit points within the model architecture. Dynamic control mechanism for costperformance tradeoff. We require a mechanism with the ability to adaptively regulate the balance between computational cost and performance according to userdefined priorities. Such a mechanism empowers the model to exit at the optimal layer based on specific needs and desired outcomes, ensuring efficient resource allocation and maximizing utility in various application scenarios, particularly in resourceconstrained environments like edge computing or realtime applications.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 25,
                                    "key": "doc/body/sec2/par1/txl2",
                                    "block type": "txl",
                                    "content": "Driven by these considerations, {\\ours} offers a novel training process that enables an adaptive early exiting mechanism designed to bolster computational efficiency while preserving satisfactory model accuracy. For better understanding, we'll begin with a general overview of model training and inference before diving into the specific details of our training process.",
                                    "leftover": "Driven by these considerations, {\\ours} offers a novel training process that enables an adaptive early exiting mechanism designed to bolster computational efficiency while preserving satisfactory model accuracy. For better understanding, we'll begin with a general overview of model training and inference before diving into the specific details of our training process.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par1/figure*3",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 26,
                                            "key": "doc/body/sec2/par1/figure*3/cpt0",
                                            "block type": "cpt",
                                            "content": "{\\ours} framework.",
                                            "leftover": "{\\ours} framework.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par2",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec2/par2/txl0",
                                    "block type": "txl",
                                    "content": "Training and Inference overview. As shown in \\Figref{fig:mainframework}, the training phase of {\\ours} involves following three main steps:",
                                    "leftover": "Training and Inference overview. As shown in \\Figref{fig:mainframework}, the training phase of {\\ours} involves following three main steps:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 28,
                                    "key": "doc/body/sec2/par2/enumerate1",
                                    "block type": "enumerate",
                                    "content": "Step \\stepA: Train parent model for early exit via the transformer encoder. Step \\stepB: Derive a dataset (which we call the Derived dataset) from the dynamic model obtained in Step \\stepA. Step \\stepC: Train the Gating Network}to learn optimal exit points in the encoder tailored to users' requirements.",
                                    "leftover": "Step \\stepA: Train parent model for early exit via the transformer encoder. Step \\stepB: Derive a dataset (which we call the Derived dataset) from the dynamic model obtained in Step \\stepA. Step \\stepC: Train the Gating Network}to learn optimal exit points in the encoder tailored to users' requirements.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec2/par2/txl2",
                                    "block type": "txl",
                                    "content": "We refer to Step \\stepA and \\stepB together as model preprocessing}and Step \\stepC as model adaptation. The former is required only once, whereas the latter is repeated as per user requirements. All these steps use the training data subset.",
                                    "leftover": "We refer to Step \\stepA and \\stepB together as model preprocessing}and Step \\stepC as model adaptation. The former is required only once, whereas the latter is repeated as per user requirements. All these steps use the training data subset.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 30,
                                    "key": "doc/body/sec2/par2/txl3",
                                    "block type": "txl",
                                    "content": "During inference, the gating network guides the parent model by selecting the optimal exit point based on features extracted from the backbone with just one forward pass for final predictions.",
                                    "leftover": "During inference, the gating network guides the parent model by selecting the optimal exit point based on features extracted from the backbone with just one forward pass for final predictions.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub4",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 31,
                                            "key": "doc/body/sec2/par2/sub4/tit",
                                            "block type": "title",
                                            "content": "Step \\stepA: Training the Model with Weighted Stochastic Depth",
                                            "leftover": "Step \\stepA: Training the Model with Weighted Stochastic Depth",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec2/par2/sub4/txl0",
                                            "block type": "txl",
                                            "content": "In this step, we enable the model to allow exiting at the encoder. To maintain consistently high performance at each exit point, we input each stochastic depth's output to a shared transformer decoder. We then apply \\Eqref{eq:origloss} to compute the loss  for each exit point . However, we observe that direct training in this fashion does not encourage the model to use fewer layers to extract and prioritize informative representations, as shown in \\Tabref{tab:loss}. To address this, we introduce a set of coefficients  to emphasize the quality of representations at later layers more, enabling earlier layers to also concentrate on producing effective intermediate representations. As the layer depth increases, the corresponding coefficient  grows, ensuring a progressively stricter standard for feature quality. The new loss function is then expressed as",
                                            "leftover": "In this step, we enable the model to allow exiting at the encoder. To maintain consistently high performance at each exit point, we input each stochastic depth's output to a shared transformer decoder. We then apply \\Eqref{eq:origloss} to compute the loss  for each exit point . However, we observe that direct training in this fashion does not encourage the model to use fewer layers to extract and prioritize informative representations, as shown in \\Tabref{tab:loss}. To address this, we introduce a set of coefficients  to emphasize the quality of representations at later layers more, enabling earlier layers to also concentrate on producing effective intermediate representations. As the layer depth increases, the corresponding coefficient  grows, ensuring a progressively stricter standard for feature quality. The new loss function is then expressed as",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 33,
                                            "key": "doc/body/sec2/par2/sub4/equation1",
                                            "block type": "equation",
                                            "content": "total = {1N∑^N∑^, where ∀<', <',",
                                            "leftover": "total = {1N∑^N∑^, where ∀<', <',",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 34,
                                            "key": "doc/body/sec2/par2/sub4/txl2",
                                            "block type": "txl",
                                            "content": "where N is the number of images in the training set, and  is from \\Eqref{eq:origloss}. [18]R{0.45} 2em \\includegraphics[width=0.45]{figures/images/PQvslayertrainset.pdf}",
                                            "leftover": "where N is the number of images in the training set, and  is from \\Eqref{eq:origloss}. [18]R{0.45} 2em \\includegraphics[width=0.45]{figures/images/PQvslayertrainset.pdf}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 35,
                                            "key": "doc/body/sec2/par2/sub4/cpt3",
                                            "block type": "cpt",
                                            "content": "Intuition for",
                                            "leftover": "Intuition for",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub5",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 36,
                                            "key": "doc/body/sec2/par2/sub5/tit",
                                            "block type": "title",
                                            "content": "Step \\stepB: Deriving the Gating Network Training Dataset",
                                            "leftover": "Step \\stepB: Deriving the Gating Network Training Dataset",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 37,
                                            "key": "doc/body/sec2/par2/sub5/txl0",
                                            "block type": "txl",
                                            "content": "To facilitate informed exit decisions during inference, our approach is to train a gating network to learn optimal exit strategies. In this step, we facilitate this gating network training by first deriving an intermediate dataset.",
                                            "leftover": "To facilitate informed exit decisions during inference, our approach is to train a gating network to learn optimal exit strategies. In this step, we facilitate this gating network training by first deriving an intermediate dataset.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 38,
                                            "key": "doc/body/sec2/par2/sub5/txl1",
                                            "block type": "txl",
                                            "content": "To this end, we record the performance of the pretrained stochastic depth model (obtained from Step \\stepA) at all potential exit points for each image within the training dataset and create a Derived}dataset . Specifically, we associate the th input image with a vector of length . Each element  of represents the predicted panoptic quality upon exiting at the encoder layer . Hence, each sample of can be represented as (, ) ∈.",
                                            "leftover": "To this end, we record the performance of the pretrained stochastic depth model (obtained from Step \\stepA) at all potential exit points for each image within the training dataset and create a Derived}dataset . Specifically, we associate the th input image with a vector of length . Each element  of represents the predicted panoptic quality upon exiting at the encoder layer . Hence, each sample of can be represented as (, ) ∈.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub6",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 39,
                                            "key": "doc/body/sec2/par2/sub6/tit",
                                            "block type": "title",
                                            "content": "Step \\stepC: Training for Gating Network",
                                            "leftover": "Step \\stepC: Training for Gating Network",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 40,
                                            "key": "doc/body/sec2/par2/sub6/txl0",
                                            "block type": "txl",
                                            "content": "In this step, we train the gating network on dataset (obtained from Step \\stepB) to selfselect the number of encoder layers based on the input image. Ideally, this module should allow exiting at the encoder layer which would result in the highest quality segmentation map. With this in mind, we first establish the target exit for the gating network. Note that the panoptic quality generally increases with increasing encoder layers (see \\Figref{fig:PQvslayers}). However, we would like the gating network to prioritize increasing the panoptic quality while also reducing the number of layers (to reduce the overall computations). Consequently, we introduce a utility function expressed as the linear combination of segmentation quality and the depth of the network. This function is formulated as",
                                            "leftover": "In this step, we train the gating network on dataset (obtained from Step \\stepB) to selfselect the number of encoder layers based on the input image. Ideally, this module should allow exiting at the encoder layer which would result in the highest quality segmentation map. With this in mind, we first establish the target exit for the gating network. Note that the panoptic quality generally increases with increasing encoder layers (see \\Figref{fig:PQvslayers}). However, we would like the gating network to prioritize increasing the panoptic quality while also reducing the number of layers (to reduce the overall computations). Consequently, we introduce a utility function expressed as the linear combination of segmentation quality and the depth of the network. This function is formulated as",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 41,
                                            "key": "doc/body/sec2/par2/sub6/equation1",
                                            "block type": "equation",
                                            "content": "() = q,",
                                            "leftover": "() = q,",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 42,
                                            "key": "doc/body/sec2/par2/sub6/txl2",
                                            "block type": "txl",
                                            "content": "where serves as an adaptation factor}governing the tradeoff between segmentation quality and computational cost. Clearly, a higher value of signifies a greater emphasis on efficiency over segmentation quality. Using \\Eqref{eq:utilityfunc}, we determine a target exit point for each image using",
                                            "leftover": "where serves as an adaptation factor}governing the tradeoff between segmentation quality and computational cost. Clearly, a higher value of signifies a greater emphasis on efficiency over segmentation quality. Using \\Eqref{eq:utilityfunc}, we determine a target exit point for each image using",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 43,
                                            "key": "doc/body/sec2/par2/sub6/equation3",
                                            "block type": "equation",
                                            "content": "= (()) .",
                                            "leftover": "= (()) .",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 44,
                                            "key": "doc/body/sec2/par2/sub6/txl4",
                                            "block type": "txl",
                                            "content": "With a target designated for each image using \\Eqref{eq:target}, the gating decision can be approached as a straightforward classification problem. The gating architecture consists of a pooling operation (·) on the token length dimension followed by a linear layer with weights . Its output logits can be represented as",
                                            "leftover": "With a target designated for each image using \\Eqref{eq:target}, the gating decision can be approached as a straightforward classification problem. The gating architecture consists of a pooling operation (·) on the token length dimension followed by a linear layer with weights . Its output logits can be represented as",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 45,
                                            "key": "doc/body/sec2/par2/sub6/align5",
                                            "block type": "align",
                                            "content": "= (1) .",
                                            "leftover": "= (1) .",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 46,
                                            "key": "doc/body/sec2/par2/sub6/txl6",
                                            "block type": "txl",
                                            "content": "In consideration of having minimal impact on the computations due to the gating network, we use the output of the lowest resolution feature map 1 as input to the pooling operation. To optimize the gating network, we use the standard crossentropy loss between the output logits and the onehot version of target exit as our training objective. During inference, the gating network identifies the layer with the highest predicted logits, \\ie, (g), as the optimal exit layer for image . Note that while there can be more complex choices for the gating network, our simple linear layer in \\Eqref{eq:gating} works well in our experiments.",
                                            "leftover": "In consideration of having minimal impact on the computations due to the gating network, we use the output of the lowest resolution feature map 1 as input to the pooling operation. To optimize the gating network, we use the standard crossentropy loss between the output logits and the onehot version of target exit as our training objective. During inference, the gating network identifies the layer with the highest predicted logits, \\ie, (g), as the optimal exit layer for image . Note that while there can be more complex choices for the gating network, our simple linear layer in \\Eqref{eq:gating} works well in our experiments.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par3",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 47,
                                    "key": "doc/body/sec2/par3/txl0",
                                    "block type": "txl",
                                    "content": "Saving training costs through Step \\stepC. {\\ours} presents a distinct advantage in terms of its adaptability to varying computational constraints. In scenarios where a smaller model is desired, {\\ours} necessitates training solely the gating network (\\ie, repeat Step \\stepC). Assuming that the computational load is proportional to the depth of the network, \\Eqref{eq:utilityfunc} enables us to weigh the performance gain against the computational overhead for each exit layer. We achieve this by setting the total number of layers to a smaller number depending on user preferences. For instance, as illustrated in \\Figref{fig:mainframework}, User X preferring a smaller model compared to User Y may opt for a smaller, \\ie, X<Y. Then, given the importance of segmentation quality, we choose β. With these two variables set in \\Eqref{eq:target}, we train the gating network. This capability shows that {\\ours} is versatile and resourceefficient as it adapts to diverse needs and optimizes allocations.",
                                    "leftover": "Saving training costs through Step \\stepC. {\\ours} presents a distinct advantage in terms of its adaptability to varying computational constraints. In scenarios where a smaller model is desired, {\\ours} necessitates training solely the gating network (\\ie, repeat Step \\stepC). Assuming that the computational load is proportional to the depth of the network, \\Eqref{eq:utilityfunc} enables us to weigh the performance gain against the computational overhead for each exit layer. We achieve this by setting the total number of layers to a smaller number depending on user preferences. For instance, as illustrated in \\Figref{fig:mainframework}, User X preferring a smaller model compared to User Y may opt for a smaller, \\ie, X<Y. Then, given the importance of segmentation quality, we choose β. With these two variables set in \\Eqref{eq:target}, we train the gating network. This capability shows that {\\ours} is versatile and resourceefficient as it adapts to diverse needs and optimizes allocations.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par3/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 48,
                                            "key": "doc/body/sec2/par3/sub1/tit",
                                            "block type": "title",
                                            "content": "Inference",
                                            "leftover": "Inference",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 49,
                                            "key": "doc/body/sec2/par3/sub1/txl0",
                                            "block type": "txl",
                                            "content": "In the inference phase, the gating network guides the parent model toward an optimal exit point tailored to each input image. Similar to the training phase, the gating mechanism receives lowresolution features from the backbone and produces a vector of length for each image. The value of remains consistent with that determined in Step \\stepC. Subsequently, the gating network identifies the layer with the highest predicted logits as the optimal exit layer for each image. The parent model adheres to this decision, exiting at the determined layer, and subsequently progresses through the subsequent components to make the final prediction. This dynamic process ensures that the model adaptively selects the most optimal layer for exit during inference, enhancing its efficiency in handling diverse input data.",
                                            "leftover": "In the inference phase, the gating network guides the parent model toward an optimal exit point tailored to each input image. Similar to the training phase, the gating mechanism receives lowresolution features from the backbone and produces a vector of length for each image. The value of remains consistent with that determined in Step \\stepC. Subsequently, the gating network identifies the layer with the highest predicted logits as the optimal exit layer for each image. The parent model adheres to this decision, exiting at the determined layer, and subsequently progresses through the subsequent components to make the final prediction. This dynamic process ensures that the model adaptively selects the most optimal layer for exit during inference, enhancing its efficiency in handling diverse input data.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 50,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Experiments",
                            "leftover": "Experiments",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 51,
                                    "key": "doc/body/sec3/par0/txl0",
                                    "block type": "txl",
                                    "content": "Datasets. Our study illustrates the adaptability of {\\ours} in dynamically managing the tradeoff between computation and performance based on M2F metaarchitecture. We do this on two widely used image segmentation datasets: COCO and Cityscapes . COCO comprises 80 ''things'' and 53 ''stuff'' categories, with 118k training images and 5k validation images. Cityscapes consists of 8 ''things'' and 11 ''stuff'' categories, with approximately 3k training images and 500 validation images. The evaluation is conducted over the union of ''things'' and ''stuff'' categories.",
                                    "leftover": "Datasets. Our study illustrates the adaptability of {\\ours} in dynamically managing the tradeoff between computation and performance based on M2F metaarchitecture. We do this on two widely used image segmentation datasets: COCO and Cityscapes . COCO comprises 80 ''things'' and 53 ''stuff'' categories, with 118k training images and 5k validation images. Cityscapes consists of 8 ''things'' and 11 ''stuff'' categories, with approximately 3k training images and 500 validation images. The evaluation is conducted over the union of ''things'' and ''stuff'' categories.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 52,
                                    "key": "doc/body/sec3/par1/txl0",
                                    "block type": "txl",
                                    "content": "Evaluation metrics. We follow the evaluation setting of for evaluation of ''universal'' segmentation, \\ie, we train the model solely with panoptic segmentation annotations but evaluate it for panoptic, semantic, and instance segmentation tasks. We use the standard PQ}(Panoptic Quality ) metric to evaluate panoptic segmentation performance. We report APp (Average Precision ) computed across all categories for instance segmentation, and mIOUp(mean Intersection over Union ) for semantic segmentation by merging instance masks from the same category. The subscript p denotes that these metrics are computed for the model trained solely with panoptic segmentation annotations. In terms of computational cost, we use GFLOPs calculated as the average GFLOPs across all validation images. All models are trained on the train}split and evaluated on the validation}split.",
                                    "leftover": "Evaluation metrics. We follow the evaluation setting of for evaluation of ''universal'' segmentation, \\ie, we train the model solely with panoptic segmentation annotations but evaluate it for panoptic, semantic, and instance segmentation tasks. We use the standard PQ}(Panoptic Quality ) metric to evaluate panoptic segmentation performance. We report APp (Average Precision ) computed across all categories for instance segmentation, and mIOUp(mean Intersection over Union ) for semantic segmentation by merging instance masks from the same category. The subscript p denotes that these metrics are computed for the model trained solely with panoptic segmentation annotations. In terms of computational cost, we use GFLOPs calculated as the average GFLOPs across all validation images. All models are trained on the train}split and evaluated on the validation}split.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par2",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 53,
                                    "key": "doc/body/sec3/par2/txl0",
                                    "block type": "txl",
                                    "content": "Baseline models. We compare \\ours with two sets of efficient segmentation methods. First, we compare with our baseline universal segmentation architecture M2F . Further, we also integrate recently proposed transformer encoder designs (LiteDETR and RTDETR ) for efficient object detection into M2F and named them LiteM2F and RTM2F, respectively. Second, we include comparisons with recent efficient architectures that proposed taskspecific components, namely YOSO, RAPSAM, and ReMax .",
                                    "leftover": "Baseline models. We compare \\ours with two sets of efficient segmentation methods. First, we compare with our baseline universal segmentation architecture M2F . Further, we also integrate recently proposed transformer encoder designs (LiteDETR and RTDETR ) for efficient object detection into M2F and named them LiteM2F and RTM2F, respectively. Second, we include comparisons with recent efficient architectures that proposed taskspecific components, namely YOSO, RAPSAM, and ReMax .",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par3",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec3/par3/txl0",
                                    "block type": "txl",
                                    "content": "Architecture details. We focus on standard backbones Res50 and SWINTiny pretrained on ImageNet1K, unless specified otherwise. We set the total number of encoder layers to be 6 following . We consider layers 2 to 6 as potential exit points, unless stated otherwise. In our gating network, we use a straightforward 1D adaptive average pooling operation as our pooling function.",
                                    "leftover": "Architecture details. We focus on standard backbones Res50 and SWINTiny pretrained on ImageNet1K, unless specified otherwise. We set the total number of encoder layers to be 6 following . We consider layers 2 to 6 as potential exit points, unless stated otherwise. In our gating network, we use a straightforward 1D adaptive average pooling operation as our pooling function.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par4",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 55,
                                    "key": "doc/body/sec3/par4/txl0",
                                    "block type": "txl",
                                    "content": "Training settings. The experimental setup closely mirrors that of M2F, with all model configurations and training specifics following identical procedures. We use Detectron2 and PyTorch for our implementation. For the stochastic depth training phase (Step \\stepA), we initialize weights as provided by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations for Cityscapes, with a batch size of 16. For the training of the gating network (Step \\stepC), we perform 2 epochs of training on the COCO dataset and 20k iterations on the Cityscapes dataset, employing the Adam optimizer . The adaptation factor in the utility function, as discussed in \\Secref{sec:gating}, is set to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed training is performed using 8 A6000 GPUs. On the COCO dataset, the training time of Step \\stepA is 280 GPU hours, Step \\stepB is 17 GPU hours, and Step \\stepC 7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step \\stepA is 45 GPU hours, Step \\stepB is 1 GPU hours, and Step \\stepC is 7.2 GPU hours. In Step \\stepA, we use identical settings as M2F for the loss between the predicted segment and ground truth segment, \\ie, . The weight λmask is fixed at 5.0, while λclass is set to 2.0 for all classes, except 0.1 for the ''no object'' class.",
                                    "leftover": "Training settings. The experimental setup closely mirrors that of M2F, with all model configurations and training specifics following identical procedures. We use Detectron2 and PyTorch for our implementation. For the stochastic depth training phase (Step \\stepA), we initialize weights as provided by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations for Cityscapes, with a batch size of 16. For the training of the gating network (Step \\stepC), we perform 2 epochs of training on the COCO dataset and 20k iterations on the Cityscapes dataset, employing the Adam optimizer . The adaptation factor in the utility function, as discussed in \\Secref{sec:gating}, is set to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed training is performed using 8 A6000 GPUs. On the COCO dataset, the training time of Step \\stepA is 280 GPU hours, Step \\stepB is 17 GPU hours, and Step \\stepC 7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step \\stepA is 45 GPU hours, Step \\stepB is 1 GPU hours, and Step \\stepC is 7.2 GPU hours. In Step \\stepA, we use identical settings as M2F for the loss between the predicted segment and ground truth segment, \\ie, . The weight λmask is fixed at 5.0, while λclass is set to 2.0 for all classes, except 0.1 for the ''no object'' class.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par4/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 56,
                                            "key": "doc/body/sec3/par4/sub1/tit",
                                            "block type": "title",
                                            "content": "Main Results",
                                            "leftover": "Main Results",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/par4/sub1/table0",
                                            "block_type": "table",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec3/par4/sub1/table0/cpt0",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 57,
                                                            "key": "doc/body/sec3/par4/sub1/table0/cpt0/txl0",
                                                            "block type": "txl",
                                                            "content": "COCO evaluation.",
                                                            "leftover": "COCO evaluation.",
                                                            "matches": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 58,
                                                    "key": "doc/body/sec3/par4/sub1/table0/txl1",
                                                    "block type": "txl",
                                                    "content": "{\\hlours{Our method}}, {\\hltask{Taskspecific architectures}}} \\columnwidth{!}{",
                                                    "leftover": "{\\hlours{Our method}}, {\\hltask{Taskspecific architectures}}} \\columnwidth{!}{",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 59,
                                                    "key": "doc/body/sec3/par4/sub1/table0/tabular2",
                                                    "block type": "tabular",
                                                    "content": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Model & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\multicolumn{7}c{Backbone: SWINT} RTM2F & 41.36 & 61.54 & 24.68 && 158.30 & 59.66 LiteM2F & 52.70 & 63.08 & 41.10 && 188.00 & 79.78 M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 \\rowcolor{eccvblue!20} {\\ours}(=0.0005) & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\rowcolor{eccvblue!20} {\\ours}(=0.02) & 50.79 & 62.25 & 39.71 && 181.64 & 67.71 \\rowcolor{eccvblue!20} Lite{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\multicolumn{7}c{Backbone: Res50} M2F & 51.73 & 61.94 & 41.72 && 229.10 & 135.00 MF & 46.50 & 57.80 & 33.00 && 181.00 &  \\rowcolor{gray!20} YOSO & 48.40 & 58.74 & 36.87 && 114.50 &  \\rowcolor{gray!20} RAPSAM & 46.90 &  &  && 123.00 &  \\rowcolor{gray!20} ReMax & 53.50 &  &  &&  &  \\rowcolor{eccvblue!20} {\\ours} & 51.89 & 61.07 & 41.25 && 195.55 & 92.37",
                                                    "leftover": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Model & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\multicolumn{7}c{Backbone: SWINT} RTM2F & 41.36 & 61.54 & 24.68 && 158.30 & 59.66 LiteM2F & 52.70 & 63.08 & 41.10 && 188.00 & 79.78 M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 \\rowcolor{eccvblue!20} {\\ours}(=0.0005) & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\rowcolor{eccvblue!20} {\\ours}(=0.02) & 50.79 & 62.25 & 39.71 && 181.64 & 67.71 \\rowcolor{eccvblue!20} Lite{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\multicolumn{7}c{Backbone: Res50} M2F & 51.73 & 61.94 & 41.72 && 229.10 & 135.00 MF & 46.50 & 57.80 & 33.00 && 181.00 &  \\rowcolor{gray!20} YOSO & 48.40 & 58.74 & 36.87 && 114.50 &  \\rowcolor{gray!20} RAPSAM & 46.90 &  &  && 123.00 &  \\rowcolor{gray!20} ReMax & 53.50 &  &  &&  &  \\rowcolor{eccvblue!20} {\\ours} & 51.89 & 61.07 & 41.25 && 195.55 & 92.37",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 60,
                                                    "key": "doc/body/sec3/par4/sub1/table0/txl3",
                                                    "block type": "txl",
                                                    "content": "} l [!ht]{0.495}",
                                                    "leftover": "} l [!ht]{0.495}",
                                                    "matches": []
                                                },
                                                {
                                                    "key": "doc/body/sec3/par4/sub1/table0/cpt4",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 61,
                                                            "key": "doc/body/sec3/par4/sub1/table0/cpt4/txl0",
                                                            "block type": "txl",
                                                            "content": "Cityscapes evaluation.",
                                                            "leftover": "Cityscapes evaluation.",
                                                            "matches": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 62,
                                                    "key": "doc/body/sec3/par4/sub1/table0/tabular5",
                                                    "block type": "tabular",
                                                    "content": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Model & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\multicolumn{7}c{Backbone: SWINT} RTM2F & 59.73 & 77.89 & 31.35 && 361.10 & 130.00 LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 M2F & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 \\rowcolor{eccvblue!20} {\\ours}(=0.003) & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 \\rowcolor{eccvblue!20} {\\ours}(=0.01) & 62.09 & 79.58 & 36.04 && 439.67 & 182.95 \\rowcolor{eccvblue!20} Lite\\ours & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\multicolumn{7}c{Backbone: Res50} M2F & 61.86 & 76.94 & 37.35 && 524.11 & 281.13 \\rowcolor{gray!20} YOSO & 59.70 & 76.05 & 33.76 && 265.1 &  \\rowcolor{gray!20} ReMax & 65.40 &  &  && 294.7 &  \\rowcolor{eccvblue!20} {\\ours} & 62.20 & 77.34 & 37.21 && 453.50 & 220.59",
                                                    "leftover": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Model & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\multicolumn{7}c{Backbone: SWINT} RTM2F & 59.73 & 77.89 & 31.35 && 361.10 & 130.00 LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 M2F & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 \\rowcolor{eccvblue!20} {\\ours}(=0.003) & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 \\rowcolor{eccvblue!20} {\\ours}(=0.01) & 62.09 & 79.58 & 36.04 && 439.67 & 182.95 \\rowcolor{eccvblue!20} Lite\\ours & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\multicolumn{7}c{Backbone: Res50} M2F & 61.86 & 76.94 & 37.35 && 524.11 & 281.13 \\rowcolor{gray!20} YOSO & 59.70 & 76.05 & 33.76 && 265.1 &  \\rowcolor{gray!20} ReMax & 65.40 &  &  && 294.7 &  \\rowcolor{eccvblue!20} {\\ours} & 62.20 & 77.34 & 37.21 && 453.50 & 220.59",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 63,
                                                    "key": "doc/body/sec3/par4/sub1/table0/txl6",
                                                    "block type": "txl",
                                                    "content": "}",
                                                    "leftover": "}",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 64,
                                            "key": "doc/body/sec3/par4/sub1/txl1",
                                            "block type": "txl",
                                            "content": "In \\tabref{tab:resultcoco} and \\tabref{tab:resultcityscapes}, we compare {\\ours} with our baseline prior works on the validation set of COCO and Cityscapes dataset, respectively. In \\tabref{tab:resultcoco}, we observe that {\\ours} effectively reduces computational costs while upholding performance levels in comparison to M2F using both SWINT and Res50 backbones. Additionally, {\\ours} can be seamlessly integrated into efficient encoder designs, such as LiteM2F, further reducing GLOPs by approximately 12.6%. With Res50 as the backbone, MF, YOSO, and RAPSAM exhibit inferior performance compared to {\\ours}. Although ReMax demonstrates competitive accuracy, its focus on specialized panoptic segmentation models limits its applicability. Our work, however, aims for a broader impact by creating efficient segmentation architectures that can be used for various segmentation tasks. We make similar observations on the Cityscapes dataset as presented in \\tabref{tab:resultcityscapes}.",
                                            "leftover": "In \\tabref{tab:resultcoco} and \\tabref{tab:resultcityscapes}, we compare {\\ours} with our baseline prior works on the validation set of COCO and Cityscapes dataset, respectively. In \\tabref{tab:resultcoco}, we observe that {\\ours} effectively reduces computational costs while upholding performance levels in comparison to M2F using both SWINT and Res50 backbones. Additionally, {\\ours} can be seamlessly integrated into efficient encoder designs, such as LiteM2F, further reducing GLOPs by approximately 12.6%. With Res50 as the backbone, MF, YOSO, and RAPSAM exhibit inferior performance compared to {\\ours}. Although ReMax demonstrates competitive accuracy, its focus on specialized panoptic segmentation models limits its applicability. Our work, however, aims for a broader impact by creating efficient segmentation architectures that can be used for various segmentation tasks. We make similar observations on the Cityscapes dataset as presented in \\tabref{tab:resultcityscapes}.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/par4/sub2",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 65,
                                            "key": "doc/body/sec3/par4/sub2/tit",
                                            "block type": "title",
                                            "content": "Ablation Studies",
                                            "leftover": "Ablation Studies",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par5",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 66,
                                    "key": "doc/body/sec3/par5/txl0",
                                    "block type": "txl",
                                    "content": "Balancing computational cost and performance. Within \\ours, the parameter serves as an adaptation factor that governs the tradeoff between computational cost and performance. Its value, however, is contingent upon the backbone and dataset characteristics. This dependency arises due to the disparate ranges of GFLOPs and segmentation quality (represented by PQ) which are a function of the architecture components and the training data distribution. \\Figref{fig:perato} illustrates {\\ours} with different values of in Step \\stepC using the exact same weights for the parent model from Step \\stepA during inference. In comparison to the M2Fi model (where each is trained standalone with i layers), adjusting the value of provides a tradeoff between GFLOPs and PQ.",
                                    "leftover": "Balancing computational cost and performance. Within \\ours, the parameter serves as an adaptation factor that governs the tradeoff between computational cost and performance. Its value, however, is contingent upon the backbone and dataset characteristics. This dependency arises due to the disparate ranges of GFLOPs and segmentation quality (represented by PQ) which are a function of the architecture components and the training data distribution. \\Figref{fig:perato} illustrates {\\ours} with different values of in Step \\stepC using the exact same weights for the parent model from Step \\stepA during inference. In comparison to the M2Fi model (where each is trained standalone with i layers), adjusting the value of provides a tradeoff between GFLOPs and PQ.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par5/table1",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/par5/table1/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 67,
                                                    "key": "doc/body/sec3/par5/table1/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of .",
                                                    "leftover": "Impact of .",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 68,
                                            "key": "doc/body/sec3/par5/table1/tabular1",
                                            "block type": "tabular",
                                            "content": "cccccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Data & \\multirow{2}{*}{β} & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\rowcolor{gray!20} \\cellcolorwhite\\multirow{5}{*}{\\rotCOCO} & Baseline & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & 0.0 & 52.24 & 62.95 & 41.61 && 220.61 & 107.18 & 0.0005 & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 & 0.001 & 51.72 & 62.60 & 41.12 && 193.10 & 79.18 & 0.02 & 50.79 & 62.25 & 39.71 && 181.64 & 67.71 \\rowcolor{gray!20} \\cellcolorwhite\\multirow{6}{*}{\\rotCityscapes} & Baseline & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 & 0.0 & 64.58 & 80.35 & 40.31 && 536.09 & 279.37 & 0.003 & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 & 0.005 & 63.24 & 79.73 & 37.97 && 469.38 & 212.66 & 0.01 & 62.09 & 79.58 & 36.04 && 439.67 & 182.95 & 0.1 & 60.71 & 78.15 & 33.86 && 411.98 & 155.26",
                                            "leftover": "cccccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Data & \\multirow{2}{*}{β} & PQ}& mIoUp & APp && Total}& Tx. Enc.} \\rowcolor{gray!20} \\cellcolorwhite\\multirow{5}{*}{\\rotCOCO} & Baseline & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & 0.0 & 52.24 & 62.95 & 41.61 && 220.61 & 107.18 & 0.0005 & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 & 0.001 & 51.72 & 62.60 & 41.12 && 193.10 & 79.18 & 0.02 & 50.79 & 62.25 & 39.71 && 181.64 & 67.71 \\rowcolor{gray!20} \\cellcolorwhite\\multirow{6}{*}{\\rotCityscapes} & Baseline & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 & 0.0 & 64.58 & 80.35 & 40.31 && 536.09 & 279.37 & 0.003 & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 & 0.005 & 63.24 & 79.73 & 37.97 && 469.38 & 212.66 & 0.01 & 62.09 & 79.58 & 36.04 && 439.67 & 182.95 & 0.1 & 60.71 & 78.15 & 33.86 && 411.98 & 155.26",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 69,
                                            "key": "doc/body/sec3/par5/table1/txl2",
                                            "block type": "txl",
                                            "content": "} l {0.495}",
                                            "leftover": "} l {0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/par5/table1/cpt3",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 70,
                                                    "key": "doc/body/sec3/par5/table1/cpt3/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of . The baseline M2F refers to the complete M2F model trained with 6/5/4 layers for 50 epochs. {\\ours} pertains to the training of only the gating network for 2 epochs by setting =6,5,4 respectively. (Dataset: COCO; Backbone: SWINT)",
                                                    "leftover": "Impact of . The baseline M2F refers to the complete M2F model trained with 6/5/4 layers for 50 epochs. {\\ours} pertains to the training of only the gating network for 2 epochs by setting =6,5,4 respectively. (Dataset: COCO; Backbone: SWINT)",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 71,
                                            "key": "doc/body/sec3/par5/table1/tabular4",
                                            "block type": "tabular",
                                            "content": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}{} & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multirow{2}{*}{6} & M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & {\\ours} & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\multirow{2}{*}{5} & M2F & 51.61 & 61.93 & 41.55 && 221.95 & 108.07 & {\\ours} & 52.26 & 62.67 & 41.56 && 208.82 & 94.59 \\multirow{2}{*}{4} & M2F & 51.38 & 62.30 & 41.11 && 208.33 & 94.45 & {\\ours} & 52.20 & 62.58 & 41.55 && 202.47 & 88.65",
                                            "leftover": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}{} & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multirow{2}{*}{6} & M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & {\\ours} & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\multirow{2}{*}{5} & M2F & 51.61 & 61.93 & 41.55 && 221.95 & 108.07 & {\\ours} & 52.26 & 62.67 & 41.56 && 208.82 & 94.59 \\multirow{2}{*}{4} & M2F & 51.38 & 62.30 & 41.11 && 208.33 & 94.45 & {\\ours} & 52.20 & 62.58 & 41.55 && 202.47 & 88.65",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 72,
                                            "key": "doc/body/sec3/par5/table1/txl5",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 73,
                                    "key": "doc/body/sec3/par5/itemize2",
                                    "block type": "itemize",
                                    "content": "\\setlengthsep{0.0em} Impact of adaptation factor . We analyze the impact of on \\ours and present our analysis in \\tabref{tab:resultbeta}. As expected, a smaller prioritizes segmentation quality over computations resulting in superior performance. Conversely, a larger signifies a greater emphasis on GFLOPs. This results in a slight sacrifice in PQ leading to a significant reduction in GFLOPs. Impact of total encoder layers in the parent architecture.}In situations where computational resources are limited, we present an approach using {\\ours} to create a scaleddown version of the parent model. As illustrated in \\Tabref{tab:resultsmallermodels}, we analyze the impact of set to 5 and 4 (in place of the default value of 6). We set =0.0005 for all the models. It can be observed that {\\ours} not only reduces the computational load but also preserves the performance of the parent model. Importantly, we only}execute Step \\stepC for each configuration which involves training the gating network.",
                                    "leftover": "\\setlengthsep{0.0em} Impact of adaptation factor . We analyze the impact of on \\ours and present our analysis in \\tabref{tab:resultbeta}. As expected, a smaller prioritizes segmentation quality over computations resulting in superior performance. Conversely, a larger signifies a greater emphasis on GFLOPs. This results in a slight sacrifice in PQ leading to a significant reduction in GFLOPs. Impact of total encoder layers in the parent architecture.}In situations where computational resources are limited, we present an approach using {\\ours} to create a scaleddown version of the parent model. As illustrated in \\Tabref{tab:resultsmallermodels}, we analyze the impact of set to 5 and 4 (in place of the default value of 6). We set =0.0005 for all the models. It can be observed that {\\ours} not only reduces the computational load but also preserves the performance of the parent model. Importantly, we only}execute Step \\stepC for each configuration which involves training the gating network.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par5/table3",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/par5/table3/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 74,
                                                    "key": "doc/body/sec3/par5/table3/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Stochastic Depth (SD) training.",
                                                    "leftover": "Stochastic Depth (SD) training.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 75,
                                            "key": "doc/body/sec3/par5/table3/txl1",
                                            "block type": "txl",
                                            "content": "Here, all models (w/ 6 encoder layers) deterministically exit at the \\hllayer{marked layer} at inference. ''USD'': Unweighted; ''WSD'': Weighted. (Baseline: M2F w/ SWINT)} \\columnwidth{!}{ \\setlength{\\tabcolsep}{10pt}",
                                            "leftover": "Here, all models (w/ 6 encoder layers) deterministically exit at the \\hllayer{marked layer} at inference. ''USD'': Unweighted; ''WSD'': Weighted. (Baseline: M2F w/ SWINT)} \\columnwidth{!}{ \\setlength{\\tabcolsep}{10pt}",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 76,
                                            "key": "doc/body/sec3/par5/table3/tabular2",
                                            "block type": "tabular",
                                            "content": "lccccc \\cellcolorwhite & \\multicolumn{5}cPQ(↑)} \\cline{26} \\rowcolor{gray!20} \\cellcolorwhite \\multirow{2}{*}Model& 2}& 3}& 4}& 5}& 6} \\multicolumn{6}c{Dataset: Cityscapes} Baseline & 03.21 & 14.37 & 26.66 & 42.50 & 64.00 w/ USD & 59.96 & 61.85 & 63.18 & 62.98 & 63.73 w/ WSD & 60.71 & 62.14 & 62.94 & 63.89 & 64.60 \\multicolumn{6}c{Dataset: COCO} Baseline & 10.16 & 17.02 & 23.43 & 33.63 & 51.71 w/ USD & 49.40 & 50.25 & 50.49 & 50.51 & 50.44 w/ WSD & 50.70 & 51.76 & 52.30 & 52.39 & 52.48",
                                            "leftover": "lccccc \\cellcolorwhite & \\multicolumn{5}cPQ(↑)} \\cline{26} \\rowcolor{gray!20} \\cellcolorwhite \\multirow{2}{*}Model& 2}& 3}& 4}& 5}& 6} \\multicolumn{6}c{Dataset: Cityscapes} Baseline & 03.21 & 14.37 & 26.66 & 42.50 & 64.00 w/ USD & 59.96 & 61.85 & 63.18 & 62.98 & 63.73 w/ WSD & 60.71 & 62.14 & 62.94 & 63.89 & 64.60 \\multicolumn{6}c{Dataset: COCO} Baseline & 10.16 & 17.02 & 23.43 & 33.63 & 51.71 w/ USD & 49.40 & 50.25 & 50.49 & 50.51 & 50.44 w/ WSD & 50.70 & 51.76 & 52.30 & 52.39 & 52.48",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 77,
                                            "key": "doc/body/sec3/par5/table3/txl3",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/par5/table3/minipage4",
                                            "block_type": "minipage",
                                            "children": [
                                                {
                                                    "leaf id": 78,
                                                    "key": "doc/body/sec3/par5/table3/minipage4/txl0",
                                                    "block type": "txl",
                                                    "content": "{0.595} \\includegraphics[width=\\columnwidth]{figures/images/paretoplot.pdf}",
                                                    "leftover": "{0.595} \\includegraphics[width=\\columnwidth]{figures/images/paretoplot.pdf}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 79,
                                                    "key": "doc/body/sec3/par5/table3/minipage4/txl1",
                                                    "block type": "txl",
                                                    "content": "{0.395} \\captionoffigure{\\textcolor{mapcolor}{M2Fi} is trained w/ total i layers. \\textcolor{r1color}{ℓi} is result of same model from our Step \\stepA. \\textcolor{ao(english)}Values denote β. Dataset: COCO.",
                                                    "leftover": "{0.395} \\captionoffigure{\\textcolor{mapcolor}{M2Fi} is trained w/ total i layers. \\textcolor{r1color}{ℓi} is result of same model from our Step \\stepA. \\textcolor{ao(english)}Values denote β. Dataset: COCO.",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 80,
                                                    "key": "doc/body/sec3/par5/table3/minipage4/txl2",
                                                    "block type": "txl",
                                                    "content": "}",
                                                    "leftover": "}",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec3/par5/table3/cpt5",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 81,
                                                    "key": "doc/body/sec3/par5/table3/cpt5/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of backbone size.",
                                                    "leftover": "Impact of backbone size.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 82,
                                            "key": "doc/body/sec3/par5/table3/tabular6",
                                            "block type": "tabular",
                                            "content": "llccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multicolumn{8}c{Dataset: COCO} \\multirow{2}{*}{SWINT} & M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & {\\ours} & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\multirow{2}{*}{SWINS} & M2F & 54.63 & 64.24 & 44.69 && 316.50 & 121.69 & {\\ours} & 54.76 & 64.46 & 44.48 && 275.96 & 81.05 \\multirow{2}{*}{SWINB^†} & M2F & 56.40 & 67.09 & 46.29 && 470.98 & 122.56 & {\\ours} & 56.49 & 66.56 & 46.40 && 425.17 & 76.50 \\multicolumn{8}c{Dataset: Cityscapes} \\multirow{2}{*}{SWINT} & M2F & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 & {\\ours} & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 \\multirow{2}{*}{SWINS} & M2F & 64.84 & 81.76 & 40.73 && 724.29 & 281.13 & {\\ours} & 65.12 & 81.64 & 41.17 && 665.38 & 222.22 \\multirow{2}{*}{SWINB^†} & M2F & 66.12 & 82.70 & 42.84 && 1051.19 & 283.14 & {\\ours} & 65.44 & 82.05 & 40.51 && 984.73 & 216.69",
                                            "leftover": "llccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multicolumn{8}c{Dataset: COCO} \\multirow{2}{*}{SWINT} & M2F & 52.03 & 62.49 & 42.18 && 235.57 & 121.69 & {\\ours} & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 \\multirow{2}{*}{SWINS} & M2F & 54.63 & 64.24 & 44.69 && 316.50 & 121.69 & {\\ours} & 54.76 & 64.46 & 44.48 && 275.96 & 81.05 \\multirow{2}{*}{SWINB^†} & M2F & 56.40 & 67.09 & 46.29 && 470.98 & 122.56 & {\\ours} & 56.49 & 66.56 & 46.40 && 425.17 & 76.50 \\multicolumn{8}c{Dataset: Cityscapes} \\multirow{2}{*}{SWINT} & M2F & 64.00 & 80.77 & 39.26 && 537.85 & 281.13 & {\\ours} & 64.18 & 80.49 & 39.64 && 507.51 & 250.80 \\multirow{2}{*}{SWINS} & M2F & 64.84 & 81.76 & 40.73 && 724.29 & 281.13 & {\\ours} & 65.12 & 81.64 & 41.17 && 665.38 & 222.22 \\multirow{2}{*}{SWINB^†} & M2F & 66.12 & 82.70 & 42.84 && 1051.19 & 283.14 & {\\ours} & 65.44 & 82.05 & 40.51 && 984.73 & 216.69",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 83,
                                            "key": "doc/body/sec3/par5/table3/txl7",
                                            "block type": "txl",
                                            "content": "} \\vspace*{2\\baselineskip}",
                                            "leftover": "} \\vspace*{2\\baselineskip}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par6",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 84,
                                    "key": "doc/body/sec3/par6/txl0",
                                    "block type": "txl",
                                    "content": "Impact of backbone features. We analyze the impact of backbone features on \\ours in \\Tabref{tab:resultbackbones} and \\Tabref{tab:resultweights}, since our gating network mechanism takes these features as input.",
                                    "leftover": "Impact of backbone features. We analyze the impact of backbone features on \\ours in \\Tabref{tab:resultbackbones} and \\Tabref{tab:resultweights}, since our gating network mechanism takes these features as input.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 85,
                                    "key": "doc/body/sec3/par6/itemize1",
                                    "block type": "itemize",
                                    "content": "\\setlengthsep{0.0em} Size variations.}In \\tabref{tab:resultbackbones}, we evaluate {\\ours} with backbones of different sizes. Specifically, we observe the performance with SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . We can observe the adaptability of {\\ours} in delivering robust performance efficiency across a range of these backbone sizes. Furthermore, this versatility of {\\ours} extends to LiteM2F metaarchitecture as well (see supplementary material). Pretrained weight variations.}We explore how different pretraining strategies for the backbones affect the performance of {\\ours} in \\tabref{tab:resultweights}. We initialize the backbone weights using both supervised learning (SL) and selfsupervised learning (SSL) techniques on the ImageNet1K dataset . For SSL pretraining on the SWINT backbone, we utilize MoBY . For SSL pretraining on the Res50 backbone, we use DINO . With MoBY weights on the SWINT backbone, {\\ours} maintains 99.7% of the PQ of M2F while reducing GFLOPs in the transformer encoder by 26.34%. With DINO weights on the Res50 backbone, we observe a reduction of 29.79% in GFLOPs in the transformer encoder, alongside a slight improvement in PQ.",
                                    "leftover": "\\setlengthsep{0.0em} Size variations.}In \\tabref{tab:resultbackbones}, we evaluate {\\ours} with backbones of different sizes. Specifically, we observe the performance with SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . We can observe the adaptability of {\\ours} in delivering robust performance efficiency across a range of these backbone sizes. Furthermore, this versatility of {\\ours} extends to LiteM2F metaarchitecture as well (see supplementary material). Pretrained weight variations.}We explore how different pretraining strategies for the backbones affect the performance of {\\ours} in \\tabref{tab:resultweights}. We initialize the backbone weights using both supervised learning (SL) and selfsupervised learning (SSL) techniques on the ImageNet1K dataset . For SSL pretraining on the SWINT backbone, we utilize MoBY . For SSL pretraining on the Res50 backbone, we use DINO . With MoBY weights on the SWINT backbone, {\\ours} maintains 99.7% of the PQ of M2F while reducing GFLOPs in the transformer encoder by 26.34%. With DINO weights on the Res50 backbone, we observe a reduction of 29.79% in GFLOPs in the transformer encoder, alongside a slight improvement in PQ.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par6/table2",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/par6/table2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 86,
                                                    "key": "doc/body/sec3/par6/table2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact on DETR.",
                                                    "leftover": "Impact on DETR.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 87,
                                            "key": "doc/body/sec3/par6/table2/tabular1",
                                            "block type": "tabular",
                                            "content": "lcccccccc & \\multicolumn{5}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{26} \\cline{89} \\multirow{2}{*}Model & AP}& AP50 & APS & APM & APL && Total}& Tx. Enc.} DETR & 42.0 & 62.4 & 20.5 & 45.8 & 61.1 && 83.59 & 9.92 {\\oursdetr}(=0.0001) & 41.9 & 62.2 & 20.8 & 45.8 & 60.4 && 81.87 & 6.41 {\\oursdetr}(=0.001) & 40.9 & 61.5 & 20.2 & 44.5 & 59.1 && 79.92 & 4.52 {\\oursdetr}(=0.01) & 40.2 & 61.0 & 18.7 & 43.6 & 58.8 && 79.33 & 3.94",
                                            "leftover": "lcccccccc & \\multicolumn{5}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{26} \\cline{89} \\multirow{2}{*}Model & AP}& AP50 & APS & APM & APL && Total}& Tx. Enc.} DETR & 42.0 & 62.4 & 20.5 & 45.8 & 61.1 && 83.59 & 9.92 {\\oursdetr}(=0.0001) & 41.9 & 62.2 & 20.8 & 45.8 & 60.4 && 81.87 & 6.41 {\\oursdetr}(=0.001) & 40.9 & 61.5 & 20.2 & 44.5 & 59.1 && 79.92 & 4.52 {\\oursdetr}(=0.01) & 40.2 & 61.0 & 18.7 & 43.6 & 58.8 && 79.33 & 3.94",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 88,
                                            "key": "doc/body/sec3/par6/table2/txl2",
                                            "block type": "txl",
                                            "content": "} l {0.495}",
                                            "leftover": "} l {0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/par6/table2/cpt3",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 89,
                                                    "key": "doc/body/sec3/par6/table2/cpt3/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of backbone weights.",
                                                    "leftover": "Impact of backbone weights.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 90,
                                            "key": "doc/body/sec3/par6/table2/txl4",
                                            "block type": "txl",
                                            "content": "{\\ours} applies to backbone weights obtained through selfsupervised pretraining as well. (Dataset: COCO)} \\columnwidth{!}{",
                                            "leftover": "{\\ours} applies to backbone weights obtained through selfsupervised pretraining as well. (Dataset: COCO)} \\columnwidth{!}{",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 91,
                                            "key": "doc/body/sec3/par6/table2/tabular5",
                                            "block type": "tabular",
                                            "content": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multirow{2}{*}{SWINT(MoBY)} & M2F & 51.64 & 62.42 & 41.93 && 235.57 & 121.69 & {\\ours} & 51.48 & 62.49 & 41.27 && 203.56 & 89.64 \\multirow{2}{*}{Res50(DINO)} & M2F & 51.57 & 61.65 & 41.24 && 229.41 & 126.05 & {\\ours} & 51.89 & 61.67 & 41.30 && 191.97 & 88.50",
                                            "leftover": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multirow{2}{*}{SWINT(MoBY)} & M2F & 51.64 & 62.42 & 41.93 && 235.57 & 121.69 & {\\ours} & 51.48 & 62.49 & 41.27 && 203.56 & 89.64 \\multirow{2}{*}{Res50(DINO)} & M2F & 51.57 & 61.65 & 41.24 && 229.41 & 126.05 & {\\ours} & 51.89 & 61.67 & 41.30 && 191.97 & 88.50",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 92,
                                            "key": "doc/body/sec3/par6/table2/txl6",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par7",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 93,
                                    "key": "doc/body/sec3/par7/txl0",
                                    "block type": "txl",
                                    "content": "Performance in object detection. To demonstrate the applicability of our method beyond segmentation, we extend our proposed three step recipe for object detection task using DETR for object detection task and name it \\oursdetr. In particular, we vary the values of the adaptation factor to analyze performancecomputation tradeoffs of \\oursdetr in \\tabref{tab:resultdetr}. Clearly, the resultant architecture maintains the performance but helps in reducing the computations of the encoder (\\eg, it achieves a 35.38% reduction in GFLOPs in the transformer encoder without significantly impacting performance).",
                                    "leftover": "Performance in object detection. To demonstrate the applicability of our method beyond segmentation, we extend our proposed three step recipe for object detection task using DETR for object detection task and name it \\oursdetr. In particular, we vary the values of the adaptation factor to analyze performancecomputation tradeoffs of \\oursdetr in \\tabref{tab:resultdetr}. Clearly, the resultant architecture maintains the performance but helps in reducing the computations of the encoder (\\eg, it achieves a 35.38% reduction in GFLOPs in the transformer encoder without significantly impacting performance).",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par7/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 94,
                                            "key": "doc/body/sec3/par7/sub1/tit",
                                            "block type": "title",
                                            "content": "Qualitative Comparisons",
                                            "leftover": "Qualitative Comparisons",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec3/par7/sub1/figure0",
                                            "block_type": "figure",
                                            "children": [
                                                {
                                                    "leaf id": 95,
                                                    "key": "doc/body/sec3/par7/sub1/figure0/tcbraster0",
                                                    "block type": "tcbraster",
                                                    "content": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp10/000000297427.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp10/000000297427m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp10/000000297427ours.jpg} \\vspace*{2\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp11/000000300913.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp11/000000300913m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp11/000000300913ours.jpg} \\vspace*{3\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bitours.png} \\vspace*{6.25\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bitours.png} \\vspace*{2\\baselineskip}",
                                                    "leftover": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp10/000000297427.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp10/000000297427m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp10/000000297427ours.jpg} \\vspace*{2\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp11/000000300913.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp11/000000300913m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp11/000000300913ours.jpg} \\vspace*{3\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp1/frankfurt000000016286leftImg8bitours.png} \\vspace*{6.25\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp2/frankfurt000001004327leftImg8bitours.png} \\vspace*{2\\baselineskip}",
                                                    "matches": []
                                                },
                                                {
                                                    "leaf id": 96,
                                                    "key": "doc/body/sec3/par7/sub1/figure0/cpt1",
                                                    "block type": "cpt",
                                                    "content": "Qualitative visualizations.",
                                                    "leftover": "Qualitative visualizations.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 97,
                                            "key": "doc/body/sec3/par7/sub1/txl1",
                                            "block type": "txl",
                                            "content": "We present a few examples of predicted segmentation maps in \\Figref{fig:qualresult} with SWINT backbone. Compared to the parent architecture, \\ours consistently shows strong performance while selfselecting the encoder layers based on the input examples, both in everyday scenes (on COCO dataset) as well as intricate traffic scenes (on Cityscapes dataset).",
                                            "leftover": "We present a few examples of predicted segmentation maps in \\Figref{fig:qualresult} with SWINT backbone. Compared to the parent architecture, \\ours consistently shows strong performance while selfselecting the encoder layers based on the input examples, both in everyday scenes (on COCO dataset) as well as intricate traffic scenes (on Cityscapes dataset).",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 98,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Conclusions",
                            "leftover": "Conclusions",
                            "matches": []
                        },
                        {
                            "leaf id": 99,
                            "key": "doc/body/sec4/txl0",
                            "block type": "txl",
                            "content": "In this paper, we propose an efficient transformer encoder design {\\ours} for the Mask2Formerstyle frameworks. \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. The first step involves training the parent model to be dynamic}by allowing stochastic depths at the transformer encoder. The second step involves creating a derived dataset from the training dataset which contains a pair of image and layer number that provides the highest segmentation quality. Finally, the third step involves training a gating network, whose function is to decide the number of layers to be used given the input image. Extensive experiments demonstrate that {\\ours} achieves significantly reduced computational complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\\ours}'s ability to dynamically tradeoff between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks.",
                            "leftover": "In this paper, we propose an efficient transformer encoder design {\\ours} for the Mask2Formerstyle frameworks. \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. The first step involves training the parent model to be dynamic}by allowing stochastic depths at the transformer encoder. The second step involves creating a derived dataset from the training dataset which contains a pair of image and layer number that provides the highest segmentation quality. Finally, the third step involves training a gating network, whose function is to decide the number of layers to be used given the input image. Extensive experiments demonstrate that {\\ours} achieves significantly reduced computational complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\\ours}'s ability to dynamically tradeoff between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec4/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 100,
                                    "key": "doc/body/sec4/par1/txl0",
                                    "block type": "txl",
                                    "content": "Limitations. While {\\ours} offers dynamic tradeoffs between performance and efficiency according to specific needs, the adaptation factor is a hyperparameter that needs separate tuning for each use case. This is because it relies on the model configuration and dataset characteristics. \\clearpage",
                                    "leftover": "Limitations. While {\\ours} offers dynamic tradeoffs between performance and efficiency according to specific needs, the adaptation factor is a hyperparameter that needs separate tuning for each use case. This is because it relies on the model configuration and dataset characteristics. \\clearpage",
                                    "matches": []
                                },
                                {
                                    "leaf id": 101,
                                    "key": "doc/body/sec4/par1/center1",
                                    "block type": "center",
                                    "content": "\\Large{Efficient Transformer Encoders for Mask2Formerstyle models} (Supplementary Material)} \\vspace*{2em}",
                                    "leftover": "\\Large{Efficient Transformer Encoders for Mask2Formerstyle models} (Supplementary Material)} \\vspace*{2em}",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 102,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Additional Experiments",
                            "leftover": "Additional Experiments",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec5/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 103,
                                    "key": "doc/body/sec5/par0/txl0",
                                    "block type": "txl",
                                    "content": "Impact of backbone size on LiteM2F. We apply {\\ours} on LiteM2F using various backbone sizes, including SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . LiteM2F is a specific variant based on LiteDETR . We used the configuration named ''LiteDETR H3L1(6+1)×1'' given its strong performance in detection relative to the computations required. However, we adjust this configuration to (5+1) when applying our approach to LiteM2F. Further, we use their without the keyaware deformable attention proposed in their paper. This adjustment is necessary because LiteM2F actually has 6 encoder layers, and the original configuration might introduce an additional layer that isn't present in the model. Following this, we identify layers 2 to 5 as potential exits, followed by the last layer, layer 6 in the transformer encoder. We retain layer 6 and do not consider it as a feasible exit point as it leverages features from all scales provided by the backbone, making it essential to the model's functionality. As shown in \\Tabref{tab:suppresultbackbones}, we observe that {\\ours} effectively reduces computational cost while maintaining performance across LiteM2F variants, which underscores the versatility and robustness of {\\ours} across different model architectures and sizes.",
                                    "leftover": "Impact of backbone size on LiteM2F. We apply {\\ours} on LiteM2F using various backbone sizes, including SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . LiteM2F is a specific variant based on LiteDETR . We used the configuration named ''LiteDETR H3L1(6+1)×1'' given its strong performance in detection relative to the computations required. However, we adjust this configuration to (5+1) when applying our approach to LiteM2F. Further, we use their without the keyaware deformable attention proposed in their paper. This adjustment is necessary because LiteM2F actually has 6 encoder layers, and the original configuration might introduce an additional layer that isn't present in the model. Following this, we identify layers 2 to 5 as potential exits, followed by the last layer, layer 6 in the transformer encoder. We retain layer 6 and do not consider it as a feasible exit point as it leverages features from all scales provided by the backbone, making it essential to the model's functionality. As shown in \\Tabref{tab:suppresultbackbones}, we observe that {\\ours} effectively reduces computational cost while maintaining performance across LiteM2F variants, which underscores the versatility and robustness of {\\ours} across different model architectures and sizes.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 104,
                                    "key": "doc/body/sec5/par1/txl0",
                                    "block type": "txl",
                                    "content": "Impact of target and loss settings for gating network training. We investigate various target and loss settings during the training of the gating network. Specifically, we compare the approach detailed in the main paper, using onehot target and crossentropy loss (referred to as ''hardCE'' in \\Tabref{tab:tgtloss}), with three alternative methods that do not involve setting a specific target exit for each image.",
                                    "leftover": "Impact of target and loss settings for gating network training. We investigate various target and loss settings during the training of the gating network. Specifically, we compare the approach detailed in the main paper, using onehot target and crossentropy loss (referred to as ''hardCE'' in \\Tabref{tab:tgtloss}), with three alternative methods that do not involve setting a specific target exit for each image.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 105,
                                    "key": "doc/body/sec5/par1/txl1",
                                    "block type": "txl",
                                    "content": "First, we consider using crossentropy loss between the output of the utility function (·) and the predicted logit passed through a softmax function (referred to as ''uCE''), \\ie,",
                                    "leftover": "First, we consider using crossentropy loss between the output of the utility function (·) and the predicted logit passed through a softmax function (referred to as ''uCE''), \\ie,",
                                    "matches": []
                                },
                                {
                                    "leaf id": 106,
                                    "key": "doc/body/sec5/par1/frm2",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^()ln[softmax(g)] .",
                                    "leftover": "gating=∑i^N∑^()ln[softmax(g)] .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 107,
                                    "key": "doc/body/sec5/par1/txl3",
                                    "block type": "txl",
                                    "content": "Second, we apply a softmax function to the utility function u() and use crossentropy as the loss function (referred to as ''softCE''), \\ie,",
                                    "leftover": "Second, we apply a softmax function to the utility function u() and use crossentropy as the loss function (referred to as ''softCE''), \\ie,",
                                    "matches": []
                                },
                                {
                                    "leaf id": 108,
                                    "key": "doc/body/sec5/par1/frm4",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^softmax(())ln[softmax(g)] .",
                                    "leftover": "gating=∑i^N∑^softmax(())ln[softmax(g)] .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 109,
                                    "key": "doc/body/sec5/par1/txl5",
                                    "block type": "txl",
                                    "content": "Third, we apply a softmax function to the utility function, but use mean squared error (MSE) loss instead (referred to as ''softMSE''), \\ie,",
                                    "leftover": "Third, we apply a softmax function to the utility function, but use mean squared error (MSE) loss instead (referred to as ''softMSE''), \\ie,",
                                    "matches": []
                                },
                                {
                                    "leaf id": 110,
                                    "key": "doc/body/sec5/par1/frm6",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^[softmax(())softmax(g)]^2 .",
                                    "leftover": "gating=∑i^N∑^[softmax(())softmax(g)]^2 .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 111,
                                    "key": "doc/body/sec5/par1/txl7",
                                    "block type": "txl",
                                    "content": "The analysis in \\Tabref{tab:tgtloss} is conducted using the SWINT backbone on the COCO dataset. We observe that ''hardCE'' yields the most favorable results. As a result, we use this approach consistently in the main paper.",
                                    "leftover": "The analysis in \\Tabref{tab:tgtloss} is conducted using the SWINT backbone on the COCO dataset. We observe that ''hardCE'' yields the most favorable results. As a result, we use this approach consistently in the main paper.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec5/par1/table8",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "leaf id": 112,
                                            "key": "doc/body/sec5/par1/table8/txl0",
                                            "block type": "txl",
                                            "content": "[!hb]{0.495}",
                                            "leftover": "[!hb]{0.495}",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec5/par1/table8/cpt1",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 113,
                                                    "key": "doc/body/sec5/par1/table8/cpt1/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of backbone size on LiteM2F.",
                                                    "leftover": "Impact of backbone size on LiteM2F.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 114,
                                            "key": "doc/body/sec5/par1/table8/txl2",
                                            "block type": "txl",
                                            "content": "Our Lite{\\ours} maintains the performance of LiteM2F while reducing GFLOPs for different datasets and for different backbones.} \\columnwidth{!}{",
                                            "leftover": "Our Lite{\\ours} maintains the performance of LiteM2F while reducing GFLOPs for different datasets and for different backbones.} \\columnwidth{!}{",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 115,
                                            "key": "doc/body/sec5/par1/table8/tabular3",
                                            "block type": "tabular",
                                            "content": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multicolumn{8}c{Dataset: Cityscapes} \\multirow{2}{*}{SWINT} & LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 & Lite{\\ours} & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\multirow{2}{*}{SWINS} & LiteM2F & 63.54 & 79.74 & 39.12 && 615.15 & 171.99 & Lite{\\ours} & 63.32 & 80.21 & 37.91 && 588.82 & 145.66 \\multirow{2}{*}{SWINB} & LiteM2F & 64.48 & 82.34 & 39.21 && 942.05 & 174.01 & Lite{\\ours} & 64.66 & 81.40 & 39.52 && 921.15 & 153.11 \\multicolumn{8}c{Dataset: COCO} \\multirow{2}{*}{}SWINT & LiteM2F & 52.70 & 63.08 & 41.10 && 193.79 & 79.78 & Lite{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\multirow{2}{*}{SWINS} & LiteM2F & 54.30 & 64.81 & 43.94 && 269.26 & 74.45 & Lite{\\ours} & 54.47 & 64.14 & 43.55 && 258.00 & 63.96",
                                            "leftover": "llcccccccc && \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{35} \\cline{78} \\multirow{2}{*}Bakbone & \\multirow{2}{*}Model & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\multicolumn{8}c{Dataset: Cityscapes} \\multirow{2}{*}{SWINT} & LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 & Lite{\\ours} & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\multirow{2}{*}{SWINS} & LiteM2F & 63.54 & 79.74 & 39.12 && 615.15 & 171.99 & Lite{\\ours} & 63.32 & 80.21 & 37.91 && 588.82 & 145.66 \\multirow{2}{*}{SWINB} & LiteM2F & 64.48 & 82.34 & 39.21 && 942.05 & 174.01 & Lite{\\ours} & 64.66 & 81.40 & 39.52 && 921.15 & 153.11 \\multicolumn{8}c{Dataset: COCO} \\multirow{2}{*}{}SWINT & LiteM2F & 52.70 & 63.08 & 41.10 && 193.79 & 79.78 & Lite{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\multirow{2}{*}{SWINS} & LiteM2F & 54.30 & 64.81 & 43.94 && 269.26 & 74.45 & Lite{\\ours} & 54.47 & 64.14 & 43.55 && 258.00 & 63.96",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 116,
                                            "key": "doc/body/sec5/par1/table8/txl4",
                                            "block type": "txl",
                                            "content": "} l",
                                            "leftover": "} l",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec5/par1/table8/cpt5",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 117,
                                                    "key": "doc/body/sec5/par1/table8/cpt5/txl0",
                                                    "block type": "txl",
                                                    "content": "Impact of target and loss in gating network training.",
                                                    "leftover": "Impact of target and loss in gating network training.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 118,
                                            "key": "doc/body/sec5/par1/table8/txl6",
                                            "block type": "txl",
                                            "content": "We use ''hardCE'' loss for training our gating network in the main paper. (Backbone: SWINT; Dataset: COCO)} \\columnwidth{!}{",
                                            "leftover": "We use ''hardCE'' loss for training our gating network in the main paper. (Backbone: SWINT; Dataset: COCO)} \\columnwidth{!}{",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 119,
                                            "key": "doc/body/sec5/par1/table8/tabular7",
                                            "block type": "tabular",
                                            "content": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Method & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\rowcolor{eccvblue!20} hardCE & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 uCE & 52.16 & 62.58 & 41.57 && 207.49 & 94.06 softCE & 51.64 & 62.75 & 40.88 && 202.08 & 87.85 softMSE & 51.54 & 62.73 & 40.91 && 198.46 & 84.53",
                                            "leftover": "lcccccccc & \\multicolumn{3}cPerformance(↑)} && \\multicolumn{2}cGFLOPs(↓)} \\cline{24} \\cline{67} \\multirow{2}{*}Method & PQ}& mIOUp & APp && Total}& Tx. Enc.} \\rowcolor{eccvblue!20} hardCE & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 uCE & 52.16 & 62.58 & 41.57 && 207.49 & 94.06 softCE & 51.64 & 62.75 & 40.88 && 202.08 & 87.85 softMSE & 51.54 & 62.73 & 40.91 && 198.46 & 84.53",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 120,
                                            "key": "doc/body/sec5/par1/table8/txl8",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 121,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Additional Qualitative Results",
                            "leftover": "Additional Qualitative Results",
                            "matches": []
                        },
                        {
                            "leaf id": 122,
                            "key": "doc/body/sec6/txl0",
                            "block type": "txl",
                            "content": "We provide additional examples of predicted segmentation maps in \\Figref{fig:suppqualresult}.",
                            "leftover": "We provide additional examples of predicted segmentation maps in \\Figref{fig:suppqualresult}.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec6/figure1",
                            "block_type": "figure",
                            "children": [
                                {
                                    "leaf id": 123,
                                    "key": "doc/body/sec6/figure1/tcbraster0",
                                    "block type": "tcbraster",
                                    "content": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp9/000000176232ours.jpg} \\vspace*{2.5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp2/000000002149ours.jpg} \\vspace*{5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitours.png} \\vspace*{8\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitours.png} \\vspace*{2\\baselineskip}",
                                    "leftover": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp9/000000176232ours.jpg} \\vspace*{2.5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp2/000000002149ours.jpg} \\vspace*{5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitours.png} \\vspace*{8\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitours.png} \\vspace*{2\\baselineskip}",
                                    "matches": []
                                },
                                {
                                    "leaf id": 124,
                                    "key": "doc/body/sec6/figure1/cpt1",
                                    "block type": "cpt",
                                    "content": "Qualitative visualizations.",
                                    "leftover": "Qualitative visualizations.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 125,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Ammar, A., Khalil, M.I., Salama, C.: Rtyoso: Revisiting yoso for realtime panoptic segmentation. In: 2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES). pp. 306311 (2023). \\doi{10.1109/NILES59815.2023.10296714}",
            "leftover": "Ammar, A., Khalil, M.I., Salama, C.: Rtyoso: Revisiting yoso for realtime panoptic segmentation. In: 2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES). pp. 306311 (2023). \\doi{10.1109/NILES59815.2023.10296714}",
            "matches": []
        },
        {
            "leaf id": 126,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endtoend object detection with transformers. In: European conference on computer vision. pp. 213229. Springer (2020)",
            "leftover": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endtoend object detection with transformers. In: European conference on computer vision. pp. 213229. Springer (2020)",
            "matches": []
        },
        {
            "leaf id": 127,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Caron, M., Touvron, H., Misra, I., J\\'egou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in selfsupervised vision transformers. In: Proceedings of the International Conference on Computer Vision (ICCV) (2021)",
            "leftover": "Caron, M., Touvron, H., Misra, I., J\\'egou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in selfsupervised vision transformers. In: Proceedings of the International Conference on Computer Vision (ICCV) (2021)",
            "matches": []
        },
        {
            "leaf id": 128,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: Panopticdeeplab: A simple, strong, and fast baseline for bottomup panoptic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1247512485 (2020)",
            "leftover": "Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: Panopticdeeplab: A simple, strong, and fast baseline for bottomup panoptic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1247512485 (2020)",
            "matches": []
        },
        {
            "leaf id": 129,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention mask transformer for universal image segmentation. In: CVPR (2022)",
            "leftover": "Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention mask transformer for universal image segmentation. In: CVPR (2022)",
            "matches": []
        },
        {
            "leaf id": 130,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Cheng, B., Schwing, A., Kirillov, A.: Perpixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems \\textbf{34}, 1786417875 (2021)",
            "leftover": "Cheng, B., Schwing, A., Kirillov, A.: Perpixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems \\textbf{34}, 1786417875 (2021)",
            "matches": []
        },
        {
            "leaf id": 131,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 32133223 (2016)",
            "leftover": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 32133223 (2016)",
            "matches": []
        },
        {
            "leaf id": 132,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., FeiFei, L.: Imagenet: A largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee (2009)",
            "leftover": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., FeiFei, L.: Imagenet: A largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee (2009)",
            "matches": []
        },
        {
            "leaf id": 133,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Everingham, M., Eslami, S.A., Van~Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes challenge: A retrospective. International journal of computer vision \\textbf{111}, 98136 (2015)",
            "leftover": "Everingham, M., Eslami, S.A., Van~Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes challenge: A retrospective. International journal of computer vision \\textbf{111}, 98136 (2015)",
            "matches": []
        },
        {
            "leaf id": 134,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for realtime semantic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 97169725 (2021)",
            "leftover": "Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for realtime semantic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 97169725 (2021)",
            "matches": []
        },
        {
            "leaf id": 135,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Gu, X., Cui, Y., Huang, J., Rashwan, A., Yang, X., Zhou, X., Ghiasi, G., Kuo, W., Chen, H., Chen, L.C., et~al.: Dataseg: Taming a universal multidataset multitask segmentation model. Advances in Neural Information Processing Systems \\textbf{36} (2024)",
            "leftover": "Gu, X., Cui, Y., Huang, J., Rashwan, A., Yang, X., Zhou, X., Ghiasi, G., Kuo, W., Chen, H., Chen, L.C., et~al.: Dataseg: Taming a universal multidataset multitask segmentation model. Advances in Neural Information Processing Systems \\textbf{36} (2024)",
            "matches": []
        },
        {
            "leaf id": 136,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "He, K., Gkioxari, G., Doll{\\'a}r, P., Girshick, R.: Mask rcnn. In: Proceedings of the IEEE international conference on computer vision. pp. 29612969 (2017)",
            "leftover": "He, K., Gkioxari, G., Doll{\\'a}r, P., Girshick, R.: Mask rcnn. In: Proceedings of the IEEE international conference on computer vision. pp. 29612969 (2017)",
            "matches": []
        },
        {
            "leaf id": 137,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778 (2016)",
            "leftover": "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778 (2016)",
            "matches": []
        },
        {
            "leaf id": 138,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Hou, R., Li, J., Bhargava, A., Raventos, A., Guizilini, V., Fang, C., Lynch, J., Gaidon, A.: Realtime panoptic segmentation from dense detections. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 85238532 (2020)",
            "leftover": "Hou, R., Li, J., Bhargava, A., Raventos, A., Guizilini, V., Fang, C., Lynch, J., Gaidon, A.: Realtime panoptic segmentation from dense detections. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 85238532 (2020)",
            "matches": []
        },
        {
            "leaf id": 139,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Hu, J., Huang, L., Ren, T., Zhang, S., Ji, R., Cao, L.: You only segment once: Towards realtime panoptic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1781917829 (2023)",
            "leftover": "Hu, J., Huang, L., Ren, T., Zhang, S., Ji, R., Cao, L.: You only segment once: Towards realtime panoptic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1781917829 (2023)",
            "matches": []
        },
        {
            "leaf id": 140,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer to rule universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 29892998 (2023)",
            "leftover": "Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer to rule universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 29892998 (2023)",
            "matches": []
        },
        {
            "leaf id": 141,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jiang, Z., Gong, Z., Xu, Y., Wang, J.: Multiexit vision transformer with custom finetuning for finegrained image recognition. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 52335237 (2023)",
            "leftover": "Jiang, Z., Gong, Z., Xu, Y., Wang, J.: Multiexit vision transformer with custom finetuning for finegrained image recognition. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 52335237 (2023)",
            "matches": []
        },
        {
            "leaf id": 142,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)",
            "leftover": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)",
            "matches": []
        },
        {
            "leaf id": 143,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Kirillov, A., He, K., Girshick, R., Rother, C., Dollár, P.: Panoptic segmentation (2019)",
            "leftover": "Kirillov, A., He, K., Girshick, R., Rother, C., Dollár, P.: Panoptic segmentation (2019)",
            "matches": []
        },
        {
            "leaf id": 144,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr: An interleaved multiscale encoder for efficient detr. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1855818567 (2023)",
            "leftover": "Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr: An interleaved multiscale encoder for efficient detr. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1855818567 (2023)",
            "matches": []
        },
        {
            "leaf id": 145,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll{\\'a}r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 612, 2014, Proceedings, Part V 13. pp. 740755. Springer (2014)",
            "leftover": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll{\\'a}r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 612, 2014, Proceedings, Part V 13. pp. 740755. Springer (2014)",
            "matches": []
        },
        {
            "leaf id": 146,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1001210022 (2021)",
            "leftover": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1001210022 (2021)",
            "matches": []
        },
        {
            "leaf id": 147,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Liu, Z., Sun, Y., Li, Y., Zhou, Z., Hu, J., Li, F.: Multiexit vision transformer for dynamic inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 52145223 (2021)",
            "leftover": "Liu, Z., Sun, Y., Li, Y., Zhou, Z., Hu, J., Li, F.: Multiexit vision transformer for dynamic inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 52145223 (2021)",
            "matches": []
        },
        {
            "leaf id": 148,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: Detrs beat yolos on realtime object detection. arXiv preprint arXiv:2304.08069 (2023)",
            "leftover": "Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: Detrs beat yolos on realtime object detection. arXiv preprint arXiv:2304.08069 (2023)",
            "matches": []
        },
        {
            "leaf id": 149,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, highperformance deep learning library. In: Advances in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc. (2019)",
            "leftover": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, highperformance deep learning library. In: Advances in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc. (2019)",
            "matches": []
        },
        {
            "leaf id": 150,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Sun, S., Wang, W., Yu, Q., Howard, A., Torr, P., Chen, L.C.: Remax: Relaxing for better training on efficient panoptic segmentation. arXiv preprint arXiv:2306.17319 (2023)",
            "leftover": "Sun, S., Wang, W., Yu, Q., Howard, A., Torr, P., Chen, L.C.: Remax: Relaxing for better training on efficient panoptic segmentation. arXiv preprint arXiv:2306.17319 (2023)",
            "matches": []
        },
        {
            "leaf id": 151,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Tang, J., Liu, Z., Li, Y., Sun, Y., Zhou, Z., Hu, J., Li, F.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1350413513 (2023)",
            "leftover": "Tang, J., Liu, Z., Li, Y., Sun, Y., Zhou, Z., Hu, J., Li, F.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1350413513 (2023)",
            "matches": []
        },
        {
            "leaf id": 152,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Tang, S., Wang, Y., Kong, Z., Zhang, T., Li, Y., Ding, C., Wang, Y., Liang, Y., Xu, D.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1078110791 (2023)",
            "leftover": "Tang, S., Wang, Y., Kong, Z., Zhang, T., Li, Y., Ding, C., Wang, Y., Liang, Y., Xu, D.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1078110791 (2023)",
            "matches": []
        },
        {
            "leaf id": 153,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Tu, Z.: Autocontext and its application to highlevel vision tasks. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition. pp.~18. IEEE (2008)",
            "leftover": "Tu, Z.: Autocontext and its application to highlevel vision tasks. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition. pp.~18. IEEE (2008)",
            "matches": []
        },
        {
            "leaf id": 154,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Valade, F., Hebiri, M., Gay, P.: Eero: Early exit with reject option for efficient classification with limited budget (2024)",
            "leftover": "Valade, F., Hebiri, M., Gay, P.: Eero: Early exit with reject option for efficient classification with limited budget (2024)",
            "matches": []
        },
        {
            "leaf id": 155,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, {\\L}., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems \\textbf{30} (2017)",
            "leftover": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, {\\L}., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems \\textbf{30} (2017)",
            "matches": []
        },
        {
            "leaf id": 156,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y., Zhang, Q., Chowdhury, M., et~al.: Efficient large language models: A survey. arXiv preprint arXiv:2312.03863 \\textbf{1} (2023)",
            "leftover": "Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y., Zhang, Q., Chowdhury, M., et~al.: Efficient large language models: A survey. arXiv preprint arXiv:2312.03863 \\textbf{1} (2023)",
            "matches": []
        },
        {
            "leaf id": 157,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Wang, X., Zhou, W., He, X., Peng, X., Wei, F., Guo, Y.: Singlelayer vision transformers for more accurate early exits with less overhead. Pattern Recognition \\textbf{136}, 102243 (2022)",
            "leftover": "Wang, X., Zhou, W., He, X., Peng, X., Wei, F., Guo, Y.: Singlelayer vision transformers for more accurate early exits with less overhead. Pattern Recognition \\textbf{136}, 102243 (2022)",
            "matches": []
        },
        {
            "leaf id": 158,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://github.com/facebookresearch/detectron2 (2019)",
            "leftover": "Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://github.com/facebookresearch/detectron2 (2019)",
            "matches": []
        },
        {
            "leaf id": 159,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "leftover": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "matches": []
        },
        {
            "leaf id": 160,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "leftover": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "matches": []
        },
        {
            "leaf id": 161,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Xu, F., Zhang, X., Ma, Z., Wang, J., Hu, J., Sun, J.: Lgvit: Dynamic early exiting for accelerating vision transformer. In: Proceedings of the 32nd ACM International Conference on Multimedia. pp. 19581966 (2023)",
            "leftover": "Xu, F., Zhang, X., Ma, Z., Wang, J., Hu, J., Sun, J.: Lgvit: Dynamic early exiting for accelerating vision transformer. In: Proceedings of the 32nd ACM International Conference on Multimedia. pp. 19581966 (2023)",
            "matches": []
        },
        {
            "leaf id": 162,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Xu, J., Xiong, Z., Bhattacharyya, S.P.: Pidnet: A realtime semantic segmentation network inspired by pid controllers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1952919539 (2023)",
            "leftover": "Xu, J., Xiong, Z., Bhattacharyya, S.P.: Pidnet: A realtime semantic segmentation network inspired by pid controllers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1952919539 (2023)",
            "matches": []
        },
        {
            "leaf id": 163,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, C., Wang, S., et~al.: A survey of resourceefficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024)",
            "leftover": "Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, C., Wang, S., et~al.: A survey of resourceefficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024)",
            "matches": []
        },
        {
            "leaf id": 164,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Xu, S., Yuan, H., Shi, Q., Qi, L., Wang, J., Yang, Y., Li, Y., Chen, K., Tong, Y., Ghanem, B., et~al.: Rapsam: Towards realtime allpurpose segment anything. arXiv preprint arXiv:2401.10228 (2024)",
            "leftover": "Xu, S., Yuan, H., Shi, Q., Qi, L., Wang, J., Yang, Y., Li, Y., Chen, K., Tong, Y., Ghanem, B., et~al.: Rapsam: Towards realtime allpurpose segment anything. arXiv preprint arXiv:2401.10228 (2024)",
            "matches": []
        },
        {
            "leaf id": 165,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Yang, J., Zhang, X., Zhang, X., Tang, J., Li, X.: Exploiting face recognizability with early exit vision transformers. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 63416345 (2023)",
            "leftover": "Yang, J., Zhang, X., Zhang, X., Tang, J., Li, X.: Exploiting face recognizability with early exit vision transformers. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 63416345 (2023)",
            "matches": []
        },
        {
            "leaf id": 166,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Yu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N.: Bisenet v2: Bilateral network with guided aggregation for realtime semantic segmentation. International Journal of Computer Vision \\textbf{129}, 30513068 (2021)",
            "leftover": "Yu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N.: Bisenet v2: Bilateral network with guided aggregation for realtime semantic segmentation. International Journal of Computer Vision \\textbf{129}, 30513068 (2021)",
            "matches": []
        },
        {
            "leaf id": 167,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmentation network for realtime semantic segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 325341 (2018)",
            "leftover": "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmentation network for realtime semantic segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 325341 (2018)",
            "matches": []
        },
        {
            "leaf id": 168,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Zhang, T., He, X., Qin, Z., Sun, J.: Adaptive deep neural network inference optimization with eenet. In: Proceedings of the 37th International Conference on Machine Learning. pp. 1598315993 (2023)",
            "leftover": "Zhang, T., He, X., Qin, Z., Sun, J.: Adaptive deep neural network inference optimization with eenet. In: Proceedings of the 37th International Conference on Machine Learning. pp. 1598315993 (2023)",
            "matches": []
        }
    ]
}