{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "Efficient Transformer Encoders for Mask2Formerstyle models",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "0.32",
                    "matching_string": "Efficient Transformer Encoders for "
                },
                {
                    "pdf_id": "0.33",
                    "matching_string": "Mask2Formerstyle models"
                }
            ]
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{Manyi Yao2, Abhishek Aich1, Yumin Suh1, Amit RoyChowdhury2, Christian Shelton2, Manmohan Chandraker1,3 }",
            "leftover": "{}",
            "matches": [
                {
                    "pdf_id": "0.34",
                    "matching_string": "Manyi Yao2, Abhishek Aich1, Yumin Suh1, Amit RoyChowdhury2, "
                },
                {
                    "pdf_id": "0.35",
                    "matching_string": "Christian Shelton2, Manmohan Chandraker1,3 "
                }
            ]
        },
        {
            "leaf id": 2,
            "key": "doc/aut1",
            "block type": "author",
            "content": "running{M.~Yao et al.} \\institute{NEC Laboratories, America, San Jose CA 95110, USA, University of California, Riverside, CA 92521, USA, University of California, San Diego, CA 92093, USA Corresponding author: aaich@neclabs.com",
            "leftover": "running{M.~Yao et al.} \\institute{USA, USA, ",
            "matches": [
                {
                    "pdf_id": "0.39",
                    "matching_string": "Corresponding author: aaich@neclabs.com"
                },
                {
                    "pdf_id": "0.38",
                    "matching_string": "University of California, San Diego, CA 92093, USA "
                },
                {
                    "pdf_id": "0.36",
                    "matching_string": "NEC Laboratories, America, San Jose CA 95110, "
                },
                {
                    "pdf_id": "0.37",
                    "matching_string": "University of California, Riverside, CA 92521, "
                }
            ]
        },
        {
            "leaf id": 3,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices. One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current onesizefitsall approach. To this end, we introduce \\ours or EffiCient TransfOrmer Encoders for Mask2Formerstyle models. Noting that the encoder module of M2Fstyle models incur high resourceintensive computations, \\ours provides a strategy to selfselect the number of hidden layers in the encoder, conditioned on the input image. To enable this selfselection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The first step is to train the parent architecture to enable early exiting from the encoder. The second step is to create an derived dataset of the ideal number of encoder layers required for each training example. The third step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on input image. Additionally, to change the computationalaccuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "0.41",
                    "matching_string": "for image segmentation tasks. Although these architectures offer "
                },
                {
                    "pdf_id": "0.43",
                    "matching_string": "use of computational resources can be taxing on deployed devices. One "
                },
                {
                    "pdf_id": "0.45",
                    "matching_string": "to the specific needs of the input image rather than the current onesizefitsall "
                },
                {
                    "pdf_id": "0.48",
                    "matching_string": "encoder module of M2Fstyle models incur high resourceintensive computations, "
                },
                {
                    "pdf_id": "0.50",
                    "matching_string": "layers in the encoder, conditioned on the input image. To enable "
                },
                {
                    "pdf_id": "0.52",
                    "matching_string": "and computational efficiency, we present a three step recipe. The first "
                },
                {
                    "pdf_id": "0.55",
                    "matching_string": "of encoder layers required for each training example. The third step "
                },
                {
                    "pdf_id": "0.57",
                    "matching_string": "that predicts the number of encoder layers to be used, conditioned on "
                },
                {
                    "pdf_id": "0.60",
                    "matching_string": "retraining time. Experiments on the public datasets show that the "
                },
                {
                    "pdf_id": "0.61",
                    "matching_string": "proposed approach reduces expected encoder computational cost while "
                },
                {
                    "pdf_id": "0.63",
                    "matching_string": "flexible in architecture configurations, and can be extended beyond the "
                },
                {
                    "pdf_id": "0.40",
                    "matching_string": "Vision transformer based models bring significant improvements "
                },
                {
                    "pdf_id": "0.42",
                    "matching_string": "powerful capabilities irrespective of specific segmentation tasks, their "
                },
                {
                    "pdf_id": "0.44",
                    "matching_string": "way to overcome this challenge is by adapting the computation level "
                },
                {
                    "pdf_id": "0.46",
                    "matching_string": "approach. To this end, we introduce \\ours or EffiCient "
                },
                {
                    "pdf_id": "0.47",
                    "matching_string": "TransfOrmer Encoders for Mask2Formerstyle models. Noting that the "
                },
                {
                    "pdf_id": "0.49",
                    "matching_string": "provides a strategy to selfselect the number of hidden "
                },
                {
                    "pdf_id": "0.51",
                    "matching_string": "this selfselection ability for providing a balance between performance "
                },
                {
                    "pdf_id": "0.53",
                    "matching_string": "step is to train the parent architecture to enable early exiting from the "
                },
                {
                    "pdf_id": "0.54",
                    "matching_string": "encoder. The second step is to create an derived dataset of the ideal number "
                },
                {
                    "pdf_id": "0.56",
                    "matching_string": "is to use the aforementioned derived dataset to train a gating network "
                },
                {
                    "pdf_id": "0.58",
                    "matching_string": "input image. Additionally, to change the computationalaccuracy tradeoff, "
                },
                {
                    "pdf_id": "0.59",
                    "matching_string": "only steps two and three need to be repeated which significantly reduces "
                },
                {
                    "pdf_id": "0.62",
                    "matching_string": "maintaining performance, adapts to various user compute resources, is "
                },
                {
                    "pdf_id": "0.64",
                    "matching_string": "segmentation task to object detection."
                },
                {
                    "pdf_id": "1.0",
                    "matching_string": "\\ours "
                }
            ]
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec0/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "0.65",
                                    "matching_string": "Introduction"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec0/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 5,
                                    "key": "doc/body/sec0/txl0/txl0",
                                    "block type": "txl",
                                    "content": "With the advent of powerful universal image segmentation architectures, it is highly desirable to prioritize the computational efficiency of these architectures for their enhanced scalability, \\eg, use on resourcelimited edge devices. These architectures are extremely useful in tackling instance, semantic, and panoptic segmentation tasks using one generalized architecture, owing to the transformerbased modules. These universal architectures leverage DEtection TRansformers or DETRstyle modules and represent both stuff and things categories using general feature tokens. This is an incredible advantage over preceding segmentation methods in literature that require careful considerations in design specifications. Hence, these segmentation architectures reduce the need for taskspecific choices which favor performance of one task over the other .",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.13",
                                            "matching_string": "and things categories using general feature tokens. This is an incredible "
                                        },
                                        {
                                            "pdf_id": "1.15",
                                            "matching_string": "careful considerations in design specifications. Hence, these segmentation "
                                        },
                                        {
                                            "pdf_id": "1.16",
                                            "matching_string": "architectures reduce the need for taskspecific choices which favor performance "
                                        },
                                        {
                                            "pdf_id": "0.68",
                                            "matching_string": "for their enhanced scalability, \\eg, use on resourcelimited edge devices. "
                                        },
                                        {
                                            "pdf_id": "0.70",
                                            "matching_string": "and panoptic segmentation tasks using one generalized architecture, owing "
                                        },
                                        {
                                            "pdf_id": "0.71",
                                            "matching_string": "to the transformerbased modules. These universal architectures leverage "
                                        },
                                        {
                                            "pdf_id": "1.12",
                                            "matching_string": "DEtection TRansformers or DETRstyle modules and represent both stuff "
                                        },
                                        {
                                            "pdf_id": "1.14",
                                            "matching_string": "advantage over preceding segmentation methods in literature that require "
                                        },
                                        {
                                            "pdf_id": "1.17",
                                            "matching_string": "of one task over the other ."
                                        },
                                        {
                                            "pdf_id": "1.7",
                                            "matching_string": "With the advent of powerful universal image segmentation architectures, it is highly desirable to prioritize the computational efficiency of these architectures These architectures are extremely useful in tackling instance, semantic, "
                                        },
                                        {
                                            "pdf_id": "0.66",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "0.67",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "0.69",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec0/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 6,
                                    "key": "doc/body/sec0/txl1/txl0",
                                    "block type": "txl",
                                    "content": "Stateoftheart models for universal segmentation like Mask2former (M2F) are built on the key idea inspired from DETR: ''mask'' classification is versatile enough to address both semantic and instancelevel segmentation tasks. However, the problem of efficient M2Fstyle architectures have been underexplored. With backbone architectures (\\eg, Resnet50, SWINTiny ), showed that DETRstyle models incur the highest computations from the transformer encoder due to maintaining full length token representations from multiscale backbone features. While existing works like primarily focus on scaling the input token to improve efficiency, this approach often neglects other aspects of model optimization and leads to a ''onesizefitsall'' solution (Figure~). This limitation leaves significant room for further efficiency improvements.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.18",
                                            "matching_string": "Stateoftheart models for universal segmentation like Mask2former (M2F) "
                                        },
                                        {
                                            "pdf_id": "1.20",
                                            "matching_string": "enough to address both semantic and instancelevel segmentation tasks. However, "
                                        },
                                        {
                                            "pdf_id": "1.23",
                                            "matching_string": "that DETRstyle models incur the highest computations from the transformer "
                                        },
                                        {
                                            "pdf_id": "1.26",
                                            "matching_string": "the input token to improve efficiency, this approach often neglects other aspects "
                                        },
                                        {
                                            "pdf_id": "1.28",
                                            "matching_string": "limitation leaves significant room for further efficiency improvements."
                                        },
                                        {
                                            "pdf_id": "1.19",
                                            "matching_string": "are built on the key idea inspired from DETR: ''mask'' classification is versatile "
                                        },
                                        {
                                            "pdf_id": "1.21",
                                            "matching_string": "the problem of efficient M2Fstyle architectures have been underexplored. "
                                        },
                                        {
                                            "pdf_id": "1.24",
                                            "matching_string": "encoder due to maintaining full length token representations from multiscale "
                                        },
                                        {
                                            "pdf_id": "1.25",
                                            "matching_string": "backbone features. While existing works like primarily focus on scaling "
                                        },
                                        {
                                            "pdf_id": "1.27",
                                            "matching_string": "of model optimization and leads to a ''onesizefitsall'' solution (Figure~). This "
                                        },
                                        {
                                            "pdf_id": "1.22",
                                            "matching_string": "With backbone architectures (\\eg, Resnet50, SWINTiny ), showed "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec0/txl2",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 7,
                                    "key": "doc/body/sec0/txl2/txl0",
                                    "block type": "txl",
                                    "content": "Given this growing importance of M2Fstyle architectures and indispensable need for efficiency for realworld deployment, we introduce \\ours or 'EffiCient TransfOrmer Encoders' for M2Fstyle architectures. Our key idea comes from our observation made on the training set of COCO and Cityscapes dataset demonstrated in \\Figref{fig:teaserb}. We plot a histogram of the number of hidden encoder layers that produces the best panoptic segmentation quality for each image. It can be seen that not all images require the use of all hidden layers of the transformer encoder in order to achieve the maximum panoptic segmentation quality . With this insight, we propose to create a dynamic transformer encoder that economically uses the hidden layers, guided by a gating network that can select different depths for different images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.29",
                                            "matching_string": "Given this growing importance of M2Fstyle architectures and indispensable "
                                        },
                                        {
                                            "pdf_id": "1.31",
                                            "matching_string": "TransfOrmer Encoders' for M2Fstyle architectures. Our key idea comes from "
                                        },
                                        {
                                            "pdf_id": "1.34",
                                            "matching_string": "layers that produces the best panoptic segmentation quality for each image. "
                                        },
                                        {
                                            "pdf_id": "1.36",
                                            "matching_string": "transformer encoder in order to achieve the maximum panoptic segmentation "
                                        },
                                        {
                                            "pdf_id": "1.38",
                                            "matching_string": "encoder that economically uses the hidden layers, guided by a gating network "
                                        },
                                        {
                                            "pdf_id": "1.30",
                                            "matching_string": "need for efficiency for realworld deployment, we introduce \\ours or 'EffiCient "
                                        },
                                        {
                                            "pdf_id": "1.32",
                                            "matching_string": "our observation made on the training set of COCO and Cityscapes dataset "
                                        },
                                        {
                                            "pdf_id": "1.33",
                                            "matching_string": "demonstrated in \\Figref{fig:teaserb}. We plot a histogram of the number of hidden encoder "
                                        },
                                        {
                                            "pdf_id": "1.35",
                                            "matching_string": "It can be seen that not all images require the use of all hidden layers of the "
                                        },
                                        {
                                            "pdf_id": "1.37",
                                            "matching_string": "quality . With this insight, we propose to create a dynamic transformer "
                                        },
                                        {
                                            "pdf_id": "1.39",
                                            "matching_string": "that can select different depths for different images."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec0/txl3",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 8,
                                    "key": "doc/body/sec0/txl3/txl0",
                                    "block type": "txl",
                                    "content": "To achieve the aforementioned ability, \\ours leverages the wellstudied early exiting strategy to create stochastic depths for the transformer encoder to improve inference efficiency. Previous exit mechanisms have primarily relied on confidence scores or uncertainty scores, typically applied in classification tasks. However, implementing such mechanisms in our context would necessitate the inclusion of a decoder and a prediction head to generate a reliable confidence score. This additional complexity introduces a significant number of FLOPs, rendering it impractical for our purposes. By contrast, \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. Step \\stepA involves training the parent model to be dynamic by allowing stochastic depths at the transformer encoder. Using the fact that the transformer encoder maintains the token length of the input throughout the hidden layers constant, Step \\stepB involves creating a Derived dataset from the training dataset whose each sample contains a pair of image and layer number that provides the highest segmentation quality. Finally, Step \\stepC involves training a Gating Network using the derived dataset, whose function is to decide the number of layers to be used given the input image.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.43",
                                            "matching_string": "have primarily relied on confidence scores or uncertainty scores, typically "
                                        },
                                        {
                                            "pdf_id": "2.1",
                                            "matching_string": "applied in classification tasks. However, implementing such mechanisms in our "
                                        },
                                        {
                                            "pdf_id": "2.3",
                                            "matching_string": "generate a reliable confidence score. This additional complexity introduces a significant "
                                        },
                                        {
                                            "pdf_id": "2.7",
                                            "matching_string": "the parent model to be dynamic by allowing stochastic depths at the transformer "
                                        },
                                        {
                                            "pdf_id": "2.10",
                                            "matching_string": "Derived dataset from the training dataset whose each sample contains a pair of "
                                        },
                                        {
                                            "pdf_id": "2.11",
                                            "matching_string": "image and layer number that provides the highest segmentation quality. Finally, "
                                        },
                                        {
                                            "pdf_id": "2.13",
                                            "matching_string": "function is to decide the number of layers to be used given the input image."
                                        },
                                        {
                                            "pdf_id": "1.40",
                                            "matching_string": "To achieve the aforementioned ability, \\ours leverages the wellstudied "
                                        },
                                        {
                                            "pdf_id": "1.42",
                                            "matching_string": "for the transformer encoder to improve inference efficiency. Previous exit mechanisms "
                                        },
                                        {
                                            "pdf_id": "2.2",
                                            "matching_string": "context would necessitate the inclusion of a decoder and a prediction head to "
                                        },
                                        {
                                            "pdf_id": "2.4",
                                            "matching_string": "number of FLOPs, rendering it impractical for our purposes. By contrast, "
                                        },
                                        {
                                            "pdf_id": "2.8",
                                            "matching_string": "encoder. Using the fact that the transformer encoder maintains the token length "
                                        },
                                        {
                                            "pdf_id": "2.9",
                                            "matching_string": "of the input throughout the hidden layers constant, Step \\stepB involves creating a "
                                        },
                                        {
                                            "pdf_id": "2.5",
                                            "matching_string": "provides a threestep training recipe that can be used to customize the "
                                        },
                                        {
                                            "pdf_id": "1.8",
                                            "matching_string": "early exiting strategy to create stochastic depths \\ours transformer encoder on the fly given the input image. Step \\stepA involves training Step \\stepC involves training a Gating Network using the derived dataset, whose "
                                        },
                                        {
                                            "pdf_id": "1.41",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.6",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.12",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec0/txl4",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 9,
                                    "key": "doc/body/sec0/txl4/txl0",
                                    "block type": "txl",
                                    "content": "The key contributions of \\ours are multifold. First, given a trained M2Fstyle architecture, \\ours finetunes the model into one that allows the ability to randomly exit from the encoder by leveraging the fact that the token length remains constant in the hidden layers. Second, it introduces an accessory to the parent architecture \\via the Gating network that provides the ability to smartly use the encoder layers. Using this module, \\ours enables the parent architecture to decide the optimal amount of layers for the given input without any performance degradation as well as any confidence threshold (unlike prior early exiting strategies). Third, as a result of our Gating network module's training strategy, \\ours can adapt the parent architecture to varying computational budgets using only Step \\stepC. On COCO dataset, the computational cost of Step \\stepB and \\stepC are only ∼6% and ∼2.5% of Step \\stepA cost, respectively. Finally, \\ours can also incorporate recent advances in making transformer encoder efficient using token length scaling, bringing best of the both methods in pushing the limits of the efficiency. To summarize, we make the following contributions:",
                                    "leftover": "\\ours ",
                                    "matches": [
                                        {
                                            "pdf_id": "2.16",
                                            "matching_string": "to randomly exit from the encoder by leveraging the fact that the token length "
                                        },
                                        {
                                            "pdf_id": "2.18",
                                            "matching_string": "parent architecture \\via the Gating network that provides the ability to smartly "
                                        },
                                        {
                                            "pdf_id": "2.20",
                                            "matching_string": "to decide the optimal amount of layers for the given input without any "
                                        },
                                        {
                                            "pdf_id": "2.22",
                                            "matching_string": "exiting strategies). Third, as a result of our Gating network module's training "
                                        },
                                        {
                                            "pdf_id": "2.27",
                                            "matching_string": "efficient using token length scaling, bringing best of the both methods in pushing "
                                        },
                                        {
                                            "pdf_id": "2.14",
                                            "matching_string": "The key contributions of \\ours are multifold. First, given a trained M2Fstyle "
                                        },
                                        {
                                            "pdf_id": "2.15",
                                            "matching_string": "architecture, \\ours finetunes the model into one that allows the ability "
                                        },
                                        {
                                            "pdf_id": "2.21",
                                            "matching_string": "performance degradation as well as any confidence threshold (unlike prior early "
                                        },
                                        {
                                            "pdf_id": "2.23",
                                            "matching_string": "strategy, \\ours can adapt the parent architecture to varying computational "
                                        },
                                        {
                                            "pdf_id": "2.25",
                                            "matching_string": "Step \\stepB and \\stepC are only ∼6% and ∼2.5% of Step \\stepA cost, respectively. Finally, "
                                        },
                                        {
                                            "pdf_id": "2.28",
                                            "matching_string": "the limits of the efficiency. To summarize, we make the following contributions:"
                                        },
                                        {
                                            "pdf_id": "2.19",
                                            "matching_string": "use the encoder layers. Using this module, \\ours enables the parent architecture "
                                        },
                                        {
                                            "pdf_id": "2.24",
                                            "matching_string": "budgets using only Step \\stepC. On COCO dataset, the computational cost of "
                                        },
                                        {
                                            "pdf_id": "2.26",
                                            "matching_string": "can also incorporate recent advances in making transformer encoder "
                                        },
                                        {
                                            "pdf_id": "2.17",
                                            "matching_string": "remains constant in the hidden layers. Second, it introduces an accessory to the "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec0/itemize5",
                            "block type": "itemize",
                            "content": "\\setlengthsep{0.0em} We present a dynamic transformer encoder {\\ours} for M2Fstyle universal segmentation that maintains performance but reduces the computational load. {\\ours} consists of a novel training recipe that leverages input image based early exiting (in Step \\stepA), creating a derived dataset (based on training set segmentation performance in Step \\stepB), which in turn is used to train a gating function (using Step \\stepC) that allows adapting the number of hidden layers to reduce computations. Extensive experiments show that {\\ours} improves the overall performanceefficiency tradeoff, and adaptable to diverse architecture settings and can be extended beyond segmentation to the detection task.",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "2.30",
                                    "matching_string": "segmentation that maintains performance but reduces the computational load. "
                                },
                                {
                                    "pdf_id": "2.35",
                                    "matching_string": "reduce computations. "
                                },
                                {
                                    "pdf_id": "2.37",
                                    "matching_string": "tradeoff, and adaptable to diverse architecture settings and can be "
                                },
                                {
                                    "pdf_id": "2.31",
                                    "matching_string": "consists of a novel training recipe that leverages input image based "
                                },
                                {
                                    "pdf_id": "2.32",
                                    "matching_string": "early exiting (in Step \\stepA), creating a derived dataset (based on training set "
                                },
                                {
                                    "pdf_id": "2.33",
                                    "matching_string": "segmentation performance in Step \\stepB), which in turn is used to train a gating "
                                },
                                {
                                    "pdf_id": "2.34",
                                    "matching_string": "function (using Step \\stepC) that allows adapting the number of hidden layers to "
                                },
                                {
                                    "pdf_id": "2.38",
                                    "matching_string": "extended beyond segmentation to the detection task."
                                },
                                {
                                    "pdf_id": "2.29",
                                    "matching_string": "We present a dynamic transformer encoder {\\ours} for M2Fstyle universal "
                                },
                                {
                                    "pdf_id": "2.36",
                                    "matching_string": "Extensive experiments show that {\\ours} improves the overall performanceefficiency "
                                },
                                {
                                    "pdf_id": "2.0",
                                    "matching_string": "\\setlengthsep{0.0em} {\\ours} "
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Related Works",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "2.39",
                                    "matching_string": "Related Works"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 12,
                                    "key": "doc/body/sec1/par0/tit",
                                    "block type": "title",
                                    "content": "Efficient image segmentation.",
                                    "leftover": "Efficient image segmentation.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 13,
                                    "key": "doc/body/sec1/par0/txl0",
                                    "block type": "txl",
                                    "content": "With the rise of transformers, researchers are increasingly interested in creating image segmentation models that work effectively in various settings, without requiring segmentation type specific modifications to the model itself. Building on DETR, multiple universal segmentation architectures were proposed that use transformer decoder to predict masks for each entity in the input image. However, despite the significant progress in overall performance across various tasks, these models still face challenges in deployment on resourceconstrained devices. Current emphasis for efficiency for image segmentation has mostly been on specialized architectures tailored to a single segmentation task. Unlike these preceding works, \\ours makes no such assumption on the segmentation task and addresses the limitation of inefficiency in M2Fstyle universal architectures that are taskagnostic.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.41",
                                            "matching_string": "are increasingly interested in creating image segmentation models that work effectively "
                                        },
                                        {
                                            "pdf_id": "3.1",
                                            "matching_string": "predict masks for each entity in the input image. However, despite the significant "
                                        },
                                        {
                                            "pdf_id": "3.3",
                                            "matching_string": "face challenges in deployment on resourceconstrained devices. Current emphasis "
                                        },
                                        {
                                            "pdf_id": "3.5",
                                            "matching_string": "on specialized architectures tailored to a single segmentation task. Unlike these "
                                        },
                                        {
                                            "pdf_id": "3.7",
                                            "matching_string": "and addresses the limitation of inefficiency in M2Fstyle universal architectures "
                                        },
                                        {
                                            "pdf_id": "2.42",
                                            "matching_string": "in various settings, without requiring segmentation type specific modifications "
                                        },
                                        {
                                            "pdf_id": "3.2",
                                            "matching_string": "progress in overall performance across various tasks, these models still "
                                        },
                                        {
                                            "pdf_id": "3.8",
                                            "matching_string": "that are taskagnostic."
                                        },
                                        {
                                            "pdf_id": "3.6",
                                            "matching_string": "preceding works, \\ours makes no such assumption on the segmentation task "
                                        },
                                        {
                                            "pdf_id": "2.40",
                                            "matching_string": "With the rise of transformers, researchers to the model itself. Building on DETR, multiple universal segmentation architectures were proposed that use transformer decoder to for efficiency for image segmentation has mostly been "
                                        },
                                        {
                                            "pdf_id": "2.43",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.44",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "3.4",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 14,
                                    "key": "doc/body/sec1/par1/tit",
                                    "block type": "title",
                                    "content": "Earlyexiting in vision transformers.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.0",
                                            "matching_string": "Earlyexiting in vision transformers."
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 15,
                                    "key": "doc/body/sec1/par1/txl0",
                                    "block type": "txl",
                                    "content": "Recent works on early exiting aim to boost inference efficiency for large transformers. Some works used early exiting for classification tasks along with manually chosen confidence threshold in vision transformers. For example, proposed an early exiting framework for classification task ViTs combining heterogeneous task heads. Similarly, proposed an early exiting strategy for visionlanguage models by measuring layerwise similarities by checking multiple times to exit early. Applying early exiting solely to the encoder (like ) is infeasible due to the dependency on separate decoders, leading to an unacceptable optimization load. In contrast, methods like suffer from redundant computations for exit decisions at all possible choices, hindering efficient resource allocation. In contrast, \\ours only trains one decoder for all possible exit routes, as well as uses a gating module to decide the number of encoder layers required for the model depending on the input image.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.12",
                                            "matching_string": "along with manually chosen confidence threshold in vision transformers. For example, "
                                        },
                                        {
                                            "pdf_id": "3.15",
                                            "matching_string": "for visionlanguage models by measuring layerwise similarities by checking "
                                        },
                                        {
                                            "pdf_id": "3.17",
                                            "matching_string": "(like ) is infeasible due to the dependency on separate decoders, leading to "
                                        },
                                        {
                                            "pdf_id": "3.19",
                                            "matching_string": "redundant computations for exit decisions at all possible choices, hindering efficient "
                                        },
                                        {
                                            "pdf_id": "3.21",
                                            "matching_string": "possible exit routes, as well as uses a gating module to decide the number of "
                                        },
                                        {
                                            "pdf_id": "3.11",
                                            "matching_string": "transformers. Some works used early exiting for classification tasks "
                                        },
                                        {
                                            "pdf_id": "3.13",
                                            "matching_string": "proposed an early exiting framework for classification task ViTs combining "
                                        },
                                        {
                                            "pdf_id": "3.14",
                                            "matching_string": "heterogeneous task heads. Similarly, proposed an early exiting strategy "
                                        },
                                        {
                                            "pdf_id": "3.16",
                                            "matching_string": "multiple times to exit early. Applying early exiting solely to the encoder "
                                        },
                                        {
                                            "pdf_id": "3.18",
                                            "matching_string": "an unacceptable optimization load. In contrast, methods like suffer from "
                                        },
                                        {
                                            "pdf_id": "3.22",
                                            "matching_string": "encoder layers required for the model depending on the input image."
                                        },
                                        {
                                            "pdf_id": "3.20",
                                            "matching_string": "resource allocation. In contrast, \\ours only trains one decoder for all "
                                        },
                                        {
                                            "pdf_id": "3.9",
                                            "matching_string": "Recent works on early exiting aim to boost inference efficiency for large "
                                        },
                                        {
                                            "pdf_id": "3.10",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 16,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Proposed Methodology: \\ours",
                            "leftover": "\\ours",
                            "matches": [
                                {
                                    "pdf_id": "3.23",
                                    "matching_string": "Proposed Methodology: "
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec2/par0/tit",
                                    "block type": "title",
                                    "content": "Model preliminaries.",
                                    "leftover": "Model preliminaries.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par0/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 18,
                                            "key": "doc/body/sec2/par0/txl0/txl0",
                                            "block type": "txl",
                                            "content": "We first review the metaarchitecture of M2F upon which {\\ours} is based, along with the notation. This class of models contains",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.25",
                                                    "matching_string": "which {\\ours} is based, along with the notation. This class of models contains"
                                                },
                                                {
                                                    "pdf_id": "3.24",
                                                    "matching_string": "We first review the metaarchitecture of M2F upon "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par0/itemize1",
                                    "block_type": "itemize",
                                    "children": [
                                        {
                                            "leaf id": 19,
                                            "key": "doc/body/sec2/par0/itemize1/txl0",
                                            "block type": "txl",
                                            "content": "a backbone (·) which takes the th image as input to generate multiscale feature maps (), represented as 1, 2, 3, 4. These multiscale feature maps correspond to spatial resolutions typically set at 1/32, 1/16, 1/8, and 1/4 of the original image size, respectively. a transformer encoder (called the ''pixel decoder'' ), which is composed of multiple layers of transformer encoders. The function of this module is to generate rich token representation from 1, 2, 3 and generate perpixel embeddings from 4. Each layer in the transformer encoder, denoted as (·) (where ∈1,2,…,) is successively applied to (), with (·) being the last layer in the transformer encoder. a transformer decoder (along with a segmentation head) that takes two inputs: the output of the transformer encoder and the object queries. The object queries are decoded to output a binary mask along with the corresponding class label.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.28",
                                                    "matching_string": "feature maps correspond to spatial resolutions typically set at 1/32, 1/16, 1/8, "
                                                },
                                                {
                                                    "pdf_id": "3.31",
                                                    "matching_string": "multiple layers of transformer encoders. The function of this module is to "
                                                },
                                                {
                                                    "pdf_id": "3.35",
                                                    "matching_string": "the last layer in the transformer encoder. "
                                                },
                                                {
                                                    "pdf_id": "3.37",
                                                    "matching_string": "the output of the transformer encoder and the object queries. The "
                                                },
                                                {
                                                    "pdf_id": "3.39",
                                                    "matching_string": "class label."
                                                },
                                                {
                                                    "pdf_id": "3.29",
                                                    "matching_string": "and 1/4 of the original image size, respectively. "
                                                },
                                                {
                                                    "pdf_id": "3.30",
                                                    "matching_string": "a transformer encoder (called the ''pixel decoder'' ), which is composed of "
                                                },
                                                {
                                                    "pdf_id": "3.33",
                                                    "matching_string": "embeddings from 4. Each layer in the transformer encoder, denoted as "
                                                },
                                                {
                                                    "pdf_id": "3.36",
                                                    "matching_string": "a transformer decoder (along with a segmentation head) that takes two inputs: "
                                                },
                                                {
                                                    "pdf_id": "3.38",
                                                    "matching_string": "object queries are decoded to output a binary mask along with the corresponding "
                                                },
                                                {
                                                    "pdf_id": "3.27",
                                                    "matching_string": "feature maps (), represented as 1, 2, 3, 4. These multiscale "
                                                },
                                                {
                                                    "pdf_id": "3.32",
                                                    "matching_string": "generate rich token representation from 1, 2, 3 and generate perpixel "
                                                },
                                                {
                                                    "pdf_id": "3.26",
                                                    "matching_string": "a backbone (·) which takes the th image as input to generate multiscale "
                                                },
                                                {
                                                    "pdf_id": "3.34",
                                                    "matching_string": "(·) (where ∈1,2,…,) is successively applied to (), with (·) being "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par0/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 20,
                                            "key": "doc/body/sec2/par0/txl2/txl0",
                                            "block type": "txl",
                                            "content": "For brevity, we collectively refer to the operations in the transformer decoder and segmentation head together as (·). Thus, the output of the metaarchitecture with K encoder layers (a predicted mask  and corresponding label ℓ̃) can be written as",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.40",
                                                    "matching_string": "ctively refer to the operations in the transformer decoder and segmentation "
                                                },
                                                {
                                                    "pdf_id": "0.21",
                                                    "matching_string": "ou"
                                                },
                                                {
                                                    "pdf_id": "3.41",
                                                    "matching_string": "For brevity, we collehead together as (·). Thus, the tput of the metaarchitecture with K encoder layers (a predicted mask  and corresponding label ℓ̃) can be written as"
                                                },
                                                {
                                                    "pdf_id": "0.14",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 21,
                                    "key": "doc/body/sec2/par0/equation3",
                                    "block type": "equation",
                                    "content": ", ℓ̃ = h∘f∘⋯∘f2∘f1∘b() .",
                                    "leftover": ", ℓ̃ = h∘f∘⋯∘f2∘f1∘b() .",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par0/txl4",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 22,
                                            "key": "doc/body/sec2/par0/txl4/txl0",
                                            "block type": "txl",
                                            "content": "Here, the operation ∘ represents function composition, \\eg, g∘ f(x) = g(f(x)) and subscript denotes output predicted using K encoder layers. With, ℓ as the pair of ground truth segmentation map and corresponding label of image, the final loss is computed as",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.2",
                                                    "matching_string": "Here, the operation ∘ represents function composition, \\eg, g∘ f(x) = g(f(x)) and subscript denotes output predicted using K encoder layers. With, ℓ as the pair of ground truth segmentation map and corresponding label of image, the final loss is computed as"
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 23,
                                    "key": "doc/body/sec2/par0/align5",
                                    "block type": "align",
                                    "content": "= λmaskmask(, ) + λclassclass(ℓ̃, ℓ),",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "4.12",
                                            "matching_string": "= λmaskmask(, ) + λclassclass(ℓ̃, ℓ),"
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec2/par0/txl6",
                                    "block type": "txl",
                                    "content": "where mask(·,·) is a binary mask loss and class(·,·) is the corresponding classification loss. λmask and λclass represent the associated loss weights.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "4.16",
                                            "matching_string": "loss. λmask and λclass represent the associated loss weights."
                                        },
                                        {
                                            "pdf_id": "4.15",
                                            "matching_string": "where mask(·,·) is a binary mask loss and class(·,·) is the corresponding classification "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 25,
                                    "key": "doc/body/sec2/par1/tit",
                                    "block type": "title",
                                    "content": "Method motivation.",
                                    "leftover": "Method motivation.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par1/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 26,
                                            "key": "doc/body/sec2/par1/txl0/txl0",
                                            "block type": "txl",
                                            "content": "Our motivation stems from the observation that layers within the transformer encoder of M2F exhibit nonuniform contributions to Panoptic Quality (PQ), as discussed in \\Secref{sec:intro}. This prompts us to question the necessity of all K=6 layers for every image and target minimizing layer usage according to the user's computational constraints while ensuring that overall performance remains within acceptable bounds. Hence, we adopt an adaptive early exiting approach driven by three critical components:",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.18",
                                                    "matching_string": "within the transformer encoder of M2F exhibit nonuniform contributions to "
                                                },
                                                {
                                                    "pdf_id": "4.21",
                                                    "matching_string": "usage according to the user's computational constraints while ensuring that overall "
                                                },
                                                {
                                                    "pdf_id": "4.23",
                                                    "matching_string": "early exiting approach driven by three critical components:"
                                                },
                                                {
                                                    "pdf_id": "4.20",
                                                    "matching_string": "the necessity of all K=6 layers for every image and target minimizing layer "
                                                },
                                                {
                                                    "pdf_id": "4.22",
                                                    "matching_string": "performance remains within acceptable bounds. Hence, we adopt an adaptive "
                                                },
                                                {
                                                    "pdf_id": "4.19",
                                                    "matching_string": "Panoptic Quality (PQ), as discussed in \\Secref{sec:intro}. This prompts us to question "
                                                },
                                                {
                                                    "pdf_id": "4.17",
                                                    "matching_string": "Our motivation stems from the observation that layers "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec2/par1/enumerate1",
                                    "block type": "enumerate",
                                    "content": "Model suitability for early exiting. Traditional early exiting techniques often face challenges in maintaining satisfactory performance levels at potential exit points throughout the neural network. We recognize the importance of a model architecture that not only allows for early exiting but also ensures that the performance remains consistently high. Therefore, we aim to develop a model that not only permits early exits but also for which the accuracy steadily improves as the network delves deeper into its architecture. By prioritizing this aspect, we seek to establish a framework where early exiting does not compromise the overall performance of the model. Efficient and effective gating network for optimal exit decision making. The efficacy of an early exiting strategy heavily depends on the ability to make informed exit decisions. A gating network must strike a delicate balance, minimizing computational overhead while effectively identifying components that can be bypassed without compromising accuracy. Our objective is to design a lightweight yet powerful gating mechanism capable of discerning optimal exit points within the model architecture. Dynamic control mechanism for costperformance tradeoff. We require a mechanism with the ability to adaptively regulate the balance between computational cost and performance according to userdefined priorities. Such a mechanism empowers the model to exit at the optimal layer based on specific needs and desired outcomes, ensuring efficient resource allocation and maximizing utility in various application scenarios, particularly in resourceconstrained environments like edge computing or realtime applications.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "4.26",
                                            "matching_string": "performance levels at potential exit points throughout the neural network. "
                                        },
                                        {
                                            "pdf_id": "4.28",
                                            "matching_string": "for early exiting but also ensures that the performance remains consistently "
                                        },
                                        {
                                            "pdf_id": "4.30",
                                            "matching_string": "exits but also for which the accuracy steadily improves as the network delves "
                                        },
                                        {
                                            "pdf_id": "4.32",
                                            "matching_string": "framework where early exiting does not compromise the overall performance "
                                        },
                                        {
                                            "pdf_id": "4.35",
                                            "matching_string": "efficacy of an early exiting strategy heavily depends on the ability to make "
                                        },
                                        {
                                            "pdf_id": "4.37",
                                            "matching_string": "minimizing computational overhead while effectively identifying components "
                                        },
                                        {
                                            "pdf_id": "4.39",
                                            "matching_string": "design a lightweight yet powerful gating mechanism capable of discerning "
                                        },
                                        {
                                            "pdf_id": "4.42",
                                            "matching_string": "mechanism with the ability to adaptively regulate the balance between computational "
                                        },
                                        {
                                            "pdf_id": "4.44",
                                            "matching_string": "mechanism empowers the model to exit at the optimal layer based on specific "
                                        },
                                        {
                                            "pdf_id": "4.46",
                                            "matching_string": "maximizing utility in various application scenarios, particularly in resourceconstrained "
                                        },
                                        {
                                            "pdf_id": "4.24",
                                            "matching_string": "Model suitability for early exiting. Traditional early exiting techniques "
                                        },
                                        {
                                            "pdf_id": "4.27",
                                            "matching_string": "We recognize the importance of a model architecture that not only allows "
                                        },
                                        {
                                            "pdf_id": "4.29",
                                            "matching_string": "high. Therefore, we aim to develop a model that not only permits early "
                                        },
                                        {
                                            "pdf_id": "4.31",
                                            "matching_string": "deeper into its architecture. By prioritizing this aspect, we seek to establish a "
                                        },
                                        {
                                            "pdf_id": "4.34",
                                            "matching_string": "Efficient and effective gating network for optimal exit decision making. The "
                                        },
                                        {
                                            "pdf_id": "4.36",
                                            "matching_string": "informed exit decisions. A gating network must strike a delicate balance, "
                                        },
                                        {
                                            "pdf_id": "4.38",
                                            "matching_string": "that can be bypassed without compromising accuracy. Our objective is to "
                                        },
                                        {
                                            "pdf_id": "4.40",
                                            "matching_string": "optimal exit points within the model architecture. "
                                        },
                                        {
                                            "pdf_id": "4.41",
                                            "matching_string": "Dynamic control mechanism for costperformance tradeoff. We require a "
                                        },
                                        {
                                            "pdf_id": "4.43",
                                            "matching_string": "cost and performance according to userdefined priorities. Such a "
                                        },
                                        {
                                            "pdf_id": "4.45",
                                            "matching_string": "needs and desired outcomes, ensuring efficient resource allocation and "
                                        },
                                        {
                                            "pdf_id": "4.47",
                                            "matching_string": "environments like edge computing or realtime applications."
                                        },
                                        {
                                            "pdf_id": "4.33",
                                            "matching_string": "of the model. "
                                        },
                                        {
                                            "pdf_id": "5.0",
                                            "matching_string": "often face challenges in maintaining satisfactory "
                                        },
                                        {
                                            "pdf_id": "4.25",
                                            "matching_string": ""
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 28,
                                    "key": "doc/body/sec2/par1/txl2",
                                    "block type": "txl",
                                    "content": "Driven by these considerations, {\\ours} offers a novel training process that enables an adaptive early exiting mechanism designed to bolster computational efficiency while preserving satisfactory model accuracy. For better understanding, we'll begin with a general overview of model training and inference before diving into the specific details of our training process. \\input{figures/mainframework}",
                                    "leftover": "\\input{figures/mainframework}",
                                    "matches": [
                                        {
                                            "pdf_id": "5.9",
                                            "matching_string": "enables an adaptive early exiting mechanism designed to bolster computational "
                                        },
                                        {
                                            "pdf_id": "5.11",
                                            "matching_string": "we'll begin with a general overview of model training and inference before "
                                        },
                                        {
                                            "pdf_id": "5.8",
                                            "matching_string": "Driven by these considerations, {\\ours} offers a novel training process that "
                                        },
                                        {
                                            "pdf_id": "5.10",
                                            "matching_string": "efficiency while preserving satisfactory model accuracy. For better understanding, "
                                        },
                                        {
                                            "pdf_id": "5.12",
                                            "matching_string": "diving into the specific details of our training process. "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par2",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec2/par2/tit",
                                    "block type": "title",
                                    "content": "Training and Inference overview.",
                                    "leftover": "Training and Inference overview.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par2/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 30,
                                            "key": "doc/body/sec2/par2/txl0/txl0",
                                            "block type": "txl",
                                            "content": "As shown in \\Figref{fig:mainframework}, the training phase of {\\ours} involves following three main steps:",
                                            "leftover": "{\\ours} ",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.14",
                                                    "matching_string": "involves following three main steps:"
                                                },
                                                {
                                                    "pdf_id": "5.13",
                                                    "matching_string": "As shown in \\Figref{fig:mainframework}, the training phase of "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 31,
                                    "key": "doc/body/sec2/par2/enumerate1",
                                    "block type": "enumerate",
                                    "content": "Step \\stepA: Train parent model for early exit via the transformer encoder. Step \\stepB: Derive a dataset (which we call the Derived dataset) from the dynamic model obtained in Step \\stepA. Step \\stepC: Train the Gating Network to learn optimal exit points in the encoder tailored to users' requirements.",
                                    "leftover": "\\stepA. ",
                                    "matches": [
                                        {
                                            "pdf_id": "5.19",
                                            "matching_string": "tailored to users' requirements."
                                        },
                                        {
                                            "pdf_id": "5.15",
                                            "matching_string": "Step \\stepA: Train parent model for early exit via the transformer encoder. "
                                        },
                                        {
                                            "pdf_id": "5.16",
                                            "matching_string": "Step \\stepB: Derive a dataset (which we call the Derived dataset) from the dynamic "
                                        },
                                        {
                                            "pdf_id": "5.17",
                                            "matching_string": "model obtained in Step "
                                        },
                                        {
                                            "pdf_id": "5.18",
                                            "matching_string": "Step \\stepC: Train the Gating Network to learn optimal exit points in the encoder "
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec2/par2/txl2/txl0",
                                            "block type": "txl",
                                            "content": "We refer to Step \\stepA and \\stepB together as model preprocessing and Step \\stepC as model adaptation. The former is required only once, whereas the latter is repeated as per user requirements. All these steps use the training data subset.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.21",
                                                    "matching_string": "adaptation. The former is required only once, whereas the latter is repeated as "
                                                },
                                                {
                                                    "pdf_id": "5.22",
                                                    "matching_string": "per user requirements. All these steps use the training data subset."
                                                },
                                                {
                                                    "pdf_id": "5.20",
                                                    "matching_string": "We refer to Step \\stepA and \\stepB together as model preprocessing and Step \\stepC as model "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/txl3",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 33,
                                            "key": "doc/body/sec2/par2/txl3/txl0",
                                            "block type": "txl",
                                            "content": "During inference, the gating network guides the parent model by selecting the optimal exit point based on features extracted from the backbone with just one forward pass for final predictions.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.23",
                                                    "matching_string": "During inference, the gating network guides the parent model by selecting "
                                                },
                                                {
                                                    "pdf_id": "5.24",
                                                    "matching_string": "the optimal exit point based on features extracted from the backbone with just "
                                                },
                                                {
                                                    "pdf_id": "5.25",
                                                    "matching_string": "one forward pass for final predictions."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub4",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 34,
                                            "key": "doc/body/sec2/par2/sub4/tit",
                                            "block type": "title",
                                            "content": "Step \\stepA: Training the Model with Weighted Stochastic Depth",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.1",
                                                    "matching_string": "Step \\stepA: Training the Model with Weighted Stochastic Depth"
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/par2/sub4/txl0",
                                            "block_type": "txl",
                                            "children": [
                                                {
                                                    "leaf id": 35,
                                                    "key": "doc/body/sec2/par2/sub4/txl0/txl0",
                                                    "block type": "txl",
                                                    "content": "In this step, we enable the model to allow exiting at the encoder. To maintain consistently high performance at each exit point, we input each stochastic depth's output to a shared transformer decoder. We then apply \\Eqref{eq:origloss} to compute the loss  for each exit point . However, we observe that direct training in this fashion does not encourage the model to use fewer layers to extract and prioritize informative representations, as shown in \\Tabref{tab:loss}. To address this, we introduce a set of coefficients  to emphasize the quality of representations at later layers more, enabling earlier layers to also concentrate on producing effective intermediate representations. As the layer depth increases, the corresponding coefficient  grows, ensuring a progressively stricter standard for feature quality. The new loss function is then expressed as",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "6.2",
                                                            "matching_string": "performance at each exit point, we input each stochastic depth's ou"
                                                        },
                                                        {
                                                            "pdf_id": "6.6",
                                                            "matching_string": "ayers to extract and prioritize informative representations, as shown in"
                                                        },
                                                        {
                                                            "pdf_id": "6.8",
                                                            "matching_string": "asize the quality of representations at later layers more, enabling ea"
                                                        },
                                                        {
                                                            "pdf_id": "6.10",
                                                            "matching_string": "ntations. As the layer depth increases, the corresponding coefficient  "
                                                        },
                                                        {
                                                            "pdf_id": "6.12",
                                                            "matching_string": "loss function is then expressed as"
                                                        },
                                                        {
                                                            "pdf_id": "6.5",
                                                            "matching_string": "t . However, we observe that direct training in this fashion does not"
                                                        },
                                                        {
                                                            "pdf_id": "6.4",
                                                            "matching_string": "er. We then apply \\Eqref{eq:origloss} to compute the loss  for each ex"
                                                        },
                                                        {
                                                            "pdf_id": "6.11",
                                                            "matching_string": "ensuring a progressively stricter standard for feature quality. The new "
                                                        },
                                                        {
                                                            "pdf_id": "6.3",
                                                            "matching_string": "In this step, we enable the model to allow exiting at the encoder. To maintain consistently high tput to a shared transformer decodit poin encourage the model to use fewer l \\Tabref{tab:loss}. To address this, we introduce a set of coefficients  to emphrlier layers to also concentrate on producing effective intermediate represegrows, "
                                                        },
                                                        {
                                                            "pdf_id": "6.7",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "6.9",
                                                            "matching_string": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 36,
                                            "key": "doc/body/sec2/par2/sub4/equation1",
                                            "block type": "equation",
                                            "content": "total = 1/N∑^N∑^, where ∀<', <',",
                                            "leftover": "total = 1/N∑^N∑^, where ∀<', <',",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 37,
                                            "key": "doc/body/sec2/par2/sub4/txl2",
                                            "block type": "txl",
                                            "content": "where N is the number of images in the training set, and  is from \\Eqref{eq:origloss}. \\input{figures/PQvslayers}",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.15",
                                                    "matching_string": "nu"
                                                },
                                                {
                                                    "pdf_id": "6.0",
                                                    "matching_string": "where N is the mber of images in the training set, and  is from \\Eqref{eq:origloss}. \\input{figures/PQvslayers}"
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub5",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 38,
                                            "key": "doc/body/sec2/par2/sub5/tit",
                                            "block type": "title",
                                            "content": "Step \\stepB: Deriving the Gating Network Training Dataset",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.21",
                                                    "matching_string": "Step \\stepB: Deriving the Gating Network Training Dataset"
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/par2/sub5/txl0",
                                            "block_type": "txl",
                                            "children": [
                                                {
                                                    "leaf id": 39,
                                                    "key": "doc/body/sec2/par2/sub5/txl0/txl0",
                                                    "block type": "txl",
                                                    "content": "To facilitate informed exit decisions during inference, our approach is to train a gating network to learn optimal exit strategies. In this step, we facilitate this gating network training by first deriving an intermediate dataset.",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "6.22",
                                                            "matching_string": "To facilitate informed exit decisions during "
                                                        },
                                                        {
                                                            "pdf_id": "6.24",
                                                            "matching_string": "a gating network to learn optimal exit "
                                                        },
                                                        {
                                                            "pdf_id": "6.26",
                                                            "matching_string": "gating network training by first deriving "
                                                        },
                                                        {
                                                            "pdf_id": "6.23",
                                                            "matching_string": "inference, our approach is to train "
                                                        },
                                                        {
                                                            "pdf_id": "6.25",
                                                            "matching_string": "strategies. In this step, we facilitate this "
                                                        },
                                                        {
                                                            "pdf_id": "6.27",
                                                            "matching_string": "an intermediate dataset."
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 40,
                                            "key": "doc/body/sec2/par2/sub5/txl1",
                                            "block type": "txl",
                                            "content": "To this end, we record the performance of the pretrained stochastic depth model (obtained from Step \\stepA) at all potential exit points for each image within the training dataset and create a Derived dataset . Specifically, we associate the th input image with a vector of length . Each element  of represents the predicted panoptic quality upon exiting at the encoder layer . Hence, each sample of can be represented as (, ) ∈.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.28",
                                                    "matching_string": " the pretrained stochastic depth "
                                                },
                                                {
                                                    "pdf_id": "6.32",
                                                    "matching_string": "and create a Derived dataset . Specifi"
                                                },
                                                {
                                                    "pdf_id": "6.31",
                                                    "matching_string": "mage within the training datas"
                                                },
                                                {
                                                    "pdf_id": "6.38",
                                                    "matching_string": "upon exiting at the encoder layer "
                                                },
                                                {
                                                    "pdf_id": "6.30",
                                                    "matching_string": "A) at all potential exit points for each "
                                                },
                                                {
                                                    "pdf_id": "6.37",
                                                    "matching_string": "represents the predicted panoptic quality "
                                                },
                                                {
                                                    "pdf_id": "6.39",
                                                    "matching_string": "Hence, each sample of can be represented "
                                                },
                                                {
                                                    "pdf_id": "6.35",
                                                    "matching_string": "ngth . Each element  "
                                                },
                                                {
                                                    "pdf_id": "6.41",
                                                    "matching_string": "To this end, we record the performance ofmodel (obtained from Step \\stepiet cally, we associate the th input image with a vector of leof . as (, ) ∈."
                                                },
                                                {
                                                    "pdf_id": "6.29",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "6.33",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "6.34",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "6.36",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "6.40",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par2/sub6",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 41,
                                            "key": "doc/body/sec2/par2/sub6/tit",
                                            "block type": "title",
                                            "content": "Step \\stepC: Training for Gating Network",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.47",
                                                    "matching_string": "Step \\stepC: Training for Gating Network"
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/par2/sub6/txl0",
                                            "block_type": "txl",
                                            "children": [
                                                {
                                                    "leaf id": 42,
                                                    "key": "doc/body/sec2/par2/sub6/txl0/txl0",
                                                    "block type": "txl",
                                                    "content": "In this step, we train the gating network on dataset (obtained from Step \\stepB) to selfselect the number of encoder layers based on the input image. Ideally, this module should allow exiting at the encoder layer which would result in the highest quality segmentation map. With this in mind, we first establish the target exit for the gating network. Note that the panoptic quality generally increases with increasing encoder layers (see \\Figref{fig:PQvslayers}). However, we would like the gating network to prioritize increasing the panoptic quality while also reducing the number of layers (to reduce the overall computations). Consequently, we introduce a utility function expressed as the linear combination of segmentation quality and the depth of the network. This function is formulated as",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.1",
                                                            "matching_string": "target exit for the gating network. Note that the panoptic quality generally "
                                                        },
                                                        {
                                                            "pdf_id": "7.3",
                                                            "matching_string": "gating network to prioritize increasing the panoptic quality while also reducing "
                                                        },
                                                        {
                                                            "pdf_id": "7.5",
                                                            "matching_string": "introduce a utility function expressed as the linear combination of segmentation "
                                                        },
                                                        {
                                                            "pdf_id": "7.6",
                                                            "matching_string": "quality and the depth of the network. This function is formulated as"
                                                        },
                                                        {
                                                            "pdf_id": "6.48",
                                                            "matching_string": "In this step, we train the gating network on dataset (obtained from Step "
                                                        },
                                                        {
                                                            "pdf_id": "6.49",
                                                            "matching_string": "to selfselect the number of encoder layers based on the input image. Ideally, "
                                                        },
                                                        {
                                                            "pdf_id": "7.4",
                                                            "matching_string": "the number of layers (to reduce the overall computations). Consequently, we "
                                                        },
                                                        {
                                                            "pdf_id": "7.2",
                                                            "matching_string": "\\stepB) this module should allow exiting at the encoder layer which would result in the highest quality segmentation map. With this in mind, we first establish the increases with increasing encoder layers (see \\Figref{fig:PQvslayers}). However, we would like the "
                                                        },
                                                        {
                                                            "pdf_id": "6.50",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "6.51",
                                                            "matching_string": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 43,
                                            "key": "doc/body/sec2/par2/sub6/equation1",
                                            "block type": "equation",
                                            "content": "() = q,",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.0",
                                                    "matching_string": "() = q,"
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/par2/sub6/txl2",
                                            "block_type": "txl",
                                            "children": [
                                                {
                                                    "leaf id": 44,
                                                    "key": "doc/body/sec2/par2/sub6/txl2/txl0",
                                                    "block type": "txl",
                                                    "content": "where serves as an adaptation factor governing the tradeoff between segmentation quality and computational cost. Clearly, a higher value of signifies a greater emphasis on efficiency over segmentation quality. Using \\Eqref{eq:utilityfunc}, we determine a target exit point for each image using",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.8",
                                                            "matching_string": "quality and computational cost. Clearly, a higher value of signifies a "
                                                        },
                                                        {
                                                            "pdf_id": "7.10",
                                                            "matching_string": "a target exit point for each image using"
                                                        },
                                                        {
                                                            "pdf_id": "7.7",
                                                            "matching_string": "where serves as an adaptation factor governing the tradeoff between segmentation greater emphasis on efficiency over segmentation quality. Using \\Eqref{eq:utilityfunc}, we determine "
                                                        },
                                                        {
                                                            "pdf_id": "7.9",
                                                            "matching_string": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 45,
                                            "key": "doc/body/sec2/par2/sub6/equation3",
                                            "block type": "equation",
                                            "content": "= (()) .",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.11",
                                                    "matching_string": "= (()) ."
                                                },
                                                {
                                                    "pdf_id": "7.17",
                                                    "matching_string": ""
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec2/par2/sub6/txl4",
                                            "block_type": "txl",
                                            "children": [
                                                {
                                                    "leaf id": 46,
                                                    "key": "doc/body/sec2/par2/sub6/txl4/txl0",
                                                    "block type": "txl",
                                                    "content": "With a target designated for each image using \\Eqref{eq:target}, the gating decision can be approached as a straightforward classification problem. The gating architecture consists of a pooling operation (·) on the token length dimension followed by a linear layer with weights . Its output logits can be represented as",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.19",
                                                            "matching_string": "approached as a straightforward classification problem. The gating architecture "
                                                        },
                                                        {
                                                            "pdf_id": "7.21",
                                                            "matching_string": "linear layer with weights . Its output logits can be represented as"
                                                        },
                                                        {
                                                            "pdf_id": "7.18",
                                                            "matching_string": "With a target designated for each image using \\Eqref{eq:target}, the gating decision can be "
                                                        },
                                                        {
                                                            "pdf_id": "7.20",
                                                            "matching_string": "consists of a pooling operation (·) on the token length dimension followed by a "
                                                        },
                                                        {
                                                            "pdf_id": "7.27",
                                                            "matching_string": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 47,
                                            "key": "doc/body/sec2/par2/sub6/align5",
                                            "block type": "align",
                                            "content": "= (1) .",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.13",
                                                    "matching_string": "= (1) ."
                                                },
                                                {
                                                    "pdf_id": "7.29",
                                                    "matching_string": ""
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 48,
                                            "key": "doc/body/sec2/par2/sub6/txl6",
                                            "block type": "txl",
                                            "content": "In consideration of having minimal impact on the computations due to the gating network, we use the output of the lowest resolution feature map 1 as input to the pooling operation. To optimize the gating network, we use the standard crossentropy loss between the output logits and the onehot version of target exit as our training objective. During inference, the gating network identifies the layer with the highest predicted logits, \\ie, (g), as the optimal exit layer for image . Note that while there can be more complex choices for the gating network, our simple linear layer in \\Eqref{eq:gating} works well in our experiments.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.23",
                                                    "matching_string": "network, we use the output of the lowest resolution feature map 1 as input to the "
                                                },
                                                {
                                                    "pdf_id": "7.24",
                                                    "matching_string": "pooling operation. To optimize the gating network, we use the standard crossentropy "
                                                },
                                                {
                                                    "pdf_id": "7.32",
                                                    "matching_string": "layer for image . Note that while there can be more complex choices for the "
                                                },
                                                {
                                                    "pdf_id": "7.33",
                                                    "matching_string": "gating network, our simple linear layer in \\Eqref{eq:gating} works well in our experiments."
                                                },
                                                {
                                                    "pdf_id": "7.22",
                                                    "matching_string": "In consideration of having minimal impact on the computations due to the gating "
                                                },
                                                {
                                                    "pdf_id": "7.25",
                                                    "matching_string": "loss between the output logits and the onehot version of target exit "
                                                },
                                                {
                                                    "pdf_id": "7.26",
                                                    "matching_string": "as our training objective. During inference, the gating network identifies the "
                                                },
                                                {
                                                    "pdf_id": "7.31",
                                                    "matching_string": "as the optimal exit "
                                                },
                                                {
                                                    "pdf_id": "7.30",
                                                    "matching_string": "layer with the highest predicted logits, \\ie, (g), "
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/par3",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 49,
                                    "key": "doc/body/sec2/par3/tit",
                                    "block type": "title",
                                    "content": "Saving training costs through Step \\stepC.",
                                    "leftover": "Saving training costs through Step \\stepC.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/par3/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 50,
                                            "key": "doc/body/sec2/par3/txl0/txl0",
                                            "block type": "txl",
                                            "content": "{\\ours} presents a distinct advantage in terms of its adaptability to varying computational constraints. In scenarios where a smaller model is desired, {\\ours} necessitates training solely the gating network (\\ie, repeat Step \\stepC). Assuming that the computational load is proportional to the depth of the network, \\Eqref{eq:utilityfunc} enables us to weigh the performance gain against the computational overhead for each exit layer. We achieve this by setting the total number of layers to a smaller number depending on user preferences. For instance, as illustrated in \\Figref{fig:mainframework}, User X preferring a smaller model compared to User Y may opt for a smaller, \\ie, X<Y. Then, given the importance of segmentation quality, we choose β. With these two variables set in \\Eqref{eq:target}, we train the gating network. This capability shows that {\\ours} is versatile and resourceefficient as it adapts to diverse needs and optimizes allocations.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "0.29",
                                                    "matching_string": "X "
                                                },
                                                {
                                                    "pdf_id": "7.35",
                                                    "matching_string": "in terms of its adaptability to varying computational constraints. In "
                                                },
                                                {
                                                    "pdf_id": "7.39",
                                                    "matching_string": "gain against the computational overhead for each exit layer. We achieve "
                                                },
                                                {
                                                    "pdf_id": "7.46",
                                                    "matching_string": "optimizes allocations."
                                                },
                                                {
                                                    "pdf_id": "7.40",
                                                    "matching_string": "this by setting the total number of layers to a smaller number depending "
                                                },
                                                {
                                                    "pdf_id": "7.43",
                                                    "matching_string": "Then, given the importance of segmentation quality, we choose β. With these "
                                                },
                                                {
                                                    "pdf_id": "7.44",
                                                    "matching_string": "two variables set in \\Eqref{eq:target}, we train the gating network. This capability shows "
                                                },
                                                {
                                                    "pdf_id": "7.36",
                                                    "matching_string": "scenarios where a smaller model is desired, {\\ours} necessitates training solely "
                                                },
                                                {
                                                    "pdf_id": "7.37",
                                                    "matching_string": "the gating network (\\ie, repeat Step \\stepC). Assuming that the computational load "
                                                },
                                                {
                                                    "pdf_id": "7.45",
                                                    "matching_string": "that {\\ours} is versatile and resourceefficient as it adapts to diverse needs and "
                                                },
                                                {
                                                    "pdf_id": "8.0",
                                                    "matching_string": "{\\ours} presents a distinct advantage is proportional to the depth of the network, \\Eqref{eq:utilityfunc} enables us to weigh the performance on user preferences. For instance, as illustrated in \\Figref{fig:mainframework}, User preferring a smaller model compared to User Y may opt for a smaller, \\ie, X<Y. "
                                                },
                                                {
                                                    "pdf_id": "7.34",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "7.38",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "7.41",
                                                    "matching_string": ""
                                                },
                                                {
                                                    "pdf_id": "7.42",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/par3/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 51,
                                            "key": "doc/body/sec2/par3/sub1/tit",
                                            "block type": "title",
                                            "content": "Inference",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "8.1",
                                                    "matching_string": "Inference"
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 52,
                                            "key": "doc/body/sec2/par3/sub1/txl0",
                                            "block type": "txl",
                                            "content": "In the inference phase, the gating network guides the parent model toward an optimal exit point tailored to each input image. Similar to the training phase, the gating mechanism receives lowresolution features from the backbone and produces a vector of length for each image. The value of remains consistent with that determined in Step \\stepC. Subsequently, the gating network identifies the layer with the highest predicted logits as the optimal exit layer for each image. The parent model adheres to this decision, exiting at the determined layer, and subsequently progresses through the subsequent components to make the final prediction. This dynamic process ensures that the model adaptively selects the most optimal layer for exit during inference, enhancing its efficiency in handling diverse input data.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "8.2",
                                                    "matching_string": "In the inference phase, the gating network guides the parent model toward an "
                                                },
                                                {
                                                    "pdf_id": "8.4",
                                                    "matching_string": "the gating mechanism receives lowresolution features from the backbone and "
                                                },
                                                {
                                                    "pdf_id": "8.7",
                                                    "matching_string": "layer with the highest predicted logits as the optimal exit layer for each image. "
                                                },
                                                {
                                                    "pdf_id": "8.8",
                                                    "matching_string": "The parent model adheres to this decision, exiting at the determined layer, and "
                                                },
                                                {
                                                    "pdf_id": "8.10",
                                                    "matching_string": "prediction. This dynamic process ensures that the model adaptively selects the "
                                                },
                                                {
                                                    "pdf_id": "8.11",
                                                    "matching_string": "most optimal layer for exit during inference, enhancing its efficiency in handling "
                                                },
                                                {
                                                    "pdf_id": "8.3",
                                                    "matching_string": "optimal exit point tailored to each input image. Similar to the training phase, "
                                                },
                                                {
                                                    "pdf_id": "8.5",
                                                    "matching_string": "produces a vector of length for each image. The value of remains consistent "
                                                },
                                                {
                                                    "pdf_id": "8.6",
                                                    "matching_string": "with that determined in Step \\stepC. Subsequently, the gating network identifies the "
                                                },
                                                {
                                                    "pdf_id": "8.9",
                                                    "matching_string": "subsequently progresses through the subsequent components to make the final "
                                                },
                                                {
                                                    "pdf_id": "8.12",
                                                    "matching_string": "diverse input data."
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 53,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Experiments",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "8.13",
                                    "matching_string": "Experiments"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec3/par0/tit",
                                    "block type": "title",
                                    "content": "Datasets.",
                                    "leftover": "Datasets.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 55,
                                    "key": "doc/body/sec3/par0/txl0",
                                    "block type": "txl",
                                    "content": "Our study illustrates the adaptability of {\\ours} in dynamically managing the tradeoff between computation and performance based on M2F metaarchitecture. We do this on two widely used image segmentation datasets: COCO and Cityscapes . COCO comprises 80 ''things'' and 53 ''stuff'' categories, with 118k training images and 5k validation images. Cityscapes consists of 8 ''things'' and 11 ''stuff'' categories, with approximately 3k training images and 500 validation images. The evaluation is conducted over the union of ''things'' and ''stuff'' categories.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "8.18",
                                            "matching_string": "with 118k training images and 5k validation images. Cityscapes consists "
                                        },
                                        {
                                            "pdf_id": "8.20",
                                            "matching_string": "and 500 validation images. The evaluation is conducted over the union of ''things'' "
                                        },
                                        {
                                            "pdf_id": "8.14",
                                            "matching_string": "Our study illustrates the adaptability of {\\ours} in dynamically "
                                        },
                                        {
                                            "pdf_id": "8.15",
                                            "matching_string": "managing the tradeoff between computation and performance based on M2F "
                                        },
                                        {
                                            "pdf_id": "8.16",
                                            "matching_string": "metaarchitecture. We do this on two widely used image segmentation datasets: "
                                        },
                                        {
                                            "pdf_id": "8.19",
                                            "matching_string": "of 8 ''things'' and 11 ''stuff'' categories, with approximately 3k training images "
                                        },
                                        {
                                            "pdf_id": "8.21",
                                            "matching_string": "and ''stuff'' categories."
                                        },
                                        {
                                            "pdf_id": "8.17",
                                            "matching_string": "COCO and Cityscapes . COCO comprises 80 ''things'' and 53 ''stuff'' categories, "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 56,
                                    "key": "doc/body/sec3/par1/tit",
                                    "block type": "title",
                                    "content": "Evaluation metrics.",
                                    "leftover": "Evaluation metrics.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 57,
                                    "key": "doc/body/sec3/par1/txl0",
                                    "block type": "txl",
                                    "content": "We follow the evaluation setting of for evaluation of ''universal'' segmentation, \\ie, we train the model solely with panoptic segmentation annotations but evaluate it for panoptic, semantic, and instance segmentation tasks. We use the standard PQ (Panoptic Quality ) metric to evaluate panoptic segmentation performance. We report APp (Average Precision ) computed across all categories for instance segmentation, and mIOUp(mean Intersection over Union ) for semantic segmentation by merging instance masks from the same category. The subscript p denotes that these metrics are computed for the model trained solely with panoptic segmentation annotations. In terms of computational cost, we use GFLOPs calculated as the average GFLOPs across all validation images. All models are trained on the train split and evaluated on the validation split.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "8.29",
                                            "matching_string": "from the same category. The subscript p denotes that these metrics are computed "
                                        },
                                        {
                                            "pdf_id": "8.30",
                                            "matching_string": "for the model trained solely with panoptic segmentation annotations. In terms of "
                                        },
                                        {
                                            "pdf_id": "8.31",
                                            "matching_string": "computational cost, we use GFLOPs calculated as the average GFLOPs across "
                                        },
                                        {
                                            "pdf_id": "8.32",
                                            "matching_string": "all validation images. All models are trained on the train split and evaluated on "
                                        },
                                        {
                                            "pdf_id": "8.25",
                                            "matching_string": "tasks. We use the standard PQ (Panoptic Quality ) metric to evaluate "
                                        },
                                        {
                                            "pdf_id": "8.26",
                                            "matching_string": "panoptic segmentation performance. We report APp (Average Precision ) "
                                        },
                                        {
                                            "pdf_id": "8.27",
                                            "matching_string": "computed across all categories for instance segmentation, and mIOUp(mean Intersection "
                                        },
                                        {
                                            "pdf_id": "8.28",
                                            "matching_string": "over Union ) for semantic segmentation by merging instance masks "
                                        },
                                        {
                                            "pdf_id": "8.24",
                                            "matching_string": "annotations but evaluate it for panoptic, semantic, and instance segmentation "
                                        },
                                        {
                                            "pdf_id": "8.33",
                                            "matching_string": "the validation split."
                                        },
                                        {
                                            "pdf_id": "8.23",
                                            "matching_string": "''universal'' segmentation, \\ie, we train the model solely with panoptic segmentation "
                                        },
                                        {
                                            "pdf_id": "8.22",
                                            "matching_string": "We follow the evaluation setting of for evaluation of "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par2",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 58,
                                    "key": "doc/body/sec3/par2/tit",
                                    "block type": "title",
                                    "content": "Baseline models.",
                                    "leftover": "Baseline models.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 59,
                                    "key": "doc/body/sec3/par2/txl0",
                                    "block type": "txl",
                                    "content": "We compare \\ours with two sets of efficient segmentation methods. First, we compare with our baseline universal segmentation architecture M2F . Further, we also integrate recently proposed transformer encoder designs (LiteDETR and RTDETR ) for efficient object detection into M2F and named them LiteM2F and RTM2F, respectively. Second, we include comparisons with recent efficient architectures that proposed taskspecific components, namely YOSO, RAPSAM, and ReMax .",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "8.35",
                                            "matching_string": "methods. First, we compare with our baseline universal segmentation architecture "
                                        },
                                        {
                                            "pdf_id": "8.38",
                                            "matching_string": "M2F and named them LiteM2F and RTM2F, respectively. Second, we include "
                                        },
                                        {
                                            "pdf_id": "8.36",
                                            "matching_string": "M2F . Further, we also integrate recently proposed transformer encoder "
                                        },
                                        {
                                            "pdf_id": "8.39",
                                            "matching_string": "comparisons with recent efficient architectures that proposed taskspecific components, "
                                        },
                                        {
                                            "pdf_id": "8.37",
                                            "matching_string": "designs (LiteDETR and RTDETR ) for efficient object detection into "
                                        },
                                        {
                                            "pdf_id": "8.34",
                                            "matching_string": "We compare \\ours with two sets of efficient segmentation namely YOSO, RAPSAM, and ReMax ."
                                        },
                                        {
                                            "pdf_id": "8.40",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par3",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 60,
                                    "key": "doc/body/sec3/par3/tit",
                                    "block type": "title",
                                    "content": "Architecture details.",
                                    "leftover": "Architecture details.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 61,
                                    "key": "doc/body/sec3/par3/txl0",
                                    "block type": "txl",
                                    "content": "We focus on standard backbones Res50 and SWINTiny pretrained on ImageNet1K, unless specified otherwise. We set the total number of encoder layers to be 6 following . We consider layers 2 to 6 as potential exit points, unless stated otherwise. In our gating network, we use a straightforward 1D adaptive average pooling operation as our pooling function.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "9.37",
                                            "matching_string": "total number of encoder layers to be 6 following . We consider layers 2 to 6 as "
                                        },
                                        {
                                            "pdf_id": "9.39",
                                            "matching_string": "straightforward 1D adaptive average pooling operation as our pooling function."
                                        },
                                        {
                                            "pdf_id": "9.38",
                                            "matching_string": "potential exit points, unless stated otherwise. In our gating network, we use a "
                                        },
                                        {
                                            "pdf_id": "9.36",
                                            "matching_string": "pretrained on ImageNet1K, unless specified otherwise. We set the "
                                        },
                                        {
                                            "pdf_id": "9.35",
                                            "matching_string": "We focus on standard backbones Res50 and SWINTiny "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par4",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 62,
                                    "key": "doc/body/sec3/par4/tit",
                                    "block type": "title",
                                    "content": "Training settings.",
                                    "leftover": "Training settings.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par4/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 63,
                                            "key": "doc/body/sec3/par4/txl0/txl0",
                                            "block type": "txl",
                                            "content": "The experimental setup closely mirrors that of M2F, with all model configurations and training specifics following identical procedures. We use Detectron2 and PyTorch for our implementation. For the stochastic depth training phase (Step \\stepA), we initialize weights as provided by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations for Cityscapes, with a batch size of 16. For the training of the gating network (Step \\stepC), we perform 2 epochs of training on the COCO dataset and 20k iterations on the Cityscapes dataset, employing the Adam optimizer . The adaptation factor in the utility function, as discussed in \\Secref{sec:gating}, is set to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed training is performed using 8 A6000 GPUs. On the COCO dataset, the training time of Step \\stepA is 280 GPU hours, Step \\stepB is 17 GPU hours, and Step \\stepC 7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step \\stepA is 45 GPU hours, Step \\stepB is 1 GPU hours, and Step \\stepC is 7.2 GPU hours. In Step \\stepA, we use identical settings as M2F for the loss between the predicted segment and ground truth segment, \\ie, . The weight λmask is fixed at 5.0, while λclass is set to 2.0 for all classes, except 0.1 for the ''no object'' class.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "9.45",
                                                    "matching_string": "for Cityscapes, with a batch size of 16. For the training of the gating "
                                                },
                                                {
                                                    "pdf_id": "9.49",
                                                    "matching_string": "to 0.0005 for COCO and 0.003 for Cityscapes, unless otherwise specified. Distributed "
                                                },
                                                {
                                                    "pdf_id": "9.56",
                                                    "matching_string": "is set to 2.0 for all classes, except 0.1 for the ''no object'' class."
                                                },
                                                {
                                                    "pdf_id": "9.41",
                                                    "matching_string": "with all model configurations and training specifics following identical procedures. "
                                                },
                                                {
                                                    "pdf_id": "9.43",
                                                    "matching_string": "the stochastic depth training phase (Step \\stepA), we initialize weights as provided "
                                                },
                                                {
                                                    "pdf_id": "9.46",
                                                    "matching_string": "network (Step \\stepC), we perform 2 epochs of training on the COCO dataset and "
                                                },
                                                {
                                                    "pdf_id": "9.51",
                                                    "matching_string": "training time of Step \\stepA is 280 GPU hours, Step \\stepB is 17 GPU hours, and Step "
                                                },
                                                {
                                                    "pdf_id": "9.52",
                                                    "matching_string": "7.2 GPU hours. Similarly for Cityscapes dataset, the training time of Step "
                                                },
                                                {
                                                    "pdf_id": "9.53",
                                                    "matching_string": "is 45 GPU hours, Step \\stepB is 1 GPU hours, and Step \\stepC is 7.2 GPU hours. In Step "
                                                },
                                                {
                                                    "pdf_id": "9.54",
                                                    "matching_string": "we use identical settings as M2F for the loss between the predicted segment "
                                                },
                                                {
                                                    "pdf_id": "9.48",
                                                    "matching_string": "The adaptation factor in the utility function, as discussed in \\Secref{sec:gating}, is set "
                                                },
                                                {
                                                    "pdf_id": "9.50",
                                                    "matching_string": "training is performed using 8 A6000 GPUs. On the COCO dataset, the "
                                                },
                                                {
                                                    "pdf_id": "9.55",
                                                    "matching_string": "and ground truth segment, \\ie, . The weight λmask is fixed at 5.0, while λclass "
                                                },
                                                {
                                                    "pdf_id": "9.47",
                                                    "matching_string": "20k iterations on the Cityscapes dataset, employing the Adam optimizer . "
                                                },
                                                {
                                                    "pdf_id": "9.44",
                                                    "matching_string": "by M2F and subsequently train 50 epochs for the COCO dataset and 90k iterations "
                                                },
                                                {
                                                    "pdf_id": "9.40",
                                                    "matching_string": "The experimental setup closely mirrors that of M2F, We use Detectron2 and PyTorch for our implementation. For \\stepC \\stepA \\stepA, "
                                                },
                                                {
                                                    "pdf_id": "9.42",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/par4/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 64,
                                            "key": "doc/body/sec3/par4/sub1/tit",
                                            "block type": "title",
                                            "content": "Main Results",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "9.57",
                                                    "matching_string": "Main Results"
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 65,
                                            "key": "doc/body/sec3/par4/sub1/txl0",
                                            "block type": "txl",
                                            "content": "In \\tabref{tab:resultcoco} and \\tabref{tab:resultcityscapes}, we compare {\\ours} with our baseline prior works on the validation set of COCO and Cityscapes dataset, respectively. In \\tabref{tab:resultcoco}, we observe that {\\ours} effectively reduces computational costs while upholding performance levels in comparison to M2F using both SWINT and Res50 backbones. Additionally, {\\ours} can be seamlessly integrated into efficient encoder designs, such as LiteM2F, further reducing GLOPs by approximately 12.6%. With Res50 as the backbone, MF, YOSO, and RAPSAM exhibit inferior performance compared to {\\ours}. Although ReMax demonstrates competitive accuracy, its focus on specialized panoptic segmentation models limits its applicability. Our work, however, aims for a broader impact by creating efficient segmentation architectures that can be used for various segmentation tasks. We make similar observations on the Cityscapes dataset as presented in \\tabref{tab:resultcityscapes}.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "10.50",
                                                    "matching_string": "segmentation models limits its applicability. Our work, however, aims for a "
                                                },
                                                {
                                                    "pdf_id": "10.52",
                                                    "matching_string": "for various segmentation tasks. We make similar observations on the Cityscapes "
                                                },
                                                {
                                                    "pdf_id": "9.59",
                                                    "matching_string": "the validation set of COCO and Cityscapes dataset, respectively. In "
                                                },
                                                {
                                                    "pdf_id": "10.46",
                                                    "matching_string": "efficient encoder designs, such as LiteM2F, further reducing GLOPs by "
                                                },
                                                {
                                                    "pdf_id": "10.48",
                                                    "matching_string": "RAPSAM exhibit inferior performance compared to {\\ours}. Although "
                                                },
                                                {
                                                    "pdf_id": "10.49",
                                                    "matching_string": "ReMax demonstrates competitive accuracy, its focus on specialized panoptic "
                                                },
                                                {
                                                    "pdf_id": "10.51",
                                                    "matching_string": "broader impact by creating efficient segmentation architectures that can be used "
                                                },
                                                {
                                                    "pdf_id": "9.60",
                                                    "matching_string": "we observe that {\\ours} effectively reduces computational costs while upholding "
                                                },
                                                {
                                                    "pdf_id": "10.45",
                                                    "matching_string": "Res50 backbones. Additionally, {\\ours} can be seamlessly integrated into "
                                                },
                                                {
                                                    "pdf_id": "10.47",
                                                    "matching_string": "approximately 12.6%. With Res50 as the backbone, MF, YOSO, and "
                                                },
                                                {
                                                    "pdf_id": "10.53",
                                                    "matching_string": "dataset as presented in "
                                                },
                                                {
                                                    "pdf_id": "9.61",
                                                    "matching_string": "performance levels in comparison to M2F using both SWINT and "
                                                },
                                                {
                                                    "pdf_id": "9.58",
                                                    "matching_string": "In \\tabref{tab:resultcoco} and \\tabref{tab:resultcityscapes}, we compare {\\ours} with our baseline prior works on \\tabref{tab:resultcoco}, \\tabref{tab:resultcityscapes}."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/par4/sub2",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 66,
                                            "key": "doc/body/sec3/par4/sub2/tit",
                                            "block type": "title",
                                            "content": "Ablation Studies",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "10.54",
                                                    "matching_string": "Ablation Studies"
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par5",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 67,
                                    "key": "doc/body/sec3/par5/tit",
                                    "block type": "title",
                                    "content": "Balancing computational cost and performance.",
                                    "leftover": "Balancing computational cost and performance.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par5/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 68,
                                            "key": "doc/body/sec3/par5/txl0/txl0",
                                            "block type": "txl",
                                            "content": "Within \\ours, the parameter serves as an adaptation factor that governs the tradeoff between computational cost and performance. Its value, however, is contingent upon the backbone and dataset characteristics. This dependency arises due to the disparate ranges of GFLOPs and segmentation quality (represented by PQ) which are a function of the architecture components and the training data distribution. \\Figref{fig:perato} illustrates {\\ours} with different values of in Step \\stepC using the exact same weights for the parent model from Step \\stepA during inference. In comparison to the M2Fi model (where each is trained standalone with i layers), adjusting the value of provides a tradeoff between GFLOPs and PQ. \\input{tables/lossandsmallermodels}",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "0.28",
                                                    "matching_string": "i "
                                                },
                                                {
                                                    "pdf_id": "10.56",
                                                    "matching_string": "parameter serves as an adaptation factor that governs the tradeoff between "
                                                },
                                                {
                                                    "pdf_id": "10.58",
                                                    "matching_string": "backbone and dataset characteristics. This dependency arises due to the disparate "
                                                },
                                                {
                                                    "pdf_id": "10.60",
                                                    "matching_string": "are a function of the architecture components and the training data distribution. "
                                                },
                                                {
                                                    "pdf_id": "10.63",
                                                    "matching_string": "to the M2Fi model (where each is trained standalone with layers), adjusting "
                                                },
                                                {
                                                    "pdf_id": "10.57",
                                                    "matching_string": "computational cost and performance. Its value, however, is contingent upon the "
                                                },
                                                {
                                                    "pdf_id": "10.59",
                                                    "matching_string": "ranges of GFLOPs and segmentation quality (represented by PQ) which "
                                                },
                                                {
                                                    "pdf_id": "10.62",
                                                    "matching_string": "same weights for the parent model from Step \\stepA during inference. In comparison "
                                                },
                                                {
                                                    "pdf_id": "10.64",
                                                    "matching_string": "the value of provides a tradeoff between GFLOPs and PQ. "
                                                },
                                                {
                                                    "pdf_id": "10.61",
                                                    "matching_string": "illustrates {\\ours} with different values of in Step \\stepC using the exact "
                                                },
                                                {
                                                    "pdf_id": "10.55",
                                                    "matching_string": "Within \\ours, the \\Figref{fig:perato} \\input{tables/lossandsmallermodels}"
                                                },
                                                {
                                                    "pdf_id": "10.66",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 69,
                                    "key": "doc/body/sec3/par5/itemize1",
                                    "block type": "itemize",
                                    "content": "\\setlengthsep{0.0em} Impact of adaptation factor . We analyze the impact of on \\ours and present our analysis in \\tabref{tab:resultbeta}. As expected, a smaller prioritizes segmentation quality over computations resulting in superior performance. Conversely, a larger signifies a greater emphasis on GFLOPs. This results in a slight sacrifice in PQ leading to a significant reduction in GFLOPs. Impact of total encoder layers in the parent architecture. In situations where computational resources are limited, we present an approach using {\\ours} to create a scaleddown version of the parent model. As illustrated in \\Tabref{tab:resultsmallermodels}, we analyze the impact of set to 5 and 4 (in place of the default value of 6). We set =0.0005 for all the models. It can be observed that {\\ours} not only reduces the computational load but also preserves the performance of the parent model. Importantly, we only execute Step \\stepC for each configuration which involves training the gating network.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "10.67",
                                            "matching_string": "quality over computations resulting in superior performance. Conversely, "
                                        },
                                        {
                                            "pdf_id": "10.69",
                                            "matching_string": "sacrifice in PQ leading to a significant reduction in GFLOPs. "
                                        },
                                        {
                                            "pdf_id": "11.52",
                                            "matching_string": "which involves training the gating network."
                                        },
                                        {
                                            "pdf_id": "10.68",
                                            "matching_string": "a larger signifies a greater emphasis on GFLOPs. This results in a slight "
                                        },
                                        {
                                            "pdf_id": "11.45",
                                            "matching_string": "Impact of total encoder layers in the parent architecture. In situations where "
                                        },
                                        {
                                            "pdf_id": "11.46",
                                            "matching_string": "computational resources are limited, we present an approach using "
                                        },
                                        {
                                            "pdf_id": "11.47",
                                            "matching_string": "to create a scaleddown version of the parent model. As illustrated in "
                                        },
                                        {
                                            "pdf_id": "11.48",
                                            "matching_string": "we analyze the impact of set to 5 and 4 (in place of the default value of "
                                        },
                                        {
                                            "pdf_id": "11.49",
                                            "matching_string": "6). We set =0.0005 for all the models. It can be observed that "
                                        },
                                        {
                                            "pdf_id": "11.50",
                                            "matching_string": "not only reduces the computational load but also preserves the performance of "
                                        },
                                        {
                                            "pdf_id": "11.51",
                                            "matching_string": "the parent model. Importantly, we only execute Step \\stepC for each configuration "
                                        },
                                        {
                                            "pdf_id": "0.11",
                                            "matching_string": ". "
                                        },
                                        {
                                            "pdf_id": "10.65",
                                            "matching_string": "Impact of adaptation factor We analyze the impact of on \\ours and "
                                        },
                                        {
                                            "pdf_id": "0.31",
                                            "matching_string": "a "
                                        },
                                        {
                                            "pdf_id": "0.30",
                                            "matching_string": "\\setlengthsep{0.0em} present our analysis in \\tabref{tab:resultbeta}. As expected, smaller prioritizes segmentation {\\ours} \\Tabref{tab:resultsmallermodels}, {\\ours} "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par6",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 70,
                                    "key": "doc/body/sec3/par6/tit",
                                    "block type": "title",
                                    "content": "Impact of backbone features.",
                                    "leftover": "Impact of backbone features.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/par6/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 71,
                                            "key": "doc/body/sec3/par6/txl0/txl0",
                                            "block type": "txl",
                                            "content": "We analyze the impact of backbone features on \\ours in \\Tabref{tab:resultbackbones} and \\Tabref{tab:resultweights}, since our gating network mechanism takes these features as input.",
                                            "leftover": "We analyze the impact of backbone features on \\ours in \\Tabref{tab:resultbackbones} and \\Tabref{tab:resultweights}, since our gating network mechanism takes ",
                                            "matches": [
                                                {
                                                    "pdf_id": "11.55",
                                                    "matching_string": "these features as input."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 72,
                                    "key": "doc/body/sec3/par6/itemize1",
                                    "block type": "itemize",
                                    "content": "\\setlengthsep{0.0em} Size variations. In \\tabref{tab:resultbackbones}, we evaluate {\\ours} with backbones of different sizes. Specifically, we observe the performance with SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . We can observe the adaptability of {\\ours} in delivering robust performance efficiency across a range of these backbone sizes. Furthermore, this versatility of {\\ours} extends to LiteM2F metaarchitecture as well (see supplementary material). Pretrained weight variations. We explore how different pretraining strategies for the backbones affect the performance of {\\ours} in \\tabref{tab:resultweights}. We initialize the backbone weights using both supervised learning (SL) and selfsupervised learning (SSL) techniques on the ImageNet1K dataset . For SSL pretraining on the SWINT backbone, we utilize MoBY . For SSL pretraining on the Res50 backbone, we use DINO . With MoBY weights on the SWINT backbone, {\\ours} maintains 99.7% of the PQ of M2F while reducing GFLOPs in the transformer encoder by 26.34%. With DINO weights on the Res50 backbone, we observe a reduction of 29.79% in GFLOPs in the transformer encoder, alongside a slight improvement in PQ.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "11.57",
                                            "matching_string": "sizes. Specifically, we observe the performance with SWINTiny (T), SWINSmall "
                                        },
                                        {
                                            "pdf_id": "11.61",
                                            "matching_string": "LiteM2F metaarchitecture as well (see supplementary material). "
                                        },
                                        {
                                            "pdf_id": "11.64",
                                            "matching_string": "initialize the backbone weights using both supervised learning (SL) and selfsupervised "
                                        },
                                        {
                                            "pdf_id": "12.21",
                                            "matching_string": "reducing GFLOPs in the transformer encoder by 26.34%. With DINO weights "
                                        },
                                        {
                                            "pdf_id": "12.22",
                                            "matching_string": "on the Res50 backbone, we observe a reduction of 29.79% in GFLOPs in the "
                                        },
                                        {
                                            "pdf_id": "12.23",
                                            "matching_string": "transformer encoder, alongside a slight improvement in PQ."
                                        },
                                        {
                                            "pdf_id": "11.58",
                                            "matching_string": "(S), and SWINBase (B) architectures . We can observe the adaptability "
                                        },
                                        {
                                            "pdf_id": "11.60",
                                            "matching_string": "of these backbone sizes. Furthermore, this versatility of {\\ours} extends to "
                                        },
                                        {
                                            "pdf_id": "11.62",
                                            "matching_string": "Pretrained weight variations. We explore how different pretraining strategies "
                                        },
                                        {
                                            "pdf_id": "11.66",
                                            "matching_string": "pretraining on the SWINT backbone, we utilize MoBY . For SSL pretraining "
                                        },
                                        {
                                            "pdf_id": "11.68",
                                            "matching_string": "on the SWINT backbone, {\\ours} maintains 99.7% of the PQ of M2F while "
                                        },
                                        {
                                            "pdf_id": "11.59",
                                            "matching_string": "of different of {\\ours} in delivering robust performance efficiency across a range "
                                        },
                                        {
                                            "pdf_id": "11.63",
                                            "matching_string": "for the backbones affect the performance of {\\ours} in \\tabref{tab:resultweights}. We "
                                        },
                                        {
                                            "pdf_id": "11.65",
                                            "matching_string": "learning (SSL) techniques on the ImageNet1K dataset . For SSL "
                                        },
                                        {
                                            "pdf_id": "11.56",
                                            "matching_string": "Size variations. In \\tabref{tab:resultbackbones}, we evaluate {\\ours} with backbones "
                                        },
                                        {
                                            "pdf_id": "11.53",
                                            "matching_string": "\\setlengthsep{0.0em} on the Res50 backbone, we use DINO . With MoBY weights "
                                        },
                                        {
                                            "pdf_id": "11.54",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "11.67",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/par7",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 73,
                                    "key": "doc/body/sec3/par7/tit",
                                    "block type": "title",
                                    "content": "Performance in object detection.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "0.13",
                                            "matching_string": "Performance in object detection."
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/par7/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 74,
                                            "key": "doc/body/sec3/par7/txl0/txl0",
                                            "block type": "txl",
                                            "content": "To demonstrate the applicability of our method beyond segmentation, we extend our proposed three step recipe for object detection task using DETR for object detection task and name it \\oursdetr. In particular, we vary the values of the adaptation factor to analyze performancecomputation tradeoffs of \\oursdetr in \\tabref{tab:resultdetr}. Clearly, the resultant architecture maintains the performance but helps in reducing the computations of the encoder (\\eg, it achieves a 35.38% reduction in GFLOPs in the transformer encoder without significantly impacting performance).",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "12.29",
                                                    "matching_string": "architecture maintains the performance but helps in reducing the computations "
                                                },
                                                {
                                                    "pdf_id": "12.31",
                                                    "matching_string": "transformer encoder without significantly impacting performance)."
                                                },
                                                {
                                                    "pdf_id": "12.26",
                                                    "matching_string": "detection task using DETR for object detection task and name it "
                                                },
                                                {
                                                    "pdf_id": "12.27",
                                                    "matching_string": "In particular, we vary the values of the adaptation factor to analyze "
                                                },
                                                {
                                                    "pdf_id": "12.30",
                                                    "matching_string": "of the encoder (\\eg, it achieves a 35.38% reduction in GFLOPs in the "
                                                },
                                                {
                                                    "pdf_id": "12.25",
                                                    "matching_string": "method beyond segmentation, we extend our proposed three step recipe for object "
                                                },
                                                {
                                                    "pdf_id": "12.24",
                                                    "matching_string": "To demonstrate the applicability of our \\oursdetr. performancecomputation tradeoffs of \\oursdetr in \\tabref{tab:resultdetr}. Clearly, the resultant "
                                                },
                                                {
                                                    "pdf_id": "12.28",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/par7/sub1",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 75,
                                            "key": "doc/body/sec3/par7/sub1/tit",
                                            "block type": "title",
                                            "content": "Qualitative Comparisons",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "12.32",
                                                    "matching_string": "Qualitative Comparisons"
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 76,
                                            "key": "doc/body/sec3/par7/sub1/txl0",
                                            "block type": "txl",
                                            "content": "We present a few examples of predicted segmentation maps in \\Figref{fig:qualresult} with SWINT backbone. Compared to the parent architecture, \\ours consistently shows strong performance while selfselecting the encoder layers based on the input examples, both in everyday scenes (on COCO dataset) as well as intricate traffic scenes (on Cityscapes dataset).",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "12.35",
                                                    "matching_string": "shows strong performance while selfselecting the encoder layers based on the "
                                                },
                                                {
                                                    "pdf_id": "12.36",
                                                    "matching_string": "input examples, both in everyday scenes (on COCO dataset) as well as intricate "
                                                },
                                                {
                                                    "pdf_id": "12.37",
                                                    "matching_string": "traffic scenes (on Cityscapes dataset)."
                                                },
                                                {
                                                    "pdf_id": "12.34",
                                                    "matching_string": "backbone. Compared to the parent architecture, \\ours consistently "
                                                },
                                                {
                                                    "pdf_id": "12.33",
                                                    "matching_string": "We present a few examples of predicted segmentation maps in \\Figref{fig:qualresult} with SWINT "
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 77,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Conclusions",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.38",
                                    "matching_string": "Conclusions"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 78,
                                    "key": "doc/body/sec4/txl0/txl0",
                                    "block type": "txl",
                                    "content": "In this paper, we propose an efficient transformer encoder design {\\ours} for the Mask2Formerstyle frameworks. \\ours provides a threestep training recipe that can be used to customize the transformer encoder on the fly given the input image. The first step involves training the parent model to be dynamic by allowing stochastic depths at the transformer encoder. The second step involves creating a derived dataset from the training dataset which contains a pair of image and layer number that provides the highest segmentation quality. Finally, the third step involves training a gating network, whose function is to decide the number of layers to be used given the input image. Extensive experiments demonstrate that {\\ours} achieves significantly reduced computational complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\\ours}'s ability to dynamically tradeoff between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.42",
                                            "matching_string": "input image. The first step involves training the parent model to be dynamic "
                                        },
                                        {
                                            "pdf_id": "12.44",
                                            "matching_string": "creating a derived dataset from the training dataset which contains a "
                                        },
                                        {
                                            "pdf_id": "12.45",
                                            "matching_string": "pair of image and layer number that provides the highest segmentation quality. "
                                        },
                                        {
                                            "pdf_id": "12.47",
                                            "matching_string": "decide the number of layers to be used given the input image. Extensive experiments "
                                        },
                                        {
                                            "pdf_id": "0.12",
                                            "matching_string": "s "
                                        },
                                        {
                                            "pdf_id": "12.39",
                                            "matching_string": "In this paper, we propose an efficient transformer encoder design {\\ours} for the "
                                        },
                                        {
                                            "pdf_id": "12.41",
                                            "matching_string": "that can be used to customize the transformer encoder on the fly given the "
                                        },
                                        {
                                            "pdf_id": "12.46",
                                            "matching_string": "Finally, the third step involves training a gating network, whose function is to "
                                        },
                                        {
                                            "pdf_id": "12.40",
                                            "matching_string": "Mask2Formerstyle frameworks. \\ours provides a threestep training recipe "
                                        },
                                        {
                                            "pdf_id": "12.43",
                                            "matching_string": "by allowing stochastic depths at the transformer encoder. The second step involves "
                                        },
                                        {
                                            "pdf_id": "12.48",
                                            "matching_string": "demonstrate that {\\ours} achieves significantly reduced computational "
                                        },
                                        {
                                            "pdf_id": "12.49",
                                            "matching_string": "complexity compared to established methods while maintaining competitive performance in universal segmentation. Our results highlight {\\ours}'ability to dynamically tradeoff between performance and efficiency as per requirements, showcasing its adaptability across diverse architectural configurations, and can be applied to models for object detection tasks."
                                        },
                                        {
                                            "pdf_id": "12.50",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 79,
                                    "key": "doc/body/sec4/par1/tit",
                                    "block type": "title",
                                    "content": "Limitations.",
                                    "leftover": "Limitations.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/par1/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 80,
                                            "key": "doc/body/sec4/par1/txl0/txl0",
                                            "block type": "txl",
                                            "content": "While {\\ours} offers dynamic tradeoffs between performance and efficiency according to specific needs, the adaptation factor is a hyperparameter that needs separate tuning for each use case. This is because it relies on the model configuration and dataset characteristics. \\clearpage",
                                            "leftover": "While {\\ours} offers dynamic tradeoffs between performance and efficiency according to specific needs, the adaptation factor is a hyperparameter that needs separate tuning for each use case. This is because it relies on the model configuration and dataset characteristics. \\clearpage",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 81,
                                    "key": "doc/body/sec4/par1/center1",
                                    "block type": "center",
                                    "content": "\\Large{Efficient Transformer Encoders for Mask2Formerstyle models (Supplementary Material)} \\vspace*{2em}",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "14.2",
                                            "matching_string": "Mask2Formerstyle models "
                                        },
                                        {
                                            "pdf_id": "14.1",
                                            "matching_string": "Efficient Transformer Encoders for "
                                        },
                                        {
                                            "pdf_id": "14.0",
                                            "matching_string": "\\Large{(Supplementary Material)} \\vspace*{2em}"
                                        },
                                        {
                                            "pdf_id": "14.3",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 82,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Additional Experiments",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "14.4",
                                    "matching_string": "Additional Experiments"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/par0",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 83,
                                    "key": "doc/body/sec5/par0/tit",
                                    "block type": "title",
                                    "content": "Impact of backbone size on LiteM2F.",
                                    "leftover": "Impact of backbone size on LiteM2F.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 84,
                                    "key": "doc/body/sec5/par0/txl0",
                                    "block type": "txl",
                                    "content": "We apply {\\ours} on LiteM2F using various backbone sizes, including SWINTiny (T), SWINSmall (S), and SWINBase (B) architectures . LiteM2F is a specific variant based on LiteDETR . We used the configuration named ''LiteDETR H3L1(6+1)×1'' given its strong performance in detection relative to the computations required. However, we adjust this configuration to (5+1) when applying our approach to LiteM2F. Further, we use their without the keyaware deformable attention proposed in their paper. This adjustment is necessary because LiteM2F actually has 6 encoder layers, and the original configuration might introduce an additional layer that isn't present in the model. Following this, we identify layers 2 to 5 as potential exits, followed by the last layer, layer 6 in the transformer encoder. We retain layer 6 and do not consider it as a feasible exit point as it leverages features from all scales provided by the backbone, making it essential to the model's functionality. As shown in \\Tabref{tab:suppresultbackbones}, we observe that {\\ours} effectively reduces computational cost while maintaining performance across LiteM2F variants, which underscores the versatility and robustness of {\\ours} across different model architectures and sizes.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "14.6",
                                            "matching_string": "using various backbone sizes, including SWINTiny (T), SWINSmall (S), and "
                                        },
                                        {
                                            "pdf_id": "14.8",
                                            "matching_string": ". We used the configuration named ''LiteDETR H3L1(6+1)×1'' given "
                                        },
                                        {
                                            "pdf_id": "14.10",
                                            "matching_string": "we adjust this configuration to (5+1) when applying our approach to LiteM2F. "
                                        },
                                        {
                                            "pdf_id": "14.13",
                                            "matching_string": "has 6 encoder layers, and the original configuration might introduce an "
                                        },
                                        {
                                            "pdf_id": "14.15",
                                            "matching_string": "2 to 5 as potential exits, followed by the last layer, layer 6 in the transformer "
                                        },
                                        {
                                            "pdf_id": "14.17",
                                            "matching_string": "it leverages features from all scales provided by the backbone, making it essential "
                                        },
                                        {
                                            "pdf_id": "14.7",
                                            "matching_string": "SWINBase (B) architectures . LiteM2F is a specific variant based on LiteDETR "
                                        },
                                        {
                                            "pdf_id": "14.9",
                                            "matching_string": "its strong performance in detection relative to the computations required. However, "
                                        },
                                        {
                                            "pdf_id": "14.11",
                                            "matching_string": "Further, we use their without the keyaware deformable attention "
                                        },
                                        {
                                            "pdf_id": "14.12",
                                            "matching_string": "proposed in their paper. This adjustment is necessary because LiteM2F actually "
                                        },
                                        {
                                            "pdf_id": "14.14",
                                            "matching_string": "additional layer that isn't present in the model. Following this, we identify layers "
                                        },
                                        {
                                            "pdf_id": "14.16",
                                            "matching_string": "encoder. We retain layer 6 and do not consider it as a feasible exit point as "
                                        },
                                        {
                                            "pdf_id": "14.19",
                                            "matching_string": "effectively reduces computational cost while maintaining performance across "
                                        },
                                        {
                                            "pdf_id": "14.20",
                                            "matching_string": "LiteM2F variants, which underscores the versatility and robustness of "
                                        },
                                        {
                                            "pdf_id": "14.21",
                                            "matching_string": "across different model architectures and sizes."
                                        },
                                        {
                                            "pdf_id": "14.18",
                                            "matching_string": "to the model's functionality. As shown in \\Tabref{tab:suppresultbackbones}, we observe that "
                                        },
                                        {
                                            "pdf_id": "14.5",
                                            "matching_string": "We apply {\\ours} on LiteM2F {\\ours} {\\ours} "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/par1",
                            "block_type": "par",
                            "children": [
                                {
                                    "leaf id": 85,
                                    "key": "doc/body/sec5/par1/tit",
                                    "block type": "title",
                                    "content": "Impact of target and loss settings for gating network training.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "14.22",
                                            "matching_string": "Impact of target and loss settings for gating network training."
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 86,
                                            "key": "doc/body/sec5/par1/txl0/txl0",
                                            "block type": "txl",
                                            "content": "We investigate various target and loss settings during the training of the gating network. Specifically, we compare the approach detailed in the main paper, using onehot target and crossentropy loss (referred to as ''hardCE'' in \\Tabref{tab:tgtloss}), with three alternative methods that do not involve setting a specific target exit for each image.",
                                            "leftover": "We ",
                                            "matches": [
                                                {
                                                    "pdf_id": "14.23",
                                                    "matching_string": "investigate various target and loss settings during the training of the gating "
                                                },
                                                {
                                                    "pdf_id": "14.24",
                                                    "matching_string": "network. Specifically, we compare the approach detailed in the main paper, using "
                                                },
                                                {
                                                    "pdf_id": "14.26",
                                                    "matching_string": "three alternative methods that do not involve setting a specific target exit for "
                                                },
                                                {
                                                    "pdf_id": "14.25",
                                                    "matching_string": "onehot target and crossentropy loss (referred to as ''hardCE'' in \\Tabref{tab:tgtloss}), with "
                                                },
                                                {
                                                    "pdf_id": "14.27",
                                                    "matching_string": "each image."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/txl1",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 87,
                                            "key": "doc/body/sec5/par1/txl1/txl0",
                                            "block type": "txl",
                                            "content": "First, we consider using crossentropy loss between the output of the utility function (·) and the predicted logit passed through a softmax function (referred to as ''uCE''), \\ie,",
                                            "leftover": "\\ie,",
                                            "matches": [
                                                {
                                                    "pdf_id": "14.28",
                                                    "matching_string": "First, we consider using crossentropy loss between the output of the utility "
                                                },
                                                {
                                                    "pdf_id": "14.29",
                                                    "matching_string": "function (·) and the predicted logit passed through a softmax function (referred "
                                                },
                                                {
                                                    "pdf_id": "14.30",
                                                    "matching_string": "to as ''uCE''), "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 88,
                                    "key": "doc/body/sec5/par1/frm2",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^()ln[softmax(g)] .",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "15.22",
                                            "matching_string": "gating=∑i^N∑^()ln[softmax(g)] ."
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/txl3",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 89,
                                            "key": "doc/body/sec5/par1/txl3/txl0",
                                            "block type": "txl",
                                            "content": "Second, we apply a softmax function to the utility function u() and use crossentropy as the loss function (referred to as ''softCE''), \\ie,",
                                            "leftover": "\\ie,",
                                            "matches": [
                                                {
                                                    "pdf_id": "14.38",
                                                    "matching_string": "Second, we apply a softmax function to the utility function u() and use "
                                                },
                                                {
                                                    "pdf_id": "14.39",
                                                    "matching_string": "crossentropy as the loss function (referred to as ''softCE''), "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 90,
                                    "key": "doc/body/sec5/par1/frm4",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^softmax(())ln[softmax(g)] .",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "15.0",
                                            "matching_string": "gating=∑i^N∑^softmax(())ln[softmax(g)] ."
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/txl5",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 91,
                                            "key": "doc/body/sec5/par1/txl5/txl0",
                                            "block type": "txl",
                                            "content": "Third, we apply a softmax function to the utility function, but use mean squared error (MSE) loss instead (referred to as ''softMSE''), \\ie,",
                                            "leftover": "\\ie,",
                                            "matches": [
                                                {
                                                    "pdf_id": "15.1",
                                                    "matching_string": "Third, we apply a softmax function to the utility function, but use mean "
                                                },
                                                {
                                                    "pdf_id": "15.2",
                                                    "matching_string": "squared error (MSE) loss instead (referred to as ''softMSE''), "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 92,
                                    "key": "doc/body/sec5/par1/frm6",
                                    "block type": "frm",
                                    "content": "gating=∑i^N∑^[softmax(())softmax(g)]^2 .",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "9.10",
                                            "matching_string": "gating=∑i^N∑^[softmax(())softmax(g)]^2 ."
                                        },
                                        {
                                            "pdf_id": "15.31",
                                            "matching_string": ""
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/txl7",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 93,
                                            "key": "doc/body/sec5/par1/txl7/txl0",
                                            "block type": "txl",
                                            "content": "The analysis in \\Tabref{tab:tgtloss} is conducted using the SWINT backbone on the COCO dataset. We observe that ''hardCE'' yields the most favorable results. As a result, we use this approach consistently in the main paper.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "15.11",
                                                    "matching_string": "COCO dataset. We observe that ''hardCE'' yields the most favorable results. As "
                                                },
                                                {
                                                    "pdf_id": "15.12",
                                                    "matching_string": "a result, we use this approach consistently in the main paper."
                                                },
                                                {
                                                    "pdf_id": "15.10",
                                                    "matching_string": "The analysis in \\Tabref{tab:tgtloss} is conducted using the SWINT backbone on the "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec5/par1/table8",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec5/par1/table8/minipage0",
                                            "block_type": "minipage",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage0/txl0",
                                                    "block_type": "txl",
                                                    "children": [
                                                        {
                                                            "leaf id": 94,
                                                            "key": "doc/body/sec5/par1/table8/minipage0/txl0/txl0",
                                                            "block type": "txl",
                                                            "content": "{0.495\\textwidth} \\centering",
                                                            "leftover": "",
                                                            "matches": [
                                                                {
                                                                    "pdf_id": "9.11",
                                                                    "matching_string": "{0.495\\textwidth} \\centering"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage0/cpt1",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 95,
                                                            "key": "doc/body/sec5/par1/table8/minipage0/cpt1/txl0",
                                                            "block type": "txl",
                                                            "content": "Impact of backbone size on LiteM2F. Our Lite{\\ours} maintains the performance of LiteM2F while reducing GFLOPs for different datasets and for different backbones.",
                                                            "leftover": "",
                                                            "matches": [
                                                                {
                                                                    "pdf_id": "15.15",
                                                                    "matching_string": "the performance of LiteM2F while reducing "
                                                                },
                                                                {
                                                                    "pdf_id": "15.19",
                                                                    "matching_string": "different backbones."
                                                                },
                                                                {
                                                                    "pdf_id": "15.13",
                                                                    "matching_string": "Impact of backbone size on "
                                                                },
                                                                {
                                                                    "pdf_id": "15.14",
                                                                    "matching_string": "LiteM2F. Our Lite{\\ours} maintains "
                                                                },
                                                                {
                                                                    "pdf_id": "15.17",
                                                                    "matching_string": "GFLOPs for different datasets and for "
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2",
                                                    "block_type": "tabular",
                                                    "children": [
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl0",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 96,
                                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl0/txl0",
                                                                    "block type": "txl",
                                                                    "content": "{llcccccccc} \\toprule && \\multicolumn{3}{c}{Performance (↑)} && \\multicolumn{2}{c}{GFLOPs (↓)} \\cline{35} \\cline{78} \\multirow{2}{*}{Bakbone} & \\multirow{2}{*}{Model} & PQ & mIOUp & APp && Total & Tx. Enc. \\midrule \\multicolumn{8}{c}{Dataset: Cityscapes} \\hline \\multirow{2}{*}{SWINT}",
                                                                    "leftover": "",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "9.31",
                                                                            "matching_string": "{llcccccccc} \\toprule && \\multicolumn{3}{c}{Performance (↑)} && \\multicolumn{2}{c}{GFLOPs (↓)} \\cline{35} \\cline{78} \\multirow{2}{*}{Bakbone} & \\multirow{2}{*}{Model} & PQ & mIOUp & APp && Total & Tx. Enc. \\midrule \\multicolumn{8}{c}{Dataset: Cityscapes} \\hline \\multirow{2}{*}{SWINT}"
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.30",
                                                                            "matching_string": ""
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.33",
                                                                            "matching_string": ""
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl1",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 97,
                                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl1/txl0",
                                                                    "block type": "txl",
                                                                    "content": "& LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 & Lite{\\ours} & 62.64 & 79.99 & 36.52 && 412.88 & 156.17 \\hline \\multirow{2}{*}{SWINS}",
                                                                    "leftover": "",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "9.15",
                                                                            "matching_string": "LiteM2F & 62.29 & 79.43 & 36.57 && 428.71 & 172.00 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.23",
                                                                            "matching_string": "62.64 & 79.99 & 36.52 && 412.88 & 156.17 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.13",
                                                                            "matching_string": "& & Lite{\\ours} & \\hline \\multirow{2}{*}{SWINS}"
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.12",
                                                                            "matching_string": ""
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.17",
                                                                            "matching_string": ""
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.19",
                                                                            "matching_string": ""
                                                                        },
                                                                        {
                                                                            "pdf_id": "9.21",
                                                                            "matching_string": ""
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl2",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 98,
                                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl2/txl0",
                                                                    "block type": "txl",
                                                                    "content": "& LiteM2F & 63.54 & 79.74 & 39.12 && 615.15 & 171.99 & Lite{\\ours} & 63.32 & 80.21 & 37.91 && 588.82 & 145.66 \\hline \\multirow{2}{*}{SWINB}",
                                                                    "leftover": "& & Lite{\\ours} & \\hline \\multirow{2}{*}{SWINB}",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "15.27",
                                                                            "matching_string": "LiteM2F & 63.54 & 79.74 & 39.12 && 615.15 & 171.99 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "15.28",
                                                                            "matching_string": "63.32 & 80.21 & 37.91 && 588.82 & 145.66 "
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl3",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 99,
                                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl3/txl0",
                                                                    "block type": "txl",
                                                                    "content": "& LiteM2F & 64.48 & 82.34 & 39.21 && 942.05 & 174.01 & Lite{\\ours} & 64.66 & 81.40 & 39.52 && 921.15 & 153.11 \\hline \\multicolumn{8}{c}{Dataset: COCO} \\hline \\multirow{2}{*}{}SWINT",
                                                                    "leftover": "& & Lite{\\ours} & \\hline \\multicolumn{8}{c}{COCO} \\hline \\multirow{2}{*}{}SWINT",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "15.29",
                                                                            "matching_string": "LiteM2F & 64.48 & 82.34 & 39.21 && 942.05 & 174.01 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "15.30",
                                                                            "matching_string": "64.66 & 81.40 & 39.52 && 921.15 & 153.11 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "15.32",
                                                                            "matching_string": "Dataset: "
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl4",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 100,
                                                                    "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl4/txl0",
                                                                    "block type": "txl",
                                                                    "content": "& LiteM2F & 52.70 & 63.08 & 41.10 && 193.79 & 79.78 & Lite{\\ours} & 52.84 & 63.23 & 42.18 && 178.43 & 64.42 \\hline \\multirow{2}{*}{SWINS}",
                                                                    "leftover": "& & Lite{\\ours} & \\hline \\multirow{2}{*}{SWINS}",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "15.34",
                                                                            "matching_string": "52.84 & 63.23 & 42.18 && 178.43 & 64.42 "
                                                                        },
                                                                        {
                                                                            "pdf_id": "15.33",
                                                                            "matching_string": "LiteM2F & 52.70 & 63.08 & 41.10 && 193.79 & 79.78 "
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "leaf id": 101,
                                                            "key": "doc/body/sec5/par1/table8/minipage0/tabular2/txl5",
                                                            "block type": "txl",
                                                            "content": "& LiteM2F & 54.30 & 64.81 & 43.94 && 269.26 & 74.45 & Lite{\\ours} & 54.47 & 64.14 & 43.55 && 258.00 & 63.96 \\bottomrule",
                                                            "leftover": "& & Lite{\\ours} & \\bottomrule",
                                                            "matches": [
                                                                {
                                                                    "pdf_id": "15.35",
                                                                    "matching_string": "LiteM2F & 54.30 & 64.81 & 43.94 && 269.26 & 74.45 "
                                                                },
                                                                {
                                                                    "pdf_id": "15.36",
                                                                    "matching_string": "54.47 & 64.14 & 43.55 && 258.00 & 63.96 "
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 102,
                                                    "key": "doc/body/sec5/par1/table8/minipage0/txl3",
                                                    "block type": "txl",
                                                    "content": "}",
                                                    "leftover": "}",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec5/par1/table8/minipage1",
                                            "block_type": "minipage",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage1/txl0",
                                                    "block_type": "txl",
                                                    "children": [
                                                        {
                                                            "leaf id": 103,
                                                            "key": "doc/body/sec5/par1/table8/minipage1/txl0/txl0",
                                                            "block type": "txl",
                                                            "content": "{0.495\\textwidth} \\centering",
                                                            "leftover": "{0.495\\textwidth} \\centering",
                                                            "matches": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage1/cpt1",
                                                    "block_type": "cpt",
                                                    "children": [
                                                        {
                                                            "leaf id": 104,
                                                            "key": "doc/body/sec5/par1/table8/minipage1/cpt1/txl0",
                                                            "block type": "txl",
                                                            "content": "Impact of target and loss in gating network training. We use ''hardCE'' loss for training our gating network in the main paper. (Backbone: SWINT; Dataset: COCO)",
                                                            "leftover": "",
                                                            "matches": [
                                                                {
                                                                    "pdf_id": "15.18",
                                                                    "matching_string": "gating network training. We use ''hardCE'' "
                                                                },
                                                                {
                                                                    "pdf_id": "15.21",
                                                                    "matching_string": "in the main paper. (Backbone: SWINT; "
                                                                },
                                                                {
                                                                    "pdf_id": "15.16",
                                                                    "matching_string": "Impact of target and loss in "
                                                                },
                                                                {
                                                                    "pdf_id": "15.20",
                                                                    "matching_string": "loss for training our gating network "
                                                                },
                                                                {
                                                                    "pdf_id": "15.24",
                                                                    "matching_string": "Dataset: COCO)"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "key": "doc/body/sec5/par1/table8/minipage1/tabular2",
                                                    "block_type": "tabular",
                                                    "children": [
                                                        {
                                                            "key": "doc/body/sec5/par1/table8/minipage1/tabular2/txl0",
                                                            "block_type": "txl",
                                                            "children": [
                                                                {
                                                                    "leaf id": 105,
                                                                    "key": "doc/body/sec5/par1/table8/minipage1/tabular2/txl0/txl0",
                                                                    "block type": "txl",
                                                                    "content": "{lcccccccc} \\toprule & \\multicolumn{3}{c}{Performance (↑)} && \\multicolumn{2}{c}{GFLOPs (↓)} \\cline{24} \\cline{67} \\multirow{2}{*}{Method} & PQ & mIOUp & APp && Total & Tx. Enc. \\midrule \\rowcolor{eccvblue!20}",
                                                                    "leftover": "",
                                                                    "matches": [
                                                                        {
                                                                            "pdf_id": "15.23",
                                                                            "matching_string": "{lcccccccc} \\toprule & \\multicolumn{3}{c}{Performance (↑)} && \\multicolumn{2}{c}{GFLOPs (↓)} \\cline{24} \\cline{67} \\multirow{2}{*}{Method} & PQ & mIOUp & APp && Total & Tx. Enc. \\midrule \\rowcolor{eccvblue!20}"
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "leaf id": 106,
                                                            "key": "doc/body/sec5/par1/table8/minipage1/tabular2/txl1",
                                                            "block type": "txl",
                                                            "content": "hardCE & 52.06 & 62.76 & 41.51 && 202.39 & 88.47 uCE & 52.16 & 62.58 & 41.57 && 207.49 & 94.06 softCE & 51.64 & 62.75 & 40.88 && 202.08 & 87.85 softMSE & 51.54 & 62.73 & 40.91 && 198.46 & 84.53 \\bottomrule",
                                                            "leftover": "",
                                                            "matches": [
                                                                {
                                                                    "pdf_id": "10.40",
                                                                    "matching_string": "52.06 & 62.76 & 41.51 && 202.39 & 88.47 "
                                                                },
                                                                {
                                                                    "pdf_id": "15.40",
                                                                    "matching_string": "softCE & 51.64 & 62.75 & 40.88 && 202.08 & 87.85 "
                                                                },
                                                                {
                                                                    "pdf_id": "15.39",
                                                                    "matching_string": "uCE & 52.16 & 62.58 & 41.57 && 207.49 & 94.06 "
                                                                },
                                                                {
                                                                    "pdf_id": "15.41",
                                                                    "matching_string": "softMSE & 51.54 & 62.73 & 40.91 && 198.46 & 84.53 "
                                                                },
                                                                {
                                                                    "pdf_id": "15.37",
                                                                    "matching_string": "hardCE & \\bottomrule"
                                                                },
                                                                {
                                                                    "pdf_id": "15.38",
                                                                    "matching_string": ""
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "leaf id": 107,
                                                    "key": "doc/body/sec5/par1/table8/minipage1/txl3",
                                                    "block type": "txl",
                                                    "content": "}",
                                                    "leftover": "}",
                                                    "matches": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 108,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Additional Qualitative Results",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "15.42",
                                    "matching_string": "Additional Qualitative Results"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 109,
                                    "key": "doc/body/sec6/txl0/txl0",
                                    "block type": "txl",
                                    "content": "We provide additional examples of predicted segmentation maps in \\Figref{fig:suppqualresult}.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "15.43",
                                            "matching_string": "We provide additional examples of predicted segmentation maps in "
                                        },
                                        {
                                            "pdf_id": "13.8",
                                            "matching_string": "\\Figref{fig:suppqualresult}."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/figure1",
                            "block_type": "figure",
                            "children": [
                                {
                                    "leaf id": 110,
                                    "key": "doc/body/sec6/figure1/tcbraster0",
                                    "block type": "tcbraster",
                                    "content": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp9/000000176232ours.jpg} \\vspace*{2.5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp2/000000002149ours.jpg} \\vspace*{5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitours.png} \\vspace*{8\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitours.png} \\vspace*{2\\baselineskip}",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "13.5",
                                            "matching_string": "\\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp9/000000176232m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp9/000000176232ours.jpg} \\vspace*{2.5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149.jpg} \\tcbincludegraphics[]{figures/qualresultimages/coco/exp2/000000002149m2f.jpg} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/coco/exp2/000000002149ours.jpg} \\vspace*{5\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp6/munster000008000019leftImg8bitours.png} \\vspace*{8\\baselineskip} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bit.png} \\tcbincludegraphics[]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitm2f.png} \\tcbincludegraphics[flip title={boxsep=0.75mm}]{figures/qualresultimages/cityscapes/exp7/munster000013000019leftImg8bitours.png} \\vspace*{2\\baselineskip}"
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 111,
                                    "key": "doc/body/sec6/figure1/cpt1",
                                    "block type": "cpt",
                                    "content": "Qualitative visualizations. We provide additional examples of predicted segmentation maps from M2F (middle column) and \\ours (last column). Top two rows are from the COCO dataset, whereas bottom two rows are from the Cityscapes dataset. Please zoom in for a clearer view of the details. (Backbone: SWINT)",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "13.3",
                                            "matching_string": "are from the COCO dataset, whereas bottom two rows are from the Cityscapes dataset. "
                                        },
                                        {
                                            "pdf_id": "13.2",
                                            "matching_string": "maps from M2F (middle column) and \\ours (last column). Top two rows "
                                        },
                                        {
                                            "pdf_id": "16.1",
                                            "matching_string": "Qualitative visualizations. We provide additional examples of predicted "
                                        },
                                        {
                                            "pdf_id": "16.4",
                                            "matching_string": "Please zoom in for a clearer view of the details. (Backbone: SWINT)"
                                        },
                                        {
                                            "pdf_id": "16.5",
                                            "matching_string": "segmentation "
                                        },
                                        {
                                            "pdf_id": "13.1",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "13.4",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "16.2",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "16.3",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 112,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Ammar, A., Khalil, M.I., Salama, C.: Rtyoso: Revisiting yoso for realtime panoptic segmentation. In: 2023 5th Novel Intelligent and Leading Emerging Sciences Conference (NILES). pp. 306311 (2023). \\doi{10.1109/NILES59815.2023.10296714}",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "16.6",
                    "matching_string": "Ammar, A., Khalil, M.I., Salama, C.: Rtyoso: Revisiting yoso for realtime "
                },
                {
                    "pdf_id": "16.7",
                    "matching_string": "panoptic segmentation. In: 2023 5th Novel Intelligent and Leading Emerging "
                },
                {
                    "pdf_id": "17.1",
                    "matching_string": "Sciences Conference (NILES). pp. 306311 (2023). \\doi{10.1109/N"
                },
                {
                    "pdf_id": "13.0",
                    "matching_string": "ILES59815.2023.10296714}"
                },
                {
                    "pdf_id": "17.2",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 113,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endtoend object detection with transformers. In: European conference on computer vision. pp. 213229. Springer (2020)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.3",
                    "matching_string": "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endtoend "
                },
                {
                    "pdf_id": "17.4",
                    "matching_string": "object detection with transformers. In: European conference on computer "
                },
                {
                    "pdf_id": "17.5",
                    "matching_string": "vision. pp. 213229. Springer (2020)"
                }
            ]
        },
        {
            "leaf id": 114,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Caron, M., Touvron, H., Misra, I., J\\'egou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in selfsupervised vision transformers. In: Proceedings of the International Conference on Computer Vision (ICCV) (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.7",
                    "matching_string": "A.: Emerging properties in selfsupervised vision transformers. In: Proceedings of "
                },
                {
                    "pdf_id": "17.6",
                    "matching_string": "Caron, M., Touvron, H., Misra, I., J\\'egou, H., Mairal, J., Bojanowski, P., Joulin, "
                },
                {
                    "pdf_id": "17.8",
                    "matching_string": "the International Conference on Computer Vision (ICCV) (2021)"
                }
            ]
        },
        {
            "leaf id": 115,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: Panopticdeeplab: A simple, strong, and fast baseline for bottomup panoptic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1247512485 (2020)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.9",
                    "matching_string": "Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: "
                },
                {
                    "pdf_id": "17.11",
                    "matching_string": "In: Proceedings of the IEEE/CVF conference on computer vision and "
                },
                {
                    "pdf_id": "17.10",
                    "matching_string": "Panopticdeeplab: A simple, strong, and fast baseline for bottomup panoptic segmentation. "
                },
                {
                    "pdf_id": "17.12",
                    "matching_string": "pattern recognition. pp. 1247512485 (2020)"
                }
            ]
        },
        {
            "leaf id": 116,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention mask transformer for universal image segmentation. In: CVPR (2022)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.13",
                    "matching_string": "Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention "
                },
                {
                    "pdf_id": "17.14",
                    "matching_string": "mask transformer for universal image segmentation. In: CVPR (2022)"
                },
                {
                    "pdf_id": "17.15",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 117,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Cheng, B., Schwing, A., Kirillov, A.: Perpixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems \\textbf{34}, 1786417875 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.16",
                    "matching_string": "Cheng, B., Schwing, A., Kirillov, A.: Perpixel classification is not all you need for "
                },
                {
                    "pdf_id": "17.17",
                    "matching_string": "semantic segmentation. Advances in Neural Information Processing Systems "
                },
                {
                    "pdf_id": "16.0",
                    "matching_string": "\\textbf{34}, 1786417875 (2021)"
                },
                {
                    "pdf_id": "17.18",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 118,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 32133223 (2016)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.19",
                    "matching_string": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., "
                },
                {
                    "pdf_id": "17.20",
                    "matching_string": "Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene "
                },
                {
                    "pdf_id": "17.21",
                    "matching_string": "understanding. In: Proceedings of the IEEE conference on computer vision and "
                },
                {
                    "pdf_id": "17.22",
                    "matching_string": "pattern recognition. pp. 32133223 (2016)"
                }
            ]
        },
        {
            "leaf id": 119,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., FeiFei, L.: Imagenet: A largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248255. Ieee (2009)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.23",
                    "matching_string": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., FeiFei, L.: Imagenet: A largescale "
                },
                {
                    "pdf_id": "17.24",
                    "matching_string": "hierarchical image database. In: 2009 IEEE conference on computer vision "
                },
                {
                    "pdf_id": "17.25",
                    "matching_string": "and pattern recognition. pp. 248255. Ieee (2009)"
                }
            ]
        },
        {
            "leaf id": 120,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Everingham, M., Eslami, S.A., Van~Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes challenge: A retrospective. International journal of computer vision \\textbf{111}, 98136 (2015)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.27",
                    "matching_string": "A.: The pascal visual object classes challenge: A retrospective. International journal "
                },
                {
                    "pdf_id": "17.26",
                    "matching_string": "Everingham, M., Eslami, S.A., Van~Gool, L., Williams, C.K., Winn, J., Zisserman, "
                },
                {
                    "pdf_id": "17.28",
                    "matching_string": "of computer vision \\textbf{111}, 98136 (2015)"
                }
            ]
        },
        {
            "leaf id": 121,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet for realtime semantic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 97169725 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.29",
                    "matching_string": "Fan, M., Lai, S., Huang, J., Wei, X., Chai, Z., Luo, J., Wei, X.: Rethinking bisenet "
                },
                {
                    "pdf_id": "17.30",
                    "matching_string": "for realtime semantic segmentation. In: Proceedings of the IEEE/CVF conference "
                },
                {
                    "pdf_id": "17.31",
                    "matching_string": "on computer vision and pattern recognition. pp. 97169725 (2021)"
                }
            ]
        },
        {
            "leaf id": 122,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Gu, X., Cui, Y., Huang, J., Rashwan, A., Yang, X., Zhou, X., Ghiasi, G., Kuo, W., Chen, H., Chen, L.C., et~al.: Dataseg: Taming a universal multidataset multitask segmentation model. Advances in Neural Information Processing Systems \\textbf{36} (2024)",
            "leftover": "\\textbf{36} ",
            "matches": [
                {
                    "pdf_id": "17.32",
                    "matching_string": "Gu, X., Cui, Y., Huang, J., Rashwan, A., Yang, X., Zhou, X., Ghiasi, G., Kuo, "
                },
                {
                    "pdf_id": "17.33",
                    "matching_string": "W., Chen, H., Chen, L.C., et~al.: Dataseg: Taming a universal multidataset multitask "
                },
                {
                    "pdf_id": "17.34",
                    "matching_string": "segmentation model. Advances in Neural Information Processing Systems "
                },
                {
                    "pdf_id": "17.35",
                    "matching_string": "(2024)"
                }
            ]
        },
        {
            "leaf id": 123,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "He, K., Gkioxari, G., Doll{\\'a}r, P., Girshick, R.: Mask rcnn. In: Proceedings of the IEEE international conference on computer vision. pp. 29612969 (2017)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.36",
                    "matching_string": "He, K., Gkioxari, G., Doll{\\'a}r, P., Girshick, R.: Mask rcnn. In: Proceedings of the "
                },
                {
                    "pdf_id": "17.37",
                    "matching_string": "IEEE international conference on computer vision. pp. 29612969 (2017)"
                }
            ]
        },
        {
            "leaf id": 124,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778 (2016)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.38",
                    "matching_string": "He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: "
                },
                {
                    "pdf_id": "17.39",
                    "matching_string": "Proceedings of the IEEE conference on computer vision and pattern recognition. "
                },
                {
                    "pdf_id": "17.40",
                    "matching_string": "pp. 770778 (2016)"
                }
            ]
        },
        {
            "leaf id": 125,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Hou, R., Li, J., Bhargava, A., Raventos, A., Guizilini, V., Fang, C., Lynch, J., Gaidon, A.: Realtime panoptic segmentation from dense detections. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 85238532 (2020)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.41",
                    "matching_string": "Hou, R., Li, J., Bhargava, A., Raventos, A., Guizilini, V., Fang, C., Lynch, J., "
                },
                {
                    "pdf_id": "17.42",
                    "matching_string": "Gaidon, A.: Realtime panoptic segmentation from dense detections. In: Proceedings "
                },
                {
                    "pdf_id": "17.43",
                    "matching_string": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. "
                },
                {
                    "pdf_id": "17.44",
                    "matching_string": "pp. 85238532 (2020)"
                }
            ]
        },
        {
            "leaf id": 126,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Hu, J., Huang, L., Ren, T., Zhang, S., Ji, R., Cao, L.: You only segment once: Towards realtime panoptic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1781917829 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "17.45",
                    "matching_string": "Hu, J., Huang, L., Ren, T., Zhang, S., Ji, R., Cao, L.: You only segment once: "
                },
                {
                    "pdf_id": "17.46",
                    "matching_string": "Towards realtime panoptic segmentation. In: Proceedings of the IEEE/CVF Conference "
                },
                {
                    "pdf_id": "18.1",
                    "matching_string": "on Computer Vision and Pattern Recognition. pp. 1781917829 (2023)"
                },
                {
                    "pdf_id": "17.48",
                    "matching_string": ""
                },
                {
                    "pdf_id": "18.2",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 127,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer to rule universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 29892998 (2023)",
            "leftover": "Conference ",
            "matches": [
                {
                    "pdf_id": "17.49",
                    "matching_string": "Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer "
                },
                {
                    "pdf_id": "17.50",
                    "matching_string": "to rule universal image segmentation. In: Proceedings of the IEEE/CVF "
                },
                {
                    "pdf_id": "17.47",
                    "matching_string": "on Computer Vision and Pattern Recognition. pp. 29892998 (2023)"
                }
            ]
        },
        {
            "leaf id": 128,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jiang, Z., Gong, Z., Xu, Y., Wang, J.: Multiexit vision transformer with custom finetuning for finegrained image recognition. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 52335237 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.3",
                    "matching_string": "Jiang, Z., Gong, Z., Xu, Y., Wang, J.: Multiexit vision transformer with custom "
                },
                {
                    "pdf_id": "18.4",
                    "matching_string": "finetuning for finegrained image recognition. In: 2023 IEEE International Conference "
                },
                {
                    "pdf_id": "18.5",
                    "matching_string": "on Image Processing (ICIP). pp. 52335237 (2023)"
                }
            ]
        },
        {
            "leaf id": 129,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.6",
                    "matching_string": "Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)"
                }
            ]
        },
        {
            "leaf id": 130,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Kirillov, A., He, K., Girshick, R., Rother, C., Dollár, P.: Panoptic segmentation (2019)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.7",
                    "matching_string": "Kirillov, A., He, K., Girshick, R., Rother, C., Dollár, P.: Panoptic segmentation "
                },
                {
                    "pdf_id": "18.8",
                    "matching_string": "(2019)"
                }
            ]
        },
        {
            "leaf id": 131,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr: An interleaved multiscale encoder for efficient detr. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1855818567 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.9",
                    "matching_string": "Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: Lite detr: An "
                },
                {
                    "pdf_id": "18.10",
                    "matching_string": "interleaved multiscale encoder for efficient detr. In: Proceedings of the IEEE/CVF "
                },
                {
                    "pdf_id": "18.11",
                    "matching_string": "Conference on Computer Vision and Pattern Recognition. pp. 1855818567 (2023)"
                },
                {
                    "pdf_id": "18.12",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 132,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll{\\'a}r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 612, 2014, Proceedings, Part V 13. pp. 740755. Springer (2014)",
            "leftover": "VisionECCV ",
            "matches": [
                {
                    "pdf_id": "18.13",
                    "matching_string": "Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll{\\'a}r, P., "
                },
                {
                    "pdf_id": "18.14",
                    "matching_string": "Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer "
                },
                {
                    "pdf_id": "18.15",
                    "matching_string": "2014: 13th European Conference, Zurich, Switzerland, September 612, "
                },
                {
                    "pdf_id": "18.16",
                    "matching_string": "2014, Proceedings, Part V 13. pp. 740755. Springer (2014)"
                }
            ]
        },
        {
            "leaf id": 133,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1001210022 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.17",
                    "matching_string": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin "
                },
                {
                    "pdf_id": "18.18",
                    "matching_string": "transformer: Hierarchical vision transformer using shifted windows. In: Proceedings "
                },
                {
                    "pdf_id": "18.19",
                    "matching_string": "of the IEEE/CVF international conference on computer vision. pp. "
                },
                {
                    "pdf_id": "18.20",
                    "matching_string": "1001210022 (2021)"
                }
            ]
        },
        {
            "leaf id": 134,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Liu, Z., Sun, Y., Li, Y., Zhou, Z., Hu, J., Li, F.: Multiexit vision transformer for dynamic inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 52145223 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.21",
                    "matching_string": "Liu, Z., Sun, Y., Li, Y., Zhou, Z., Hu, J., Li, F.: Multiexit vision transformer for "
                },
                {
                    "pdf_id": "18.22",
                    "matching_string": "dynamic inference. In: Proceedings of the IEEE/CVF Conference on Computer "
                },
                {
                    "pdf_id": "18.23",
                    "matching_string": "Vision and Pattern Recognition (CVPR). pp. 52145223 (2021)"
                }
            ]
        },
        {
            "leaf id": 135,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: Detrs beat yolos on realtime object detection. arXiv preprint arXiv:2304.08069 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.24",
                    "matching_string": "Lv, W., Xu, S., Zhao, Y., Wang, G., Wei, J., Cui, C., Du, Y., Dang, Q., Liu, Y.: "
                },
                {
                    "pdf_id": "18.25",
                    "matching_string": "Detrs beat yolos on realtime object detection. arXiv preprint arXiv:2304.08069 "
                },
                {
                    "pdf_id": "18.26",
                    "matching_string": "(2023)"
                }
            ]
        },
        {
            "leaf id": 136,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, highperformance deep learning library. In: Advances in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc. (2019)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.27",
                    "matching_string": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., "
                },
                {
                    "pdf_id": "18.28",
                    "matching_string": "Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., "
                },
                {
                    "pdf_id": "18.29",
                    "matching_string": "Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: "
                },
                {
                    "pdf_id": "18.30",
                    "matching_string": "Pytorch: An imperative style, highperformance deep learning library. In: Advances "
                },
                {
                    "pdf_id": "18.31",
                    "matching_string": "in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, "
                },
                {
                    "pdf_id": "18.32",
                    "matching_string": "Inc. (2019)"
                }
            ]
        },
        {
            "leaf id": 137,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Sun, S., Wang, W., Yu, Q., Howard, A., Torr, P., Chen, L.C.: Remax: Relaxing for better training on efficient panoptic segmentation. arXiv preprint arXiv:2306.17319 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.33",
                    "matching_string": "Sun, S., Wang, W., Yu, Q., Howard, A., Torr, P., Chen, L.C.: Remax: Relaxing for "
                },
                {
                    "pdf_id": "18.34",
                    "matching_string": "better training on efficient panoptic segmentation. arXiv preprint arXiv:2306.17319 "
                },
                {
                    "pdf_id": "18.35",
                    "matching_string": "(2023)"
                }
            ]
        },
        {
            "leaf id": 138,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Tang, J., Liu, Z., Li, Y., Sun, Y., Zhou, Z., Hu, J., Li, F.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1350413513 (2023)",
            "leftover": "Dynamic early exiting for accelerating unified vision language model. In: Proceedings ",
            "matches": [
                {
                    "pdf_id": "18.36",
                    "matching_string": "Tang, J., Liu, Z., Li, Y., Sun, Y., Zhou, Z., Hu, J., Li, F.: You need multiple exiting: "
                },
                {
                    "pdf_id": "18.38",
                    "matching_string": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition "
                },
                {
                    "pdf_id": "18.39",
                    "matching_string": "(CVPR). pp. 1350413513 (2023)"
                }
            ]
        },
        {
            "leaf id": 139,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Tang, S., Wang, Y., Kong, Z., Zhang, T., Li, Y., Ding, C., Wang, Y., Liang, Y., Xu, D.: You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1078110791 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.37",
                    "matching_string": "Dynamic early exiting for accelerating unified vision language model. In: Proceedings "
                },
                {
                    "pdf_id": "18.40",
                    "matching_string": "Tang, S., Wang, Y., Kong, Z., Zhang, T., Li, Y., Ding, C., Wang, Y., Liang, Y., "
                },
                {
                    "pdf_id": "18.43",
                    "matching_string": "Vision and Pattern Recognition. pp. 1078110791 (2023)"
                },
                {
                    "pdf_id": "18.41",
                    "matching_string": "Xu, D.: You need multiple exiting: of the IEEE/CVF Conference on Computer "
                },
                {
                    "pdf_id": "18.42",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 140,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Tu, Z.: Autocontext and its application to highlevel vision tasks. In: 2008 IEEE Conference on Computer Vision and Pattern Recognition. pp.~18. IEEE (2008)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.44",
                    "matching_string": "Tu, Z.: Autocontext and its application to highlevel vision tasks. In: 2008 IEEE "
                },
                {
                    "pdf_id": "18.45",
                    "matching_string": "Conference on Computer Vision and Pattern Recognition. pp.~18. IEEE (2008)"
                }
            ]
        },
        {
            "leaf id": 141,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Valade, F., Hebiri, M., Gay, P.: Eero: Early exit with reject option for efficient classification with limited budget (2024)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "18.46",
                    "matching_string": "Valade, F., Hebiri, M., Gay, P.: Eero: Early exit with reject option for efficient "
                },
                {
                    "pdf_id": "18.47",
                    "matching_string": "classification with limited budget (2024)"
                }
            ]
        },
        {
            "leaf id": 142,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, {\\L}., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems \\textbf{30} (2017)",
            "leftover": "{\\L}., ",
            "matches": [
                {
                    "pdf_id": "18.48",
                    "matching_string": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, "
                },
                {
                    "pdf_id": "18.49",
                    "matching_string": "Polosukhin, I.: Attention is all you need. Advances in neural information processing "
                },
                {
                    "pdf_id": "18.50",
                    "matching_string": "systems \\textbf{30} (2017)"
                }
            ]
        },
        {
            "leaf id": 143,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y., Zhang, Q., Chowdhury, M., et~al.: Efficient large language models: A survey. arXiv preprint arXiv:2312.03863 \\textbf{1} (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.1",
                    "matching_string": "Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y., Zhang, "
                },
                {
                    "pdf_id": "19.2",
                    "matching_string": "Q., Chowdhury, M., et~al.: Efficient large language models: A survey. arXiv preprint "
                },
                {
                    "pdf_id": "19.3",
                    "matching_string": "arXiv:2312.03863 \\textbf{1} (2023)"
                }
            ]
        },
        {
            "leaf id": 144,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Wang, X., Zhou, W., He, X., Peng, X., Wei, F., Guo, Y.: Singlelayer vision transformers for more accurate early exits with less overhead. Pattern Recognition \\textbf{136}, 102243 (2022)",
            "leftover": "\\textbf{136}, ",
            "matches": [
                {
                    "pdf_id": "19.4",
                    "matching_string": "Wang, X., Zhou, W., He, X., Peng, X., Wei, F., Guo, Y.: Singlelayer vision transformers "
                },
                {
                    "pdf_id": "19.5",
                    "matching_string": "for more accurate early exits with less overhead. Pattern Recognition "
                },
                {
                    "pdf_id": "19.6",
                    "matching_string": "102243 (2022)"
                }
            ]
        },
        {
            "leaf id": 145,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://github.com/facebookresearch/detectron2 (2019)",
            "leftover": "ithub.",
            "matches": [
                {
                    "pdf_id": "19.7",
                    "matching_string": "Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://g"
                },
                {
                    "pdf_id": "19.8",
                    "matching_string": "com/facebookresearch/detectron2 (2019)"
                }
            ]
        },
        {
            "leaf id": 146,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.9",
                    "matching_string": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised "
                },
                {
                    "pdf_id": "19.10",
                    "matching_string": "learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)"
                }
            ]
        },
        {
            "leaf id": 147,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.11",
                    "matching_string": "Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Selfsupervised "
                },
                {
                    "pdf_id": "19.12",
                    "matching_string": "learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)"
                }
            ]
        },
        {
            "leaf id": 148,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Xu, F., Zhang, X., Ma, Z., Wang, J., Hu, J., Sun, J.: Lgvit: Dynamic early exiting for accelerating vision transformer. In: Proceedings of the 32nd ACM International Conference on Multimedia. pp. 19581966 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.13",
                    "matching_string": "Xu, F., Zhang, X., Ma, Z., Wang, J., Hu, J., Sun, J.: Lgvit: Dynamic early exiting "
                },
                {
                    "pdf_id": "19.14",
                    "matching_string": "for accelerating vision transformer. In: Proceedings of the 32nd ACM International "
                },
                {
                    "pdf_id": "19.15",
                    "matching_string": "Conference on Multimedia. pp. 19581966 (2023)"
                }
            ]
        },
        {
            "leaf id": 149,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Xu, J., Xiong, Z., Bhattacharyya, S.P.: Pidnet: A realtime semantic segmentation network inspired by pid controllers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1952919539 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.16",
                    "matching_string": "Xu, J., Xiong, Z., Bhattacharyya, S.P.: Pidnet: A realtime semantic segmentation "
                },
                {
                    "pdf_id": "19.17",
                    "matching_string": "network inspired by pid controllers. In: Proceedings of the IEEE/CVF Conference "
                },
                {
                    "pdf_id": "19.18",
                    "matching_string": "on Computer Vision and Pattern Recognition. pp. 1952919539 (2023)"
                }
            ]
        },
        {
            "leaf id": 150,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, C., Wang, S., et~al.: A survey of resourceefficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.19",
                    "matching_string": "Xu, M., Yin, W., Cai, D., Yi, R., Xu, D., Wang, Q., Wu, B., Zhao, Y., Yang, "
                },
                {
                    "pdf_id": "19.21",
                    "matching_string": "models. arXiv preprint arXiv:2401.08092 (2024)"
                },
                {
                    "pdf_id": "19.20",
                    "matching_string": "C., Wang, S., et~al.: A survey of resourceefficient llm and multimodal foundation "
                }
            ]
        },
        {
            "leaf id": 151,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Xu, S., Yuan, H., Shi, Q., Qi, L., Wang, J., Yang, Y., Li, Y., Chen, K., Tong, Y., Ghanem, B., et~al.: Rapsam: Towards realtime allpurpose segment anything. arXiv preprint arXiv:2401.10228 (2024)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.22",
                    "matching_string": "Xu, S., Yuan, H., Shi, Q., Qi, L., Wang, J., Yang, Y., Li, Y., Chen, K., Tong, "
                },
                {
                    "pdf_id": "19.23",
                    "matching_string": "Y., Ghanem, B., et~al.: Rapsam: Towards realtime allpurpose segment anything. "
                },
                {
                    "pdf_id": "19.24",
                    "matching_string": "arXiv preprint arXiv:2401.10228 (2024)"
                }
            ]
        },
        {
            "leaf id": 152,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Yang, J., Zhang, X., Zhang, X., Tang, J., Li, X.: Exploiting face recognizability with early exit vision transformers. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 63416345 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.25",
                    "matching_string": "Yang, J., Zhang, X., Zhang, X., Tang, J., Li, X.: Exploiting face recognizability "
                },
                {
                    "pdf_id": "19.26",
                    "matching_string": "with early exit vision transformers. In: 2023 IEEE International Conference on "
                },
                {
                    "pdf_id": "19.27",
                    "matching_string": "Image Processing (ICIP). pp. 63416345 (2023)"
                }
            ]
        },
        {
            "leaf id": 153,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Yu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N.: Bisenet v2: Bilateral network with guided aggregation for realtime semantic segmentation. International Journal of Computer Vision \\textbf{129}, 30513068 (2021)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.28",
                    "matching_string": "Yu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N.: Bisenet v2: Bilateral network "
                },
                {
                    "pdf_id": "19.29",
                    "matching_string": "with guided aggregation for realtime semantic segmentation. International Journal "
                },
                {
                    "pdf_id": "19.30",
                    "matching_string": "of Computer Vision \\textbf{129}, 30513068 (2021)"
                }
            ]
        },
        {
            "leaf id": 154,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmentation network for realtime semantic segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 325341 (2018)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.31",
                    "matching_string": "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmentation "
                },
                {
                    "pdf_id": "19.32",
                    "matching_string": "network for realtime semantic segmentation. In: Proceedings of the European "
                },
                {
                    "pdf_id": "19.33",
                    "matching_string": "conference on computer vision (ECCV). pp. 325341 (2018)"
                }
            ]
        },
        {
            "leaf id": 155,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Zhang, T., He, X., Qin, Z., Sun, J.: Adaptive deep neural network inference optimization with eenet. In: Proceedings of the 37th International Conference on Machine Learning. pp. 1598315993 (2023)",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "19.34",
                    "matching_string": "Zhang, T., He, X., Qin, Z., Sun, J.: Adaptive deep neural network inference optimization "
                },
                {
                    "pdf_id": "19.35",
                    "matching_string": "with eenet. In: Proceedings of the 37th International Conference on "
                },
                {
                    "pdf_id": "19.36",
                    "matching_string": "Machine Learning. pp. 1598315993 (2023)"
                }
            ]
        }
    ]
}