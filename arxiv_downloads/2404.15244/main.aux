\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{cheng2021per,cheng2021mask2former,jain2023oneformer,gu2024dataseg}
\citation{he2017mask}
\citation{tu2008auto}
\citation{kirillov2019panoptic}
\citation{vaswani2017attention}
\citation{carion2020end}
\citation{kirillov2019panoptic}
\citation{sun2023remax,hu2023you,xu2024rap}
\citation{cheng2021mask2former}
\@writefile{toc}{\contentsline {title}{Efficient Transformer Encoders for Mask2Former-style models}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {6}}
\@writefile{toc}{\contentsline {author}{Manyi Yao \and Abhishek Aich \and Yumin Suh \and Amit Roy-Chowdhury \and \unskip \ \ignorespaces Christian Shelton\and Manmohan Chandraker }{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{cheng2021per, cheng2021mask2former, jain2023oneformer, gu2024dataseg}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{he2017mask}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{tu2008auto}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{vaswani2017attention}{{1}{1}{figure.caption.2}}}
\citation{cheng2021mask2former}
\citation{he2016deep}
\citation{liu2021swin}
\citation{li2023lite}
\citation{li2023lite,lv2023detrs}
\citation{lin2014microsoft}
\citation{cordts2016cityscapes}
\citation{kirillov2019panoptic}
\citation{kirillov2019panoptic}
\citation{tang2023you,xu2023lgvit,liu2021mevt,wang2022single,jiang2023multi,yang2023exploiting,valade2024eero,tang2023need,zhang2023adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Comparison to prior works.} Instead of conventional M2F-style architecture that provides ``one-size-fits-all'' solution, our method \texttt  {ECO-M2F}\xspace  focuses on training such models in order to directly run at various resource encoder depths by leveraging a gating function. Here, \textbf  {B}, \textbf  {E}, \textbf  {D}, and \textbf  {G} denote the backbone, encoder, decoder, and (our proposed) gating network, respectively.}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser_a}{{1}{2}{\textbf {Comparison to prior works.} Instead of conventional M2F-style architecture that provides ``one-size-fits-all'' solution, our method \ours focuses on training such models in order to directly run at various resource encoder depths by leveraging a gating function. Here, \textbf {B}, \textbf {E}, \textbf {D}, and \textbf {G} denote the backbone, encoder, decoder, and (our proposed) gating network, respectively}{figure.caption.2}{}}
\newlabel{fig:teaser_a@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histogram of images achieving best panoptic segmentation by number of encoder layers.}}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:teaser_b}{{2}{2}{Histogram of images achieving best panoptic segmentation by number of encoder layers}{figure.caption.2}{}}
\newlabel{fig:teaser_b@cref}{{[figure][2][]2}{[1][1][]2}}
\@writefile{brf}{\backcite{carion2020end}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{sun2023remax, hu2023you, xu2024rap}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{he2016deep}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{liu2021swin}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{li2023lite}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{li2023lite, lv2023detrs}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{lin2014microsoft}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{cordts2016cityscapes}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive}{{2}{1}{figure.caption.2}}}
\citation{lin2014microsoft}
\citation{li2023lite}
\citation{vaswani2017attention}
\citation{carion2020end}
\citation{cheng2021per,cheng2021mask2former,jain2023oneformer,gu2024dataseg}
\citation{cheng2020panoptic,fan2021rethinking,hou2020real,hu2023you,10296714,xu2023pidnet,yu2018bisenet,yu2021bisenet}
\@writefile{brf}{\backcite{lin2014microsoft}{{3}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{li2023lite}{{3}{1}{figure.caption.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Works}{3}{section.1.2}\protected@file@percent }
\newlabel{sec:related_works}{{2}{3}{Related Works}{section.1.2}{}}
\newlabel{sec:related_works@cref}{{[section][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {paragraph}{Efficient image segmentation.}{3}{section*.3}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2017attention}{{3}{2}{section*.3}}}
\@writefile{brf}{\backcite{carion2020end}{{3}{2}{section*.3}}}
\@writefile{brf}{\backcite{cheng2021per,cheng2021mask2former, jain2023oneformer, gu2024dataseg}{{3}{2}{section*.3}}}
\citation{wan2023efficient,xu2024survey,tang2023you,xu2023lgvit,liu2021mevt,wang2022single,jiang2023multi,yang2023exploiting,valade2024eero,tang2023need,zhang2023adaptive}
\citation{xu2023lgvit,liu2021mevt,tang2023you}
\citation{xu2023lgvit}
\citation{tang2023you}
\citation{xu2023lgvit}
\citation{tang2023you}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\@writefile{brf}{\backcite{cheng2020panoptic, fan2021rethinking, hou2020real, hu2023you, 10296714, xu2023pidnet, yu2018bisenet, yu2021bisenet}{{4}{2}{section*.3}}}
\@writefile{toc}{\contentsline {paragraph}{Early-exiting in vision transformers.}{4}{section*.4}\protected@file@percent }
\@writefile{brf}{\backcite{wan2023efficient, xu2024survey, tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive}{{4}{2}{section*.4}}}
\@writefile{brf}{\backcite{xu2023lgvit, liu2021mevt, tang2023you}{{4}{2}{section*.4}}}
\@writefile{brf}{\backcite{xu2023lgvit}{{4}{2}{section*.4}}}
\@writefile{brf}{\backcite{tang2023you}{{4}{2}{section*.4}}}
\@writefile{brf}{\backcite{xu2023lgvit}{{4}{2}{section*.4}}}
\@writefile{brf}{\backcite{tang2023you}{{4}{2}{section*.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proposed Methodology: \texttt  {ECO-M2F}\xspace  }{4}{section.1.3}\protected@file@percent }
\newlabel{sec:method}{{3}{4}{Proposed Methodology: \ours }{section.1.3}{}}
\newlabel{sec:method@cref}{{[section][3][]3}{[1][4][]4}}
\@writefile{toc}{\contentsline {paragraph}{Model preliminaries.}{4}{section*.5}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{4}{3}{section*.5}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{4}{3}{section*.5}}}
\citation{cheng2021mask2former}
\citation{kirillov2019panoptic}
\citation{tang2023you,xu2023lgvit,liu2021mevt,wang2022single,jiang2023multi,yang2023exploiting,valade2024eero,tang2023need,zhang2023adaptive}
\@writefile{brf}{\backcite{cheng2021mask2former}{{5}{3}{equation.1.3.1}}}
\newlabel{eq:orig_loss}{{2}{5}{Model preliminaries}{equation.1.3.2}{}}
\newlabel{eq:orig_loss@cref}{{[equation][2][]2}{[1][5][]5}}
\@writefile{toc}{\contentsline {paragraph}{Method motivation.}{5}{section*.6}\protected@file@percent }
\@writefile{brf}{\backcite{kirillov2019panoptic}{{5}{3}{section*.6}}}
\@writefile{brf}{\backcite{tang2023you, xu2023lgvit, liu2021mevt, wang2022single, jiang2023multi, yang2023exploiting, valade2024eero, tang2023need, zhang2023adaptive}{{5}{1}{Item.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {{\texttt  {ECO-M2F}\xspace  } framework.} During the \textit  {model pre-processing} phase, we train the model to exit stochastically at $K$ potential exits using Step {\color  {cobalt}\textit  {\textbf  {A}}}\xspace  . This is followed by Step {\color  {darkpastelgreen}\textit  {\textbf  {B}}}\xspace  , where we use this model to perform inference on the training images at each exit to create a dataset $\mathcal  {D}$. In the \textit  {model adaptation} phase, we perform Step {\color  {cerise}\textit  {\textbf  {C}}}\xspace  to establish a gating target based on the computational budget and train a lightweight gating network. During \textit  {inference}, the network adheres to the gating network's output and exits at its designated output layer.}}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:main_framework}{{3}{6}{\textbf {{\ours } framework.} During the \textit {model pre-processing} phase, we train the model to exit stochastically at $\NGlayer $ potential exits using Step \stepA . This is followed by Step \stepB , where we use this model to perform inference on the training images at each exit to create a dataset $\train $. In the \textit {model adaptation} phase, we perform Step \stepC to establish a gating target based on the computational budget and train a lightweight gating network. During \textit {inference}, the network adheres to the gating network's output and exits at its designated output layer}{figure.caption.7}{}}
\newlabel{fig:main_framework@cref}{{[figure][3][]3}{[1][5][]6}}
\@writefile{toc}{\contentsline {paragraph}{Training and Inference overview.}{6}{section*.8}\protected@file@percent }
\citation{kirillov2019panoptic}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Step {\color  {cobalt}\textit  {\textbf  {A}}}\xspace  : Training the Model with Weighted Stochastic Depth}{7}{subsection.1.3.1}\protected@file@percent }
\newlabel{sec:SDwithP}{{3.1}{7}{Step \stepA : Training the Model with Weighted Stochastic Depth}{subsection.1.3.1}{}}
\newlabel{sec:SDwithP@cref}{{[subsection][1][3]3.1}{[1][6][]7}}
\newlabel{eq:loss}{{3}{7}{Step \stepA : Training the Model with Weighted Stochastic Depth}{equation.1.3.3}{}}
\newlabel{eq:loss@cref}{{[equation][3][]3}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Step {\color  {darkpastelgreen}\textit  {\textbf  {B}}}\xspace  : Deriving the Gating Network Training Dataset}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Intuition for} Eq.~\ref {eq:utility_func}. This figure shows that prioritizing PQ would need more encoder layers, and conversely, prioritizing lesser layers would result in poorer PQ. (Backbone: SWIN-T; training set).}}{7}{figure.caption.9}\protected@file@percent }
\newlabel{fig:PQ_vs_layers}{{4}{7}{\textbf {Intuition for} \Eqref {eq:utility_func}. This figure shows that prioritizing PQ would need more encoder layers, and conversely, prioritizing lesser layers would result in poorer PQ. (Backbone: SWIN-T; training set)}{figure.caption.9}{}}
\newlabel{fig:PQ_vs_layers@cref}{{[figure][4][]4}{[1][7][]7}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{7}{3.2}{subsection.1.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Step {\color  {cerise}\textit  {\textbf  {C}}}\xspace  : Training for Gating Network}{7}{subsection.1.3.3}\protected@file@percent }
\newlabel{sec:gating}{{3.3}{7}{Step \stepC : Training for Gating Network}{subsection.1.3.3}{}}
\newlabel{sec:gating@cref}{{[subsection][3][3]3.3}{[1][7][]7}}
\newlabel{eq:utility_func}{{4}{8}{Step \stepC : Training for Gating Network}{equation.1.3.4}{}}
\newlabel{eq:utility_func@cref}{{[equation][4][]4}{[1][8][]8}}
\newlabel{eq:target}{{5}{8}{Step \stepC : Training for Gating Network}{equation.1.3.5}{}}
\newlabel{eq:target@cref}{{[equation][5][]5}{[1][8][]8}}
\newlabel{eq:gating}{{6}{8}{Step \stepC : Training for Gating Network}{equation.1.3.6}{}}
\newlabel{eq:gating@cref}{{[equation][6][]6}{[1][8][]8}}
\@writefile{toc}{\contentsline {paragraph}{Saving training costs through Step {\color  {cerise}\textit  {\textbf  {C}}}\xspace  .}{8}{section*.10}\protected@file@percent }
\citation{cheng2021mask2former}
\citation{lin2014microsoft}
\citation{cordts2016cityscapes}
\citation{cheng2021mask2former}
\citation{kirillov2019panoptic}
\citation{lin2014microsoft}
\citation{everingham2015pascal}
\citation{cheng2021mask2former}
\citation{li2023lite}
\citation{lv2023detrs}
\citation{hu2023you}
\citation{xu2024rap}
\citation{sun2023remax}
\citation{he2016deep}
\citation{liu2021swin}
\citation{deng2009imagenet}
\citation{cheng2021mask2former}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Inference}{9}{subsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{9}{section.1.4}\protected@file@percent }
\newlabel{sec:exp}{{4}{9}{Experiments}{section.1.4}{}}
\newlabel{sec:exp@cref}{{[section][4][]4}{[1][9][]9}}
\@writefile{toc}{\contentsline {paragraph}{Datasets.}{9}{section*.11}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{9}{4}{section*.11}}}
\@writefile{brf}{\backcite{lin2014microsoft}{{9}{4}{section*.11}}}
\@writefile{brf}{\backcite{cordts2016cityscapes}{{9}{4}{section*.11}}}
\@writefile{toc}{\contentsline {paragraph}{Evaluation metrics.}{9}{section*.12}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{9}{4}{section*.12}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{9}{4}{section*.12}}}
\@writefile{brf}{\backcite{lin2014microsoft}{{9}{4}{section*.12}}}
\@writefile{brf}{\backcite{everingham2015pascal}{{9}{4}{section*.12}}}
\@writefile{toc}{\contentsline {paragraph}{Baseline models.}{9}{section*.13}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{9}{4}{section*.13}}}
\@writefile{brf}{\backcite{li2023lite}{{9}{4}{section*.13}}}
\@writefile{brf}{\backcite{lv2023detrs}{{9}{4}{section*.13}}}
\@writefile{brf}{\backcite{hu2023you}{{9}{4}{section*.13}}}
\@writefile{brf}{\backcite{xu2024rap}{{9}{4}{section*.13}}}
\@writefile{brf}{\backcite{sun2023remax}{{9}{4}{section*.13}}}
\citation{cheng2021mask2former}
\citation{wu2019detectron2}
\citation{PyTorch}
\citation{kingma2017adam}
\citation{lv2023detrs}
\citation{li2023lite}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021per}
\citation{hu2023you}
\citation{xu2024rap}
\citation{sun2023remax}
\citation{lv2023detrs}
\citation{li2023lite}
\citation{cheng2021mask2former}
\citation{hu2023you}
\citation{sun2023remax}
\citation{cheng2021mask2former}
\citation{liu2021swin}
\citation{he2016deep}
\citation{cheng2021mask2former}
\citation{li2023lite}
\citation{cheng2021per}
\citation{hu2023you}
\citation{xu2024rap}
\citation{sun2023remax}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {COCO evaluation.} {\hlours  {Our method}}, {\hltask  {Task-specific architectures}}}}{10}{table.caption.16}\protected@file@percent }
\@writefile{brf}{\backcite{lv2023detrs}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{li2023lite}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021per}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{hu2023you}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{xu2024rap}{{10}{1}{table.caption.16}}}
\@writefile{brf}{\backcite{sun2023remax}{{10}{1}{table.caption.16}}}
\newlabel{tab:result_coco}{{1}{10}{\textbf {COCO evaluation.} {\hlours {Our method}}, {\hltask {Task-specific architectures}}}{table.caption.16}{}}
\newlabel{tab:result_coco@cref}{{[table][1][]1}{[1][10][]10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Cityscapes evaluation.} {\hlours  {Our method}}, {\hltask  {Task-specific architectures}}}}{10}{table.caption.16}\protected@file@percent }
\@writefile{brf}{\backcite{lv2023detrs}{{10}{2}{table.caption.16}}}
\@writefile{brf}{\backcite{li2023lite}{{10}{2}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{2}{table.caption.16}}}
\@writefile{brf}{\backcite{hu2023you}{{10}{2}{table.caption.16}}}
\@writefile{brf}{\backcite{sun2023remax}{{10}{2}{table.caption.16}}}
\newlabel{tab:result_cityscapes}{{2}{10}{\textbf {Cityscapes evaluation.} {\hlours {Our method}}, {\hltask {Task-specific architectures}}}{table.caption.16}{}}
\newlabel{tab:result_cityscapes@cref}{{[table][2][]2}{[1][10][]10}}
\@writefile{toc}{\contentsline {paragraph}{Architecture details.}{10}{section*.14}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{10}{4}{section*.14}}}
\@writefile{brf}{\backcite{liu2021swin}{{10}{4}{section*.14}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{10}{4}{section*.14}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{4}{section*.14}}}
\@writefile{toc}{\contentsline {paragraph}{Training settings.}{10}{section*.15}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{4}{section*.15}}}
\@writefile{brf}{\backcite{wu2019detectron2}{{10}{4}{section*.15}}}
\@writefile{brf}{\backcite{PyTorch}{{10}{4}{section*.15}}}
\@writefile{brf}{\backcite{kingma2017adam}{{10}{4}{section*.15}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Main Results}{10}{subsection.1.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{10}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{liu2021swin}{{10}{4.1}{table.caption.16}}}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Impact of $\beta $.} As the value of $\beta $ increases, the model places greater emphasis on reducing GFLOPs over performance both in COCO and Cityscapes datasets. Baseline is M2F \cite  {cheng2021mask2former}. (Backbone: SWIN-T)}}{11}{table.caption.18}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{3}{table.caption.18}}}
\newlabel{tab:result_beta}{{3}{11}{\textbf {Impact of $\GPRatio $.} As the value of $\GPRatio $ increases, the model places greater emphasis on reducing GFLOPs over performance both in COCO and Cityscapes datasets. Baseline is M2F \cite {cheng2021mask2former}. (Backbone: SWIN-T)}{table.caption.18}{}}
\newlabel{tab:result_beta@cref}{{[table][3][]3}{[1][11][]11}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Impact of $K$}. The baseline M2F \cite  {cheng2021mask2former} refers to the complete M2F model trained with 6/5/4 layers for 50 epochs. {\texttt  {ECO-M2F}\xspace  } pertains to the training of only the gating network for 2 epochs by setting $K=6,5,4$ respectively. (Dataset: COCO; Backbone: SWIN-T)}}{11}{table.caption.18}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{4}{table.caption.18}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{4}{table.caption.18}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{4}{table.caption.18}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{4}{table.caption.18}}}
\newlabel{tab:result_smaller_models}{{4}{11}{\textbf {Impact of $\NGlayer $}. The baseline M2F \cite {cheng2021mask2former} refers to the complete M2F model trained with 6/5/4 layers for 50 epochs. {\ours } pertains to the training of only the gating network for 2 epochs by setting $\NGlayer =6,5,4$ respectively. (Dataset: COCO; Backbone: SWIN-T)}{table.caption.18}{}}
\newlabel{tab:result_smaller_models@cref}{{[table][4][]4}{[1][11][]11}}
\@writefile{brf}{\backcite{he2016deep}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{li2023lite}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{cheng2021per}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{hu2023you}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{xu2024rap}{{11}{4.1}{table.caption.16}}}
\@writefile{brf}{\backcite{sun2023remax}{{11}{4.1}{table.caption.16}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ablation Studies}{11}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balancing computational cost and performance.}{11}{section*.17}\protected@file@percent }
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{liu2021swin}
\citation{deng2009imagenet}
\citation{liu2021swin}
\citation{xie2021self}
\citation{he2016deep}
\citation{caron2021emerging}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Stochastic Depth (SD) training.} Here, all models (w/ 6 encoder layers) deterministically exit at the \hllayer  {marked layer} at inference. ``USD'': Unweighted; ``WSD'': Weighted. (Baseline: M2F w/ SWIN-T)}}{12}{table.caption.19}\protected@file@percent }
\newlabel{tab:loss}{{5}{12}{\textbf {Stochastic Depth (SD) training.} Here, all models (w/ 6 encoder layers) deterministically exit at the \hllayer {marked layer} at inference. ``USD'': Unweighted; ``WSD'': Weighted. (Baseline: M2F w/ SWIN-T)}{table.caption.19}{}}
\newlabel{tab:loss@cref}{{[table][5][]5}{[1][12][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textcolor {map-color}{M2F$_i$} is trained w/ total $i$ layers. \textcolor {r1-color}{$\ell _i$} is result of same model from our Step {\color  {cobalt}\textit  {\textbf  {A}}}\xspace  . \textcolor {ao(english)}{Values} denote $\beta $. Dataset: COCO. }}{12}{table.caption.19}\protected@file@percent }
\newlabel{fig:perato}{{5}{12}{\textcolor {map-color}{M2F$_i$} is trained w/ total $i$ layers. \textcolor {r1-color}{$\ell _i$} is result of same model from our Step \stepA . \textcolor {ao(english)}{Values} denote $\beta $. Dataset: COCO}{table.caption.19}{}}
\newlabel{fig:perato@cref}{{[figure][5][]5}{[1][12][]12}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \textbf  {Impact of backbone size.} {\texttt  {ECO-M2F}\xspace  } shows strong performance w.r.t\onedot  baselines across all backbone sizes on both COCO and Cityscapes datasets while reducing GFLOPs in the transformer encoder. $\dagger $ImageNet-21K pre-trained}}{12}{table.caption.19}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{12}{6}{table.caption.19}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{12}{6}{table.caption.19}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{12}{6}{table.caption.19}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{12}{6}{table.caption.19}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{12}{6}{table.caption.19}}}
\newlabel{tab:result_backbones}{{6}{12}{\textbf {Impact of backbone size.} {\ours } shows strong performance \wrt baselines across all backbone sizes on both COCO and Cityscapes datasets while reducing GFLOPs in the transformer encoder. $\dagger $ImageNet-21K pre-trained}{table.caption.19}{}}
\newlabel{tab:result_backbones@cref}{{[table][6][]6}{[1][12][]12}}
\@writefile{toc}{\contentsline {paragraph}{Impact of backbone features.}{12}{section*.20}\protected@file@percent }
\@writefile{brf}{\backcite{liu2021swin}{{12}{4.2}{section*.20}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{12}{4.2}{section*.20}}}
\@writefile{brf}{\backcite{liu2021swin}{{12}{4.2}{section*.20}}}
\@writefile{brf}{\backcite{xie2021self}{{12}{4.2}{section*.20}}}
\@writefile{brf}{\backcite{he2016deep}{{12}{4.2}{section*.20}}}
\@writefile{brf}{\backcite{caron2021emerging}{{12}{4.2}{section*.20}}}
\citation{carion2020end}
\citation{carion2020end}
\citation{carion2020end}
\citation{xie2021moby}
\citation{cheng2021mask2former}
\citation{caron2021emerging}
\citation{cheng2021mask2former}
\citation{carion2020end}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\citation{liu2021swin}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \textbf  {Impact on DETR.} We extend our approach to DETR \cite  {carion2020end} for object detection tasks as \texttt  {ECO-DETR}\xspace  . (Dataset: COCO; Backbone: Res50)}}{13}{table.caption.21}\protected@file@percent }
\@writefile{brf}{\backcite{carion2020end}{{13}{7}{table.caption.21}}}
\@writefile{brf}{\backcite{carion2020end}{{13}{7}{table.caption.21}}}
\newlabel{tab:result_detr}{{7}{13}{\textbf {Impact on DETR.} We extend our approach to DETR \cite {carion2020end} for object detection tasks as \oursdetr . (Dataset: COCO; Backbone: Res50)}{table.caption.21}{}}
\newlabel{tab:result_detr@cref}{{[table][7][]7}{[1][13][]13}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces \textbf  {Impact of backbone weights.} {\texttt  {ECO-M2F}\xspace  } applies to backbone weights obtained through self-supervised pre-training as well. (Dataset: COCO)}}{13}{table.caption.21}\protected@file@percent }
\@writefile{brf}{\backcite{xie2021moby}{{13}{8}{table.caption.21}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{13}{8}{table.caption.21}}}
\@writefile{brf}{\backcite{caron2021emerging}{{13}{8}{table.caption.21}}}
\@writefile{brf}{\backcite{cheng2021mask2former}{{13}{8}{table.caption.21}}}
\newlabel{tab:result_weights}{{8}{13}{\textbf {Impact of backbone weights.} {\ours } applies to backbone weights obtained through self-supervised pre-training as well. (Dataset: COCO)}{table.caption.21}{}}
\newlabel{tab:result_weights@cref}{{[table][8][]8}{[1][13][]13}}
\@writefile{toc}{\contentsline {paragraph}{Performance in object detection.}{13}{section*.22}\protected@file@percent }
\@writefile{brf}{\backcite{carion2020end}{{13}{4.2}{section*.22}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Qualitative Comparisons}{13}{subsection.1.4.3}\protected@file@percent }
\@writefile{brf}{\backcite{liu2021swin}{{13}{4.3}{figure.caption.23}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{13}{section.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Qualitative visualizations.} We illustrate few examples of predicted segmentation maps from M2F \cite  {cheng2021mask2former} (\textit  {middle} column) and \texttt  {ECO-M2F}\xspace  (\textit  {last} column). \textit  {Top} two rows are from the COCO dataset, whereas \textit  {bottom} two rows are from the Cityscapes dataset. (Backbone: Swin-T) \textit  {Zoom-in for best view}.}}{14}{figure.caption.23}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{14}{6}{figure.caption.23}}}
\newlabel{fig:qual_result}{{6}{14}{\textbf {Qualitative visualizations.} We illustrate few examples of predicted segmentation maps from M2F \cite {cheng2021mask2former} (\textit {middle} column) and \ours (\textit {last} column). \textit {Top} two rows are from the COCO dataset, whereas \textit {bottom} two rows are from the Cityscapes dataset. (Backbone: Swin-T) \textit {Zoom-in for best view}}{figure.caption.23}{}}
\newlabel{fig:qual_result@cref}{{[figure][6][]6}{[1][13][]14}}
\@writefile{toc}{\contentsline {paragraph}{Limitations.}{14}{section*.24}\protected@file@percent }
\citation{liu2021swin}
\citation{li2023lite}
\citation{li2023lite}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Experiments}{15}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact of backbone size on Lite-M2F.}{15}{section*.25}\protected@file@percent }
\@writefile{brf}{\backcite{liu2021swin}{{15}{A}{section*.25}}}
\@writefile{brf}{\backcite{li2023lite}{{15}{A}{section*.25}}}
\@writefile{brf}{\backcite{li2023lite}{{15}{A}{section*.25}}}
\@writefile{toc}{\contentsline {paragraph}{Impact of target and loss settings for gating network training.}{15}{section*.26}\protected@file@percent }
\citation{liu2021swin}
\citation{li2023lite}
\citation{li2023lite}
\citation{li2023lite}
\citation{li2023lite}
\citation{li2023lite}
\citation{cheng2021mask2former}
\citation{cheng2021mask2former}
\@writefile{brf}{\backcite{liu2021swin}{{16}{A}{section*.26}}}
\@writefile{lot}{\contentsline {table}{\numberline {T1}{\ignorespaces \textbf  {Impact of backbone size on Lite-M2F.} Our Lite-{\texttt  {ECO-M2F}\xspace  } maintains the performance of Lite-M2F while reducing GFLOPs for different datasets and for different backbones.}}{16}{table.caption.27}\protected@file@percent }
\@writefile{brf}{\backcite{li2023lite}{{16}{T1}{table.caption.27}}}
\@writefile{brf}{\backcite{li2023lite}{{16}{T1}{table.caption.27}}}
\@writefile{brf}{\backcite{li2023lite}{{16}{T1}{table.caption.27}}}
\@writefile{brf}{\backcite{li2023lite}{{16}{T1}{table.caption.27}}}
\@writefile{brf}{\backcite{li2023lite}{{16}{T1}{table.caption.27}}}
\newlabel{tab:supp_result_backbones}{{T1}{16}{\textbf {Impact of backbone size on Lite-M2F.} Our Lite-{\ours } maintains the performance of Lite-M2F while reducing GFLOPs for different datasets and for different backbones}{table.caption.27}{}}
\newlabel{tab:supp_result_backbones@cref}{{[table][1][]T1}{[1][16][]16}}
\@writefile{lot}{\contentsline {table}{\numberline {T2}{\ignorespaces \textbf  {Impact of target and loss in gating network training.} We use ``hard-CE'' loss for training our gating network in the main paper. (Backbone: SWIN-T; Dataset: COCO)}}{16}{table.caption.27}\protected@file@percent }
\newlabel{tab:tgt_loss}{{T2}{16}{\textbf {Impact of target and loss in gating network training.} We use ``hard-CE'' loss for training our gating network in the main paper. (Backbone: SWIN-T; Dataset: COCO)}{table.caption.27}{}}
\newlabel{tab:tgt_loss@cref}{{[table][2][]T2}{[1][16][]16}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Qualitative Results}{16}{section.1.2}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{ref}
\bibcite{10296714}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {F1}{\ignorespaces \textbf  {Qualitative visualizations.} We provide additional examples of predicted segmentation maps from M2F \cite  {cheng2021mask2former} (\textit  {middle} column) and \texttt  {ECO-M2F}\xspace  (\textit  {last} column). \textit  {Top} two rows are from the COCO dataset, whereas \textit  {bottom} two rows are from the Cityscapes dataset. Please zoom in for a clearer view of the details. (Backbone: SWIN-T)}}{17}{figure.caption.28}\protected@file@percent }
\@writefile{brf}{\backcite{cheng2021mask2former}{{17}{F1}{figure.caption.28}}}
\newlabel{fig:supp_qual_result}{{F1}{17}{\textbf {Qualitative visualizations.} We provide additional examples of predicted segmentation maps from M2F \cite {cheng2021mask2former} (\textit {middle} column) and \ours (\textit {last} column). \textit {Top} two rows are from the COCO dataset, whereas \textit {bottom} two rows are from the Cityscapes dataset. Please zoom in for a clearer view of the details. (Backbone: SWIN-T)}{figure.caption.28}{}}
\newlabel{fig:supp_qual_result@cref}{{[figure][1][]F1}{[1][16][]17}}
\bibcite{carion2020end}{2}
\bibcite{caron2021emerging}{3}
\bibcite{cheng2020panoptic}{4}
\bibcite{cheng2021mask2former}{5}
\bibcite{cheng2021per}{6}
\bibcite{cordts2016cityscapes}{7}
\bibcite{deng2009imagenet}{8}
\bibcite{everingham2015pascal}{9}
\bibcite{fan2021rethinking}{10}
\bibcite{gu2024dataseg}{11}
\bibcite{he2017mask}{12}
\bibcite{he2016deep}{13}
\bibcite{hou2020real}{14}
\bibcite{hu2023you}{15}
\bibcite{jain2023oneformer}{16}
\bibcite{jiang2023multi}{17}
\bibcite{kingma2017adam}{18}
\bibcite{kirillov2019panoptic}{19}
\bibcite{li2023lite}{20}
\bibcite{lin2014microsoft}{21}
\bibcite{liu2021swin}{22}
\bibcite{liu2021mevt}{23}
\bibcite{lv2023detrs}{24}
\bibcite{PyTorch}{25}
\bibcite{sun2023remax}{26}
\bibcite{tang2023need}{27}
\bibcite{tang2023you}{28}
\bibcite{tu2008auto}{29}
\bibcite{valade2024eero}{30}
\bibcite{vaswani2017attention}{31}
\bibcite{wan2023efficient}{32}
\bibcite{wang2022single}{33}
\bibcite{wu2019detectron2}{34}
\bibcite{xie2021self}{35}
\bibcite{xie2021moby}{36}
\bibcite{xu2023lgvit}{37}
\bibcite{xu2023pidnet}{38}
\bibcite{xu2024survey}{39}
\bibcite{xu2024rap}{40}
\bibcite{yang2023exploiting}{41}
\bibcite{yu2021bisenet}{42}
\bibcite{yu2018bisenet}{43}
\bibcite{zhang2023adaptive}{44}
\csgdef{tcb@ehg@height@REG@1}{123.48839pt}
\csgdef{tcb@ehg@height@REG@2}{149.21922pt}
\gdef \@abspage@last{20}
