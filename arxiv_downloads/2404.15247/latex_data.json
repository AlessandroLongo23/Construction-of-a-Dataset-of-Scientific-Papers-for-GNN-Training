{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "\\ours: Unlocking the Power of Code Instruction Tuning \\\\ by Simply Merging Upcycled Mixture-of-Experts"
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "\\author{Yifeng Ding, Jiawei Liu, Yuxiang Wei, Terry Yue Zhuo, Lingming Zhang \\\\   University of Illinois Urbana-Champaign \\\\   \\texttt{\\{yifeng6, lingming\\}@illinois.edu} \\\\}"
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": " We introduce \\ours, a simple yet powerful training scheme, by simply merging upcycled \\moefull (\\moe) to unleash the performance limit of instruction-tuned code \\llmfull{s} (\\llm{s}).  While vanilla \\sparseupcycle fails to improve instruction tuning, \\ours introduces a shared expert mechanism with a novel routing weight normalization strategy into \\sparseupcycle,  which significantly boosts instruction tuning. After fine-tuning the upcycled \\moe model,  \\ours introduces a learnable model merging mechanism to compile the upcycled \\moe back to a dense model, achieving upcycled \\moe{-level} performance with only dense-model compute. By applying \\ours to a 1.3B model,  we create a new state-of-the-art tiny code \\llm{} (<3B) with 67.1 and 64.6 \\passat{1} on \\humaneval and \\humanevalp{} respectively. With the same data and model architecture, \\ours improves supervised fine-tuning (SFT) by 13\\% on \\humanevalp, along with consistent improvements from 2\\% to 13\\% on \\mbppp, \\multiple, and \\dsonek, demonstrating its generalizability. \\ours is fully orthogonal to existing techniques such as \\evolinstruct{} and \\ossinstruct{}, opening a new dimension for improving code instruction tuning. Codes are available at \\url{https://github.com/ise-uiuc/xft}.  "
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "leaf id": 3,
                    "key": "doc/body/sec0/tit",
                    "block type": "section",
                    "content": "Introduction"
                },
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec0/par0",
                            "block type": "par",
                            "content": "Program synthesis (or code generation) is a long-standing problem explored since the early days of computer science~\\cite{manna1971toward}.  Recently, instruction tuning of code \\llmfull{s} (\\llm{s}) has been used to improve many coding tasks~\\cite{codealpaca, luo2023wizardcoder, wei2023magicoder}, such as text-to-code generation~\\cite{chen2021evaluating, austin2021program}, code completion~\\cite{cassano2022multiple}, and data science engineering~\\cite{lai2022ds1000}."
                        },
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec0/par1",
                            "block type": "par",
                            "content": "A typical instruction tuning flow involves two steps~\\cite{zhang2023instruction}:  (i) curating an instruction dataset of instruction-output pairs, where the instruction reflects human intents in natural language and the output includes explained code snippets that correspond to the intent;  and  (ii) supervised fine-tuning of pre-trained \\llm on the instruction dataset.  In the realm of code, multiple instruction-tuning methods have been proposed to curate high-quality instruction datasets. For example, \\textit{Code \\evolinstruct}~\\cite{luo2023wizardcoder} uses \\chatgpt to obtain complex synthetic code instructions with heuristic prompts, while \\ossinstruct~\\cite{wei2023magicoder} prompts \\chatgpt to generate new coding problems by drawing inspiration from open source code snippets. While existing work focuses on the data perspectives of instruction tuning,  they all follow the standard SFT, leaving room for exploring advanced training schemes."
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec0/par2",
                            "block type": "par",
                            "content": "We argue that prior works largely overlook the possibility of improving the code instruction tuning by advancing the training schemes. Figure~\\ref{fig:comparison} depicts the supervised fine-tuning (SFT), which directly starts with the pre-trained weights and architecture for fine-tuning. The model is dense here because all parameters are activated to compute the next token (assuming it is a decoder-only \\llm{}). In contrast to fine-tuning a dense model,  following the scaling laws~\\cite{kaplan2020scaling} (i.e.,\\xspace more parameters, better performance),"
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec0/par3",
                            "block type": "par",
                            "content": "An \\moe model is efficient because the generation of the next token only involves a subset of parameters (i.e.,\\xspace experts) and thus is sparsely activated. For example, Mixtral-8x7B~\\cite{jiang2024mixtral}, compared to a dense 7B model,  uses approximately 8\u00d7 parameters and 2\u00d7 computation, i.e.,\\xspace only 2 out of 8 experts are dynamically selected to compute the next token. However, there are two key limitations when using \\sparseupcycle in instruction tuning: (i) Slow scaling: \\citet{komatsuzaki2023sparse} show that \\sparseupcycle improves the dense SFT marginally at the early phase, requiring orders of magnitude of extra compute to achieve decent improvement; and (ii) Inference cost: though \\moe is more efficient than directly scaling the size of dense \\llm{s},"
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec0/par4",
                            "block type": "par",
                            "content": "as it introduces significantly more parameters (i.e.,\\xspace memory) and, more importantly, computes during inference, compared to its dense counterparts."
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec0/par5",
                            "block type": "par",
                            "content": "In this paper, we propose \\ours:  by simply merging upcycled \\moe models, we push the performance limit of instruction-tuned code \\llm{s}. While vanilla \\sparseupcycle fails to improve instruction tuning efficiently~\\cite{komatsuzaki2023sparse},"
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec0/par6",
                            "block type": "par",
                            "content": "After the upcycled \\moe model finishes the SFT phase, motivated by \\modelsoup~\\cite{wortsman2022model},"
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec0/par7",
                            "block type": "par",
                            "content": "achieving similar performance without paying extra inference cost as the \\sparseupcycle. With only 1.3B parameters, \\ours achieves 67.1 \\passat{1} on \\humaneval and 64.6 \\passat{1} on \\humanevalp, which is the new state-of-the-art for tiny code \\llm{s} (<3B). Compared with SFT, \\ours achieves 13\\% improvement on \\humanevalp. Surprisingly, our model merging mechanism can preserve or even further boost the general performance of the upcycled \\moe with around 18\u00d7 parameters!  We conclude our contribution as follows:"
                        },
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec0/itemize8",
                            "block type": "itemize",
                            "content": "[leftmargin=1em]     \\setlength{\\parskip}{2pt}     \\setlength\\itemsep{0pt}     \\item Dimension:      We open a new dimension of improving instruction tuning of code \\llm{s} by advancing its training scheme, using enhanced \\sparseupcycle and learnable model merging mechanism, which neither changes the final model structure nor requires more training data.     \\item Technique:      We present \\ours, a new training scheme for code instruction tuning. \\ours involves two steps: upcycling and merging. A pre-trained dense \\llm is first upcycled into an \\moe with the shared expert setting and then fine-tuned on the instruction dataset. To avoid the performance degradation caused by the scale mismatch issue,     we propose a novel routing weight normalization strategy. In addition, we introduce the first learnable mechanism      for merging the upcycled \\moe into a dense model, eliminating additional inference overhead while preserving or even improving the \\moe performance.     \\item Results: With only 1.3B parameters, \\ours achieves 67.1 \\passat{1} on \\humaneval and 64.6 \\passat{1} on \\humanevalp, which is the new state-of-the-art for tiny code \\llm{s} (<3B). Compared with normal supervised fine-tuning (SFT), \\ours achieves 13\\% improvement on \\humanevalp! \\ours also achieves a consistent improvement from 2\\% to 13\\% on \\mbpp, \\multiple, and \\dsonek over SFT, demonstrating its generalization."
                        },
                        {
                            "leaf id": 13,
                            "key": "doc/body/sec0/sec9/tit",
                            "block type": "section",
                            "content": "Related Work"
                        },
                        {
                            "key": "doc/body/sec0/sec9",
                            "block_type": "sec",
                            "children": [
                                {
                                    "leaf id": 14,
                                    "key": "doc/body/sec0/sec9/sub0/tit",
                                    "block type": "subsection",
                                    "content": "\\moefull"
                                },
                                {
                                    "key": "doc/body/sec0/sec9/sub0",
                                    "block_type": "sub",
                                    "children": [
                                        {
                                            "leaf id": 15,
                                            "key": "doc/body/sec0/sec9/sub0/par0",
                                            "block type": "par",
                                            "content": "Compared with the standard Transformer, \\moe replaces each Feed-Forward Network (FFN) layer with an \\moe layer, which uses N (i.e.,\\xspace multiple) expert networks that are structurally equivalent to the original FFN layer and uses a router that directs each input token to K out of N expert networks.  Formally, for the l-th \\moe layer, output hidden state _t^l of the t-th input token is computed as follows~\\cite{dai2024deepseekmoe}:"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec9/sub0/equation1",
                                            "block_type": "equation",
                                            "children": [
                                                {
                                                    "key": "doc/body/sec0/sec9/sub0/equation1/split0",
                                                    "block_type": "split",
                                                    "children": [
                                                        {
                                                            "leaf id": 16,
                                                            "key": "doc/body/sec0/sec9/sub0/equation1/split0/par0",
                                                            "block type": "par",
                                                            "content": "g_{i,t} &="
                                                        },
                                                        {
                                                            "leaf id": 17,
                                                            "key": "doc/body/sec0/sec9/sub0/equation1/split0/cases1",
                                                            "block type": "cases",
                                                            "content": "s_{i,t} & s_{i,t}\\in  \\mbox{Topk}(s_t, K) \\\\     0 & otherwise"
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 18,
                                            "key": "doc/body/sec0/sec9/sub0/par2",
                                            "block type": "par",
                                            "content": "where N refers to the total number of experts, g_i,t refers to the gate value for the i-th expert, _i(\u00b7) refers to the i-th expert, _t^l refers to the hidden states of the t-th token which is the input of the l-th \\moe layer, s_i,t refers to the affinity score between the i-th expert and the t-th token, (S, K) refers to a function computing K largest scores over S, and _i^l refers to the centroid of the i-th expert in the l-th \\moe layer. By definition, each token will only be assigned to and computed in the top K experts among all the N experts."
                                        },
                                        {
                                            "leaf id": 19,
                                            "key": "doc/body/sec0/sec9/sub0/par3",
                                            "block type": "par",
                                            "content": "Recently, many works have been proposed to scale model sizes with \\moe architecture~\\cite{lepikhin2020gshard, du2022glam, fedus2022switch, jiang2024mixtral, xue2024openmoe}. While  most \\moe models are trained from scratch, \\sparseupcycle~\\cite{komatsuzaki2023sparse} is proposed to initialize \\moe models based on the pre-trained dense model, which can efficiently reduce the computational costs of training \\moe models, compared with training \\moe models from scratch.  Specifically, \\sparseupcycle constructs a new \\moe model by initializing each expert of each \\moe layer as a copy of the original FFN layer in the dense model, while directly copying the remaining layers from the dense model to the new \\moe model."
                                        },
                                        {
                                            "leaf id": 20,
                                            "key": "doc/body/sec0/sec9/sub0/sub4/tit",
                                            "block type": "subsection",
                                            "content": "Instruction Tuning"
                                        },
                                        {
                                            "key": "doc/body/sec0/sec9/sub0/sub4",
                                            "block_type": "sub",
                                            "children": [
                                                {
                                                    "leaf id": 21,
                                                    "key": "doc/body/sec0/sec9/sub0/sub4/par0",
                                                    "block type": "par",
                                                    "content": "Instruction tuning is designed to improve the instruction-following ability of \\llm{s} by fine-tuning them on the instruction datasets in a supervised fashion~\\cite{wei2022finetuned}.  The quality of the instruction dataset is significant for the effectiveness of instruction tuning and researchers have proposed multiple methods to improve data quality. For example, \\textsc{\\selfinstruct}~\\cite{wang2023selfinstruct} synthesizes high-quality instruction data by prompting a foundation \\llm with specially designed prompts. To improve \\textsc{\\selfinstruct}, \\evolinstruct~\\cite{xu2023wizardlm} improves the complexity and diversity of the instruction dataset by prompting \\chatgpt with heuristic prompts. \\textsc{\\ossinstruct}~\\cite{wei2023magicoder} queries \\chatgpt to generate instruction-output pairs by getting inspiration from real-world code snippets."
                                                },
                                                {
                                                    "leaf id": 22,
                                                    "key": "doc/body/sec0/sec9/sub0/sub4/par1",
                                                    "block type": "par",
                                                    "content": "Recently, some parameter-efficient fine-tuning techniques have been proposed to use \\moe for better instruction tuning. For example, \\loramoe~\\cite{dou2023loramoe} and \\mocle~\\cite{gou2024mixture} propose \\moe-like modules that are constructed with Low-Rank Adaptations (\\lora) to improve instruction tuning, while \\pesc~\\cite{wu2024parameterefficient} proposes to integrate adapters into \\moe that are upcycled from dense models. Different from these works, \\ours focuses on full fine-tuning, which generally performs better than parameter-efficient fine-tuning~\\cite{chen2022revisiting}."
                                                },
                                                {
                                                    "leaf id": 23,
                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/tit",
                                                    "block type": "subsection",
                                                    "content": "Weight Averaging"
                                                },
                                                {
                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2",
                                                    "block_type": "sub",
                                                    "children": [
                                                        {
                                                            "leaf id": 24,
                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/par0",
                                                            "block type": "par",
                                                            "content": "Weight averaging is a commonly used technique to improve the performance of deep learning models. For example, \\modelsoup~\\cite{wortsman2022model} averages the weights of multiple models that are initialized from the same pre-trained model but finetuned with different hyperparameter configurations to improve the accuracy and robustness of the model. However, only a few works have been proposed to merge expert networks of an \\moe layer to a normal FFN layer using weight averaging. For example, OneS~\\cite{xue2022student} proposes several simple weight averaging methods to merge expert networks of a BERT-based \\moe model. Closely related to our work, \\ewafull (\\ewa)~\\cite{huang2023experts} proposes to convert an \\moe model to a dense model with two steps: (i) During \\moe training, \\ewa conducts weighted averaging of all the expert weights after each weight update of \\moe, which is based on a manually-crafted hyperparameter \u03b2; (ii) After training, \\ewa converts each \\moe layer into an FFN layer by uniformly averaging the experts."
                                                        },
                                                        {
                                                            "leaf id": 25,
                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/par1",
                                                            "block type": "par",
                                                            "content": "Different from all the aforementioned existing works, \\ours is the first work proposing a learnable mechanism to merge expert networks in the upcycled \\moe model. Furthermore, while the training scheme of \\ewa is deeply coupled to a specific \\moe architecture, \\ours can be easily adapted to different \\moe architectures by only adjusting the final merging process.  In addition, unlike \\ewa, \\ours does not introduce any hyperparameters into the training of the large \\moe models,  significantly reducing the computational resources for hyperparameter searching.  Our empirical results in Section \\ref{sec:experiment} also showcase the clear advantage of \\ours."
                                                        },
                                                        {
                                                            "leaf id": 26,
                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/tit",
                                                            "block type": "section",
                                                            "content": "\\ours"
                                                        },
                                                        {
                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2",
                                                            "block_type": "sec",
                                                            "children": [
                                                                {
                                                                    "leaf id": 27,
                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/par0",
                                                                    "block type": "par",
                                                                    "content": "We describe the details of \\ours in this section. There are two steps in our framework: \\textit{upcycling} (Section~\\ref{sec:upcycling}) and \\textit{merging} (Section~\\ref{sec:merging}). During upcycling, we construct an \\moefull (\\moe) model from the pre-trained dense model, namely \\oursmoe, which is then fine-tuned on coding instruction data. For merging, we propose a learnable model merging method to convert the instruction-tuned \\oursmoe back to a normal dense model by merging each \\moe layer into an FFN layer through weight averaging while directly copying other remaining layers. Consequently, we can obtain \\oursmerge that has the same model architecture and size as the original pre-trained dense model, which eliminates all the additional inference overhead brought by the original \\sparseupcycle, while preserving or even improving the performance of \\oursmoe. Our framework is illustrated in Figure \\ref{fig:overview}."
                                                                },
                                                                {
                                                                    "leaf id": 28,
                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/tit",
                                                                    "block type": "subsection",
                                                                    "content": "Upcycling"
                                                                },
                                                                {
                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1",
                                                                    "block_type": "sub",
                                                                    "children": [
                                                                        {
                                                                            "leaf id": 29,
                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/par0",
                                                                            "block type": "par",
                                                                            "content": "Inspired by \\sparseupcycle~\\cite{komatsuzaki2023sparse}, we convert the pre-trained dense \\llm to a new \\moe by initializing each expert of each \\moe layer as a copy of the original FFN layer in the dense model, while directly copying the remaining layers from the dense model to the new \\moe model. However, the performance gain brought by \\sparseupcycle is negligible with a very limited extra training budget~\\cite{komatsuzaki2023sparse} -- which is exactly the situation we are facing during instruction tuning. Intuitively, it is because each expert in the upcycled \\moe model is trained on fewer instruction data than the original dense model does because traditional routers used in \\sparseupcycle will assign different tokens to different experts and thus reduce the amount of data each expert is trained on~\\cite{gou2024mixture}. Consequently, inspired by \\deepseekmoe~\\cite{dai2024deepseekmoe} and \\mocle~\\cite{gou2024mixture}, \\ours introduces the shared expert setting into \\sparseupcycle to tackle this challenge. We further propose a novel routing weight normalization strategy for \\ours to avoid the potential performance degradation caused by the scale mismatch problem~\\cite{wu2022residual}."
                                                                        },
                                                                        {
                                                                            "leaf id": 30,
                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/tit",
                                                                            "block type": "subsubsection",
                                                                            "content": "Shared Expert for Upcycling"
                                                                        },
                                                                        {
                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1",
                                                                            "block_type": "ssb",
                                                                            "children": [
                                                                                {
                                                                                    "leaf id": 31,
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/par0",
                                                                                    "block type": "par",
                                                                                    "content": "During upcycling, we isolate one shared expert among all the other normal experts in each \\moe layer, where the shared expert will be deterministically assigned to handle all the tokens while other normal experts are assigned by the router. By doing so, the upcycled \\moe model can achieve a clear performance boost in instruction tuning, where the shared expert can learn general knowledge across the whole instruction dataset while other normal experts learn specific knowledge among different instructions assigned by the router. Formally, the output hidden state _t^l of the l-th \\moe layer when processing the t-th token can be expressed as:"
                                                                                },
                                                                                {
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/equation1",
                                                                                    "block_type": "equation",
                                                                                    "children": [
                                                                                        {
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/equation1/split0",
                                                                                            "block_type": "split",
                                                                                            "children": [
                                                                                                {
                                                                                                    "leaf id": 32,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/equation1/split0/par0",
                                                                                                    "block type": "par",
                                                                                                    "content": "g_{i,t} &="
                                                                                                },
                                                                                                {
                                                                                                    "leaf id": 33,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/equation1/split0/cases1",
                                                                                                    "block type": "cases",
                                                                                                    "content": "1-{s_{t}}_{\\max} & i=1 \\\\  \\mbox{Softmax}_i(s_{i,t})\\cdot {s_{t}}_{\\max} & s_{i,t}\\in  {S_t}_K \\\\     0 & {otherwise}"
                                                                                                },
                                                                                                {
                                                                                                    "leaf id": 34,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/equation1/split0/par2",
                                                                                                    "block type": "par",
                                                                                                    "content": "{S_t}_K &= \\mbox{Topk}(\\{s_{i,t} \\mid 1\\leq i\\leq N\\}, K-1) \\\\ {s_{t}}_{\\max} &= \\max (\\{s_{i,t} \\mid 1\\leq i\\leq N\\}) \\\\ s_{i,t} &= \\begin{cases}     -\\infty & i = 1 \\\\  \\mbox{Softmax}_i({\\mbox{u}_t^l}^T\\mbox{e}_i^l)& i\\geq 2"
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                },
                                                                                {
                                                                                    "leaf id": 35,
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/par2",
                                                                                    "block type": "par",
                                                                                    "content": "where h_t^l refers to the output hidden state of the l-th \\moe layer when processing the t-th token, N refers to the total number of experts, g_i,t refers to the gate value for the i-th expert, _i(\u00b7) refers to the i-th expert, u_t^l refers to the output hidden state of the l-th attention layer when processing the t-th token and also the input of the l-th \\moe layer, s_i, t refers to the affinity score between the i-th expert and the t-th token, s_t_max refers to the maximum affinity score among all the experts besides the shared expert, (S,K) refers to a function computing K largest scores over S, S_tK refers to a set of K-1 largest affinity scores among all the experts besides the shared expert, and e_i^l refers to the centroid of the i-th expert in the l-th \\moe layer."
                                                                                },
                                                                                {
                                                                                    "leaf id": 36,
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/par3",
                                                                                    "block type": "par",
                                                                                    "content": "_1 is chosen as the shared expert in each \\moe layer and each token will be assigned to top K experts including one shared expert and K-1 other normal experts. Compared with the original \\sparseupcycle, there are two major differences:"
                                                                                },
                                                                                {
                                                                                    "leaf id": 37,
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/itemize4",
                                                                                    "block type": "itemize",
                                                                                    "content": "[leftmargin=1em]     \\setlength{\\parskip}{2pt}     \\setlength\\itemsep{0pt}     \\item Weighted Shared Expert. Following \\mocle~\\cite{gou2024mixture}, with the token-to-expert affinity score s_i,t, we get the maximum affinity score s_t_max and use its complement 1-s_t_max as the routing weight of the shared expert.     \\item Routing Weight Normalization. Although the shared expert setting is also used in recent works~\\cite{dai2024deepseekmoe, gou2024mixture}, we cannot directly follow their routing strategy because they cannot handle a scale mismatch problem that is unique for \\sparseupcycle. The scale mismatch problem is that differences between the scale of the output of the upcycled \\moe layer and the original FFN layer can cause performance degradation~\\cite{wu2022residual}. To handle this problem, we need to make sure the sum of g_i,t equals 1, so that the output of the \\moe layer matches that of the FFN layer in scale. To do so, we normalize the affinity scores of top K-1 normal experts with Softmax and scale their sum to s_t_max to make sure that the sum of the g_i,t of top K experts, including one shared expert and K-1 normal experts, equals 1."
                                                                                },
                                                                                {
                                                                                    "leaf id": 38,
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/tit",
                                                                                    "block type": "subsection",
                                                                                    "content": "Merging"
                                                                                },
                                                                                {
                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5",
                                                                                    "block_type": "sub",
                                                                                    "children": [
                                                                                        {
                                                                                            "leaf id": 39,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par0",
                                                                                            "block type": "par",
                                                                                            "content": "We propose a learnable model merging method to convert the large MoE model, namely \\oursmoe, back to a dense model \\oursmerge. By doing so, we expect \\oursmerge to keep the boosted performance gained during upcycling while keeping its model size the same as the original dense model to avoid any additional inference overhead. Inspired by \\modelsoup~\\cite{wortsman2022model}, we choose to merge \\oursmoe by learning the mixing coefficients that can be used to average the parameters of all experts in each \\moe layer to obtain a normal FFN layer, while directly copying other remaining layers."
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 40,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par1",
                                                                                            "block type": "par",
                                                                                            "content": "Formally speaking, given the weights of N experts at the l-th layer W_1^l, W_2^l, \u22ef, W_N^l, the process of merging each \\moe layer to an FFN layer can be stated as below:"
                                                                                        },
                                                                                        {
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/equation2",
                                                                                            "block_type": "equation",
                                                                                            "children": [
                                                                                                {
                                                                                                    "leaf id": 41,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/equation2/split0",
                                                                                                    "block type": "split",
                                                                                                    "content": "\\overline{W^l} = \\sum_{i=1}^{N}\\alpha_{i}^lW_{i}^l"
                                                                                                }
                                                                                            ]
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 42,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par3",
                                                                                            "block type": "par",
                                                                                            "content": "where W^l denotes the merged parameter of all N experts and \u03b1_i^l denotes the learnable mixing coefficient of expert W_i^l. We consider a neural network f(x; \u03b8) with input x and parameters \u03b8. For loss \u2112 and instruction dataset {(x_i,y_i)}_i=1^m, such mixing coefficients \u03b1 of all the L layers can be learned via:"
                                                                                        },
                                                                                        {
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/equation4",
                                                                                            "block_type": "equation",
                                                                                            "children": [
                                                                                                {
                                                                                                    "leaf id": 43,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/equation4/split0",
                                                                                                    "block type": "split",
                                                                                                    "content": "\\arg\\min_{\\alpha}\\sum_{j=1}^{m}\\mathcal{L}(f(x_j; \\theta_{o}, (\\sum_{i=1}^{N}\\alpha_{i}^lW_{i}^l)_{1:L}), y_i)"
                                                                                                }
                                                                                            ]
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 44,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par5",
                                                                                            "block type": "par",
                                                                                            "content": "where \u03b8_o refers to all the remaining layers of \\oursmoe other than \\moe layers and \u03b1 is parameterized as the output of a softmax, so that each \u03b1_i^l is positive and \u2211_i=1^N\u03b1_i^l = 1."
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 45,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par6",
                                                                                            "block type": "par",
                                                                                            "content": "While the learning process defined in Eq. (\\ref{formula:merging2}) is the most intuitive way of learning \u03b1, our experiment in Section \\ref{sec:ablation_merging} shows that, due to the shared expert setting, it tends to simply increase the mixing coefficient of the shared expert at each layer as much as possible to decrease the loss. It is not helpful because, although the shared expert has learned general knowledge across the whole instruction dataset and needs a relatively large mixing coefficient, we still need to keep the scale of the mixing coefficient of other normal experts at a certain level also to keep some specific knowledge learned by other normal experts in the merged parameter W^l."
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 46,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par7",
                                                                                            "block type": "par",
                                                                                            "content": "To solve this issue, we introduce a \\textit{shared expert rate} \u03bb to fix the mixing coefficient of the shared expert and learn the mixing coefficients of the remaining normal experts which sums to 1-\u03bb in each layer. By doing so, we can easily control the scale of the mixing coefficient of the shared expert, while still being able to learn the optimal layer-wise mixing coefficients of other normal experts. Let's say W_1^l is the shared expert of the l-th layer, then Eq. (\\ref{formula:merging1}) and Eq. (\\ref{formula:merging2}) can be reformulated as below:"
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 47,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/gather8",
                                                                                            "block type": "gather",
                                                                                            "content": "\\overline{W^l} = \\lambda W_{1}^l + \\sum_{i=2}^{N}\\alpha_{i}^lW_{i}^l\\label{formula:merging1new}\\\\ \\arg\\min_{\\alpha}\\sum_{j=1}^{m}\\mathcal{L}(f(x_j; \\theta_o, \\overline{W^l}_{1:L}), y_i)\\label{formula:merging2new}"
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 48,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/par9",
                                                                                            "block type": "par",
                                                                                            "content": "In practice, we uniformly initialize the mixing coefficients \u03b1 of all the normal experts as 1-\u03bb/N-1, which is then trained on the same instruction dataset as upcycling."
                                                                                        },
                                                                                        {
                                                                                            "leaf id": 49,
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/tit",
                                                                                            "block type": "section",
                                                                                            "content": "Main Evaluation"
                                                                                        },
                                                                                        {
                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10",
                                                                                            "block_type": "sec",
                                                                                            "children": [
                                                                                                {
                                                                                                    "leaf id": 50,
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/tit",
                                                                                                    "block type": "subsection",
                                                                                                    "content": "Experimental Setup"
                                                                                                },
                                                                                                {
                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0",
                                                                                                    "block_type": "sub",
                                                                                                    "children": [
                                                                                                        {
                                                                                                            "leaf id": 51,
                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/par0",
                                                                                                            "block type": "par",
                                                                                                            "content": "We use \\dscoderbase 1.3B~\\cite{guo2024deepseekcoder} as the main base code \\llm."
                                                                                                        },
                                                                                                        {
                                                                                                            "leaf id": 52,
                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/par1",
                                                                                                            "block type": "par",
                                                                                                            "content": "It is constructed with 8 experts in one expert layer and the top 6 experts\\footnote{ 6 is the best-performing number of activated experts per our \\humanevalp{} experiments using top {2,4,6} experts.} are activated for each token, including one shared expert.  As such, we denote the model size of \\oursmoe as 8\u00d71.3B. Other hyperparameter settings are detailed in Appendix \\ref{sec:hyperparameter}.  We finally obtain \\oursmerge by using the learned mixing coefficients to merge \\moe layers inside \\oursmoe as normal FFN layers.  Note that \\oursmerge is the final instruction-tuned \\llm we produce, while \\oursmoe is only an intermediate product of \\ours framework."
                                                                                                        },
                                                                                                        {
                                                                                                            "leaf id": 53,
                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/par2",
                                                                                                            "block type": "par",
                                                                                                            "content": "To study the effectiveness of \\ours, we build a baseline model, namely \\baselineds, by directly performing SFT for \\dscoderbase 1.3B on \\evolcode.  To compare \\ours with \\ewa~\\cite{huang2023experts}, we also implement a baseline \\ewads and instruction-tune it using the same hyperparameter setting as \\baselineds, which is described in Appendix \\ref{sec:hyperparameter}. More implementation details of \\ewads can be seen in Appendix \\ref{sec:ewa_details}.  Furthermore, we incorporate multiple small open-source models (<3B) as our baselines, including \\dscoderbase 1.3B, \\dscoderinst 1.3B~\\cite{guo2024deepseekcoder}, Phi-2 2.7B, and \\stablecoder 3B~\\cite{stable-code-3b}."
                                                                                                        },
                                                                                                        {
                                                                                                            "leaf id": 54,
                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/tit",
                                                                                                            "block type": "subsection",
                                                                                                            "content": "Python Text-to-Code Generation"
                                                                                                        },
                                                                                                        {
                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3",
                                                                                                            "block_type": "sub",
                                                                                                            "children": [
                                                                                                                {
                                                                                                                    "leaf id": 55,
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/par0",
                                                                                                                    "block type": "par",
                                                                                                                    "content": "We further employ \\humanevalp and \\mbppp, which use more tests automatically generated by \\evalplus~\\cite{evalplus} for more rigorous evaluation.  We leave the details in Appendix \\ref{sec:benchmarks}."
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 56,
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/par1",
                                                                                                                    "block type": "par",
                                                                                                                    "content": "Table~\\ref{tab:python-text2code} shows the \\passat{1} results of different \\llm{s}."
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 57,
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/par2",
                                                                                                                    "block type": "par",
                                                                                                                    "content": "We can also observe that \\oursmerge has a clear improvement over the \\baselineds on both benchmarks, with 13\\% and 2\\% improvement on \\humanevalp and \\mbppp respectively, while \\ewads even performs worse than \\baselineds on \\mbpp{(+)}."
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 58,
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/par3",
                                                                                                                    "block type": "par",
                                                                                                                    "content": "Surprisingly, \\oursmerge even surpasses \\oursmoe on \\humaneval and \\humanevalp, despite only using around 18\u00d7 parameters and around 16\u00d7 computations, which showcases the effectiveness of our simple learnable merging technique. Appendix~\\ref{sec:statistic} further demonstrates the statistical significance of the improvements brought by \\ours."
                                                                                                                },
                                                                                                                {
                                                                                                                    "leaf id": 59,
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/tit",
                                                                                                                    "block type": "subsection",
                                                                                                                    "content": "Multilingual Code Generation"
                                                                                                                },
                                                                                                                {
                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4",
                                                                                                                    "block_type": "sub",
                                                                                                                    "children": [
                                                                                                                        {
                                                                                                                            "leaf id": 60,
                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/par0",
                                                                                                                            "block type": "par",
                                                                                                                            "content": "We use \\multiple~\\cite{cassano2022multiple}, a multi-programming benchmark that supports 18 programming languages in addition to Python, to evaluate the multilingual ability and generalizability of \\ours{}.  Among these, we choose 6 representative programming for their distinct language features: Java, JavaScript, C++, PHP, Swift, and Rust, following~\\citet{wei2023magicoder}. Table \\ref{tab:multilang} shows, among all 1.3B models, \\oursmerge achieves the best average multilingual performance and performs the best on 5 (out of 6) individual programming languages,  overall largely improving \\baselineds{} which uses standard SFT. Notably, the overall performance of \\ewads is on par with \\baselineds, indicating that \\ewads{} may not improve SFT on multilingual coding. Appendix~\\ref{sec:expert_analysis} further studies whether each expert in \\oursmoe specializes differently in these programming languages."
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "leaf id": 61,
                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/tit",
                                                                                                                            "block type": "subsection",
                                                                                                                            "content": "Code Generation for Data Science"
                                                                                                                        },
                                                                                                                        {
                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1",
                                                                                                                            "block_type": "sub",
                                                                                                                            "children": [
                                                                                                                                {
                                                                                                                                    "leaf id": 62,
                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/par0",
                                                                                                                                    "block type": "par",
                                                                                                                                    "content": "The \\dsonek dataset~\\cite{lai2022ds1000} is a collection of 1000 realistic data science coding problems ranging from 7 popular data science libraries in Python, including Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), and TensorFlow (tf).  We evaluate \\ours on \\dsonek{} to understand its effectiveness for practical data science engineering.  We follow the evaluation setting of prior works~\\cite{guo2024deepseekcoder, wei2023magicoder}.  In Table \\ref{tab:ds1000}, \\oursmerge achieves the best overall performance among all the evaluated 1.3B models.  Specifically, \\oursmerge consistently surpasses \\baselineds among all the seven studied libraries and also outperforms \\ewads in general."
                                                                                                                                },
                                                                                                                                {
                                                                                                                                    "leaf id": 63,
                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/tit",
                                                                                                                                    "block type": "section",
                                                                                                                                    "content": "Ablation Study"
                                                                                                                                },
                                                                                                                                {
                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1",
                                                                                                                                    "block_type": "sec",
                                                                                                                                    "children": [
                                                                                                                                        {
                                                                                                                                            "leaf id": 64,
                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/tit",
                                                                                                                                            "block type": "subsection",
                                                                                                                                            "content": "Effect of Shared Expert with Routing Weight Normalization"
                                                                                                                                        },
                                                                                                                                        {
                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0",
                                                                                                                                            "block_type": "sub",
                                                                                                                                            "children": [
                                                                                                                                                {
                                                                                                                                                    "leaf id": 65,
                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/par0",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "We demonstrate the importance of the shared expert of \\ours by comparing its performance with the \\sparseupcycle~\\cite{komatsuzaki2023sparse} baseline that does not employ any shared expert. As shown in Table \\ref{tab:ablation-moe}, the performance of the original \\sparseupcycle (with the \"- Shared Expert\" label) drops greatly compared with \\oursmoe. Notably, the \\sparseupcycle model even performs worse than \\baselineds on \\humanevalp, showing its ineffectiveness for instruction tuning."
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 66,
                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/par1",
                                                                                                                                                    "block type": "par",
                                                                                                                                                    "content": "While the shared expert setting is also employed in most recent works~\\cite{dai2024deepseekmoe, gou2024mixture}, their routing strategy will cause performance degradation due to the scale mismatch between the outputs of the upcycled \\moe layer and the original FFN layer. To understand the importance of routing weight normalization, we conduct an ablation by excluding it from \\ours. Table \\ref{tab:ablation-moe} shows that, after removing routing weight normalization, the performance substantially decreases, despite being still better than the original \\sparseupcycle that does not use the shared expert setting."
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "leaf id": 67,
                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/tit",
                                                                                                                                                    "block type": "subsection",
                                                                                                                                                    "content": "Effect of Merging Strategy"
                                                                                                                                                },
                                                                                                                                                {
                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2",
                                                                                                                                                    "block_type": "sub",
                                                                                                                                                    "children": [
                                                                                                                                                        {
                                                                                                                                                            "leaf id": 68,
                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/par0",
                                                                                                                                                            "block type": "par",
                                                                                                                                                            "content": "In this section, we demonstrate the effectiveness of our learnable merging technique by comparing it with (1) directly merging experts with initialized mixing coefficients, and (2) the learnable merging technique without the shared rate setting, which is the same setting as the learned soup in \\modelsoup~\\cite{wortsman2022model} and is described in Eq. (\\ref{formula:merging1}) and Eq. (\\ref{formula:merging2}).  Specifically, we initialize the learnable mixing coefficient of the shared expert as 0.75 and that of the other 7 normal experts as 1/28 for fair comparison. As shown in Table \\ref{tab:ablation-merge}, trained mixing coefficients outperform the initialized mixing coefficients for merging. Furthermore, removing the shared rate setting will largely degrade the performance of \\oursmerge on both \\humaneval and \\humanevalp, demonstrating its importance."
                                                                                                                                                        },
                                                                                                                                                        {
                                                                                                                                                            "leaf id": 69,
                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/par1",
                                                                                                                                                            "block type": "par",
                                                                                                                                                            "content": "We further study the effect of the shared expert rate \u03bb on the performance of the final merged dense model. We evenly choose five shared expert rates, including 0.00, 0.25, 0.50, 0.75, and 1.00, to perform the learnable merging process and evaluate each merged dense model accordingly. Note that 0.75 is the default shared expert rate used in our main experiments. If the shared expert rate is 0.00, it means that the shared expert is ignored when constructing the merged dense model from the upcycled \\moe model; if the shared expert rate is 1.00, it means that the final dense model is built by simply extracting the shared expert from the upcycled \\moe model. As shown in Table \\ref{tab:ablation-shared-expert-rate}, there are mainly three interesting observations:"
                                                                                                                                                        },
                                                                                                                                                        {
                                                                                                                                                            "leaf id": 70,
                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/itemize2",
                                                                                                                                                            "block type": "itemize",
                                                                                                                                                            "content": "[leftmargin=1em]     \\setlength{\\parskip}{2pt}     \\setlength\\itemsep{0pt}     \\item The performance of the final merged dense model improves gradually when the shared expert rate grows from 0.00 to 0.75, indicating that general knowledge learned by the shared expert is important for better performance.     \\item The performance of the final merged dense model drops significantly when the shared expert rate grows from 0.75 to 1.00, showing that specific knowledge learned by other experts is also irreplaceable and ignoring them will lead to a significant performance drop.     \\item All the final merged dense models consistently outperform the normal SFT baseline regardless of their shared expert rate, further demonstrating the effectiveness of \\ours."
                                                                                                                                                        },
                                                                                                                                                        {
                                                                                                                                                            "leaf id": 71,
                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/tit",
                                                                                                                                                            "block type": "subsection",
                                                                                                                                                            "content": "Effect of Base Code \\llm"
                                                                                                                                                        },
                                                                                                                                                        {
                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3",
                                                                                                                                                            "block_type": "sub",
                                                                                                                                                            "children": [
                                                                                                                                                                {
                                                                                                                                                                    "leaf id": 72,
                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/par0",
                                                                                                                                                                    "block type": "par",
                                                                                                                                                                    "content": "In this section, we demonstrate that the effectiveness of \\ours is not dependent on the choice of base code \\llm{s}. To show this, we conduct an ablation experiment by applying \\ours to \\stablecoder 3B~\\cite{stable-code-3b}, whose architecture is different from \\dscoderbase 1.3B~\\cite{guo2024deepseekcoder}, and see whether it can still improve its performance. Hyperparameter settings are detailed in Appendix \\ref{sec:stable_setting}. As is shown in Table \\ref{tab:ablation-stable}, \\stablemerge significantly improves \\baselinestable by 10\\% on \\humaneval and 11\\% on \\humanevalp respectively. Furthermore, \\stablemerge consistently boosts the performance of \\stablemoe while only using 14\u00d7 parameters and 12\u00d7 computations. These results show that the effectiveness of \\ours does not depend on any specific choice of base code \\llm{s}, demonstrating the generalizability of \\ours across different model architectures."
                                                                                                                                                                },
                                                                                                                                                                {
                                                                                                                                                                    "leaf id": 73,
                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/tit",
                                                                                                                                                                    "block type": "section",
                                                                                                                                                                    "content": "Discussion"
                                                                                                                                                                },
                                                                                                                                                                {
                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1",
                                                                                                                                                                    "block_type": "sec",
                                                                                                                                                                    "children": [
                                                                                                                                                                        {
                                                                                                                                                                            "leaf id": 74,
                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/tit",
                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                            "content": "Training Overhead Analysis"
                                                                                                                                                                        },
                                                                                                                                                                        {
                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0",
                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                            "children": [
                                                                                                                                                                                {
                                                                                                                                                                                    "leaf id": 75,
                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/par0",
                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                    "content": "Compared with SFT, \\ours will inevitably introduce additional overhead in the training process because \\ours needs to fine-tune the upcycled \\moe model while the normal SFT technique only needs to fine-tune the original dense model. To better understand the effect of such overhead, we conduct an experiment where we use the same training budget (i.e., the same GPU hours) instead of the same training steps for the normal SFT baseline. As shown in Table \\ref{tab:discussion-overhead}, although sharing the same training budget as \\oursmerge, the performance of \\baselineds is still significantly worse than that of \\oursmerge, demonstrating the ability of \\ours to unlock the power of code instruction tuning using the same training budget."
                                                                                                                                                                                },
                                                                                                                                                                                {
                                                                                                                                                                                    "leaf id": 76,
                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/tit",
                                                                                                                                                                                    "block type": "subsection",
                                                                                                                                                                                    "content": "Generalizability for General Tasks"
                                                                                                                                                                                },
                                                                                                                                                                                {
                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1",
                                                                                                                                                                                    "block_type": "sub",
                                                                                                                                                                                    "children": [
                                                                                                                                                                                        {
                                                                                                                                                                                            "leaf id": 77,
                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/par0",
                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                            "content": "In this section, we demonstrate that \\ours can improve the performance of \\llm{s} on general tasks across different domains by applying \\ours to general instruction tuning. We use \\tinyllama 1.1B~\\cite{zhang2024tinyllama} as the base model and use \\evolgeneral~\\cite{xu2023wizardlm} as the training dataset for general instruction tuning. Following existing work~\\cite{zhang2024tinyllama}, we use MMLU~\\cite{hendrycks2021measuring} with the 5-shot setting as our evaluation benchmark to showcase the general performance of \\llm{s}. Hyperparameter settings are detailed in Appendix \\ref{sec:tinyllama_setting}. As shown in Table \\ref{tab:discussion-generalizability}, overall, \\tinyllamamerge improves \\baselinetinyllama by 5\\% on MMLU, demonstrating the generalizable effectiveness of \\ours for general instruction tuning."
                                                                                                                                                                                        },
                                                                                                                                                                                        {
                                                                                                                                                                                            "leaf id": 78,
                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/tit",
                                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                                            "content": "Preliminary Theoretical Explanation"
                                                                                                                                                                                        },
                                                                                                                                                                                        {
                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1",
                                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                                            "children": [
                                                                                                                                                                                                {
                                                                                                                                                                                                    "leaf id": 79,
                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/par0",
                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                    "content": "We provide a preliminary theoretical explanation of \\ours by considering a simplified variant of it. Let\u2019s start by analyzing the two major steps of \\ours:"
                                                                                                                                                                                                },
                                                                                                                                                                                                {
                                                                                                                                                                                                    "leaf id": 80,
                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/itemize1",
                                                                                                                                                                                                    "block type": "itemize",
                                                                                                                                                                                                    "content": "[leftmargin=1em]     \\setlength{\\parskip}{2pt}     \\setlength\\itemsep{0pt}     \\item Step 1: Upcycling. According to the scaling law~\\cite{kaplan2020scaling}, the upcycled \\moe model performs better than the normal SFT dense model due to more trainable parameters.     \\item Step 2: Merging. We consider a simplified variant of \\ours, where the upcycled \\moe model (e.g., \\oursmoe) can be viewed as the ensembling of two dense models and the merged dense model (e.g., \\oursmerge) can be viewed as the merging of the same two dense models; see Appendix \\ref{sec:theoratical_analysis} for more details. As such, we can directly apply the theoretical analyzing process in Section 4 of~\\cite{wortsman2022model} to analyze the performance difference between the upcycled \\moe model and the merged dense model, which is initially designed to analyze the performance difference between model ensembling and model merging. According to~\\cite{wortsman2022model}, the convexity of the loss can help the merged dense model achieve a similar expected loss as that of the upcycled \\moe model."
                                                                                                                                                                                                },
                                                                                                                                                                                                {
                                                                                                                                                                                                    "leaf id": 81,
                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/par2",
                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                    "content": "Overall, the Upcycling step improves the performance with more trainable parameters, while the Merging step maintains the upcycled MoE-level performance with only dense-model compute. Consequently, we provide a preliminary theoretical explanation for the effectiveness of \\ours."
                                                                                                                                                                                                },
                                                                                                                                                                                                {
                                                                                                                                                                                                    "leaf id": 82,
                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/tit",
                                                                                                                                                                                                    "block type": "section",
                                                                                                                                                                                                    "content": "Conclusion"
                                                                                                                                                                                                },
                                                                                                                                                                                                {
                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3",
                                                                                                                                                                                                    "block_type": "sec",
                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                        {
                                                                                                                                                                                                            "leaf id": 83,
                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/par0",
                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                            "content": "This paper introduces \\ours to unlock the power of code instruction tuning by simply merging upcycled \\moe. Similar to SFT, \\ours starts with a dense \\llm{} and produces a fine-tuned dense \\llm{} with the exact size and model structure. Yet, \\ours improves SFT by upcycling the pre-trained dense \\llm{} to an \\moe{} model for fine-tuning, after which we compile the \\moe{} model back to an efficient dense \\llm{} with a learnable merging mechanism. As such, we unleash the performance limit of instruction tuning without any additional inference overhead. Using the same dataset, \\ours improves SFT on a variety of benchmarks, including HumanEval(+), MBPP(+), \\multiple{}, and \\dsonek{}, from 2\\% to 13\\%. By applying \\ours to \\dscoderbase{ 1.3B}, we create the next state-of-the-art small (<3B) \\llm{} for code. The ultimate dense \\llm{} produced by \\ours preserves or even outperforms the full upcycled \\moe which uses 8\u00d7 parameters as much as our final dense \\llm{}."
                                                                                                                                                                                                        },
                                                                                                                                                                                                        {
                                                                                                                                                                                                            "leaf id": 84,
                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/par1",
                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                            "content": "While \\ours has proven to be effective through extensive experiments in the paper, we apply our technique to \\llm{s} with no more than 3B parameters due to resource constraints. This limitation hinders our ability to showcase the impact of \\ours on larger models. In addition, to balance the general knowledge in the shared expert and the specific knowledge in other normal experts, we introduce a hyperparameter \u03bb in the merging process of \\ours, which might slightly increase the efforts for hyperparameter search. It would be interesting to explore other hyperparameter-free techniques to tackle this challenge in the future. Furthermore, while \\ours has been empirically proven powerful, it would be interesting to provide a theoretical explanation for its strong performance."
                                                                                                                                                                                                        },
                                                                                                                                                                                                        {
                                                                                                                                                                                                            "leaf id": 85,
                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/tit",
                                                                                                                                                                                                            "block type": "section",
                                                                                                                                                                                                            "content": "Appendix for \"\\ours: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts\""
                                                                                                                                                                                                        },
                                                                                                                                                                                                        {
                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2",
                                                                                                                                                                                                            "block_type": "sec",
                                                                                                                                                                                                            "children": [
                                                                                                                                                                                                                {
                                                                                                                                                                                                                    "leaf id": 86,
                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/tit",
                                                                                                                                                                                                                    "block type": "subsection",
                                                                                                                                                                                                                    "content": "Hyperparameter Settings"
                                                                                                                                                                                                                },
                                                                                                                                                                                                                {
                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0",
                                                                                                                                                                                                                    "block_type": "sub",
                                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                                        {
                                                                                                                                                                                                                            "leaf id": 87,
                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/par0",
                                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                                            "content": "We use a batch size of 64 and a learning rare of 5e-5 with a linear scheduler to fine-tune \\oursmoe for 4 epochs with 500 warmup steps, following the implementation of previous work~\\cite{wei2023magicoder}. We further use a batch size of 64, a shared expert rate \u03bb of 0.75, and a learning rare of 1e-5 with a linear schedule to fine-tune the learnable mixing coefficients for each of the experts in the instruction-tuned \\oursmoe on the instruction dataset for 1 epoch with 125 warmup steps. Detailedly, we use Softmax to keep the sum of the mixing coefficients of the other 7 normal experts as 0.25. For \\baselineds and \\ewads, we use the same hyperparameter setting as \\ours, where the batch size is 64 and the learning rate is 5e-5 with a linear scheduler. Because \\ours is trained for 4 epochs during upcycling and 1 epoch during merging, for a fair comparison, we train \\baselineds and \\ewads for 5 (= 4 + 1) epochs with 625 warmup steps."
                                                                                                                                                                                                                        },
                                                                                                                                                                                                                        {
                                                                                                                                                                                                                            "leaf id": 88,
                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/tit",
                                                                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                                                                            "content": "Implementation details of \\ewa"
                                                                                                                                                                                                                        },
                                                                                                                                                                                                                        {
                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1",
                                                                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                                                                            "children": [
                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                    "leaf id": 89,
                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/par0",
                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                    "content": "Because \\ewa~\\cite{huang2023experts} does not release their implementation, we implemented \\ewa by ourselves, including constant schedule and linear schedule. We use a share rate \u03b2 of 0.3, following the original setting of \\ewa. While \\ewa with the constant schedule achieves reasonable performance in our evaluation, the training loss of \\ewa with the linear schedule becomes very unstable, as is shown in Figure \\ref{fig:loss_ewa}, and thus cannot achieve reasonable performance. As a result, we report the results of \\ewa with the constant schedule in Section \\ref{sec:experiment}."
                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                    "leaf id": 90,
                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/tit",
                                                                                                                                                                                                                                    "block type": "subsection",
                                                                                                                                                                                                                                    "content": "Details of \\humaneval and \\mbpp"
                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1",
                                                                                                                                                                                                                                    "block_type": "sub",
                                                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                            "leaf id": 91,
                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/par0",
                                                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                                                            "content": "In these benchmarks, each task consists of a task description in English, which is sent to \\llm{s} as the prompt, and \\llm{s} are expected to generate the corresponding code to satisfy the requirements in the description. While these benchmarks provide a handful of test cases to validate the correctness of the generated code, these tests are often insufficient for more rigorous evaluation. As such, \\humanevalp and \\mbppp proposed by \\evalplus~\\cite{evalplus} are usually used to evaluate the correctness of the generated code, which provides 80\u00d7/35\u00d7 more tests compared with the original benchmarks."
                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                            "leaf id": 92,
                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/tit",
                                                                                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                                                                                            "content": "Statistical Significance Analysis"
                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1",
                                                                                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                                                                                            "children": [
                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                    "leaf id": 93,
                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/par0",
                                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                                    "content": "In this section, we show that improvements brought by \\ours are statistically significant. In our main experiments, we follow prior works~\\cite{wei2023magicoder, lozhkov2024starcoder} to conduct experiments on \\humaneval{}~(+) using greedy decoding. To demonstrate the statistical significance of our improvements, we change our setting from greedy decoding to sampling. In detail, to conduct one experiment on \\humaneval{}~(+), the model will sample one solution for each problem in \\humaneval{}~(+) with top p = 0.95 and temperature = 0.8, which is the same setting used in prior works~\\cite{evalplus, chen2021evaluating}."
                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                    "leaf id": 94,
                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/par1",
                                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                                    "content": "Following prior work~\\cite{evalplus}, we repeat this experiment 200 times for three techniques: \\oursmerge, \\ewads, and \\baselineds. \\ewads is included because it is the best-performing baseline in our main experiment. We first compute their average pass@1 performance in these 200 experiments. As is shown in Table \\ref{tab:discussion-stat-mean}, \\oursmerge outperforms both \\ewads and \\baselineds clearly."
                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                    "leaf id": 95,
                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/par2",
                                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                                    "content": "Furthermore, we use the Wilcoxon signed-rank test~\\cite{Wilcoxon1945IndividualCB, dror-etal-2018-hitchhikers}, a widely used statistical test, to check if the improvements brought by \\ours are statistically significant. As shown in Table \\ref{tab:discussion-stat-p}, the p-values for both \\oursmerge vs. \\ewads and \\oursmerge vs. \\baselineds are much smaller than both 0.0025 (the significance level recommended for NLP work by~\\cite{sogaard-etal-2014-whats}) and 0.05 (the most common significance level), demonstrating the statistical significance of the improvements brought by \\ours."
                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                    "leaf id": 96,
                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/tit",
                                                                                                                                                                                                                                                    "block type": "subsection",
                                                                                                                                                                                                                                                    "content": "Analysis on Expert Specialization"
                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3",
                                                                                                                                                                                                                                                    "block_type": "sub",
                                                                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                            "leaf id": 97,
                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/par0",
                                                                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                                                                            "content": "Inspired by recent works~\\cite{jiang2024mixtral, xue2024openmoe}, we analyze whether each expert in \\oursmoe has different specializations in different programming languages by visualizing the routing decision of the tokens from different programming languages in the \\multiple benchmark (including Python). For the \\multiple benchmark, we collect the routing decision when conducting experiments in Section \\ref{sec:multiple}. For Python, we collect the routing decision by reruning \\humaneval experiment following the same setting as Section \\ref{sec:multiple}. Following Mixtral~\\cite{jiang2024mixtral}, we get the visualization results from layers 0, 11, and 23 in \\oursmoe, where layer 0 and layer 23 are the first and the last layers of \\oursmoe. As is shown in Figure \\ref{fig:analysis}, we do not observe obvious patterns in the assignment of experts based on the programming language, which is in line with the observation reported by recent works~\\cite{jiang2024mixtral, xue2024openmoe}.  1681"
                                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                            "leaf id": 98,
                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/tit",
                                                                                                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                                                                                                            "content": "Training Settings for \\stablecoder 3B"
                                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1",
                                                                                                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                                                                                                            "children": [
                                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                                    "leaf id": 99,
                                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/par0",
                                                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                                                    "content": "We use \\evolcode as the training dataset. Since \\stablecoder 3B is the base model, we upcycle a new \\moe model from the base model, namely \\stablemoe. Due to limited computational resources, we construct \\stablemoe with 4 experts in one expert layer, where the top 2 experts are activated for each token, including one shared expert. Consequently, the size of \\stablemoe can be described as 4\u00d73B. We use a batch size of 64 and a learning rate of 5e-5 with a linear scheduler to fine-tune \\stablemoe for 4 epochs with 500 warmup steps. Similar to \\oursmerge, we obtain \\stablemerge by learning mixing coefficients to merge \\moe layers inside \\stablemoe as normal FFN layers, which is fine-tuned with a batch size of 64, a shared expert rate \u03bb of 0.85, and a learning rate of 1e-5 with a linear schedule for 1 epoch with 125 warmup steps. Our baseline model, namely \\baselinestable, is fine-tuned for 5 (= 4 + 1) epochs with a batch size of 64, a learning rate of 5e-5, and 625 warmup steps for a fair comparison."
                                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                                    "leaf id": 100,
                                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/tit",
                                                                                                                                                                                                                                                                    "block type": "subsection",
                                                                                                                                                                                                                                                                    "content": "Training Settings for \\tinyllama 1.1B"
                                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1",
                                                                                                                                                                                                                                                                    "block_type": "sub",
                                                                                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                                            "leaf id": 101,
                                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/par0",
                                                                                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                                                                                            "content": "Using \\tinyllama 1.1B as the base model, we upcycle a new \\moe model, namely \\tinyllamamoe, from the base model. Following the setting for \\oursmoe, we construct \\tinyllamamoe with 8 experts in one expert layer, where the top 6 experts are activated for each token, including one shared expert. As such, the number of parameters for \\tinyllamamoe can be written as 8\u00d71.1B. We use a batch size of 64 and a learning rate of 5e-5 with a linear scheduler to fine-tune \\tinyllamamoe for 4 epochs with 240 warmup steps. To obtain \\tinyllamamerge, we learn mixing coefficients to merge \\moe layers inside \\tinyllamamoe by fine-tuning them with a batch size of 64, a shared expert rate \u03bb of 0.85, and a learning rate of 2e-5 with a linear schedule for 1 epoch with 60 warmup steps. For a fair comparison, we fine-tune a baseline model \\baselinetinyllama for 5 (= 4 + 1) epochs with a batch size of 64, a learning rate of 5e-5, and 300 warmup steps."
                                                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                                            "leaf id": 102,
                                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/sub1/tit",
                                                                                                                                                                                                                                                                            "block type": "subsection",
                                                                                                                                                                                                                                                                            "content": "Theoratical Explanation Details"
                                                                                                                                                                                                                                                                        },
                                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/sub1",
                                                                                                                                                                                                                                                                            "block_type": "sub",
                                                                                                                                                                                                                                                                            "children": [
                                                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                                                    "leaf id": 103,
                                                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/sub1/par0",
                                                                                                                                                                                                                                                                                    "block type": "par",
                                                                                                                                                                                                                                                                                    "content": "We consider a simplified variant of \\ours as below:"
                                                                                                                                                                                                                                                                                },
                                                                                                                                                                                                                                                                                {
                                                                                                                                                                                                                                                                                    "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/sub1/itemize1",
                                                                                                                                                                                                                                                                                    "block_type": "itemize",
                                                                                                                                                                                                                                                                                    "children": [
                                                                                                                                                                                                                                                                                        {
                                                                                                                                                                                                                                                                                            "leaf id": 104,
                                                                                                                                                                                                                                                                                            "key": "doc/body/sec0/sec9/sub0/sub4/sub2/sec2/sub1/ssb1/sub5/sec10/sub0/sub3/sub4/sub1/sec1/sub0/sub2/sub3/sec1/sub0/sub1/sub1/sec3/sec2/sub0/sub1/sub1/sub1/sub3/sub1/sub1/sub1/itemize1/par0",
                                                                                                                                                                                                                                                                                            "block type": "par",
                                                                                                                                                                                                                                                                                            "content": "[leftmargin=1em]"
                                                                                                                                                                                                                                                                                        }
                                                                                                                                                                                                                                                                                    ]
                                                                                                                                                                                                                                                                                }
                                                                                                                                                                                                                                                                            ]
                                                                                                                                                                                                                                                                        }
                                                                                                                                                                                                                                                                    ]
                                                                                                                                                                                                                                                                }
                                                                                                                                                                                                                                                            ]
                                                                                                                                                                                                                                                        }
                                                                                                                                                                                                                                                    ]
                                                                                                                                                                                                                                                }
                                                                                                                                                                                                                                            ]
                                                                                                                                                                                                                                        }
                                                                                                                                                                                                                                    ]
                                                                                                                                                                                                                                }
                                                                                                                                                                                                                            ]
                                                                                                                                                                                                                        }
                                                                                                                                                                                                                    ]
                                                                                                                                                                                                                }
                                                                                                                                                                                                            ]
                                                                                                                                                                                                        }
                                                                                                                                                                                                    ]
                                                                                                                                                                                                }
                                                                                                                                                                                            ]
                                                                                                                                                                                        }
                                                                                                                                                                                    ]
                                                                                                                                                                                }
                                                                                                                                                                            ]
                                                                                                                                                                        }
                                                                                                                                                                    ]
                                                                                                                                                                }
                                                                                                                                                            ]
                                                                                                                                                        }
                                                                                                                                                    ]
                                                                                                                                                }
                                                                                                                                            ]
                                                                                                                                        }
                                                                                                                                    ]
                                                                                                                                }
                                                                                                                            ]
                                                                                                                        }
                                                                                                                    ]
                                                                                                                }
                                                                                                            ]
                                                                                                        }
                                                                                                    ]
                                                                                                }
                                                                                            ]
                                                                                        }
                                                                                    ]
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 105,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. \\newblock \\href {http://arxiv.org/abs/2108.07732} {Program synthesis with large language models}."
        },
        {
            "leaf id": 106,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Loubna Ben~Allal, Niklas Muennighoff, Logesh Kumar~Umapathi, Ben Lipkin, and Leandro von Werra. 2022. \\newblock A framework for the evaluation of code generation models. \\newblock \\url{https://github.com/bigcode-project/bigcode-evaluation-harness}."
        },
        {
            "leaf id": 107,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2022. \\newblock \\href {http://arxiv.org/abs/2208.08227} {Multipl-e: A scalable and extensible approach to benchmarking neural code generation}."
        },
        {
            "leaf id": 108,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Sahil Chaudhary. 2023. \\newblock Code alpaca: An instruction-following llama model for code generation. \\newblock \\url{https://github.com/sahil280114/codealpaca}."
        },
        {
            "leaf id": 109,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. 2022. \\newblock \\href {http://arxiv.org/abs/2202.07962} {Revisiting parameter-efficient tuning: Are we really there yet?}"
        },
        {
            "leaf id": 110,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. \\newblock \\href {http://arxiv.org/abs/2107.03374} {Evaluating large language models trained on code}."
        },
        {
            "leaf id": 111,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Damai Dai, Chengqi Deng, Chenggang Zhao, R.~X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y.~Wu, Zhenda Xie, Y.~K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024. \\newblock \\href {http://arxiv.org/abs/2401.06066} {Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models}."
        },
        {
            "leaf id": 112,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2023. \\newblock \\href {http://arxiv.org/abs/2312.09979} {Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment}."
        },
        {
            "leaf id": 113,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. \\newblock \\href {https://doi.org/10.18653/v1/P18-1128} {The hitchhiker{'}s guide to testing statistical significance in natural language processing}. \\newblock In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1383--1392, Melbourne, Australia. Association for Computational Linguistics."
        },
        {
            "leaf id": 114,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. \\newblock \\href {http://arxiv.org/abs/2112.06905} {Glam: Efficient scaling of language models with mixture-of-experts}."
        },
        {
            "leaf id": 115,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "William Fedus, Barret Zoph, and Noam Shazeer. 2022. \\newblock \\href {http://arxiv.org/abs/2101.03961} {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity}."
        },
        {
            "leaf id": 116,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James~T. Kwok, and Yu~Zhang. 2024. \\newblock \\href {http://arxiv.org/abs/2312.12379} {Mixture of cluster-conditional lora experts for vision-language instruction tuning}."
        },
        {
            "leaf id": 117,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. \\newblock \\href {http://arxiv.org/abs/2401.14196} {Deepseek-coder: When the large language model meets programming -- the rise of code intelligence}."
        },
        {
            "leaf id": 118,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. \\newblock \\href {http://arxiv.org/abs/2009.03300} {Measuring massive multitask language understanding}."
        },
        {
            "leaf id": 119,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Tong He, and Wanli Ouyang. 2023. \\newblock \\href {http://arxiv.org/abs/2308.06093} {Experts weights averaging: A new general training scheme for vision transformers}."
        },
        {
            "leaf id": 120,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William~El Sayed. 2024. \\newblock \\href {http://arxiv.org/abs/2401.04088} {Mixtral of experts}."
        },
        {
            "leaf id": 121,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. \\newblock \\href {http://arxiv.org/abs/2001.08361} {Scaling laws for neural language models}."
        },
        {
            "leaf id": 122,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos~Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi~Tay, Mostafa Dehghani, and Neil Houlsby. 2023. \\newblock \\href {http://arxiv.org/abs/2212.05055} {Sparse upcycling: Training mixture-of-experts from dense checkpoints}."
        },
        {
            "leaf id": 123,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott~Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. \\newblock \\href {http://arxiv.org/abs/2211.11501} {Ds-1000: A natural and reliable benchmark for data science code generation}."
        },
        {
            "leaf id": 124,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. \\newblock \\href {http://arxiv.org/abs/2006.16668} {Gshard: Scaling giant models with conditional computation and automatic sharding}."
        },
        {
            "leaf id": 125,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. \\newblock \\href {https://openreview.net/forum?id=1qvx610Cu7} {Is your code generated by chat{GPT} really correct? rigorous evaluation of large language models for code generation}. \\newblock In Thirty-seventh Conference on Neural Information Processing Systems."
        },
        {
            "leaf id": 126,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "{LLaMA-MoE Team}. 2023. \\newblock \\href {https://github.com/pjlab-sys4nlp/llama-moe} {Llama-moe: Building mixture-of-experts from llama with continual pre-training}."
        },
        {
            "leaf id": 127,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos~Mu\u00f1oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries. 2024. \\newblock \\href {http://arxiv.org/abs/2402.19173} {Starcoder 2 and the stack v2: The next generation}."
        },
        {
            "leaf id": 128,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. \\newblock \\href {http://arxiv.org/abs/2306.08568} {Wizardcoder: Empowering code large language models with evol-instruct}."
        },
        {
            "leaf id": 129,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Zohar Manna and Richard~J Waldinger. 1971. \\newblock Toward automatic program synthesis. \\newblock Communications of the ACM, 14(3):151--165."
        },
        {
            "leaf id": 130,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, , and Nathan Cooper. 2024. \\newblock \\href {[https://huggingface.co/stabilityai/stable-code-3b](https://huggingface.co/stabilityai/stable-code-3b)} {Stable code 3b}."
        },
        {
            "leaf id": 131,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. \\newblock \\href {http://arxiv.org/abs/1701.06538} {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}."
        },
        {
            "leaf id": 132,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Anders S{\\o}gaard, Anders Johannsen, Barbara Plank, Dirk Hovy, and Hector Mart{\\'\\i}nez~Alonso. 2014. \\newblock \\href {https://doi.org/10.3115/v1/W14-1601} {What{'}s in a p-value in {NLP}?} \\newblock In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 1--10, Ann Arbor, Michigan. Association for Computational Linguistics."
        },
        {
            "leaf id": 133,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. \\newblock \\href {http://arxiv.org/abs/2212.10560} {Self-instruct: Aligning language models with self-generated instructions}."
        },
        {
            "leaf id": 134,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2022. \\newblock \\href {http://arxiv.org/abs/2109.01652} {Finetuned language models are zero-shot learners}."
        },
        {
            "leaf id": 135,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. \\newblock Magicoder: Source code is all you need. \\newblock arXiv preprint arXiv:2312.02120."
        },
        {
            "leaf id": 136,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Frank. Wilcoxon. 1945. \\newblock \\href {https://api.semanticscholar.org/CorpusID:53662922} {Individual comparisons by ranking methods}. \\newblock Biometrics, 1:196--202."
        },
        {
            "leaf id": 137,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. \\newblock \\href {http://arxiv.org/abs/2203.05482} {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}."
        },
        {
            "leaf id": 138,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Haoyuan Wu, Haisheng Zheng, and Bei Yu. 2024. \\newblock \\href {http://arxiv.org/abs/2401.02731} {Parameter-efficient sparsity crafting from dense to mixture-of-experts for instruction tuning on general tasks}."
        },
        {
            "leaf id": 139,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu~Yuan. 2022. \\newblock \\href {http://arxiv.org/abs/2204.09636} {Residual mixture of experts}."
        },
        {
            "leaf id": 140,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. \\newblock \\href {http://arxiv.org/abs/2304.12244} {Wizardlm: Empowering large language models to follow complex instructions}."
        },
        {
            "leaf id": 141,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. 2022. \\newblock \\href {http://arxiv.org/abs/2201.10890} {One student knows all experts know: From sparse to dense}."
        },
        {
            "leaf id": 142,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. 2024. \\newblock Openmoe: An early effort on open mixture-of-experts language models. \\newblock arXiv preprint arXiv:2402.01739."
        },
        {
            "leaf id": 143,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. \\newblock \\href {http://arxiv.org/abs/2401.02385} {Tinyllama: An open-source small language model}."
        },
        {
            "leaf id": 144,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023. \\newblock \\href {http://arxiv.org/abs/2308.10792} {Instruction tuning for large language models: A survey}."
        }
    ]
}