\begin{thebibliography}{40}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton}]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021.
\newblock \href {http://arxiv.org/abs/2108.07732} {Program synthesis with large language models}.

\bibitem[{Ben~Allal et~al.(2022)Ben~Allal, Muennighoff, Kumar~Umapathi, Lipkin, and von Werra}]{bigcode-evaluation-harness}
Loubna Ben~Allal, Niklas Muennighoff, Logesh Kumar~Umapathi, Ben Lipkin, and Leandro von Werra. 2022.
\newblock A framework for the evaluation of code generation models.
\newblock \url{https://github.com/bigcode-project/bigcode-evaluation-harness}.

\bibitem[{Cassano et~al.(2022)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and Jangda}]{cassano2022multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2022.
\newblock \href {http://arxiv.org/abs/2208.08227} {Multipl-e: A scalable and extensible approach to benchmarking neural code generation}.

\bibitem[{Chaudhary(2023)}]{codealpaca}
Sahil Chaudhary. 2023.
\newblock Code alpaca: An instruction-following llama model for code generation.
\newblock \url{https://github.com/sahil280114/codealpaca}.

\bibitem[{Chen et~al.(2022)Chen, Liu, Meng, and Liang}]{chen2022revisiting}
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. 2022.
\newblock \href {http://arxiv.org/abs/2202.07962} {Revisiting parameter-efficient tuning: Are we really there yet?}

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.
\newblock \href {http://arxiv.org/abs/2107.03374} {Evaluating large language models trained on code}.

\bibitem[{Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, Xie, Li, Huang, Luo, Ruan, Sui, and Liang}]{dai2024deepseekmoe}
Damai Dai, Chengqi Deng, Chenggang Zhao, R.~X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y.~Wu, Zhenda Xie, Y.~K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024.
\newblock \href {http://arxiv.org/abs/2401.06066} {Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models}.

\bibitem[{Dou et~al.(2023)Dou, Zhou, Liu, Gao, Zhao, Shen, Zhou, Xi, Wang, Fan, Pu, Zhu, Zheng, Gui, Zhang, and Huang}]{dou2023loramoe}
Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi~Zhang, and Xuanjing Huang. 2023.
\newblock \href {http://arxiv.org/abs/2312.09979} {Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment}.

\bibitem[{Dror et~al.(2018)Dror, Baumer, Shlomov, and Reichart}]{dror-etal-2018-hitchhikers}
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1128} {The hitchhiker{'}s guide to testing statistical significance in natural language processing}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1383--1392, Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui}]{du2022glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022.
\newblock \href {http://arxiv.org/abs/2112.06905} {Glam: Efficient scaling of language models with mixture-of-experts}.

\bibitem[{Fedus et~al.(2022)Fedus, Zoph, and Shazeer}]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer. 2022.
\newblock \href {http://arxiv.org/abs/2101.03961} {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity}.

\bibitem[{Gou et~al.(2024)Gou, Liu, Chen, Hong, Xu, Li, Yeung, Kwok, and Zhang}]{gou2024mixture}
Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James~T. Kwok, and Yu~Zhang. 2024.
\newblock \href {http://arxiv.org/abs/2312.12379} {Mixture of cluster-conditional lora experts for vision-language instruction tuning}.

\bibitem[{Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, Luo, Xiong, and Liang}]{guo2024deepseekcoder}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024.
\newblock \href {http://arxiv.org/abs/2401.14196} {Deepseek-coder: When the large language model meets programming -- the rise of code intelligence}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
\newblock \href {http://arxiv.org/abs/2009.03300} {Measuring massive multitask language understanding}.

\bibitem[{Huang et~al.(2023)Huang, Ye, Huang, Li, Chen, He, and Ouyang}]{huang2023experts}
Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Tong He, and Wanli Ouyang. 2023.
\newblock \href {http://arxiv.org/abs/2308.06093} {Experts weights averaging: A new general training scheme for vision transformers}.

\bibitem[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, de~las Casas, Hanna, Bressand, Lengyel, Bour, Lample, Lavaud, Saulnier, Lachaux, Stock, Subramanian, Yang, Antoniak, Scao, Gervet, Lavril, Wang, Lacroix, and Sayed}]{jiang2024mixtral}
Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2024.
\newblock \href {http://arxiv.org/abs/2401.04088} {Mixtral of experts}.

\bibitem[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
\newblock \href {http://arxiv.org/abs/2001.08361} {Scaling laws for neural language models}.

\bibitem[{Komatsuzaki et~al.(2023)Komatsuzaki, Puigcerver, Lee-Thorp, Ruiz, Mustafa, Ainslie, Tay, Dehghani, and Houlsby}]{komatsuzaki2023sparse}
Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos~Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi~Tay, Mostafa Dehghani, and Neil Houlsby. 2023.
\newblock \href {http://arxiv.org/abs/2212.05055} {Sparse upcycling: Training mixture-of-experts from dense checkpoints}.

\bibitem[{Lai et~al.(2022)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, tau Yih, Fried, Wang, and Yu}]{lai2022ds1000}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott~Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022.
\newblock \href {http://arxiv.org/abs/2211.11501} {Ds-1000: A natural and reliable benchmark for data science code generation}.

\bibitem[{Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen}]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
\newblock \href {http://arxiv.org/abs/2006.16668} {Gshard: Scaling giant models with conditional computation and automatic sharding}.

\bibitem[{Liu et~al.(2023)Liu, Xia, Wang, and Zhang}]{evalplus}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023.
\newblock \href {https://openreview.net/forum?id=1qvx610Cu7} {Is your code generated by chat{GPT} really correct? rigorous evaluation of large language models for code generation}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{{LLaMA-MoE Team}(2023)}]{llama-moe-2023}
{LLaMA-MoE Team}. 2023.
\newblock \href {https://github.com/pjlab-sys4nlp/llama-moe} {Llama-moe: Building mixture-of-experts from llama with continual pre-training}.

\bibitem[{Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, Liu, Tian, Kocetkov, Zucker, Belkada, Wang, Liu, Abulkhanov, Paul, Li, Li, Risdal, Li, Zhu, Zhuo, Zheltonozhskii, Dade, Yu, Krauß, Jain, Su, He, Dey, Abati, Chai, Muennighoff, Tang, Oblokulov, Akiki, Marone, Mou, Mishra, Gu, Hui, Dao, Zebaze, Dehaene, Patry, Xu, McAuley, Hu, Scholak, Paquet, Robinson, Anderson, Chapados, Patwary, Tajbakhsh, Jernite, Ferrandis, Zhang, Hughes, Wolf, Guha, von Werra, and de~Vries}]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos~Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries. 2024.
\newblock \href {http://arxiv.org/abs/2402.19173} {Starcoder 2 and the stack v2: The next generation}.

\bibitem[{Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang}]{luo2023wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.
\newblock \href {http://arxiv.org/abs/2306.08568} {Wizardcoder: Empowering code large language models with evol-instruct}.

\bibitem[{Manna and Waldinger(1971)}]{manna1971toward}
Zohar Manna and Richard~J Waldinger. 1971.
\newblock Toward automatic program synthesis.
\newblock \emph{Communications of the ACM}, 14(3):151--165.

\bibitem[{Pinnaparaju et~al.(2024)Pinnaparaju, Adithyan, Phung, Tow, Baicoianu, , and Cooper}]{stable-code-3b}
Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, , and Nathan Cooper. 2024.
\newblock \href {[https://huggingface.co/stabilityai/stable-code-3b](https://huggingface.co/stabilityai/stable-code-3b)} {Stable code 3b}.

\bibitem[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean}]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.
\newblock \href {http://arxiv.org/abs/1701.06538} {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}.

\bibitem[{S{\o}gaard et~al.(2014)S{\o}gaard, Johannsen, Plank, Hovy, and Mart{\'\i}nez~Alonso}]{sogaard-etal-2014-whats}
Anders S{\o}gaard, Anders Johannsen, Barbara Plank, Dirk Hovy, and Hector Mart{\'\i}nez~Alonso. 2014.
\newblock \href {https://doi.org/10.3115/v1/W14-1601} {What{'}s in a p-value in {NLP}?}
\newblock In \emph{Proceedings of the Eighteenth Conference on Computational Natural Language Learning}, pages 1--10, Ann Arbor, Michigan. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi}]{wang2023selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
\newblock \href {http://arxiv.org/abs/2212.10560} {Self-instruct: Aligning language models with self-generated instructions}.

\bibitem[{Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le}]{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2022.
\newblock \href {http://arxiv.org/abs/2109.01652} {Finetuned language models are zero-shot learners}.

\bibitem[{Wei et~al.(2023)Wei, Wang, Liu, Ding, and Zhang}]{wei2023magicoder}
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023.
\newblock Magicoder: Source code is all you need.
\newblock \emph{arXiv preprint arXiv:2312.02120}.

\bibitem[{Wilcoxon(1945)}]{Wilcoxon1945IndividualCB}
Frank. Wilcoxon. 1945.
\newblock \href {https://api.semanticscholar.org/CorpusID:53662922} {Individual comparisons by ranking methods}.
\newblock \emph{Biometrics}, 1:196--202.

\bibitem[{Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and Schmidt}]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022.
\newblock \href {http://arxiv.org/abs/2203.05482} {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}.

\bibitem[{Wu et~al.(2024)Wu, Zheng, and Yu}]{wu2024parameterefficient}
Haoyuan Wu, Haisheng Zheng, and Bei Yu. 2024.
\newblock \href {http://arxiv.org/abs/2401.02731} {Parameter-efficient sparsity crafting from dense to mixture-of-experts for instruction tuning on general tasks}.

\bibitem[{Wu et~al.(2022)Wu, Liu, Chen, Chen, Dai, and Yuan}]{wu2022residual}
Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, and Lu~Yuan. 2022.
\newblock \href {http://arxiv.org/abs/2204.09636} {Residual mixture of experts}.

\bibitem[{Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang}]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.
\newblock \href {http://arxiv.org/abs/2304.12244} {Wizardlm: Empowering large language models to follow complex instructions}.

\bibitem[{Xue et~al.(2022)Xue, He, Ren, Lou, and You}]{xue2022student}
Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. 2022.
\newblock \href {http://arxiv.org/abs/2201.10890} {One student knows all experts know: From sparse to dense}.

\bibitem[{Xue et~al.(2024)Xue, Zheng, Fu, Ni, Zheng, Zhou, and You}]{xue2024openmoe}
Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. 2024.
\newblock Openmoe: An early effort on open mixture-of-experts language models.
\newblock \emph{arXiv preprint arXiv:2402.01739}.

\bibitem[{Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu}]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024.
\newblock \href {http://arxiv.org/abs/2401.02385} {Tinyllama: An open-source small language model}.

\bibitem[{Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, and Wang}]{zhang2023instruction}
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023.
\newblock \href {http://arxiv.org/abs/2308.10792} {Instruction tuning for large language models: A survey}.

\end{thebibliography}
