[
    {
        "content": "X FT: Unlocking the Power of Code Instruction Tuning",
        "page_index": 0,
        "bbox": [
            129.152,
            749.1406442,
            466.1261597999999,
            763.7020372000001
        ]
    },
    {
        "content": "by Simply Merging Upcycled Mixture\u2212of\u2212Experts",
        "page_index": 0,
        "bbox": [
            147.219,
            733.2006441999999,
            448.05881400000004,
            747.5468442
        ]
    },
    {
        "content": "Yifeng Ding, Jiawei Liu, Yuxiang Wei, Terry Yue Zhuo, Lingming Zhang",
        "page_index": 0,
        "bbox": [
            112.245,
            694.8883632,
            483.03552799999994,
            706.8435632
        ]
    },
    {
        "content": "University of Illinois Urbana\u2212Champaign",
        "page_index": 0,
        "bbox": [
            199.09199999999998,
            680.8576767999999,
            396.1854272,
            692.8128767999999
        ]
    },
    {
        "content": "{yifeng6, lingming}@illinois.edu",
        "page_index": 0,
        "bbox": [
            201.99599999999998,
            667.4117951999999,
            393.2791999999999,
            679.3669951999999
        ]
    },
    {
        "content": "4",
        "page_index": 0,
        "bbox": [
            16.34,
            569.74,
            36.34,
            579.74
        ]
    },
    {
        "content": "2",
        "page_index": 0,
        "bbox": [
            16.34,
            559.74,
            36.34,
            569.74
        ]
    },
    {
        "content": "0",
        "page_index": 0,
        "bbox": [
            16.34,
            549.74,
            36.34,
            559.74
        ]
    },
    {
        "content": "2",
        "page_index": 0,
        "bbox": [
            16.34,
            539.74,
            36.34,
            549.74
        ]
    },
    {
        "content": "r",
        "page_index": 0,
        "bbox": [
            16.34,
            528.0799999999999,
            36.34,
            534.7399999999999
        ]
    },
    {
        "content": "Ap",
        "page_index": 0,
        "bbox": [
            16.34,
            503.64,
            36.34,
            528.0799999999999
        ]
    },
    {
        "content": "3",
        "page_index": 0,
        "bbox": [
            16.34,
            488.64,
            36.34,
            498.64
        ]
    },
    {
        "content": "2",
        "page_index": 0,
        "bbox": [
            16.34,
            478.64,
            36.34,
            488.64
        ]
    },
    {
        "content": "] ",
        "page_index": 0,
        "bbox": [
            16.34,
            461.98,
            36.34,
            473.64
        ]
    },
    {
        "content": "L",
        "page_index": 0,
        "bbox": [
            16.34,
            449.76,
            36.34,
            461.98
        ]
    },
    {
        "content": "C",
        "page_index": 0,
        "bbox": [
            16.34,
            436.41999999999996,
            36.34,
            449.75999999999993
        ]
    },
    {
        "content": ".",
        "page_index": 0,
        "bbox": [
            16.34,
            431.41999999999996,
            36.34,
            436.41999999999996
        ]
    },
    {
        "content": "s",
        "page_index": 0,
        "bbox": [
            16.34,
            423.64,
            36.34,
            431.41999999999996
        ]
    },
    {
        "content": "[c",
        "page_index": 0,
        "bbox": [
            16.34,
            408.1,
            36.34,
            423.64
        ]
    },
    {
        "content": "1",
        "page_index": 0,
        "bbox": [
            16.34,
            388.1,
            36.34,
            398.1
        ]
    },
    {
        "content": "v",
        "page_index": 0,
        "bbox": [
            16.34,
            378.1,
            36.34,
            388.1
        ]
    },
    {
        "content": "7",
        "page_index": 0,
        "bbox": [
            16.34,
            368.1,
            36.34,
            378.1
        ]
    },
    {
        "content": "4",
        "page_index": 0,
        "bbox": [
            16.34,
            358.1,
            36.34,
            368.1
        ]
    },
    {
        "content": "2",
        "page_index": 0,
        "bbox": [
            16.34,
            348.1,
            36.34,
            358.1
        ]
    },
    {
        "content": "5",
        "page_index": 0,
        "bbox": [
            16.34,
            338.1,
            36.34,
            348.1
        ]
    },
    {
        "content": "1",
        "page_index": 0,
        "bbox": [
            16.34,
            328.1,
            36.34,
            338.1
        ]
    },
    {
        "content": ".",
        "page_index": 0,
        "bbox": [
            16.34,
            323.1,
            36.34,
            328.1
        ]
    },
    {
        "content": "4",
        "page_index": 0,
        "bbox": [
            16.34,
            313.1,
            36.34,
            323.1
        ]
    },
    {
        "content": "0",
        "page_index": 0,
        "bbox": [
            16.34,
            303.1,
            36.34,
            313.1
        ]
    },
    {
        "content": "4",
        "page_index": 0,
        "bbox": [
            16.34,
            293.1,
            36.34,
            303.1
        ]
    },
    {
        "content": "2",
        "page_index": 0,
        "bbox": [
            16.34,
            283.1,
            36.34,
            293.1
        ]
    },
    {
        "content": ":",
        "page_index": 0,
        "bbox": [
            16.34,
            277.54,
            36.34,
            283.1
        ]
    },
    {
        "content": "v",
        "page_index": 0,
        "bbox": [
            16.34,
            267.54,
            36.34,
            277.54
        ]
    },
    {
        "content": "i",
        "page_index": 0,
        "bbox": [
            16.34,
            261.98,
            36.34,
            267.54
        ]
    },
    {
        "content": "X",
        "page_index": 0,
        "bbox": [
            16.34,
            247.54,
            36.34,
            261.98
        ]
    },
    {
        "content": "r",
        "page_index": 0,
        "bbox": [
            16.34,
            240.88,
            36.34,
            247.54
        ]
    },
    {
        "content": "a",
        "page_index": 0,
        "bbox": [
            16.34,
            232.0,
            36.34,
            240.88
        ]
    },
    {
        "content": "Abstract",
        "page_index": 0,
        "bbox": [
            157.75799999999998,
            608.5253632,
            202.2432992,
            620.4805632
        ]
    },
    {
        "content": "We introduce X FT, a simple yet powerful",
        "page_index": 0,
        "bbox": [
            87.874,
            590.8810784,
            272.129491112,
            601.0628556
        ]
    },
    {
        "content": "training scheme, by simply merging upcycled",
        "page_index": 0,
        "bbox": [
            87.874,
            578.9260783999999,
            272.1254626190001,
            588.8886784
        ]
    },
    {
        "content": "Mixture\u2212of\u2212Experts (MoE) to unleash the per\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            566.9710784,
            273.7753986928001,
            576.9336784000001
        ]
    },
    {
        "content": "formance limit of instruction\u2212tuned code Large",
        "page_index": 0,
        "bbox": [
            87.874,
            555.0160784,
            272.1239383412001,
            564.9786784
        ]
    },
    {
        "content": "Language Models (LLMs). While vanilla",
        "page_index": 0,
        "bbox": [
            87.874,
            543.0610783999999,
            272.1287004640001,
            553.0236784
        ]
    },
    {
        "content": "sparse upcycling fails to improve instruction",
        "page_index": 0,
        "bbox": [
            87.874,
            531.1060784,
            272.1287004640001,
            541.0686784000001
        ]
    },
    {
        "content": "tuning, X FT introduces a shared expert mecha\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            519.1500784,
            273.77939040100006,
            529.3318556
        ]
    },
    {
        "content": "nism with a novel routing weight normalization",
        "page_index": 0,
        "bbox": [
            87.874,
            507.1950784,
            272.127903456,
            517.1576784
        ]
    },
    {
        "content": "strategy into sparse upcycling, which signif\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            495.2400784,
            273.7749204880001,
            505.20267839999997
        ]
    },
    {
        "content": "icantly boosts instruction tuning. After fine\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            483.28507840000003,
            273.774920488,
            493.24767840000004
        ]
    },
    {
        "content": "tuning the upcycled MoE model, X FT intro\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            471.33007840000005,
            273.779309068,
            481.51185560000005
        ]
    },
    {
        "content": "duces a learnable model merging mechanism",
        "page_index": 0,
        "bbox": [
            87.874,
            459.3750784,
            272.12870046400013,
            469.33767839999996
        ]
    },
    {
        "content": "to compile the upcycled MoE back to a dense",
        "page_index": 0,
        "bbox": [
            87.874,
            447.41907840000005,
            272.1223841756001,
            457.38167840000006
        ]
    },
    {
        "content": "model, achieving upcycled MoE\u2212level perfor\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            435.4640784,
            273.7749204880001,
            445.4266784
        ]
    },
    {
        "content": "mance with only dense\u2212model compute. By",
        "page_index": 0,
        "bbox": [
            87.874,
            423.5090784,
            272.474203432,
            433.4716784
        ]
    },
    {
        "content": "applying X FT to a 1.3B model, we create a",
        "page_index": 0,
        "bbox": [
            87.874,
            411.55407840000004,
            272.1279094,
            421.73585560000004
        ]
    },
    {
        "content": "new state\u2212of\u2212the\u2212art tiny code LLM (<3B) with",
        "page_index": 0,
        "bbox": [
            87.874,
            399.5990784,
            272.12238417560013,
            409.5616784
        ]
    },
    {
        "content": "67.1 and 64.6 pass@1 on HumanEval and Hu\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            387.64307840000004,
            273.77991108699996,
            397.6056784
        ]
    },
    {
        "content": "manEval+ respectively. With the same data and",
        "page_index": 0,
        "bbox": [
            87.874,
            375.6880784,
            272.127903456,
            385.65067839999995
        ]
    },
    {
        "content": "model architecture, X FT improves supervised",
        "page_index": 0,
        "bbox": [
            87.874,
            363.7330784,
            272.12966525160004,
            373.9148556
        ]
    },
    {
        "content": "fine\u2212tuning (SFT) by 13% on HumanEval+,",
        "page_index": 0,
        "bbox": [
            87.874,
            351.7780784,
            273.36844640800007,
            361.7406784
        ]
    },
    {
        "content": "along with consistent improvements from 2%",
        "page_index": 0,
        "bbox": [
            87.874,
            339.82307840000004,
            272.95181047600005,
            349.78567840000005
        ]
    },
    {
        "content": "to 13% on MBPP+, MultiPL\u2212E, and DS\u22121000,",
        "page_index": 0,
        "bbox": [
            87.874,
            327.8680784,
            273.3753803776001,
            337.8306784
        ]
    },
    {
        "content": "demonstrating its generalizability. X FT is",
        "page_index": 0,
        "bbox": [
            87.874,
            315.91207840000004,
            272.130745156,
            326.09385560000004
        ]
    },
    {
        "content": "fully orthogonal to existing techniques such",
        "page_index": 0,
        "bbox": [
            87.874,
            303.9570784,
            272.1287004640001,
            313.91967839999995
        ]
    },
    {
        "content": "as Evol\u2212Instruct and OSS\u2212INSTRUCT, open\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            292.0020784,
            273.778550064,
            301.9646784
        ]
    },
    {
        "content": "ing a new dimension for improving code in\u2212",
        "page_index": 0,
        "bbox": [
            87.874,
            280.04707840000003,
            273.774920488,
            290.0096784
        ]
    },
    {
        "content": "struction tuning. Codes are available at https:",
        "page_index": 0,
        "bbox": [
            87.874,
            268.09207840000005,
            274.6158,
            278.47310760000005
        ]
    },
    {
        "content": "//github.com/ise\u2212uiuc/xft.",
        "page_index": 0,
        "bbox": [
            87.376,
            256.13707840000006,
            214.89765,
            266.51810760000006
        ]
    },
    {
        "content": "1 Introduction",
        "page_index": 0,
        "bbox": [
            70.86600000000001,
            235.80736320000005,
            153.67967040000002,
            247.76256320000005
        ]
    },
    {
        "content": "Figure 1: Overview of SFT, sparse upcycling, and X FT.",
        "page_index": 0,
        "bbox": [
            306.142,
            496.5460784,
            524.4096641159999,
            506.7278556
        ]
    },
    {
        "content": "tion dataset of instruction\u2212output pairs, where the",
        "page_index": 0,
        "bbox": [
            306.142,
            474.82263439999997,
            524.4051746131,
            485.7317344
        ]
    },
    {
        "content": "instruction reflects human intents in natural lan\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            461.2736344,
            526.2173833960001,
            472.1827344
        ]
    },
    {
        "content": "guage and the output includes explained code snip\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            447.7246344,
            526.2198379435,
            458.63373440000004
        ]
    },
    {
        "content": "pets that correspond to the intent; and (ii) super\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            434.1746344,
            526.217383396,
            445.0837344
        ]
    },
    {
        "content": "vised fine\u2212tuning of pre\u2212trained LLM on the in\u2212",
        "page_index": 0,
        "bbox": [
            305.869,
            420.6256344,
            526.222565446,
            431.53473440000005
        ]
    },
    {
        "content": "struction dataset. In the realm of code, multiple",
        "page_index": 0,
        "bbox": [
            306.142,
            407.0766344,
            524.414763712,
            417.9857344
        ]
    },
    {
        "content": "instruction\u2212tuning methods have been proposed",
        "page_index": 0,
        "bbox": [
            306.142,
            393.5276344,
            524.414763712,
            404.43673440000003
        ]
    },
    {
        "content": "to curate high\u2212quality instruction datasets. For",
        "page_index": 0,
        "bbox": [
            306.142,
            379.9786344,
            524.592800224,
            390.8877344
        ]
    },
    {
        "content": "example, Code Evol\u2212Instruct (Luo et al., 2023)",
        "page_index": 0,
        "bbox": [
            306.142,
            366.4286344,
            525.135050594,
            377.58864370000003
        ]
    },
    {
        "content": "uses ChatGPT to obtain complex synthetic code",
        "page_index": 0,
        "bbox": [
            306.142,
            352.8796344,
            524.414763712,
            363.7887344
        ]
    },
    {
        "content": "instructions with heuristic prompts, while OSS\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            339.3306344,
            526.209912394,
            350.23973440000003
        ]
    },
    {
        "content": "INSTRUCT (Wei et al., 2023) prompts ChatGPT to",
        "page_index": 0,
        "bbox": [
            306.415,
            325.7816344,
            524.4127185798001,
            336.6907344
        ]
    },
    {
        "content": "generate new coding problems by drawing inspira\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            312.2326344,
            526.2246270384001,
            323.1417344
        ]
    },
    {
        "content": "tion from open source code snippets. While exist\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            298.6826344,
            526.2210379445,
            309.5917344
        ]
    },
    {
        "content": "ing work focuses on the data perspectives of instruc\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            285.1336344,
            526.2252379480001,
            296.04273440000003
        ]
    },
    {
        "content": "tion tuning, they all follow the standard SFT, leav\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            271.58463439999997,
            526.2251070387999,
            282.4937344
        ]
    },
    {
        "content": "ing room for exploring advanced training schemes.",
        "page_index": 0,
        "bbox": [
            306.142,
            258.0356344,
            526.3217834830001,
            268.9447344
        ]
    },
    {
        "content": "We argue that prior works largely overlook the",
        "page_index": 0,
        "bbox": [
            317.051,
            242.8386344,
            524.40790007,
            253.74773439999998
        ]
    },
    {
        "content": "possibility of improving the code instruction tun\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            229.28963439999998,
            526.2173833960001,
            240.19873439999998
        ]
    },
    {
        "content": "Program synthesis (or code generation) is a long\u2212",
        "page_index": 0,
        "bbox": [
            70.866,
            215.7406344,
            290.9413833960001,
            226.6497344
        ]
    },
    {
        "content": "ing by advancing the training schemes. Figure 1",
        "page_index": 0,
        "bbox": [
            306.142,
            215.7406344,
            525.227055298,
            226.6497344
        ]
    },
    {
        "content": "standing problem explored since the early days of",
        "page_index": 0,
        "bbox": [
            70.866,
            202.1916344,
            289.1389164394001,
            213.1007344
        ]
    },
    {
        "content": "depicts the supervised fine\u2212tuning (SFT), which",
        "page_index": 0,
        "bbox": [
            306.142,
            202.1916344,
            524.414763712,
            213.1007344
        ]
    },
    {
        "content": "computer science (Manna and Waldinger, 1971).",
        "page_index": 0,
        "bbox": [
            70.866,
            188.6416344,
            291.04152893400004,
            199.55073439999998
        ]
    },
    {
        "content": "directly starts with the pre\u2212trained weights and ar\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            188.6416344,
            526.224583402,
            199.55073439999998
        ]
    },
    {
        "content": "Recently, instruction tuning of code Large Lan\u2212",
        "page_index": 0,
        "bbox": [
            70.866,
            175.0926344,
            290.941383396,
            186.0017344
        ]
    },
    {
        "content": "chitecture for fine\u2212tuning. The model is dense here",
        "page_index": 0,
        "bbox": [
            306.142,
            175.0926344,
            524.4099426834999,
            186.25264370000002
        ]
    },
    {
        "content": "guage Models (LLMs) has been used to improve",
        "page_index": 0,
        "bbox": [
            70.866,
            161.5436344,
            289.1387637120001,
            172.4527344
        ]
    },
    {
        "content": "because all parameters are activated to compute the",
        "page_index": 0,
        "bbox": [
            306.142,
            161.5436344,
            524.407781888,
            172.4527344
        ]
    },
    {
        "content": "many coding tasks (Chaudhary, 2023; Luo et al.,",
        "page_index": 0,
        "bbox": [
            70.866,
            147.9946344,
            290.496292116,
            158.9037344
        ]
    },
    {
        "content": "next token (assuming it is a decoder\u2212only LLM). In",
        "page_index": 0,
        "bbox": [
            306.142,
            147.9946344,
            524.4077818879999,
            158.9037344
        ]
    },
    {
        "content": "2023; Wei et al., 2023), such as text\u2212to\u2212code gener\u2212",
        "page_index": 0,
        "bbox": [
            70.866,
            134.4456344,
            290.949237948,
            145.35473439999998
        ]
    },
    {
        "content": "contrast to fine\u2212tuning a dense model, following the",
        "page_index": 0,
        "bbox": [
            306.142,
            134.4456344,
            524.4087282859999,
            145.6056437
        ]
    },
    {
        "content": "ation (Chen et al., 2021; Austin et al., 2021), code",
        "page_index": 0,
        "bbox": [
            70.866,
            120.89563439999999,
            289.13514189080007,
            131.8047344
        ]
    },
    {
        "content": "scaling laws (Kaplan et al., 2020) (i.e., more param\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            120.89563439999999,
            526.219827308,
            132.0556437
        ]
    },
    {
        "content": "completion (Cassano et al., 2022), and data science",
        "page_index": 0,
        "bbox": [
            70.866,
            107.3466344,
            289.1317818880001,
            118.25573440000001
        ]
    },
    {
        "content": "eters, better performance), sparse upcycling (Ko\u2212",
        "page_index": 0,
        "bbox": [
            306.142,
            107.3466344,
            526.2173833960001,
            118.25573440000001
        ]
    },
    {
        "content": "engineering (Lai et al., 2022).",
        "page_index": 0,
        "bbox": [
            70.866,
            93.79763439999999,
            200.8370174,
            104.7067344
        ]
    },
    {
        "content": "matsuzaki et al., 2023) is proposed to efficiently",
        "page_index": 0,
        "bbox": [
            306.142,
            93.79763439999999,
            524.7930913,
            104.7067344
        ]
    },
    {
        "content": "A typical instruction tuning flow involves two",
        "page_index": 0,
        "bbox": [
            81.775,
            80.2486344,
            289.13190007,
            91.15773440000001
        ]
    },
    {
        "content": "upgrade the model sizes by upcycling a dense LLM",
        "page_index": 0,
        "bbox": [
            306.142,
            80.2486344,
            524.4077818879999,
            91.15773440000001
        ]
    },
    {
        "content": "steps (Zhang et al., 2023): (i) curating an instruc\u2212",
        "page_index": 0,
        "bbox": [
            70.866,
            66.6996344,
            290.9413833960001,
            77.6087344
        ]
    },
    {
        "content": "to a sparsely activated Mixture\u2212of\u2212Experts (MoE)",
        "page_index": 0,
        "bbox": [
            306.142,
            66.6996344,
            525.141746136,
            77.6087344
        ]
    },
    {
        "content": "1",
        "page_index": 0,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "model. An MoE model is efficient because the",
        "page_index": 1,
        "bbox": [
            70.866,
            757.7086344,
            289.13876371200007,
            768.6177344
        ]
    },
    {
        "content": "\u2022 Dimension: We open a new dimension of im\u2212",
        "page_index": 1,
        "bbox": [
            308.251,
            757.7086344,
            526.2156888459999,
            768.6940981
        ]
    },
    {
        "content": "generation of the next token only involves a subset",
        "page_index": 1,
        "bbox": [
            70.866,
            744.1596344,
            289.13514189080007,
            755.0687343999999
        ]
    },
    {
        "content": "proving instruction tuning of code LLMs by",
        "page_index": 1,
        "bbox": [
            317.051,
            744.1596344,
            524.7862276579999,
            755.0687343999999
        ]
    },
    {
        "content": "of parameters (i.e., experts) and thus is sparsely",
        "page_index": 1,
        "bbox": [
            70.866,
            730.6096344,
            289.13195808600005,
            741.7696437
        ]
    },
    {
        "content": "advancing its training scheme, using enhanced",
        "page_index": 1,
        "bbox": [
            317.051,
            730.6096344,
            524.40790007,
            741.5187344
        ]
    },
    {
        "content": "activated. For example, Mixtral\u22128x7B (Jiang et al.,",
        "page_index": 1,
        "bbox": [
            70.866,
            717.0606344,
            290.4923464588,
            728.2206437
        ]
    },
    {
        "content": "sparse upcycling and learnable model merging",
        "page_index": 1,
        "bbox": [
            317.051,
            717.0606344,
            524.40790007,
            727.9697344
        ]
    },
    {
        "content": "2024), compared to a dense 7B model, uses approx\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            703.5116344,
            290.949237948,
            714.4207344
        ]
    },
    {
        "content": "mechanism, which neither changes the final",
        "page_index": 1,
        "bbox": [
            317.051,
            703.5116344,
            524.40790007,
            714.4207344
        ]
    },
    {
        "content": "imately 8\u00d7 parameters and 2\u00d7 computation, i.e.,",
        "page_index": 1,
        "bbox": [
            70.866,
            689.9626344,
            291.03944987399996,
            701.1226436999999
        ]
    },
    {
        "content": "model structure nor requires more training data.",
        "page_index": 1,
        "bbox": [
            317.051,
            689.9626344,
            525.7966285,
            700.8717343999999
        ]
    },
    {
        "content": "only 2 out of 8 experts are dynamically selected to",
        "page_index": 1,
        "bbox": [
            70.866,
            676.4136344,
            289.13843643900003,
            687.3227344
        ]
    },
    {
        "content": "compute the next token. However, there are two",
        "page_index": 1,
        "bbox": [
            70.866,
            662.8636344,
            289.13876371199996,
            673.7727344
        ]
    },
    {
        "content": "key limitations when using sparse upcycling in in\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            649.3146344,
            290.95028522160004,
            660.2237344
        ]
    },
    {
        "content": "struction tuning: (i) Slow scaling: Komatsuzaki",
        "page_index": 1,
        "bbox": [
            70.866,
            635.7656344,
            289.138339526,
            646.9256436999999
        ]
    },
    {
        "content": "et al. (2023) show that sparse upcycling improves",
        "page_index": 1,
        "bbox": [
            70.866,
            622.2166344,
            289.13189097900005,
            633.1257343999999
        ]
    },
    {
        "content": "the dense SFT marginally at the early phase, re\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            608.6676344,
            290.94138339599994,
            619.5767344
        ]
    },
    {
        "content": "quiring orders of magnitude of extra compute to",
        "page_index": 1,
        "bbox": [
            70.866,
            595.1176344,
            289.13876371200007,
            606.0267344
        ]
    },
    {
        "content": "achieve decent improvement; and (ii) Inference",
        "page_index": 1,
        "bbox": [
            70.866,
            581.5686344,
            289.13703468,
            592.7286436999999
        ]
    },
    {
        "content": "cost: though MoE is more efficient than directly",
        "page_index": 1,
        "bbox": [
            70.866,
            568.0196344,
            289.51210588200007,
            579.1796436999999
        ]
    },
    {
        "content": "scaling the size of dense LLMs, MoE is still ex\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            554.4706344,
            290.941383396,
            565.3797344
        ]
    },
    {
        "content": "pensive, especially at inference, as it introduces",
        "page_index": 1,
        "bbox": [
            70.866,
            540.9216344,
            289.138763712,
            551.8307344
        ]
    },
    {
        "content": "significantly more parameters (i.e., memory) and,",
        "page_index": 1,
        "bbox": [
            70.866,
            527.3716344,
            290.4971537316,
            538.5316436999999
        ]
    },
    {
        "content": "more importantly, computes during inference, com\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            513.8226344,
            290.949237948,
            524.7317343999999
        ]
    },
    {
        "content": "pared to its dense counterparts.",
        "page_index": 1,
        "bbox": [
            70.866,
            500.2736344,
            206.2915674,
            511.1827344
        ]
    },
    {
        "content": "In this paper, we propose X FT: by simply",
        "page_index": 1,
        "bbox": [
            81.775,
            486.7246344,
            289.51550077,
            497.87373460000003
        ]
    },
    {
        "content": "merging upcycled MoE models, we push the per\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            473.1756344,
            290.94138339600005,
            484.0847344
        ]
    },
    {
        "content": "formance limit of instruction\u2212tuned code LLMs.",
        "page_index": 1,
        "bbox": [
            70.866,
            459.6256344,
            291.04152893400004,
            470.53473440000005
        ]
    },
    {
        "content": "While vanilla sparse upcycling fails to improve",
        "page_index": 1,
        "bbox": [
            70.353,
            446.0766344,
            289.137618684,
            456.9857344
        ]
    },
    {
        "content": "instruction tuning efficiently (Komatsuzaki et al.,",
        "page_index": 1,
        "bbox": [
            70.866,
            432.5276344,
            290.49629211599995,
            443.43673440000003
        ]
    },
    {
        "content": "2023), X FT addresses this challenge by isolating",
        "page_index": 1,
        "bbox": [
            70.866,
            418.9786344,
            289.13551149000006,
            430.1277346
        ]
    },
    {
        "content": "one expert as the shared expert among all the other",
        "page_index": 1,
        "bbox": [
            70.866,
            405.4296344,
            289.3173784063001,
            416.3387344
        ]
    },
    {
        "content": "experts in each MoE layer, inspired by DeepSeek\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            391.8796344,
            290.9487797658,
            402.7887344
        ]
    },
    {
        "content": "MoE (Dai et al., 2024) and MoCLE (Gou et al.,",
        "page_index": 1,
        "bbox": [
            70.866,
            378.3306344,
            290.49629211599995,
            389.23973440000003
        ]
    },
    {
        "content": "2024). X FT also includes a novel routing weight",
        "page_index": 1,
        "bbox": [
            70.866,
            364.7816344,
            289.135583494,
            375.9307346
        ]
    },
    {
        "content": "normalization strategy to eliminate scale mismatch",
        "page_index": 1,
        "bbox": [
            70.866,
            351.2326344,
            289.138000075,
            362.1417344
        ]
    },
    {
        "content": "between the upcycled MoE layer with the shared",
        "page_index": 1,
        "bbox": [
            70.866,
            337.6836344,
            289.138763712,
            348.59273440000004
        ]
    },
    {
        "content": "expert and the original dense layer, which will oth\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            324.1336344,
            290.948583402,
            335.04273440000003
        ]
    },
    {
        "content": "erwise lead to performance degradation (Wu et al.,",
        "page_index": 1,
        "bbox": [
            70.866,
            310.58463439999997,
            290.497055753,
            321.4937344
        ]
    },
    {
        "content": "2022). After the upcycled MoE model finishes",
        "page_index": 1,
        "bbox": [
            70.866,
            297.0356344,
            289.138763712,
            307.9447344
        ]
    },
    {
        "content": "the SFT phase, motivated by Model Soups (Worts\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            283.4866344,
            290.9420924875,
            294.39573440000004
        ]
    },
    {
        "content": "man et al., 2022), X FT uses a learnable model",
        "page_index": 1,
        "bbox": [
            70.866,
            269.9376344,
            289.13846558999995,
            281.0867346
        ]
    },
    {
        "content": "merging mechanism to output a dense model by",
        "page_index": 1,
        "bbox": [
            70.866,
            256.3876344,
            289.51709130000006,
            267.29673440000005
        ]
    },
    {
        "content": "merging all the expert networks in the upcycled",
        "page_index": 1,
        "bbox": [
            70.866,
            242.8386344,
            289.13876371200007,
            253.74773439999998
        ]
    },
    {
        "content": "MoE, i.e., the final dense model is of the same",
        "page_index": 1,
        "bbox": [
            70.866,
            229.28963439999998,
            289.13692219800004,
            240.4496437
        ]
    },
    {
        "content": "model structure and size as the original pre\u2212trained",
        "page_index": 1,
        "bbox": [
            70.866,
            215.7406344,
            289.1316946152,
            226.6497344
        ]
    },
    {
        "content": "model, achieving similar performance without pay\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            202.1916344,
            290.9484524928,
            213.1007344
        ]
    },
    {
        "content": "ing extra inference cost as the sparse upcycling.",
        "page_index": 1,
        "bbox": [
            70.866,
            188.6416344,
            291.04152893400004,
            199.55073439999998
        ]
    },
    {
        "content": "With only 1.3B parameters, X FT achieves 67.1",
        "page_index": 1,
        "bbox": [
            70.353,
            175.0926344,
            289.95688374,
            186.2417346
        ]
    },
    {
        "content": "pass@1 on HumanEval and 64.6 pass@1 on Hu\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            161.5436344,
            290.944644674,
            172.4527344
        ]
    },
    {
        "content": "manEval+, which is the new state\u2212of\u2212the\u2212art for",
        "page_index": 1,
        "bbox": [
            70.866,
            147.9946344,
            289.31680022399996,
            158.9037344
        ]
    },
    {
        "content": "tiny code LLMs (<3B). Compared with SFT, X FT",
        "page_index": 1,
        "bbox": [
            70.866,
            134.4456344,
            289.47234142269997,
            145.59473459999998
        ]
    },
    {
        "content": "achieves 13% improvement on HumanEval+. Sur\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            120.89563439999999,
            290.94991431220006,
            131.8047344
        ]
    },
    {
        "content": "prisingly, our model merging mechanism can pre\u2212",
        "page_index": 1,
        "bbox": [
            70.866,
            107.3466344,
            290.9450379445,
            118.25573440000001
        ]
    },
    {
        "content": "serve or even further boost the general performance",
        "page_index": 1,
        "bbox": [
            70.866,
            93.79763439999999,
            289.13178188800003,
            104.7067344
        ]
    },
    {
        "content": "of the upcycled MoE with around 1/8\u00d7 parameters!",
        "page_index": 1,
        "bbox": [
            70.866,
            80.2486344,
            289.49413295999994,
            91.3977346
        ]
    },
    {
        "content": "We conclude our contribution as follows:",
        "page_index": 1,
        "bbox": [
            70.353,
            66.6996344,
            249.8731496,
            77.6087344
        ]
    },
    {
        "content": "\u2022 Technique: We present X FT, a new training",
        "page_index": 1,
        "bbox": [
            308.251,
            674.4206343999999,
            524.408743812,
            685.5697346000001
        ]
    },
    {
        "content": "scheme for code instruction tuning. X FT in\u2212",
        "page_index": 1,
        "bbox": [
            317.051,
            660.8716344,
            526.2171700179999,
            672.0207346
        ]
    },
    {
        "content": "volves two steps: upcycling and merging. A pre\u2212",
        "page_index": 1,
        "bbox": [
            316.778,
            647.3226344,
            526.2197046366999,
            658.4826436999999
        ]
    },
    {
        "content": "trained dense LLM is first upcycled into an MoE",
        "page_index": 1,
        "bbox": [
            317.051,
            633.7726344,
            524.4120455280001,
            644.6817344
        ]
    },
    {
        "content": "with the shared expert setting and then fine\u2212tuned",
        "page_index": 1,
        "bbox": [
            316.658,
            620.2236344,
            524.414609494,
            631.1327344
        ]
    },
    {
        "content": "on the instruction dataset. To avoid the perfor\u2212",
        "page_index": 1,
        "bbox": [
            317.051,
            606.6746344,
            526.2216470359999,
            617.5837343999999
        ]
    },
    {
        "content": "mance degradation caused by the scale mismatch",
        "page_index": 1,
        "bbox": [
            317.051,
            593.1256344,
            524.412045528,
            604.0347343999999
        ]
    },
    {
        "content": "issue, we propose a novel routing weight nor\u2212",
        "page_index": 1,
        "bbox": [
            317.051,
            579.5766344,
            526.221647036,
            590.4857344
        ]
    },
    {
        "content": "malization strategy. In addition, we introduce",
        "page_index": 1,
        "bbox": [
            317.051,
            566.0266344,
            524.40790007,
            576.9357344
        ]
    },
    {
        "content": "the first learnable mechanism for merging the",
        "page_index": 1,
        "bbox": [
            317.051,
            552.4776343999999,
            524.40790007,
            563.3867343999999
        ]
    },
    {
        "content": "upcycled MoE into a dense model, eliminating",
        "page_index": 1,
        "bbox": [
            317.051,
            538.9286344,
            524.4047037037,
            549.8377343999999
        ]
    },
    {
        "content": "additional inference overhead while preserving",
        "page_index": 1,
        "bbox": [
            317.051,
            525.3796344,
            524.4044527944,
            536.2887344
        ]
    },
    {
        "content": "or even improving the MoE performance.",
        "page_index": 1,
        "bbox": [
            317.051,
            511.8306344,
            498.2293328,
            522.7397344
        ]
    },
    {
        "content": "\u2022 Results: With only 1.3B parameters, X FT",
        "page_index": 1,
        "bbox": [
            308.251,
            496.2886344,
            524.747538094,
            507.4377346
        ]
    },
    {
        "content": "achieves 67.1 pass@1 on HumanEval and 64.6",
        "page_index": 1,
        "bbox": [
            317.051,
            482.7396344,
            524.4106060449,
            493.6487344
        ]
    },
    {
        "content": "pass@1 on HumanEval+, which is the new state\u2212",
        "page_index": 1,
        "bbox": [
            317.051,
            469.1906344,
            526.215979996,
            480.09973440000005
        ]
    },
    {
        "content": "of\u2212the\u2212art for tiny code LLMs (<3B). Compared",
        "page_index": 1,
        "bbox": [
            317.051,
            455.6406344,
            524.4111728,
            466.54973440000003
        ]
    },
    {
        "content": "with normal supervised fine\u2212tuning (SFT), X FT",
        "page_index": 1,
        "bbox": [
            316.658,
            442.0916344,
            524.7478032621,
            453.2407346
        ]
    },
    {
        "content": "achieves 13% improvement on HumanEval+!",
        "page_index": 1,
        "bbox": [
            317.051,
            428.5426344,
            524.775100376,
            439.4517344
        ]
    },
    {
        "content": "X FT also achieves a consistent improvement",
        "page_index": 1,
        "bbox": [
            317.051,
            414.9936344,
            524.406601344,
            426.14273460000004
        ]
    },
    {
        "content": "from 2% to 13% on MBPP, MultiPL\u2212E, and DS\u2212",
        "page_index": 1,
        "bbox": [
            317.051,
            401.4446344,
            526.2245270384,
            412.3537344
        ]
    },
    {
        "content": "1000 over SFT, demonstrating its generalization.",
        "page_index": 1,
        "bbox": [
            316.233,
            387.8946344,
            526.3214950736001,
            398.8037344
        ]
    },
    {
        "content": "2 Related Work",
        "page_index": 1,
        "bbox": [
            306.142,
            362.1703632,
            395.23215039999997,
            374.1255632
        ]
    },
    {
        "content": "2.1 Mixture\u2212of\u2212Experts",
        "page_index": 1,
        "bbox": [
            306.142,
            340.17599809999996,
            421.3748233,
            351.0850981
        ]
    },
    {
        "content": "Mixture\u2212of\u2212Experts (MoE) can efficiently scale up",
        "page_index": 1,
        "bbox": [
            306.142,
            321.5816344,
            524.405381886,
            332.4907344
        ]
    },
    {
        "content": "model sizes with only sub\u2212linear increases in com\u2212",
        "page_index": 1,
        "bbox": [
            306.142,
            308.0316344,
            526.221965218,
            318.9407344
        ]
    },
    {
        "content": "putation (Shazeer et al., 2017). Compared with",
        "page_index": 1,
        "bbox": [
            306.142,
            294.4826344,
            524.414763712,
            305.3917344
        ]
    },
    {
        "content": "the standard Transformer, MoE replaces each Feed\u2212",
        "page_index": 1,
        "bbox": [
            306.142,
            280.9336344,
            526.225237948,
            291.84273440000004
        ]
    },
    {
        "content": "Forward Network (FFN) layer with an MoE layer,",
        "page_index": 1,
        "bbox": [
            306.142,
            267.3846344,
            525.7757612098,
            278.2937344
        ]
    },
    {
        "content": "which uses N (i.e., multiple) expert networks that",
        "page_index": 1,
        "bbox": [
            305.749,
            253.8356344,
            524.4128230486999,
            264.9956437
        ]
    },
    {
        "content": "are structurally equivalent to the original FFN layer",
        "page_index": 1,
        "bbox": [
            306.142,
            240.2856344,
            524.600218412,
            251.1947344
        ]
    },
    {
        "content": "and uses a router that directs each input token to",
        "page_index": 1,
        "bbox": [
            306.142,
            226.73663439999999,
            524.414763712,
            237.64573439999998
        ]
    },
    {
        "content": "K out of N expert networks. Formally, for the l\u2212th",
        "page_index": 1,
        "bbox": [
            306.142,
            213.1876344,
            524.4098499182,
            224.3367346
        ]
    },
    {
        "content": "MoE layer, output hidden state hlt of the t\u2212th input",
        "page_index": 1,
        "bbox": [
            306.142,
            197.7518006,
            524.4077291924,
            212.7799006
        ]
    },
    {
        "content": "token is computed as follows (Dai et al., 2024):",
        "page_index": 1,
        "bbox": [
            306.142,
            186.0896344,
            513.0112633,
            196.9987344
        ]
    },
    {
        "content": "hlt =",
        "page_index": 1,
        "bbox": [
            347.258,
            148.1388006,
            368.39509797999995,
            163.3099006
        ]
    },
    {
        "content": "N(cid:88)",
        "page_index": 1,
        "bbox": [
            371.42499999999995,
            156.19954,
            387.18210403999996,
            172.4419006
        ]
    },
    {
        "content": "(gi,tFFNi(ult)) + ult",
        "page_index": 1,
        "bbox": [
            387.18299999999994,
            148.1388006,
            473.6141273699999,
            163.3099006
        ]
    },
    {
        "content": "gi,t =",
        "page_index": 1,
        "bbox": [
            342.8849999999999,
            111.94480060000001,
            368.39509797999983,
            123.91973460000001
        ]
    },
    {
        "content": "i=1",
        "page_index": 1,
        "bbox": [
            372.45199999999994,
            137.7478006,
            386.1560047699999,
            145.7179006
        ]
    },
    {
        "content": "(cid:40)si,t si,t \u2208 Topk(st, K)",
        "page_index": 1,
        "bbox": [
            371.42499999999984,
            119.54980060000001,
            486.47054898999977,
            138.14564000000001
        ]
    },
    {
        "content": "0 otherwise",
        "page_index": 1,
        "bbox": [
            380.2129999999998,
            104.11663440000001,
            447.4415807999998,
            115.26573460000002
        ]
    },
    {
        "content": "st = {si,t | 1 \u2264 i \u2264 N }",
        "page_index": 1,
        "bbox": [
            348.2099999999998,
            84.83880060000001,
            453.5885499999997,
            96.81373460000002
        ]
    },
    {
        "content": "si,t = Softmaxi(ult",
        "page_index": 1,
        "bbox": [
            342.97499999999974,
            63.39780060000003,
            425.1431273699997,
            78.56890060000002
        ]
    },
    {
        "content": "T eli)",
        "page_index": 1,
        "bbox": [
            425.6409999999997,
            63.397800600000046,
            444.7125489899996,
            80.94890060000003
        ]
    },
    {
        "content": "(1)",
        "page_index": 1,
        "bbox": [
            512.4199999999996,
            112.98163440000005,
            525.1400105999996,
            123.89073440000006
        ]
    },
    {
        "content": "2",
        "page_index": 1,
        "bbox": [
            294.9109999999996,
            36.811634400000045,
            300.3655499999996,
            47.72073440000005
        ]
    },
    {
        "content": "where N refers to the total number of experts, gi,t",
        "page_index": 2,
        "bbox": [
            70.473,
            756.8818006000001,
            288.63488905,
            768.8577346000001
        ]
    },
    {
        "content": "refers to the gate value for the i\u2212th expert, FFNi(\u00b7)",
        "page_index": 2,
        "bbox": [
            70.866,
            743.3328006,
            290.41054899,
            755.3087346
        ]
    },
    {
        "content": "refers to the i\u2212th expert, ult refers to the hidden",
        "page_index": 2,
        "bbox": [
            70.866,
            728.7228006,
            289.132202252,
            743.3489006
        ]
    },
    {
        "content": "states of the t\u2212th token which is the input of the l\u2212th",
        "page_index": 2,
        "bbox": [
            70.866,
            717.0606344,
            289.133609898,
            728.2097346
        ]
    },
    {
        "content": "MoE layer, si,t refers to the affinity score between",
        "page_index": 2,
        "bbox": [
            70.866,
            702.6858006000001,
            289.1331108137,
            714.6607346000001
        ]
    },
    {
        "content": "the i\u2212th expert and the t\u2212th token, Topk(S, K)",
        "page_index": 2,
        "bbox": [
            70.866,
            689.9626344,
            290.41054899,
            701.1117346
        ]
    },
    {
        "content": "refers to a function computing K largest scores",
        "page_index": 2,
        "bbox": [
            70.866,
            676.4136344,
            289.131796436,
            687.5627346
        ]
    },
    {
        "content": "over S, and eli refers to the centroid of the i\u2212th",
        "page_index": 2,
        "bbox": [
            70.866,
            660.6188006,
            289.134410302,
            675.6029006
        ]
    },
    {
        "content": "expert in the l\u2212th MoE layer. By definition, each",
        "page_index": 2,
        "bbox": [
            70.866,
            649.3146344,
            289.135365848,
            660.4637346000001
        ]
    },
    {
        "content": "token will only be assigned to and computed in the",
        "page_index": 2,
        "bbox": [
            70.866,
            635.7656344,
            289.12976370450014,
            646.6747343999999
        ]
    },
    {
        "content": "top K experts among all the N experts.",
        "page_index": 2,
        "bbox": [
            70.866,
            622.2166344,
            243.24311929999996,
            633.3657346
        ]
    },
    {
        "content": "Recently, many works have been proposed to",
        "page_index": 2,
        "bbox": [
            81.775,
            608.5616344,
            289.1319000700001,
            619.4707344
        ]
    },
    {
        "content": "scale model sizes with MoE architecture (Lepikhin",
        "page_index": 2,
        "bbox": [
            70.866,
            595.0126344,
            289.131781888,
            605.9217344
        ]
    },
    {
        "content": "et al., 2020; Du et al., 2022; Fedus et al., 2022;",
        "page_index": 2,
        "bbox": [
            70.866,
            581.4636344,
            290.040073554,
            592.3727344
        ]
    },
    {
        "content": "Jiang et al., 2024; Xue et al., 2024). While most",
        "page_index": 2,
        "bbox": [
            70.659,
            567.9146344,
            289.13205478800006,
            578.8237343999999
        ]
    },
    {
        "content": "MoE models are trained from scratch, sparse up\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            554.3646344,
            290.94138339600005,
            565.2737344
        ]
    },
    {
        "content": "cycling (Komatsuzaki et al., 2023) is proposed to",
        "page_index": 2,
        "bbox": [
            70.866,
            540.8156344,
            289.13876371200007,
            551.7247344
        ]
    },
    {
        "content": "initialize MoE models based on the pre\u2212trained",
        "page_index": 2,
        "bbox": [
            70.866,
            527.2666344,
            289.138763712,
            538.1757344
        ]
    },
    {
        "content": "dense model, which can efficiently reduce the com\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            513.7176344,
            290.949237948,
            524.6267343999999
        ]
    },
    {
        "content": "putational costs of training MoE models, compared",
        "page_index": 2,
        "bbox": [
            70.866,
            500.1686344,
            289.13178188800003,
            511.0777344
        ]
    },
    {
        "content": "with training MoE models from scratch. Specif\u2212",
        "page_index": 2,
        "bbox": [
            70.473,
            486.6186344,
            290.94896554800005,
            497.52773440000004
        ]
    },
    {
        "content": "ically, sparse upcycling constructs a new MoE",
        "page_index": 2,
        "bbox": [
            70.866,
            473.0696344,
            289.138763712,
            483.9787344
        ]
    },
    {
        "content": "model by initializing each expert of each MoE layer",
        "page_index": 2,
        "bbox": [
            70.866,
            459.5206344,
            289.32421841199994,
            470.42973440000003
        ]
    },
    {
        "content": "as a copy of the original FFN layer in the dense",
        "page_index": 2,
        "bbox": [
            70.866,
            445.97163439999997,
            289.13876371200007,
            456.8807344
        ]
    },
    {
        "content": "model, while directly copying the remaining layers",
        "page_index": 2,
        "bbox": [
            70.866,
            432.4226344,
            289.13178188800003,
            443.3317344
        ]
    },
    {
        "content": "from the dense model to the new MoE model.",
        "page_index": 2,
        "bbox": [
            70.866,
            418.8726344,
            270.8734394,
            429.7817344
        ]
    },
    {
        "content": "2.2 Instruction Tuning",
        "page_index": 2,
        "bbox": [
            70.866,
            396.1989981,
            183.2188209,
            407.1080981
        ]
    },
    {
        "content": "Instruction tuning is designed to improve the",
        "page_index": 2,
        "bbox": [
            70.866,
            378.4366344,
            289.13876371200007,
            389.3457344
        ]
    },
    {
        "content": "instruction\u2212following ability of LLMs by fine\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            364.8866344,
            290.941383396,
            375.7957344
        ]
    },
    {
        "content": "tuning them on the instruction datasets in a su\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            351.3376344,
            290.9413833960001,
            362.24673440000004
        ]
    },
    {
        "content": "pervised fashion (Wei et al., 2022). The quality of",
        "page_index": 2,
        "bbox": [
            70.866,
            337.7886344,
            289.1351418908,
            348.6977344
        ]
    },
    {
        "content": "the instruction dataset is significant for the effec\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            324.2396344,
            290.94138339599994,
            335.1487344
        ]
    },
    {
        "content": "tiveness of instruction tuning and researchers have",
        "page_index": 2,
        "bbox": [
            70.866,
            310.6906344,
            289.1348037087,
            321.59973440000005
        ]
    },
    {
        "content": "proposed multiple methods to improve data qual\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            297.1406344,
            290.94138339600005,
            308.04973440000003
        ]
    },
    {
        "content": "ity. For example, SELF\u2212INSTRUCT (Wang et al.,",
        "page_index": 2,
        "bbox": [
            70.866,
            283.5916344,
            290.49197542599995,
            294.5007344
        ]
    },
    {
        "content": "2023) synthesizes high\u2212quality instruction data by",
        "page_index": 2,
        "bbox": [
            70.866,
            270.0426344,
            289.5154767532001,
            280.9517344
        ]
    },
    {
        "content": "prompting a foundation LLM with specially de\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            256.4936344,
            290.9413833960001,
            267.40273440000004
        ]
    },
    {
        "content": "signed prompts. To improve SELF\u2212INSTRUCT,",
        "page_index": 2,
        "bbox": [
            70.866,
            242.94463439999998,
            290.4978205,
            253.85373439999998
        ]
    },
    {
        "content": "Evol\u2212Instruct (Xu et al., 2023) improves the com\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            229.3956344,
            290.94138339600005,
            240.3047344
        ]
    },
    {
        "content": "plexity and diversity of the instruction dataset by",
        "page_index": 2,
        "bbox": [
            70.866,
            215.8456344,
            289.51709130000006,
            226.7547344
        ]
    },
    {
        "content": "prompting ChatGPT with heuristic prompts. OSS\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            202.2966344,
            290.9498972377,
            213.20573439999998
        ]
    },
    {
        "content": "INSTRUCT (Wei et al., 2023) queries ChatGPT to",
        "page_index": 2,
        "bbox": [
            71.139,
            188.7476344,
            289.138066826,
            199.6567344
        ]
    },
    {
        "content": "generate instruction\u2212output pairs by getting inspira\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            175.1986344,
            290.949237948,
            186.1077344
        ]
    },
    {
        "content": "tion from real\u2212world code snippets.",
        "page_index": 2,
        "bbox": [
            70.866,
            161.6496344,
            224.3679461,
            172.5587344
        ]
    },
    {
        "content": "2024) proposes to integrate adapters into MoE",
        "page_index": 2,
        "bbox": [
            306.142,
            757.7086344,
            524.414763712,
            768.6177344
        ]
    },
    {
        "content": "that are upcycled from dense models. Different",
        "page_index": 2,
        "bbox": [
            306.142,
            744.1596344,
            524.4147637120001,
            755.0687343999999
        ]
    },
    {
        "content": "from these works, X FT focuses on full fine\u2212tuning,",
        "page_index": 2,
        "bbox": [
            306.142,
            730.6096344,
            525.7779452059999,
            741.7587346
        ]
    },
    {
        "content": "which generally performs better than parameter\u2212",
        "page_index": 2,
        "bbox": [
            305.749,
            717.0606344,
            526.224965548,
            727.9697344
        ]
    },
    {
        "content": "efficient fine\u2212tuning (Chen et al., 2022).",
        "page_index": 2,
        "bbox": [
            306.142,
            703.5116344,
            479.1821442,
            714.4207344
        ]
    },
    {
        "content": "2.3 Weight Averaging",
        "page_index": 2,
        "bbox": [
            306.142,
            678.3219981000001,
            413.9239079999999,
            689.2310981
        ]
    },
    {
        "content": "Weight averaging is a commonly used technique",
        "page_index": 2,
        "bbox": [
            305.629,
            659.1186344,
            524.4136186840001,
            670.0277344
        ]
    },
    {
        "content": "to improve the performance of deep learning mod\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            645.5696344,
            526.2175143052001,
            656.4787344
        ]
    },
    {
        "content": "els. For example, Model Soups (Wortsman et al.,",
        "page_index": 2,
        "bbox": [
            306.142,
            632.0196344,
            525.7722921159999,
            642.9287343999999
        ]
    },
    {
        "content": "2022) averages the weights of multiple models that",
        "page_index": 2,
        "bbox": [
            306.142,
            618.4706344,
            524.407781888,
            629.3797344
        ]
    },
    {
        "content": "are initialized from the same pre\u2212trained model but",
        "page_index": 2,
        "bbox": [
            306.142,
            604.9216344,
            524.414000075,
            615.8307344
        ]
    },
    {
        "content": "finetuned with different hyperparameter configura\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            591.3726344,
            526.2246270384,
            602.2817344
        ]
    },
    {
        "content": "tions to improve the accuracy and robustness of the",
        "page_index": 2,
        "bbox": [
            306.142,
            577.8236344,
            524.407781888,
            588.7327343999999
        ]
    },
    {
        "content": "model. However, only a few works have been pro\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            564.2736344,
            526.2238634014001,
            575.1827344
        ]
    },
    {
        "content": "posed to merge expert networks of an MoE layer",
        "page_index": 2,
        "bbox": [
            306.142,
            550.7246344,
            524.5928002239999,
            561.6337344
        ]
    },
    {
        "content": "to a normal FFN layer using weight averaging. For",
        "page_index": 2,
        "bbox": [
            306.142,
            537.1756344,
            524.5983965923,
            548.0847344
        ]
    },
    {
        "content": "example, OneS (Xue et al., 2022) proposes several",
        "page_index": 2,
        "bbox": [
            306.142,
            523.6266344,
            524.4054909770001,
            534.5357343999999
        ]
    },
    {
        "content": "simple weight averaging methods to merge expert",
        "page_index": 2,
        "bbox": [
            306.142,
            510.07763439999997,
            524.4081309792,
            520.9867343999999
        ]
    },
    {
        "content": "networks of a BERT\u2212based MoE model. Closely",
        "page_index": 2,
        "bbox": [
            306.142,
            496.5276344,
            524.7930913,
            507.43673440000003
        ]
    },
    {
        "content": "related to our work, Experts Weights Averaging",
        "page_index": 2,
        "bbox": [
            306.142,
            482.9786344,
            524.414763712,
            493.8877344
        ]
    },
    {
        "content": "(EWA) (Huang et al., 2023) proposes to convert an",
        "page_index": 2,
        "bbox": [
            305.782,
            469.4296344,
            524.409200371,
            480.3387344
        ]
    },
    {
        "content": "MoE model to a dense model with two steps: (i)",
        "page_index": 2,
        "bbox": [
            306.142,
            455.8806344,
            525.138037042,
            466.78973440000004
        ]
    },
    {
        "content": "During MoE training, EWA conducts weighted av\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            442.3316344,
            526.2254997664,
            453.2407344
        ]
    },
    {
        "content": "eraging of all the expert weights after each weight",
        "page_index": 2,
        "bbox": [
            306.142,
            428.7816344,
            524.4102255263999,
            439.6907344
        ]
    },
    {
        "content": "update of MoE, which is based on a manually\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            415.2326344,
            526.2173833960001,
            426.1417344
        ]
    },
    {
        "content": "crafted hyperparameter \u03b2; (ii) After training, EWA",
        "page_index": 2,
        "bbox": [
            306.142,
            401.6836344,
            524.8021563996,
            412.83273460000004
        ]
    },
    {
        "content": "converts each MoE layer into an FFN layer by uni\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            388.1346344,
            526.2245834020001,
            399.0437344
        ]
    },
    {
        "content": "formly averaging the experts.",
        "page_index": 2,
        "bbox": [
            306.142,
            374.5856344,
            434.0511975,
            385.4947344
        ]
    },
    {
        "content": "Different from all the aforementioned existing",
        "page_index": 2,
        "bbox": [
            317.051,
            360.2046344,
            524.4079000700001,
            371.1137344
        ]
    },
    {
        "content": "works, X FT is the first work proposing a learnable",
        "page_index": 2,
        "bbox": [
            305.749,
            346.6556344,
            524.41330823,
            357.8047346
        ]
    },
    {
        "content": "mechanism to merge expert networks in the upcy\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            333.1056344,
            526.2166633954,
            344.0147344
        ]
    },
    {
        "content": "cled MoE model. Furthermore, while the training",
        "page_index": 2,
        "bbox": [
            306.142,
            319.5566344,
            524.4086873432999,
            330.46573440000003
        ]
    },
    {
        "content": "scheme of EWA is deeply coupled to a specific",
        "page_index": 2,
        "bbox": [
            306.142,
            306.0076344,
            524.4147637120001,
            316.9167344
        ]
    },
    {
        "content": "MoE architecture, X FT can be easily adapted to",
        "page_index": 2,
        "bbox": [
            306.142,
            292.4586344,
            524.413431118,
            303.6077346
        ]
    },
    {
        "content": "different MoE architectures by only adjusting the",
        "page_index": 2,
        "bbox": [
            306.142,
            278.9096344,
            524.4051746131001,
            289.81873440000004
        ]
    },
    {
        "content": "final merging process. In addition, unlike EWA,",
        "page_index": 2,
        "bbox": [
            306.142,
            265.3596344,
            525.772292116,
            276.2687344
        ]
    },
    {
        "content": "X FT does not introduce any hyperparameters into",
        "page_index": 2,
        "bbox": [
            306.142,
            251.8106344,
            524.4126140762,
            262.9597346
        ]
    },
    {
        "content": "the training of the large MoE models, significantly",
        "page_index": 2,
        "bbox": [
            306.142,
            238.2616344,
            524.7928185725,
            249.1707344
        ]
    },
    {
        "content": "reducing the computational resources for hyperpa\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            224.71263439999998,
            526.2200670346,
            235.62173439999998
        ]
    },
    {
        "content": "rameter searching. Our empirical results in Section",
        "page_index": 2,
        "bbox": [
            306.142,
            211.1636344,
            524.4077818879999,
            222.0727344
        ]
    },
    {
        "content": "4 also showcase the clear advantage of X FT.",
        "page_index": 2,
        "bbox": [
            305.76,
            197.6146344,
            501.81319470000005,
            208.7637346
        ]
    },
    {
        "content": "3 X FT",
        "page_index": 2,
        "bbox": [
            306.14200000000005,
            170.82536320000003,
            349.6317456000001,
            182.95989120000002
        ]
    },
    {
        "content": "Recently, some parameter\u2212efficient fine\u2212tuning",
        "page_index": 2,
        "bbox": [
            81.775,
            147.9946344,
            289.1319000700001,
            158.9037344
        ]
    },
    {
        "content": "We describe the details of X FT in this section.",
        "page_index": 2,
        "bbox": [
            305.629,
            147.9946344,
            526.323143914,
            159.1437346
        ]
    },
    {
        "content": "techniques have been proposed to use MoE for",
        "page_index": 2,
        "bbox": [
            70.866,
            134.4456344,
            289.3168002240001,
            145.35473439999998
        ]
    },
    {
        "content": "There are two steps in our framework: upcycling",
        "page_index": 2,
        "bbox": [
            305.804,
            134.4456344,
            524.409872416,
            145.6056437
        ]
    },
    {
        "content": "better instruction tuning. For example, Lo\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            120.89563439999999,
            290.94138339600005,
            131.8047344
        ]
    },
    {
        "content": "(Section 3.1) and merging (Section 3.2). During up\u2212",
        "page_index": 2,
        "bbox": [
            305.782,
            120.89563439999999,
            526.2167625220001,
            132.0556437
        ]
    },
    {
        "content": "RAMoE (Dou et al., 2023) and MoCLE (Gou et al.,",
        "page_index": 2,
        "bbox": [
            70.866,
            107.3466344,
            290.500219392,
            118.25573440000001
        ]
    },
    {
        "content": "cycling, we construct an Mixture\u2212of\u2212Experts (MoE)",
        "page_index": 2,
        "bbox": [
            306.142,
            107.3466344,
            525.14545523,
            118.25573440000001
        ]
    },
    {
        "content": "2024) propose MoE\u2212like modules that are con\u2212",
        "page_index": 2,
        "bbox": [
            70.866,
            93.79763439999999,
            290.9413833960001,
            104.7067344
        ]
    },
    {
        "content": "model from the pre\u2212trained dense model, namely",
        "page_index": 2,
        "bbox": [
            306.142,
            93.79763439999999,
            524.7930913,
            104.7067344
        ]
    },
    {
        "content": "structed with Low\u2212Rank Adaptations (LoRA) to",
        "page_index": 2,
        "bbox": [
            70.866,
            80.2486344,
            289.138763712,
            91.15773440000001
        ]
    },
    {
        "content": "MoEDS, which is then fine\u2212tuned on coding instruc\u2212",
        "page_index": 2,
        "bbox": [
            306.142,
            79.1852491,
            526.2236641180001,
            91.23409810000001
        ]
    },
    {
        "content": "improve instruction tuning, while PESC (Wu et al.,",
        "page_index": 2,
        "bbox": [
            70.866,
            66.6996344,
            290.4925284764999,
            77.6087344
        ]
    },
    {
        "content": "tion data. For merging, we propose a learnable",
        "page_index": 2,
        "bbox": [
            306.142,
            66.6996344,
            524.4147637120001,
            77.6087344
        ]
    },
    {
        "content": "3",
        "page_index": 2,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "model merging method to convert the instruction\u2212",
        "page_index": 3,
        "bbox": [
            70.866,
            539.7766344,
            290.94868158390005,
            550.6857344
        ]
    },
    {
        "content": "scale mismatch problem (Wu et al., 2022).",
        "page_index": 3,
        "bbox": [
            306.142,
            539.7766344,
            491.3239725,
            550.6857344
        ]
    },
    {
        "content": "Figure 2: Overview of X FT.",
        "page_index": 3,
        "bbox": [
            240.544,
            561.5000784,
            354.7320042,
            571.6818556000001
        ]
    },
    {
        "content": "tuned MoEDS back to a normal dense model by",
        "page_index": 3,
        "bbox": [
            70.866,
            525.2254584,
            289.5152000140001,
            537.1357343999999
        ]
    },
    {
        "content": "merging each MoE layer into an FFN layer through",
        "page_index": 3,
        "bbox": [
            70.866,
            512.6776344,
            289.131781888,
            523.5867344
        ]
    },
    {
        "content": "weight averaging while directly copying other re\u2212",
        "page_index": 3,
        "bbox": [
            70.473,
            499.1286344,
            290.9489655480001,
            510.03773440000003
        ]
    },
    {
        "content": "maining layers. Consequently, we can obtain",
        "page_index": 3,
        "bbox": [
            70.866,
            485.5796344,
            289.138763712,
            496.4887344
        ]
    },
    {
        "content": "X FTDS that has the same model architecture and",
        "page_index": 3,
        "bbox": [
            70.866,
            470.9672491,
            289.1288922138,
            483.1797346
        ]
    },
    {
        "content": "size as the original pre\u2212trained dense model, which",
        "page_index": 3,
        "bbox": [
            70.866,
            458.4806344,
            289.13507643620005,
            469.3897344
        ]
    },
    {
        "content": "eliminates all the additional inference overhead",
        "page_index": 3,
        "bbox": [
            70.866,
            444.9316344,
            289.138763712,
            455.84073440000003
        ]
    },
    {
        "content": "brought by the original sparse upcycling, while",
        "page_index": 3,
        "bbox": [
            70.866,
            431.3826344,
            289.13876371200007,
            442.2917344
        ]
    },
    {
        "content": "preserving or even improving the performance of",
        "page_index": 3,
        "bbox": [
            70.866,
            417.8336344,
            289.13598189149997,
            428.7427344
        ]
    },
    {
        "content": "MoEDS. Our framework is illustrated in Figure 2.",
        "page_index": 3,
        "bbox": [
            70.866,
            403.2824584,
            287.14333430000005,
            415.19373440000004
        ]
    },
    {
        "content": "3.1 Upcycling",
        "page_index": 3,
        "bbox": [
            70.86599999999999,
            382.33399810000003,
            142.08060479999997,
            393.24309810000005
        ]
    },
    {
        "content": "Inspired by sparse upcycling (Komatsuzaki et al.,",
        "page_index": 3,
        "bbox": [
            70.866,
            364.7816344,
            290.496292116,
            375.6907344
        ]
    },
    {
        "content": "2023), we convert the pre\u2212trained dense LLM to a",
        "page_index": 3,
        "bbox": [
            70.866,
            351.2326344,
            289.130254614,
            362.1417344
        ]
    },
    {
        "content": "new MoE by initializing each expert of each MoE",
        "page_index": 3,
        "bbox": [
            70.866,
            337.6836344,
            289.13357098039995,
            348.59273440000004
        ]
    },
    {
        "content": "layer as a copy of the original FFN layer in the",
        "page_index": 3,
        "bbox": [
            70.866,
            324.1336344,
            289.13876371200007,
            335.04273440000003
        ]
    },
    {
        "content": "dense model, while directly copying the remain\u2212",
        "page_index": 3,
        "bbox": [
            70.866,
            310.58463439999997,
            290.94138339600005,
            321.4937344
        ]
    },
    {
        "content": "ing layers from the dense model to the new MoE",
        "page_index": 3,
        "bbox": [
            70.866,
            297.0356344,
            289.138763712,
            307.9447344
        ]
    },
    {
        "content": "model. However, the performance gain brought by",
        "page_index": 3,
        "bbox": [
            70.866,
            283.4866344,
            289.5121822050001,
            294.39573440000004
        ]
    },
    {
        "content": "sparse upcycling is negligible with a very limited",
        "page_index": 3,
        "bbox": [
            70.866,
            269.9376344,
            289.138763712,
            280.8467344
        ]
    },
    {
        "content": "extra training budget (Komatsuzaki et al., 2023) \u2013",
        "page_index": 3,
        "bbox": [
            70.866,
            256.3876344,
            290.77512871199997,
            267.29673440000005
        ]
    },
    {
        "content": "which is exactly the situation we are facing during",
        "page_index": 3,
        "bbox": [
            70.473,
            242.8386344,
            289.1380004025,
            253.74773439999998
        ]
    },
    {
        "content": "instruction tuning. Intuitively, it is because each",
        "page_index": 3,
        "bbox": [
            70.866,
            229.28963439999998,
            289.138763712,
            240.19873439999998
        ]
    },
    {
        "content": "expert in the upcycled MoE model is trained on",
        "page_index": 3,
        "bbox": [
            70.866,
            215.7406344,
            289.13876371200007,
            226.6497344
        ]
    },
    {
        "content": "fewer instruction data than the original dense model",
        "page_index": 3,
        "bbox": [
            70.866,
            202.1916344,
            289.13178188800003,
            213.1007344
        ]
    },
    {
        "content": "does because traditional routers used in sparse up\u2212",
        "page_index": 3,
        "bbox": [
            70.866,
            188.6416344,
            290.950219767,
            199.55073439999998
        ]
    },
    {
        "content": "cycling will assign different tokens to different ex\u2212",
        "page_index": 3,
        "bbox": [
            70.866,
            175.0926344,
            290.9415143052001,
            186.0017344
        ]
    },
    {
        "content": "perts and thus reduce the amount of data each ex\u2212",
        "page_index": 3,
        "bbox": [
            70.866,
            161.5436344,
            290.94138339600005,
            172.4527344
        ]
    },
    {
        "content": "pert is trained on (Gou et al., 2024). Consequently,",
        "page_index": 3,
        "bbox": [
            70.866,
            147.9946344,
            290.49970666430016,
            158.9037344
        ]
    },
    {
        "content": "inspired by DeepSeekMoE (Dai et al., 2024) and",
        "page_index": 3,
        "bbox": [
            70.866,
            134.4456344,
            289.138763712,
            145.35473439999998
        ]
    },
    {
        "content": "MoCLE (Gou et al., 2024), X FT introduces the",
        "page_index": 3,
        "bbox": [
            70.866,
            120.89563439999999,
            289.13123060400005,
            132.0447346
        ]
    },
    {
        "content": "shared expert setting into sparse upcycling to tackle",
        "page_index": 3,
        "bbox": [
            70.866,
            107.3466344,
            289.1317818880001,
            118.25573440000001
        ]
    },
    {
        "content": "this challenge. We further propose a novel routing",
        "page_index": 3,
        "bbox": [
            70.866,
            93.79763439999999,
            289.13304734360014,
            104.7067344
        ]
    },
    {
        "content": "weight normalization strategy for X FT to avoid the",
        "page_index": 3,
        "bbox": [
            70.473,
            80.2486344,
            289.12967206400003,
            91.3977346
        ]
    },
    {
        "content": "potential performance degradation caused by the",
        "page_index": 3,
        "bbox": [
            70.866,
            66.6996344,
            289.13876371200007,
            77.6087344
        ]
    },
    {
        "content": "3.1.1 Shared Expert for Upcycling",
        "page_index": 3,
        "bbox": [
            306.142,
            519.1659981,
            472.6366842,
            530.0750981
        ]
    },
    {
        "content": "During upcycling, we isolate one shared expert",
        "page_index": 3,
        "bbox": [
            306.142,
            503.0856344,
            524.4147637120001,
            513.9947344
        ]
    },
    {
        "content": "among all the other normal experts in each MoE",
        "page_index": 3,
        "bbox": [
            306.142,
            489.53663439999997,
            524.414763712,
            500.4457344
        ]
    },
    {
        "content": "layer, where the shared expert will be deterministi\u2212",
        "page_index": 3,
        "bbox": [
            306.142,
            475.9876344,
            526.2254997663999,
            486.8967344
        ]
    },
    {
        "content": "cally assigned to handle all the tokens while other",
        "page_index": 3,
        "bbox": [
            306.142,
            462.4386344,
            524.5926365875,
            473.34773440000004
        ]
    },
    {
        "content": "normal experts are assigned by the router. By doing",
        "page_index": 3,
        "bbox": [
            306.142,
            448.8896344,
            524.407781888,
            459.7987344
        ]
    },
    {
        "content": "so, the upcycled MoE model can achieve a clear",
        "page_index": 3,
        "bbox": [
            306.142,
            435.3396344,
            524.5928002239999,
            446.24873440000005
        ]
    },
    {
        "content": "performance boost in instruction tuning, where the",
        "page_index": 3,
        "bbox": [
            306.142,
            421.7906344,
            524.4055673407,
            432.6997344
        ]
    },
    {
        "content": "shared expert can learn general knowledge across",
        "page_index": 3,
        "bbox": [
            306.142,
            408.2416344,
            524.4140218932,
            419.15073440000003
        ]
    },
    {
        "content": "the whole instruction dataset while other normal",
        "page_index": 3,
        "bbox": [
            306.142,
            394.6926344,
            524.4147637120001,
            405.6017344
        ]
    },
    {
        "content": "experts learn specific knowledge among different",
        "page_index": 3,
        "bbox": [
            306.142,
            381.1436344,
            524.4087855252001,
            392.0527344
        ]
    },
    {
        "content": "instructions assigned by the router. Formally, the",
        "page_index": 3,
        "bbox": [
            306.142,
            367.5936344,
            524.414763712,
            378.5027344
        ]
    },
    {
        "content": "output hidden state hlt of the l\u2212th MoE layer when",
        "page_index": 3,
        "bbox": [
            306.142,
            352.15780060000003,
            524.4094043377,
            367.18690060000006
        ]
    },
    {
        "content": "processing the t\u2212th token can be expressed as:",
        "page_index": 3,
        "bbox": [
            306.142,
            340.4956344,
            505.82229040000004,
            351.6447346
        ]
    },
    {
        "content": "hlt =",
        "page_index": 3,
        "bbox": [
            329.327,
            305.4968006,
            350.46409797999996,
            320.6679006
        ]
    },
    {
        "content": "N(cid:88)",
        "page_index": 3,
        "bbox": [
            353.49399999999997,
            313.55753999999996,
            369.25110404,
            329.7999006
        ]
    },
    {
        "content": "i=1",
        "page_index": 3,
        "bbox": [
            354.52099999999996,
            295.10580059999995,
            368.22500476999994,
            303.07590059999995
        ]
    },
    {
        "content": "(gi,tFFNi(ult)) + ult",
        "page_index": 3,
        "bbox": [
            369.25199999999995,
            305.4968006,
            455.68312736999985,
            320.6679006
        ]
    },
    {
        "content": "1 \u2212 stmax i = 1",
        "page_index": 3,
        "bbox": [
            363.1909999999998,
            276.51480059999994,
            493.4103777799998,
            288.98773459999995
        ]
    },
    {
        "content": "gi,t =",
        "page_index": 3,
        "bbox": [
            324.95399999999984,
            261.2778006,
            350.4640979799998,
            273.2537346
        ]
    },
    {
        "content": "Softmaxi(si,t) \u00b7 stmax si,t \u2208 StK",
        "page_index": 3,
        "bbox": [
            363.1909999999998,
            260.25580059999993,
            514.2742095999997,
            272.72873459999994
        ]
    },
    {
        "content": "0 otherwise",
        "page_index": 3,
        "bbox": [
            363.19099999999975,
            245.3206343999999,
            512.0635807999997,
            256.4697345999999
        ]
    },
    {
        "content": "\uf8f1\uf8f4\uf8f2",
        "page_index": 3,
        "bbox": [
            353.4939999999998,
            270.46054,
            363.1910989899998,
            294.46063999999996
        ]
    },
    {
        "content": "\uf8f4\uf8f3",
        "page_index": 3,
        "bbox": [
            353.4939999999998,
            247.55153999999996,
            363.1910989899998,
            261.7336399999999
        ]
    },
    {
        "content": "StK = Topk({si,t | 1 \u2264 i \u2264 N }, K \u2212 1)",
        "page_index": 3,
        "bbox": [
            320.4999999999998,
            225.6498005999999,
            499.98009898999965,
            238.1227345999999
        ]
    },
    {
        "content": "stmax = max({si,t | 1 \u2264 i \u2264 N })",
        "page_index": 3,
        "bbox": [
            314.01999999999964,
            209.1118005999999,
            464.4455489899995,
            221.58473459999988
        ]
    },
    {
        "content": "si,t =",
        "page_index": 3,
        "bbox": [
            325.04299999999955,
            182.3718005999999,
            350.4640979799995,
            194.34673459999988
        ]
    },
    {
        "content": "(cid:40)\u2212\u221e i = 1",
        "page_index": 3,
        "bbox": [
            353.4939999999995,
            191.24463459999987,
            470.23937777999953,
            208.5726399999999
        ]
    },
    {
        "content": "Softmaxi(ult",
        "page_index": 3,
        "bbox": [
            362.2819999999995,
            172.45480059999986,
            416.0001273699995,
            187.08090059999986
        ]
    },
    {
        "content": "T eli) i \u2265 2",
        "page_index": 3,
        "bbox": [
            416.4979999999995,
            172.09680059999985,
            470.2365499999994,
            189.46090059999986
        ]
    },
    {
        "content": "(2)",
        "page_index": 3,
        "bbox": [
            512.4209999999994,
            158.98363439999986,
            525.1410105999994,
            169.89273439999985
        ]
    },
    {
        "content": "where hlt refers to the output hidden state of the l\u2212th",
        "page_index": 3,
        "bbox": [
            305.749,
            132.55880059999998,
            524.4096098980001,
            147.58690059999998
        ]
    },
    {
        "content": "MoE layer when processing the t\u2212th token, N refers",
        "page_index": 3,
        "bbox": [
            306.142,
            120.89563439999999,
            524.409529368,
            132.0447346
        ]
    },
    {
        "content": "to the total number of experts, gi,t refers to the gate",
        "page_index": 3,
        "bbox": [
            306.142,
            106.5208006,
            524.4066047,
            118.4957346
        ]
    },
    {
        "content": "value for the i\u2212th expert, FFNi(\u00b7) refers to the i\u2212th",
        "page_index": 3,
        "bbox": [
            305.869,
            92.9718006,
            524.409210201,
            104.9467346
        ]
    },
    {
        "content": "expert, ult refers to the output hidden state of the",
        "page_index": 3,
        "bbox": [
            306.142,
            78.3618006,
            524.412852234,
            92.9879006
        ]
    },
    {
        "content": "l\u2212th attention layer when processing the t\u2212th token",
        "page_index": 3,
        "bbox": [
            306.142,
            66.6996344,
            524.4092360857,
            77.8487346
        ]
    },
    {
        "content": "4",
        "page_index": 3,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "and also the input of the l\u2212th MoE layer, si,t refers",
        "page_index": 4,
        "bbox": [
            70.866,
            756.8818006000001,
            289.13379515360003,
            768.8577346000001
        ]
    },
    {
        "content": "to the affinity score between the i\u2212th expert and",
        "page_index": 4,
        "bbox": [
            70.866,
            744.1596344,
            289.134671194,
            755.3087346
        ]
    },
    {
        "content": "the t\u2212th token, stmax refers to the maximum affinity",
        "page_index": 4,
        "bbox": [
            70.866,
            729.1743456,
            289.51176307599997,
            741.7587346
        ]
    },
    {
        "content": "Formally speaking, given the weights of N ex\u2212",
        "page_index": 4,
        "bbox": [
            317.051,
            757.7086344,
            526.2176298839998,
            768.8577346000001
        ]
    },
    {
        "content": "perts at the l\u2212th layer W l1, W l2, \u00b7 \u00b7 \u00b7 , W lN , the pro\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            741.7368005999999,
            526.216871396,
            756.8979006
        ]
    },
    {
        "content": "cess of merging each MoE layer to an FFN layer",
        "page_index": 4,
        "bbox": [
            306.142,
            730.6096344,
            524.5928002239999,
            741.5187344
        ]
    },
    {
        "content": "score among all the experts besides the shared ex\u2212",
        "page_index": 4,
        "bbox": [
            70.866,
            717.0606344,
            290.94858340200005,
            727.9697344
        ]
    },
    {
        "content": "can be stated as below:",
        "page_index": 4,
        "bbox": [
            306.142,
            717.0606344,
            406.45117450000004,
            727.9697344
        ]
    },
    {
        "content": "pert, Topk(S, K) refers to a function computing K",
        "page_index": 4,
        "bbox": [
            70.866,
            703.5116344,
            288.35309863000003,
            714.6607346000001
        ]
    },
    {
        "content": "largest scores over S, StK refers to a set of K \u2212 1",
        "page_index": 4,
        "bbox": [
            70.866,
            689.0838006,
            290.22454999999997,
            701.1117346
        ]
    },
    {
        "content": "largest affinity scores among all the experts besides",
        "page_index": 4,
        "bbox": [
            70.866,
            676.4136344,
            289.13178188800003,
            687.3227344
        ]
    },
    {
        "content": "the shared expert, and eli refers to the centroid of",
        "page_index": 4,
        "bbox": [
            70.866,
            660.6188006,
            289.13426977,
            675.6029006
        ]
    },
    {
        "content": "the i\u2212th expert in the l\u2212th MoE layer.",
        "page_index": 4,
        "bbox": [
            70.866,
            649.3146344,
            229.42432549999998,
            660.4637346000001
        ]
    },
    {
        "content": "FFN1 is chosen as the shared expert in each MoE",
        "page_index": 4,
        "bbox": [
            81.775,
            634.6438006000001,
            289.134859414,
            646.3787344
        ]
    },
    {
        "content": "layer and each token will be assigned to top K",
        "page_index": 4,
        "bbox": [
            70.866,
            621.9206344,
            288.35309863000003,
            633.0697346000001
        ]
    },
    {
        "content": "experts including one shared expert and K \u2212 1",
        "page_index": 4,
        "bbox": [
            70.866,
            608.3716344,
            290.22455,
            619.5207346
        ]
    },
    {
        "content": "other normal experts. Compared with the original",
        "page_index": 4,
        "bbox": [
            70.866,
            594.8226344,
            289.1293927951,
            605.7317343999999
        ]
    },
    {
        "content": "sparse upcycling, there are two major differences:",
        "page_index": 4,
        "bbox": [
            70.866,
            581.2736344,
            288.13163560000004,
            592.1827344
        ]
    },
    {
        "content": "\u2022 Weighted Shared Expert. Following Mo\u2212",
        "page_index": 4,
        "bbox": [
            72.976,
            557.5756344,
            290.940308218,
            568.5610981
        ]
    },
    {
        "content": "CLE (Gou et al., 2024), with the token\u2212to\u2212expert",
        "page_index": 4,
        "bbox": [
            81.775,
            544.0266344,
            289.13604552800007,
            554.9357344
        ]
    },
    {
        "content": "affinity score si,t, we get the maximum affinity",
        "page_index": 4,
        "bbox": [
            81.775,
            529.6518006,
            289.52031828289995,
            541.6267346
        ]
    },
    {
        "content": "score stmax and use its complement 1 \u2212 stmax",
        "page_index": 4,
        "bbox": [
            81.775,
            515.6038006,
            288.85587274999995,
            528.0777346
        ]
    },
    {
        "content": "as the routing weight of the shared expert.",
        "page_index": 4,
        "bbox": [
            81.77499999999998,
            503.37863439999995,
            265.2115165,
            514.2877344
        ]
    },
    {
        "content": "\u2022 Routing Weight Normalization. Although",
        "page_index": 4,
        "bbox": [
            72.97599999999997,
            487.83763439999996,
            289.13559205,
            498.82309810000004
        ]
    },
    {
        "content": "the shared expert setting is also used in recent",
        "page_index": 4,
        "bbox": [
            81.775,
            474.2886344,
            289.13190007,
            485.1977344
        ]
    },
    {
        "content": "works (Dai et al., 2024; Gou et al., 2024), we",
        "page_index": 4,
        "bbox": [
            81.382,
            460.7386344,
            289.13948222199997,
            471.64773440000005
        ]
    },
    {
        "content": "cannot directly follow their routing strategy be\u2212",
        "page_index": 4,
        "bbox": [
            81.775,
            447.1896344,
            290.95005431240014,
            458.0987344
        ]
    },
    {
        "content": "cause they cannot handle a scale mismatch prob\u2212",
        "page_index": 4,
        "bbox": [
            81.775,
            433.6406344,
            290.9422106695,
            444.54973440000003
        ]
    },
    {
        "content": "lem that is unique for sparse upcycling. The",
        "page_index": 4,
        "bbox": [
            81.775,
            420.0916344,
            289.1319000700001,
            431.0007344
        ]
    },
    {
        "content": "scale mismatch problem is that differences be\u2212",
        "page_index": 4,
        "bbox": [
            81.775,
            406.5426344,
            290.945647036,
            417.4517344
        ]
    },
    {
        "content": "tween the scale of the output of the upcycled",
        "page_index": 4,
        "bbox": [
            81.775,
            392.9926344,
            289.1319000700001,
            403.9017344
        ]
    },
    {
        "content": "MoE layer and the original FFN layer can cause",
        "page_index": 4,
        "bbox": [
            81.775,
            379.4436344,
            289.1366128012,
            390.35273440000003
        ]
    },
    {
        "content": "performance degradation (Wu et al., 2022). To",
        "page_index": 4,
        "bbox": [
            81.775,
            365.8946344,
            289.13190007,
            376.8037344
        ]
    },
    {
        "content": "handle this problem, we need to make sure the",
        "page_index": 4,
        "bbox": [
            81.775,
            352.3456344,
            289.1319000700001,
            363.2547344
        ]
    },
    {
        "content": "sum of gi,t equals 1, so that the output of the",
        "page_index": 4,
        "bbox": [
            81.775,
            337.96980060000004,
            289.13554687799996,
            349.94573460000004
        ]
    },
    {
        "content": "MoE layer matches that of the FFN layer in scale.",
        "page_index": 4,
        "bbox": [
            81.775,
            325.2466344,
            291.039028932,
            336.15573440000003
        ]
    },
    {
        "content": "To do so, we normalize the affinity scores of top",
        "page_index": 4,
        "bbox": [
            81.437,
            311.69763439999997,
            289.1317985334,
            322.6067344
        ]
    },
    {
        "content": "K \u2212 1 normal experts with Softmax and scale",
        "page_index": 4,
        "bbox": [
            81.775,
            298.1486344,
            289.139109348,
            309.2977346
        ]
    },
    {
        "content": "their sum to stmax to make sure that the sum of",
        "page_index": 4,
        "bbox": [
            81.775,
            283.27480060000005,
            289.1323291614,
            295.74873460000003
        ]
    },
    {
        "content": "the gi,t of top K experts, including one shared",
        "page_index": 4,
        "bbox": [
            81.775,
            270.2238006,
            289.129254678,
            282.1997346
        ]
    },
    {
        "content": "expert and K \u2212 1 normal experts, equals 1.",
        "page_index": 4,
        "bbox": [
            81.775,
            257.5006344,
            270.8520001,
            268.64973460000004
        ]
    },
    {
        "content": "3.2 Merging",
        "page_index": 4,
        "bbox": [
            70.86599999999999,
            233.87999810000002,
            135.2951446,
            244.78909810000002
        ]
    },
    {
        "content": "We propose a learnable model merging method to",
        "page_index": 4,
        "bbox": [
            70.353,
            215.7406344,
            289.1285095855,
            226.6497344
        ]
    },
    {
        "content": "convert the large MoE model, namely MoEDS, back",
        "page_index": 4,
        "bbox": [
            70.866,
            201.1894584,
            289.40420271,
            213.1007344
        ]
    },
    {
        "content": "to a dense model X FTDS. By doing so, we expect",
        "page_index": 4,
        "bbox": [
            70.866,
            187.6404584,
            289.1341592618,
            199.79073459999998
        ]
    },
    {
        "content": "X FTDS to keep the boosted performance gained",
        "page_index": 4,
        "bbox": [
            70.866,
            174.0914584,
            289.132859884,
            186.2417346
        ]
    },
    {
        "content": "during upcycling while keeping its model size the",
        "page_index": 4,
        "bbox": [
            70.866,
            161.5436344,
            289.13617825529997,
            172.4527344
        ]
    },
    {
        "content": "same as the original dense model to avoid any ad\u2212",
        "page_index": 4,
        "bbox": [
            70.866,
            147.9946344,
            290.947710674,
            158.9037344
        ]
    },
    {
        "content": "ditional inference overhead. Inspired by Model",
        "page_index": 4,
        "bbox": [
            70.866,
            134.4456344,
            289.138763712,
            145.35473439999998
        ]
    },
    {
        "content": "Soups (Wortsman et al., 2022), we choose to merge",
        "page_index": 4,
        "bbox": [
            70.866,
            120.89563439999999,
            289.13178188800003,
            131.8047344
        ]
    },
    {
        "content": "MoEDS by learning the mixing coefficients that",
        "page_index": 4,
        "bbox": [
            70.866,
            106.3454584,
            289.13589588599996,
            118.25573440000001
        ]
    },
    {
        "content": "can be used to average the parameters of all experts",
        "page_index": 4,
        "bbox": [
            70.866,
            93.79763439999999,
            289.131781888,
            104.7067344
        ]
    },
    {
        "content": "in each MoE layer to obtain a normal FFN layer,",
        "page_index": 4,
        "bbox": [
            70.866,
            80.2486344,
            290.496292116,
            91.15773440000001
        ]
    },
    {
        "content": "while directly copying other remaining layers.",
        "page_index": 4,
        "bbox": [
            70.473,
            66.6996344,
            272.1495317,
            77.6087344
        ]
    },
    {
        "content": "W l =",
        "page_index": 4,
        "bbox": [
            379.097,
            684.2146346000001,
            405.5500979799999,
            695.9069006000001
        ]
    },
    {
        "content": "N(cid:88)",
        "page_index": 4,
        "bbox": [
            408.5809999999999,
            690.1495399999999,
            424.3381040399999,
            706.3909006
        ]
    },
    {
        "content": "i=1",
        "page_index": 4,
        "bbox": [
            409.6069999999999,
            671.6978005999999,
            423.31200476999993,
            679.6679005999999
        ]
    },
    {
        "content": "\u03b1liW li (3)",
        "page_index": 4,
        "bbox": [
            426.15599999999995,
            682.0878005999999,
            525.1400105999999,
            697.2589006
        ]
    },
    {
        "content": "where W l denotes the merged parameter of all N",
        "page_index": 4,
        "bbox": [
            305.749,
            646.1246344,
            523.22046185,
            658.0569006000001
        ]
    },
    {
        "content": "experts and \u03b1li denotes the learnable mixing coeffi\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            630.3308006,
            526.2198273224,
            645.3149006
        ]
    },
    {
        "content": "cient of expert W li . We consider a neural network",
        "page_index": 4,
        "bbox": [
            306.142,
            616.7808006,
            524.6814708456,
            631.7659006
        ]
    },
    {
        "content": "f (x; \u03b8) with input x and parameters \u03b8. For loss L",
        "page_index": 4,
        "bbox": [
            306.142,
            605.4776343999999,
            524.40900627,
            616.6267346
        ]
    },
    {
        "content": "and instruction dataset {(xi, yi)}mi=1, such mixing",
        "page_index": 4,
        "bbox": [
            306.142,
            589.6828006,
            524.4104678829999,
            604.6669006
        ]
    },
    {
        "content": "coefficients \u03b1 of all the L layers can be learned via:",
        "page_index": 4,
        "bbox": [
            306.142,
            578.3786344,
            525.93054475,
            589.5277346
        ]
    },
    {
        "content": "arg min\u03b1",
        "page_index": 4,
        "bbox": [
            312.689,
            537.3048006,
            348.02793854000004,
            554.1887346
        ]
    },
    {
        "content": "m(cid:88)",
        "page_index": 4,
        "bbox": [
            349.84100000000007,
            549.2145399999999,
            365.59810404000007,
            565.4569006
        ]
    },
    {
        "content": "j=1",
        "page_index": 4,
        "bbox": [
            350.3670000000001,
            530.7628006,
            365.0720047700001,
            538.7329006
        ]
    },
    {
        "content": "L(f (xj; \u03b8o, (",
        "page_index": 4,
        "bbox": [
            367.4170000000001,
            542.2138006,
            424.20954899000014,
            554.1887346
        ]
    },
    {
        "content": "N(cid:88)",
        "page_index": 4,
        "bbox": [
            424.21000000000015,
            549.2145399999998,
            439.96710404000015,
            565.4569005999999
        ]
    },
    {
        "content": "i=1",
        "page_index": 4,
        "bbox": [
            425.23700000000014,
            530.7628005999999,
            438.9410047700001,
            538.7329005999999
        ]
    },
    {
        "content": "\u03b1liW li )1:L), yi) (4)",
        "page_index": 4,
        "bbox": [
            441.78500000000014,
            541.1528005999999,
            525.1400106000001,
            556.3249005999999
        ]
    },
    {
        "content": "where \u03b8o refers to all the remaining layers of",
        "page_index": 4,
        "bbox": [
            305.749,
            506.1068006,
            524.4067426979999,
            518.0817346
        ]
    },
    {
        "content": "MoEDS other than MoE layers and \u03b1 is parame\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            492.38245839999996,
            526.2156031659999,
            504.5327346
        ]
    },
    {
        "content": "terized as the output of a softmax, so that each \u03b1li",
        "page_index": 4,
        "bbox": [
            306.142,
            477.5888006,
            523.9107851699999,
            492.5739006
        ]
    },
    {
        "content": "is positive and (cid:80)N",
        "page_index": 4,
        "bbox": [
            306.14199999999994,
            466.2856344,
            389.70118532999993,
            481.18764
        ]
    },
    {
        "content": "i=1 \u03b1li = 1.",
        "page_index": 4,
        "bbox": [
            382.9799999999999,
            463.8698006000001,
            432.0872749999999,
            479.0239006000001
        ]
    },
    {
        "content": "While the learning process defined in Eq. (4) is",
        "page_index": 4,
        "bbox": [
            317.051,
            452.4536344,
            524.4064273414999,
            463.3627344
        ]
    },
    {
        "content": "the most intuitive way of learning \u03b1, our experi\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            438.9036344,
            526.2228831259999,
            450.0527346
        ]
    },
    {
        "content": "ment in Section 5.2 shows that, due to the shared",
        "page_index": 4,
        "bbox": [
            306.142,
            425.3546344,
            524.414763712,
            436.26373440000003
        ]
    },
    {
        "content": "expert setting, it tends to simply increase the mix\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            411.8056344,
            526.224583402,
            422.7147344
        ]
    },
    {
        "content": "ing coefficient of the shared expert at each layer as",
        "page_index": 4,
        "bbox": [
            306.142,
            398.2566344,
            524.4103127991999,
            409.1657344
        ]
    },
    {
        "content": "much as possible to decrease the loss. It is not help\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            384.7076344,
            526.2252379480001,
            395.61673440000004
        ]
    },
    {
        "content": "ful because, although the shared expert has learned",
        "page_index": 4,
        "bbox": [
            306.142,
            371.1576344,
            524.4057637044999,
            382.06673440000003
        ]
    },
    {
        "content": "general knowledge across the whole instruction",
        "page_index": 4,
        "bbox": [
            306.142,
            357.60863439999997,
            524.414763712,
            368.5177344
        ]
    },
    {
        "content": "dataset and needs a relatively large mixing coeffi\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            344.0596344,
            526.2239506742,
            354.9687344
        ]
    },
    {
        "content": "cient, we still need to keep the scale of the mixing",
        "page_index": 4,
        "bbox": [
            306.142,
            330.5106344,
            524.4112728,
            341.41973440000004
        ]
    },
    {
        "content": "coefficient of other normal experts at a certain level",
        "page_index": 4,
        "bbox": [
            306.142,
            316.9616344,
            524.407781888,
            327.8707344
        ]
    },
    {
        "content": "also to keep some specific knowledge learned by",
        "page_index": 4,
        "bbox": [
            306.142,
            303.41163439999997,
            524.7930913,
            314.3207344
        ]
    },
    {
        "content": "other normal experts in the merged parameter W l.",
        "page_index": 4,
        "bbox": [
            306.142,
            289.8626344,
            525.514275,
            301.7949006
        ]
    },
    {
        "content": "To solve this issue, we introduce a shared expert",
        "page_index": 4,
        "bbox": [
            317.051,
            276.0306344,
            524.410011888,
            287.1906437
        ]
    },
    {
        "content": "rate \u03bb to fix the mixing coefficient of the shared",
        "page_index": 4,
        "bbox": [
            306.142,
            262.4816344,
            524.408447136,
            273.64164370000003
        ]
    },
    {
        "content": "expert and learn the mixing coefficients of the re\u2212",
        "page_index": 4,
        "bbox": [
            306.142,
            248.93263439999998,
            526.217383396,
            259.8417344
        ]
    },
    {
        "content": "maining normal experts which sums to 1 \u2212 \u03bb in",
        "page_index": 4,
        "bbox": [
            306.142,
            235.3836344,
            524.410025396,
            246.5327346
        ]
    },
    {
        "content": "each layer. By doing so, we can easily control the",
        "page_index": 4,
        "bbox": [
            306.142,
            221.8336344,
            524.4090037072,
            232.7427344
        ]
    },
    {
        "content": "scale of the mixing coefficient of the shared expert,",
        "page_index": 4,
        "bbox": [
            306.142,
            208.2846344,
            525.7755975733,
            219.19373439999998
        ]
    },
    {
        "content": "while still being able to learn the optimal layer\u2212wise",
        "page_index": 4,
        "bbox": [
            305.749,
            194.7356344,
            524.410345854,
            205.6447344
        ]
    },
    {
        "content": "mixing coefficients of other normal experts. Let\u2019s",
        "page_index": 4,
        "bbox": [
            306.142,
            181.1866344,
            524.41236371,
            192.0957344
        ]
    },
    {
        "content": "say W l1 is the shared expert of the l\u2212th layer, then",
        "page_index": 4,
        "bbox": [
            306.142,
            165.5248006,
            524.407669664,
            180.3759006
        ]
    },
    {
        "content": "Eq. (3) and Eq. (4) can be reformulated as below:",
        "page_index": 4,
        "bbox": [
            306.142,
            154.08763439999998,
            523.1785445,
            164.99673439999998
        ]
    },
    {
        "content": "W l = \u03bbW l1 +",
        "page_index": 4,
        "bbox": [
            361.731,
            116.8628006,
            423.52309797999993,
            132.0339006
        ]
    },
    {
        "content": "N(cid:88)",
        "page_index": 4,
        "bbox": [
            425.94699999999995,
            124.92354000000002,
            441.70410403999995,
            141.16590060000001
        ]
    },
    {
        "content": "i=2",
        "page_index": 4,
        "bbox": [
            426.97399999999993,
            106.47180060000002,
            440.6780047699999,
            114.44190060000003
        ]
    },
    {
        "content": "\u03b1liW li (5)",
        "page_index": 4,
        "bbox": [
            443.52199999999993,
            116.86280060000003,
            525.1400105999999,
            132.03390060000004
        ]
    },
    {
        "content": "arg min\u03b1",
        "page_index": 4,
        "bbox": [
            336.7139999999998,
            76.69180060000004,
            372.05293853999984,
            93.57673460000005
        ]
    },
    {
        "content": "m(cid:88)",
        "page_index": 4,
        "bbox": [
            373.8659999999999,
            88.60154000000004,
            389.6231040399999,
            104.84390060000004
        ]
    },
    {
        "content": "j=1",
        "page_index": 4,
        "bbox": [
            374.3919999999999,
            70.14980060000005,
            389.0970047699999,
            78.11990060000005
        ]
    },
    {
        "content": "L(f (xj; \u03b8o, W l1:L), yi) (6)",
        "page_index": 4,
        "bbox": [
            391.4409999999999,
            81.54880059999999,
            525.1400106000001,
            94.3589006
        ]
    },
    {
        "content": "5",
        "page_index": 4,
        "bbox": [
            294.91100000000006,
            36.81163439999999,
            300.36555000000004,
            47.72073439999999
        ]
    },
    {
        "content": "In practice, we uniformly initialize the mix\u2212",
        "page_index": 5,
        "bbox": [
            81.775,
            757.7086344,
            290.945647036,
            768.6177344
        ]
    },
    {
        "content": "Table 1 shows the pass@1 results of different",
        "page_index": 5,
        "bbox": [
            317.051,
            757.7086344,
            524.4109060760001,
            768.6177344
        ]
    },
    {
        "content": "ing coefficients \u03b1 of all the normal experts as",
        "page_index": 5,
        "bbox": [
            70.866,
            744.1596344,
            289.13380031799994,
            755.3087346
        ]
    },
    {
        "content": "LLMs. X FT achieves 67.1 pass@1 on HumanEval",
        "page_index": 5,
        "bbox": [
            306.142,
            744.1596344,
            524.404289738,
            755.3087346
        ]
    },
    {
        "content": "1\u2212\u03bb",
        "page_index": 5,
        "bbox": [
            73.377,
            735.7158006,
            89.13786798,
            743.6859006
        ]
    },
    {
        "content": "N \u22121 , which is then trained on the same instruc\u2212",
        "page_index": 5,
        "bbox": [
            72.062,
            727.6578006,
            290.948747902,
            741.5187344
        ]
    },
    {
        "content": "tion dataset as upcycling.",
        "page_index": 5,
        "bbox": [
            70.866,
            717.0606344,
            180.99336449999996,
            727.9697344
        ]
    },
    {
        "content": "4 Main Evaluation",
        "page_index": 5,
        "bbox": [
            70.866,
            689.7843632,
            174.70886720000001,
            701.7395632
        ]
    },
    {
        "content": "4.1 Experimental Setup",
        "page_index": 5,
        "bbox": [
            70.866,
            666.6799981,
            188.4442798,
            677.5890981
        ]
    },
    {
        "content": "Training. We use DeepSeek\u2212Coder\u2212Base",
        "page_index": 5,
        "bbox": [
            70.506,
            647.1956344,
            289.139543308,
            658.1810981
        ]
    },
    {
        "content": "1.3B (Guo et al., 2024) as the main base",
        "page_index": 5,
        "bbox": [
            70.048,
            633.6466344,
            289.13305529800004,
            644.5557344
        ]
    },
    {
        "content": "code LLM. evol\u2212codealpaca\u2212v1, an open\u2212source",
        "page_index": 5,
        "bbox": [
            70.866,
            620.0976344,
            289.13014913,
            631.4649165999999
        ]
    },
    {
        "content": "Evol\u2212Instruct (Luo et al., 2023) dataset contain\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            606.5486344,
            290.94138339600005,
            617.4577343999999
        ]
    },
    {
        "content": "ing 110K samples, is used as our instruction",
        "page_index": 5,
        "bbox": [
            70.866,
            592.9996344,
            289.13876371200007,
            603.9087344
        ]
    },
    {
        "content": "dataset. MoEDS, our MoE model upcycled from",
        "page_index": 5,
        "bbox": [
            70.866,
            578.3872491,
            289.13512087000004,
            590.4350981
        ]
    },
    {
        "content": "the base model, is implemented following Llama\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            565.9006344,
            290.94771067400006,
            576.8097343999999
        ]
    },
    {
        "content": "MoE (LLaMA\u2212MoE Team, 2023). It is constructed",
        "page_index": 5,
        "bbox": [
            70.866,
            552.3516344,
            289.13178188800003,
            563.2607343999999
        ]
    },
    {
        "content": "with 8 experts in one expert layer and the top 6",
        "page_index": 5,
        "bbox": [
            70.473,
            538.8026344,
            289.13521858200005,
            549.7117344
        ]
    },
    {
        "content": "experts1 are activated for each token, including one",
        "page_index": 5,
        "bbox": [
            70.866,
            525.2536344,
            289.1315322660001,
            537.8165584
        ]
    },
    {
        "content": "shared expert. As such, we denote the model size of",
        "page_index": 5,
        "bbox": [
            70.866,
            511.70363439999994,
            289.13178188800003,
            522.6127343999999
        ]
    },
    {
        "content": "MoEDS as 8\u00d71.3B. Other hyperparameter settings",
        "page_index": 5,
        "bbox": [
            70.866,
            497.0922491,
            289.13663172610006,
            509.30373460000004
        ]
    },
    {
        "content": "are detailed in Appendix A.1. We finally obtain",
        "page_index": 5,
        "bbox": [
            70.866,
            484.6056344,
            289.138763712,
            495.5147344
        ]
    },
    {
        "content": "X FTDS by using the learned mixing coefficients to",
        "page_index": 5,
        "bbox": [
            70.866,
            469.9932491,
            289.13206010000005,
            482.2057346
        ]
    },
    {
        "content": "merge MoE layers inside MoEDS as normal FFN",
        "page_index": 5,
        "bbox": [
            70.866,
            456.50545839999995,
            289.138492204,
            468.4167344
        ]
    },
    {
        "content": "layers. Note that X FTDS is the final instruction\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            442.8952491,
            290.945066914,
            455.10673460000004
        ]
    },
    {
        "content": "tuned LLM we produce, while MoEDS is only an",
        "page_index": 5,
        "bbox": [
            70.866,
            429.3462491,
            289.131544928,
            441.3940981
        ]
    },
    {
        "content": "intermediate product of X FT framework.",
        "page_index": 5,
        "bbox": [
            70.866,
            416.8596344,
            251.03687289999996,
            428.0087346
        ]
    },
    {
        "content": "Baselines. To study the effectiveness of X FT,",
        "page_index": 5,
        "bbox": [
            81.775,
            402.3376344,
            290.497358594,
            413.48673460000003
        ]
    },
    {
        "content": "we build a baseline model, namely SFTDS, by di\u2212",
        "page_index": 5,
        "bbox": [
            70.473,
            387.72524910000004,
            290.950262828,
            399.7740981
        ]
    },
    {
        "content": "rectly performing SFT for DeepSeek\u2212Coder\u2212Base",
        "page_index": 5,
        "bbox": [
            70.866,
            375.2396344,
            289.1379782568,
            386.1487344
        ]
    },
    {
        "content": "1.3B on evol\u2212codealpaca\u2212v1. To compare X FT",
        "page_index": 5,
        "bbox": [
            70.048,
            361.6906344,
            289.4718834955,
            373.0579166
        ]
    },
    {
        "content": "with EWA (Huang et al., 2023), we also implement",
        "page_index": 5,
        "bbox": [
            70.473,
            348.1416344,
            289.13434585400006,
            359.0507344
        ]
    },
    {
        "content": "a baseline EWADS and instruction\u2212tune it using",
        "page_index": 5,
        "bbox": [
            70.866,
            333.5292491,
            289.13759797200004,
            345.5770981
        ]
    },
    {
        "content": "the same hyperparameter setting as SFTDS, which",
        "page_index": 5,
        "bbox": [
            70.866,
            320.04145839999995,
            289.1354813,
            331.9517344
        ]
    },
    {
        "content": "is described in Appendix A.1. More implementa\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            307.4936344,
            290.94138339600005,
            318.40273440000004
        ]
    },
    {
        "content": "tion details of EWADS can be seen in Appendix",
        "page_index": 5,
        "bbox": [
            70.866,
            292.94245839999996,
            289.40663947999997,
            304.8537344
        ]
    },
    {
        "content": "A.2. Furthermore, we incorporate multiple small",
        "page_index": 5,
        "bbox": [
            70.473,
            280.3956344,
            289.135218582,
            291.30473440000003
        ]
    },
    {
        "content": "open\u2212source models (<3B) as our baselines, includ\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            266.8456344,
            290.949237948,
            277.7547344
        ]
    },
    {
        "content": "ing DeepSeek\u2212Coder\u2212Base 1.3B, DeepSeek\u2212Coder\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            253.2966344,
            290.949237948,
            264.2057344
        ]
    },
    {
        "content": "Instruct 1.3B (Guo et al., 2024), Phi\u22122 2.7B, and",
        "page_index": 5,
        "bbox": [
            70.866,
            239.7476344,
            289.13876371200007,
            250.6567344
        ]
    },
    {
        "content": "STABLE\u2212CODE 3B (Pinnaparaju et al., 2024).",
        "page_index": 5,
        "bbox": [
            71.139,
            226.1986344,
            267.8400164,
            237.1077344
        ]
    },
    {
        "content": "4.2 Python Text\u2212to\u2212Code Generation",
        "page_index": 5,
        "bbox": [
            70.86599999999999,
            200.5209981,
            247.7243292,
            211.4300981
        ]
    },
    {
        "content": "HumanEval (Chen et al., 2021) and MBPP (Austin",
        "page_index": 5,
        "bbox": [
            70.866,
            181.0366344,
            289.138000075,
            191.9457344
        ]
    },
    {
        "content": "et al., 2021) benchmarks are the two most widely\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            167.4876344,
            290.94877976580005,
            178.39673439999999
        ]
    },
    {
        "content": "used collections of Python code generation tasks.",
        "page_index": 5,
        "bbox": [
            70.866,
            153.93863439999998,
            291.04152893400004,
            164.84773439999998
        ]
    },
    {
        "content": "We further employ HumanEval+ and MBPP+,",
        "page_index": 5,
        "bbox": [
            70.353,
            140.3896344,
            290.49514708799995,
            151.2987344
        ]
    },
    {
        "content": "which use more tests automatically generated by",
        "page_index": 5,
        "bbox": [
            70.473,
            126.8406344,
            289.51354617000004,
            137.7497344
        ]
    },
    {
        "content": "EvalPlus (Liu et al., 2023) for more rigorous evalu\u2212",
        "page_index": 5,
        "bbox": [
            70.866,
            113.2906344,
            290.94907431150006,
            124.19973440000001
        ]
    },
    {
        "content": "ation. We leave the details in Appendix A.3.",
        "page_index": 5,
        "bbox": [
            70.866,
            99.7416344,
            264.17525200000006,
            110.6507344
        ]
    },
    {
        "content": "16 is the best\u2212performing number of activated experts per",
        "page_index": 5,
        "bbox": [
            83.519,
            77.0812576,
            289.28394912000005,
            87.5134384
        ]
    },
    {
        "content": "our HumanEval+ experiments using top {2, 4, 6} experts.",
        "page_index": 5,
        "bbox": [
            70.866,
            67.1192576,
            277.7460672,
            76.2829184
        ]
    },
    {
        "content": "6",
        "page_index": 5,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "and 64.6 pass@1 on HumanEval+, which makes",
        "page_index": 5,
        "bbox": [
            306.142,
            730.6096344,
            524.407170028,
            741.5187344
        ]
    },
    {
        "content": "it the new state\u2212of\u2212the\u2212art small code LLM (<3B).",
        "page_index": 5,
        "bbox": [
            306.142,
            717.0606344,
            526.3166125695999,
            727.9697344
        ]
    },
    {
        "content": "We can also observe that X FTDS has a clear im\u2212",
        "page_index": 5,
        "bbox": [
            305.629,
            702.5104584,
            526.217765024,
            714.6607346000001
        ]
    },
    {
        "content": "provement over the SFTDS on both benchmarks,",
        "page_index": 5,
        "bbox": [
            306.142,
            688.9604584,
            525.7692605899999,
            700.8717343999999
        ]
    },
    {
        "content": "with 13% and 2% improvement on HumanEval+",
        "page_index": 5,
        "bbox": [
            305.749,
            676.4136344,
            525.9467834980001,
            687.3227344
        ]
    },
    {
        "content": "and MBPP+ respectively, while EWADS even per\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            661.8624583999999,
            526.2218338015,
            673.7727344
        ]
    },
    {
        "content": "forms worse than SFTDS on MBPP(+). X FTDS",
        "page_index": 5,
        "bbox": [
            306.142,
            648.3134584,
            523.7122928000001,
            660.4637346000001
        ]
    },
    {
        "content": "also outperforms EWADS on both benchmarks.",
        "page_index": 5,
        "bbox": [
            306.142,
            634.7644584,
            526.32135391,
            646.6747343999999
        ]
    },
    {
        "content": "Surprisingly, X FTDS even surpasses MoEDS on",
        "page_index": 5,
        "bbox": [
            306.142,
            621.2144584,
            524.409282,
            633.3657346
        ]
    },
    {
        "content": "HumanEval and HumanEval+, despite only using",
        "page_index": 5,
        "bbox": [
            306.142,
            608.6676344,
            524.4140218932,
            619.5767344
        ]
    },
    {
        "content": "around 1/8\u00d7 parameters and around 1/6\u00d7 compu\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            595.1176344,
            526.2208465099999,
            606.2667346
        ]
    },
    {
        "content": "tations, which showcases the effectiveness of our",
        "page_index": 5,
        "bbox": [
            306.142,
            581.5686344,
            524.5928002239999,
            592.4777343999999
        ]
    },
    {
        "content": "simple learnable merging technique. Appendix A.4",
        "page_index": 5,
        "bbox": [
            306.142,
            568.0196344,
            524.407781888,
            578.9287343999999
        ]
    },
    {
        "content": "further demonstrates the statistical significance of",
        "page_index": 5,
        "bbox": [
            306.142,
            554.4706344,
            524.4090037072,
            565.3797344
        ]
    },
    {
        "content": "the improvements brought by X FT.",
        "page_index": 5,
        "bbox": [
            306.142,
            540.9216344,
            462.1591947,
            552.0707346
        ]
    },
    {
        "content": "4.3 Multilingual Code Generation",
        "page_index": 5,
        "bbox": [
            306.14199999999994,
            518.1129981,
            471.30577399999993,
            529.0220981
        ]
    },
    {
        "content": "We use MultiPL\u2212E (Cassano et al., 2022), a multi\u2212",
        "page_index": 5,
        "bbox": [
            305.629,
            500.2736344,
            526.2259365579,
            511.1827344
        ]
    },
    {
        "content": "programming benchmark that supports 18 program\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            486.7246344,
            526.2252379480001,
            497.63373440000004
        ]
    },
    {
        "content": "ming languages in addition to Python, to evalu\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            473.1756344,
            526.2173833960001,
            484.0847344
        ]
    },
    {
        "content": "ate the multilingual ability and generalizability of",
        "page_index": 5,
        "bbox": [
            306.142,
            459.6256344,
            524.41236371,
            470.53473440000005
        ]
    },
    {
        "content": "X FT. Among these, we choose 6 representative",
        "page_index": 5,
        "bbox": [
            306.142,
            446.0766344,
            524.413464986,
            457.2257346
        ]
    },
    {
        "content": "programming for their distinct language features:",
        "page_index": 5,
        "bbox": [
            306.142,
            432.5276344,
            525.928074064,
            443.43673440000003
        ]
    },
    {
        "content": "Java, JavaScript, C++, PHP, Swift, and Rust, fol\u2212",
        "page_index": 5,
        "bbox": [
            305.935,
            418.9786344,
            526.221801754,
            429.8877344
        ]
    },
    {
        "content": "lowing Wei et al. (2023). Table 2 shows, among",
        "page_index": 5,
        "bbox": [
            306.142,
            405.4296344,
            524.414763712,
            416.3387344
        ]
    },
    {
        "content": "all 1.3B models, X FTDS achieves the best aver\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            390.8784584,
            526.2152859859999,
            403.0287346
        ]
    },
    {
        "content": "age multilingual performance and performs the",
        "page_index": 5,
        "bbox": [
            306.142,
            378.3306344,
            524.414763712,
            389.23973440000003
        ]
    },
    {
        "content": "best on 5 (out of 6) individual programming lan\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            364.7816344,
            526.2173833960001,
            375.6907344
        ]
    },
    {
        "content": "guages, overall largely improving SFTDS which",
        "page_index": 5,
        "bbox": [
            306.142,
            350.2304584,
            524.410077208,
            362.1417344
        ]
    },
    {
        "content": "uses standard SFT. Notably, the overall perfor\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            337.6836344,
            526.217383396,
            348.59273440000004
        ]
    },
    {
        "content": "mance of EWADS is on par with SFTDS, indicating",
        "page_index": 5,
        "bbox": [
            306.142,
            323.13245839999996,
            524.404749164,
            335.04273440000003
        ]
    },
    {
        "content": "that EWADS may not improve SFT on multilingual",
        "page_index": 5,
        "bbox": [
            306.142,
            309.5834584,
            524.4058261580001,
            321.4937344
        ]
    },
    {
        "content": "coding. Appendix A.5 further studies whether each",
        "page_index": 5,
        "bbox": [
            306.142,
            297.0356344,
            524.407781888,
            307.9447344
        ]
    },
    {
        "content": "expert in MoEDS specializes differently in these",
        "page_index": 5,
        "bbox": [
            306.142,
            282.4844584,
            524.407787162,
            294.39573440000004
        ]
    },
    {
        "content": "programming languages.",
        "page_index": 5,
        "bbox": [
            306.142,
            269.9376344,
            414.6111813,
            280.8467344
        ]
    },
    {
        "content": "4.4 Code Generation for Data Science",
        "page_index": 5,
        "bbox": [
            306.142,
            247.1289981,
            489.1748798,
            258.0380981
        ]
    },
    {
        "content": "The DS\u22121000 dataset (Lai et al., 2022) is a collec\u2212",
        "page_index": 5,
        "bbox": [
            305.804,
            229.28963439999998,
            526.2256164118,
            240.19873439999998
        ]
    },
    {
        "content": "tion of 1000 realistic data science coding problems",
        "page_index": 5,
        "bbox": [
            306.142,
            215.7406344,
            524.4057637045001,
            226.6497344
        ]
    },
    {
        "content": "ranging from 7 popular data science libraries in",
        "page_index": 5,
        "bbox": [
            306.142,
            202.1916344,
            524.414763712,
            213.1007344
        ]
    },
    {
        "content": "Python, including Matplotlib (plt), NumPy (np),",
        "page_index": 5,
        "bbox": [
            306.142,
            188.6416344,
            525.7722921159999,
            199.55073439999998
        ]
    },
    {
        "content": "Pandas (pd), SciPy (scp), Scikit\u2212Learn (sk), Py\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            175.0926344,
            526.217383396,
            186.0017344
        ]
    },
    {
        "content": "Torch (py), and TensorFlow (tf). We evaluate X FT",
        "page_index": 5,
        "bbox": [
            305.804,
            161.5436344,
            524.7482249848,
            172.6927346
        ]
    },
    {
        "content": "on DS\u22121000 to understand its effectiveness for prac\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            147.9946344,
            526.225237948,
            158.9037344
        ]
    },
    {
        "content": "tical data science engineering. We follow the eval\u2212",
        "page_index": 5,
        "bbox": [
            306.142,
            134.4456344,
            526.2221834000001,
            145.35473439999998
        ]
    },
    {
        "content": "uation setting of prior works (Guo et al., 2024;",
        "page_index": 5,
        "bbox": [
            306.142,
            120.89563439999999,
            525.316073554,
            131.8047344
        ]
    },
    {
        "content": "Wei et al., 2023). In Table 3, X FTDS achieves the",
        "page_index": 5,
        "bbox": [
            305.629,
            106.3454584,
            524.4144919805999,
            118.4957346
        ]
    },
    {
        "content": "best overall performance among all the evaluated",
        "page_index": 5,
        "bbox": [
            306.142,
            93.79763439999999,
            524.414763712,
            104.7067344
        ]
    },
    {
        "content": "1.3B models. Specifically, X FTDS consistently sur\u2212",
        "page_index": 5,
        "bbox": [
            305.324,
            79.2474584,
            526.216203818,
            91.3977346
        ]
    },
    {
        "content": "passes SFTDS among all the seven studied libraries",
        "page_index": 5,
        "bbox": [
            306.142,
            65.6974584,
            524.405748014,
            77.6087344
        ]
    },
    {
        "content": "Model Size Instruction",
        "page_index": 6,
        "bbox": [
            90.055,
            749.5426344,
            312.38713029999997,
            767.2267343999999
        ]
    },
    {
        "content": "Dataset",
        "page_index": 6,
        "bbox": [
            272.394,
            742.7686344,
            305.1103909,
            753.6777344
        ]
    },
    {
        "content": "Dataset",
        "page_index": 6,
        "bbox": [
            329.382,
            756.3176344,
            362.0983909,
            767.2267343999999
        ]
    },
    {
        "content": "Size",
        "page_index": 6,
        "bbox": [
            336.347,
            742.7686344,
            355.1324702,
            753.6777344
        ]
    },
    {
        "content": "HumanEval (+) MBPP (+)",
        "page_index": 6,
        "bbox": [
            374.053,
            738.4556344,
            499.24583160000003,
            749.3647344
        ]
    },
    {
        "content": "Benchmark",
        "page_index": 6,
        "bbox": [
            411.503,
            756.3176344,
            461.793951,
            767.2267343999999
        ]
    },
    {
        "content": "GPT\u22123.5 (May 2023) \u2212 Private \u2212 73.2 (66.5) \u2212",
        "page_index": 6,
        "bbox": [
            90.055,
            720.3766344,
            481.4189624999999,
            731.2857343999999
        ]
    },
    {
        "content": "STABLE\u2212CODE 3B \u2212 \u2212 28.7 (25.6) 53.6 (44.1)",
        "page_index": 6,
        "bbox": [
            90.328,
            702.2966344,
            503.69223280000006,
            713.2057344
        ]
    },
    {
        "content": "DeepSeek\u2212Coder\u2212Base 1.3B \u2212 \u2212 28.7 (25.6) 55.6 (46.9)",
        "page_index": 6,
        "bbox": [
            90.055,
            688.7476344,
            503.69534469999996,
            699.6567344
        ]
    },
    {
        "content": "Phi\u22122 2.7B \u2212 \u2212 48.8 (45.1) 62.7 (52.9)",
        "page_index": 6,
        "bbox": [
            90.055,
            675.1986344000001,
            503.69534469999996,
            686.1077344
        ]
    },
    {
        "content": "DeepSeek\u2212Coder\u2212Instruct 1.3B Private 2B 65.2 (59.8) 63.9 (53.1)",
        "page_index": 6,
        "bbox": [
            90.055,
            661.6496344000001,
            503.69534469999996,
            672.5587344
        ]
    },
    {
        "content": "SFTDS 1.3B Evol\u2212Instruct 0.3B 61.6 (57.3) 59.6 (49.1)",
        "page_index": 6,
        "bbox": [
            90.055,
            642.5674584000001,
            503.6921453,
            654.4787344
        ]
    },
    {
        "content": "EWADS 1.3B Evol\u2212Instruct 0.3B 67.1 (63.4) 58.9 (48.4)",
        "page_index": 6,
        "bbox": [
            90.05499999999998,
            629.0184584000001,
            503.69608159999996,
            641.0060981
        ]
    },
    {
        "content": "MoEDS 8\u00d71.3B Evol\u2212Instruct 0.3B 65.2 (62.2) 60.4 (50.1)",
        "page_index": 6,
        "bbox": [
            90.055,
            610.9394584,
            503.6902304,
            623.0897346
        ]
    },
    {
        "content": "X FTDS 1.3B Evol\u2212Instruct 0.3B 67.1 (64.6) 60.4 (50.1)",
        "page_index": 6,
        "bbox": [
            90.05500000000004,
            597.3894584000001,
            503.6937303,
            609.5407346000001
        ]
    },
    {
        "content": "Table 1: Pass@1 results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding,",
        "page_index": 6,
        "bbox": [
            70.557,
            580.8060783999999,
            525.6546292639998,
            590.7686784
        ]
    },
    {
        "content": "following the setting of prior works (Wei et al., 2023; Liu et al., 2023). We report the results consistently from",
        "page_index": 6,
        "bbox": [
            70.866,
            568.8510784,
            524.4097784639995,
            578.8136784000001
        ]
    },
    {
        "content": "the EvalPlus (Liu et al., 2023) Leaderboard. Note that numbers in bold refer to the highest scores among all 1.3B",
        "page_index": 6,
        "bbox": [
            70.866,
            556.8960784,
            524.4079453455997,
            566.8586784
        ]
    },
    {
        "content": "models fine\u2212tuned on public datasets, which is the same for all the other tables.",
        "page_index": 6,
        "bbox": [
            70.866,
            544.9410783999999,
            384.92700239999994,
            554.9036784
        ]
    },
    {
        "content": "Model Size Programming Language Average",
        "page_index": 6,
        "bbox": [
            103.327,
            515.0516344,
            491.9489404000001,
            532.7357344
        ]
    },
    {
        "content": "C++ PHP Java JS Swift Rust",
        "page_index": 6,
        "bbox": [
            261.23,
            503.9646344,
            442.41924190000003,
            514.8737344
        ]
    },
    {
        "content": "DeepSeek\u2212Coder\u2212Base 1.3B 28.1 22.9 27.2 28.7 10.9 18.0 22.6",
        "page_index": 6,
        "bbox": [
            103.327,
            485.8856344,
            491.95277840000006,
            496.79473440000004
        ]
    },
    {
        "content": "SFTDS 1.3B 40.4 38.5 40.2 46.2 16.4 27.7 34.9",
        "page_index": 6,
        "bbox": [
            103.327,
            466.80445839999993,
            491.9532953,
            478.7910981
        ]
    },
    {
        "content": "EWADS 1.3B 39.4 38.4 37.3 45.2 20.9 28.6 35.0",
        "page_index": 6,
        "bbox": [
            103.32699999999997,
            453.2544584,
            491.9495891,
            465.1657344
        ]
    },
    {
        "content": "MoEDS 8\u00d71.3B 42.2 42.2 35.4 49.8 24.7 30.6 37.5",
        "page_index": 6,
        "bbox": [
            103.327,
            435.17545839999997,
            491.9476742000001,
            447.32573460000003
        ]
    },
    {
        "content": "X FTDS 1.3B 42.7 41.5 36.0 49.7 25.3 32.1 37.9",
        "page_index": 6,
        "bbox": [
            103.32700000000003,
            421.6254584,
            491.9532953,
            433.77673460000005
        ]
    },
    {
        "content": "Table 2: Pass@1 results on MultiPL\u2212E (Cassano et al., 2022) following the same hyperparameter settings as",
        "page_index": 6,
        "bbox": [
            70.557,
            405.04207840000004,
            524.4039751279998,
            415.0046784
        ]
    },
    {
        "content": "prior works (Wei et al., 2023; Luo et al., 2023): temperature = 0.2, top\u209a = 0.95, max\u2097ength = 512, and",
        "page_index": 6,
        "bbox": [
            70.866,
            393.0870784,
            524.4045132599999,
            403.4681076
        ]
    },
    {
        "content": "num\u209bamples = 50. All models are evaluated using bigcode\u2212evaluation\u2212harness (Ben Allal et al., 2022).",
        "page_index": 6,
        "bbox": [
            70.866,
            381.1320784,
            506.24478460000006,
            391.5131076
        ]
    },
    {
        "content": "and also outperforms EWADS in general.",
        "page_index": 6,
        "bbox": [
            70.86599999999999,
            358.40645839999996,
            249.48431109999999,
            370.3177344
        ]
    },
    {
        "content": "5 Ablation Study",
        "page_index": 6,
        "bbox": [
            70.86600000000001,
            335.4153632,
            166.19676480000004,
            347.3705632
        ]
    },
    {
        "content": "5.1 Effect of Shared Expert with Routing",
        "page_index": 6,
        "bbox": [
            70.86600000000001,
            314.6609981,
            269.44434729999995,
            325.5700981
        ]
    },
    {
        "content": "Weight Normalization",
        "page_index": 6,
        "bbox": [
            94.86600000000001,
            301.1119981,
            198.0879042,
            312.0210981
        ]
    },
    {
        "content": "We demonstrate the importance of the shared ex\u2212",
        "page_index": 6,
        "bbox": [
            70.353,
            283.5106344,
            290.940238368,
            294.41973440000004
        ]
    },
    {
        "content": "pert of X FT by comparing its performance with",
        "page_index": 6,
        "bbox": [
            70.866,
            269.9616344,
            289.13172801200005,
            281.1107346
        ]
    },
    {
        "content": "an ablation by excluding it from X FT. Table 4",
        "page_index": 6,
        "bbox": [
            306.142,
            359.4086344,
            524.4092634419999,
            370.5577346
        ]
    },
    {
        "content": "shows that, after removing routing weight normal\u2212",
        "page_index": 6,
        "bbox": [
            306.142,
            345.8596344,
            526.2238634014001,
            356.7687344
        ]
    },
    {
        "content": "ization, the performance substantially decreases,",
        "page_index": 6,
        "bbox": [
            306.142,
            332.3096344,
            525.772292116,
            343.2187344
        ]
    },
    {
        "content": "despite being still better than the original sparse up\u2212",
        "page_index": 6,
        "bbox": [
            306.142,
            318.7606344,
            526.2252379480001,
            329.66973440000004
        ]
    },
    {
        "content": "cycling that does not use the shared expert setting.",
        "page_index": 6,
        "bbox": [
            306.142,
            305.2116344,
            525.4803646,
            316.1207344
        ]
    },
    {
        "content": "5.2 Effect of Merging Strategy",
        "page_index": 6,
        "bbox": [
            306.142,
            277.2059981,
            455.08394229999993,
            288.1150981
        ]
    },
    {
        "content": "the sparse upcycling (Komatsuzaki et al., 2023)",
        "page_index": 6,
        "bbox": [
            70.866,
            256.4126344,
            289.86203704200005,
            267.3217344
        ]
    },
    {
        "content": "In this section, we demonstrate the effectiveness of",
        "page_index": 6,
        "bbox": [
            306.142,
            256.3876344,
            524.407781888,
            267.29673440000005
        ]
    },
    {
        "content": "baseline that does not employ any shared expert.",
        "page_index": 6,
        "bbox": [
            70.866,
            242.8636344,
            291.04152893400004,
            253.7727344
        ]
    },
    {
        "content": "our learnable merging technique by comparing it",
        "page_index": 6,
        "bbox": [
            306.142,
            242.8386344,
            524.4147637120001,
            253.74773439999998
        ]
    },
    {
        "content": "As shown in Table 4, the performance of the orig\u2212",
        "page_index": 6,
        "bbox": [
            70.473,
            229.31363439999998,
            290.9422564515,
            240.22273439999998
        ]
    },
    {
        "content": "with (1) directly merging experts with initialized",
        "page_index": 6,
        "bbox": [
            305.749,
            229.28963439999998,
            524.411218582,
            240.19873439999998
        ]
    },
    {
        "content": "inal sparse upcycling (with the \"\u2212 Shared Expert\"",
        "page_index": 6,
        "bbox": [
            70.866,
            215.7646344,
            289.138763712,
            226.6737344
        ]
    },
    {
        "content": "mixing coefficients, and (2) the learnable merging",
        "page_index": 6,
        "bbox": [
            306.142,
            215.7406344,
            524.4111418908,
            226.6497344
        ]
    },
    {
        "content": "label) drops greatly compared with MoEDS. No\u2212",
        "page_index": 6,
        "bbox": [
            70.866,
            201.2144584,
            290.94829373,
            213.1247344
        ]
    },
    {
        "content": "technique without the shared rate setting, which",
        "page_index": 6,
        "bbox": [
            306.142,
            202.1916344,
            524.414763712,
            213.1007344
        ]
    },
    {
        "content": "tably, the sparse upcycling model even performs",
        "page_index": 6,
        "bbox": [
            70.866,
            188.6666344,
            289.138763712,
            199.5757344
        ]
    },
    {
        "content": "is the same setting as the learned soup in Model",
        "page_index": 6,
        "bbox": [
            306.142,
            188.6416344,
            524.4147637120001,
            199.55073439999998
        ]
    },
    {
        "content": "worse than SFTDS on HumanEval+, showing its",
        "page_index": 6,
        "bbox": [
            70.473,
            174.1154584,
            289.13465529,
            186.02673439999998
        ]
    },
    {
        "content": "Soups (Wortsman et al., 2022) and is described in",
        "page_index": 6,
        "bbox": [
            306.142,
            175.0926344,
            524.4090037072001,
            186.0017344
        ]
    },
    {
        "content": "ineffectiveness for instruction tuning.",
        "page_index": 6,
        "bbox": [
            70.866,
            161.5676344,
            233.77159030000004,
            172.4767344
        ]
    },
    {
        "content": "Eq. (3) and Eq. (4). Specifically, we initialize the",
        "page_index": 6,
        "bbox": [
            306.142,
            161.5436344,
            524.4122328008,
            172.4527344
        ]
    },
    {
        "content": "While the shared expert setting is also employed",
        "page_index": 6,
        "bbox": [
            81.775,
            147.9946344,
            289.13604552800007,
            158.9037344
        ]
    },
    {
        "content": "learnable mixing coefficient of the shared expert as",
        "page_index": 6,
        "bbox": [
            306.142,
            147.9946344,
            524.407781888,
            158.9037344
        ]
    },
    {
        "content": "in most recent works (Dai et al., 2024; Gou et al.,",
        "page_index": 6,
        "bbox": [
            70.866,
            134.4456344,
            290.49629211599995,
            145.35473439999998
        ]
    },
    {
        "content": "2024), their routing strategy will cause perfor\u2212",
        "page_index": 6,
        "bbox": [
            70.866,
            120.89563439999999,
            290.94138339599994,
            131.8047344
        ]
    },
    {
        "content": "0.75 and that of the other 7 normal experts as 1",
        "page_index": 6,
        "bbox": [
            306.142,
            134.4456344,
            521.09751413,
            147.5209006
        ]
    },
    {
        "content": "28",
        "page_index": 6,
        "bbox": [
            514.746,
            131.4938006,
            523.2150282599999,
            139.4639006
        ]
    },
    {
        "content": "for fair comparison. As shown in Table 5, trained",
        "page_index": 6,
        "bbox": [
            306.142,
            120.89563439999999,
            524.4122328007999,
            131.8047344
        ]
    },
    {
        "content": "mance degradation due to the scale mismatch be\u2212",
        "page_index": 6,
        "bbox": [
            70.866,
            107.3466344,
            290.941383396,
            118.25573440000001
        ]
    },
    {
        "content": "mixing coefficients outperform the initialized mix\u2212",
        "page_index": 6,
        "bbox": [
            306.142,
            107.3466344,
            526.2246270384,
            118.25573440000001
        ]
    },
    {
        "content": "tween the outputs of the upcycled MoE layer and",
        "page_index": 6,
        "bbox": [
            70.866,
            93.79763439999999,
            289.13876371199996,
            104.7067344
        ]
    },
    {
        "content": "ing coefficients for merging. Furthermore, remov\u2212",
        "page_index": 6,
        "bbox": [
            306.142,
            93.79763439999999,
            526.2234052192001,
            104.7067344
        ]
    },
    {
        "content": "the original FFN layer. To understand the impor\u2212",
        "page_index": 6,
        "bbox": [
            70.866,
            80.2486344,
            290.94138339600005,
            91.15773440000001
        ]
    },
    {
        "content": "ing the shared rate setting will largely degrade the",
        "page_index": 6,
        "bbox": [
            306.142,
            80.2486344,
            524.4095709804,
            91.15773440000001
        ]
    },
    {
        "content": "tance of routing weight normalization, we conduct",
        "page_index": 6,
        "bbox": [
            70.866,
            66.6996344,
            289.13418189,
            77.6087344
        ]
    },
    {
        "content": "performance of X FTDS on both HumanEval and",
        "page_index": 6,
        "bbox": [
            306.142,
            65.6974584,
            524.407309342,
            77.8487346
        ]
    },
    {
        "content": "7",
        "page_index": 6,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "Model Size Data Science Library Overall",
        "page_index": 7,
        "bbox": [
            92.505,
            749.5426344,
            502.77102920000004,
            767.2267343999999
        ]
    },
    {
        "content": "np pd plt py scp tf sk",
        "page_index": 7,
        "bbox": [
            254.499,
            738.4556344,
            451.080982,
            749.3647344
        ]
    },
    {
        "content": "DeepSeek\u2212Coder\u2212Base 1.3B 25.1 5.8 34.5 12.7 9.8 11.1 12.7 16.4",
        "page_index": 7,
        "bbox": [
            92.505,
            720.3766344,
            502.7744328,
            731.2857343999999
        ]
    },
    {
        "content": "SFTDS 1.3B 30.9 17.0 40.5 32.7 18.3 21.1 24.4 25.9",
        "page_index": 7,
        "bbox": [
            92.505,
            701.2954584,
            502.7712435000001,
            713.2057344
        ]
    },
    {
        "content": "EWADS 1.3B 32.9 19.4 41.8 25.7 17.7 22.2 33.0 27.8",
        "page_index": 7,
        "bbox": [
            92.50499999999997,
            687.7464584,
            502.7723277999999,
            699.7330981
        ]
    },
    {
        "content": "MoEDS 8\u00d71.3B 33.2 21.3 38.4 41.8 21.8 23.5 37.5 30.0",
        "page_index": 7,
        "bbox": [
            92.505,
            669.6664584,
            502.77032860000014,
            681.8167346
        ]
    },
    {
        "content": "X FTDS 1.3B 32.9 20.2 38.9 41.4 21.1 16.9 37.5 29.3",
        "page_index": 7,
        "bbox": [
            92.505,
            656.1174584,
            502.77232779999997,
            668.2677346
        ]
    },
    {
        "content": "Table 3: Pass@1 results on DS\u22121000 (completion format) with temperature = 0.2, top\u209a = 0.5, max\u2097ength =",
        "page_index": 7,
        "bbox": [
            70.557,
            639.5340784,
            524.40991028,
            649.9151076
        ]
    },
    {
        "content": "1024, and num\u209bamples = 40, following the same hyperparameter setting used in prior works (Wei et al., 2023).",
        "page_index": 7,
        "bbox": [
            70.36799999999994,
            627.5780784,
            519.0873138,
            637.9591076
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 7,
        "bbox": [
            79.024,
            604.4636344,
            274.9950724,
            615.3727344
        ]
    },
    {
        "content": "Model \u03bb HumanEval HumenEval+",
        "page_index": 7,
        "bbox": [
            311.734,
            604.4636344,
            512.8421923999999,
            615.6127346000001
        ]
    },
    {
        "content": "SFTDS 61.6 57.3",
        "page_index": 7,
        "bbox": [
            79.024,
            585.3824584,
            280.978261,
            597.2937344
        ]
    },
    {
        "content": "MoEDS 65.2 62.2",
        "page_index": 7,
        "bbox": [
            79.02399999999997,
            571.8334584,
            280.978261,
            583.8200981
        ]
    },
    {
        "content": "MoEDS",
        "page_index": 7,
        "bbox": [
            79.024,
            553.7534584,
            111.6262928,
            565.6647343999999
        ]
    },
    {
        "content": "\u2212 Normalization 63.4 59.1",
        "page_index": 7,
        "bbox": [
            84.479,
            541.2066344,
            280.97826100000003,
            558.8727344
        ]
    },
    {
        "content": "MoEDS",
        "page_index": 7,
        "bbox": [
            79.024,
            522.1254584,
            111.6262928,
            534.0357343999999
        ]
    },
    {
        "content": "\u2212 Shared Expert 61.6 56.7",
        "page_index": 7,
        "bbox": [
            84.479,
            509.57763439999997,
            280.97826100000003,
            527.2437344
        ]
    },
    {
        "content": "Table 4: Ablation over the design of MoEDS. \"\u2212 Normal\u2212",
        "page_index": 7,
        "bbox": [
            70.557,
            491.1436592,
            290.79224502399995,
            501.95467840000003
        ]
    },
    {
        "content": "ization\" removes the routing weight normalization from",
        "page_index": 7,
        "bbox": [
            70.866,
            480.03707840000004,
            289.1354078880001,
            489.9996784
        ]
    },
    {
        "content": "the router, making it the same design as MoCLE (Gou",
        "page_index": 7,
        "bbox": [
            70.866,
            468.0820784,
            289.1344913288001,
            478.04467839999995
        ]
    },
    {
        "content": "et al., 2024). \"\u2212 Shared Expert\" removes the shared ex\u2212",
        "page_index": 7,
        "bbox": [
            70.866,
            456.1270784,
            290.78601145600015,
            466.0896784
        ]
    },
    {
        "content": "pert setting, making MoEDS the same architecture as",
        "page_index": 7,
        "bbox": [
            70.866,
            443.32265920000003,
            289.129351816,
            454.1346784
        ]
    },
    {
        "content": "original sparse upcycling (Komatsuzaki et al., 2023).",
        "page_index": 7,
        "bbox": [
            70.866,
            432.21707840000005,
            281.48532660000006,
            442.17967840000006
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 7,
        "bbox": [
            70.866,
            409.1016344,
            284.28072330000003,
            420.01073440000005
        ]
    },
    {
        "content": "MoEDS 65.2 62.2",
        "page_index": 7,
        "bbox": [
            70.866,
            390.0204584,
            290.265261,
            401.93173440000004
        ]
    },
    {
        "content": "X FTDS (INIT) 66.5 64.0",
        "page_index": 7,
        "bbox": [
            70.86600000000001,
            376.4714584,
            290.26051620000004,
            388.62273460000006
        ]
    },
    {
        "content": "X FTDS 67.1 64.6",
        "page_index": 7,
        "bbox": [
            70.86600000000001,
            362.9224584,
            290.265261,
            375.07273460000005
        ]
    },
    {
        "content": "X FTDS",
        "page_index": 7,
        "bbox": [
            70.866,
            344.8424584,
            103.75929280000001,
            356.99373460000004
        ]
    },
    {
        "content": "\u2212 Shared Expert Rate 66.5 64.0",
        "page_index": 7,
        "bbox": [
            76.32100000000001,
            332.2946344,
            290.265261,
            349.9617344
        ]
    },
    {
        "content": "Table 5: Ablation over the design of X FTDS. \"(INIT)\"",
        "page_index": 7,
        "bbox": [
            70.557,
            313.8616592,
            289.138397841,
            324.89185560000004
        ]
    },
    {
        "content": "refers to directly using the initialized mixing coefficients",
        "page_index": 7,
        "bbox": [
            70.866,
            302.7550784,
            289.13540788800015,
            312.71767839999995
        ]
    },
    {
        "content": "to merge experts without training. \"\u2212 Shared Rate\" re\u2212",
        "page_index": 7,
        "bbox": [
            70.866,
            290.8000784,
            290.7888009840001,
            300.7626784
        ]
    },
    {
        "content": "moves the shared rate setting from X FTDS, which is the",
        "page_index": 7,
        "bbox": [
            70.866,
            277.99565920000003,
            289.138614532,
            289.02685560000003
        ]
    },
    {
        "content": "same as the learned soup (Wortsman et al., 2022).",
        "page_index": 7,
        "bbox": [
            70.866,
            266.8890784,
            269.00218880000006,
            276.85167839999997
        ]
    },
    {
        "content": "HumanEval+, demonstrating its importance.",
        "page_index": 7,
        "bbox": [
            70.866,
            245.1656344,
            264.3061612,
            256.0747344
        ]
    },
    {
        "content": "We further study the effect of the shared expert",
        "page_index": 7,
        "bbox": [
            81.775,
            229.28963439999998,
            289.1331109801,
            240.19873439999998
        ]
    },
    {
        "content": "rate \u03bb on the performance of the final merged dense",
        "page_index": 7,
        "bbox": [
            70.866,
            215.7406344,
            289.13145026600006,
            226.8897346
        ]
    },
    {
        "content": "model. We evenly choose five shared expert rates,",
        "page_index": 7,
        "bbox": [
            70.866,
            202.1916344,
            290.4942957507,
            213.1007344
        ]
    },
    {
        "content": "including 0.00, 0.25, 0.50, 0.75, and 1.00, to per\u2212",
        "page_index": 7,
        "bbox": [
            70.866,
            188.6416344,
            290.9413833960001,
            199.55073439999998
        ]
    },
    {
        "content": "form the learnable merging process and evaluate",
        "page_index": 7,
        "bbox": [
            70.866,
            175.0926344,
            289.138763712,
            186.0017344
        ]
    },
    {
        "content": "each merged dense model accordingly. Note that",
        "page_index": 7,
        "bbox": [
            70.866,
            161.5436344,
            289.138763712,
            172.4527344
        ]
    },
    {
        "content": "SFTDS \u2212 61.6 57.3",
        "page_index": 7,
        "bbox": [
            311.734,
            585.3824584,
            518.8161259000001,
            597.2937344
        ]
    },
    {
        "content": "0.00 62.8 59.8",
        "page_index": 7,
        "bbox": [
            360.007,
            568.3046344,
            518.8216778000001,
            579.2137344
        ]
    },
    {
        "content": "0.25 64.6 61.0",
        "page_index": 7,
        "bbox": [
            360.007,
            554.7556344000001,
            518.8216778000001,
            565.6647344
        ]
    },
    {
        "content": "X FTDS",
        "page_index": 7,
        "bbox": [
            311.734,
            540.2044584,
            344.6272928,
            552.3557346
        ]
    },
    {
        "content": "0.50 65.9 62.8",
        "page_index": 7,
        "bbox": [
            360.007,
            541.2066344000001,
            518.8216778000001,
            552.1157344000001
        ]
    },
    {
        "content": "0.75 67.1 64.6",
        "page_index": 7,
        "bbox": [
            360.007,
            527.7329981000001,
            518.8216778000001,
            538.6420981000001
        ]
    },
    {
        "content": "1.00 63.4 60.4",
        "page_index": 7,
        "bbox": [
            360.007,
            514.1076344000002,
            518.8216778000001,
            525.0167344000001
        ]
    },
    {
        "content": "Table 6: Ablation over the effect of the shared expert",
        "page_index": 7,
        "bbox": [
            305.833,
            496.52307840000003,
            524.4042746680001,
            506.4856784
        ]
    },
    {
        "content": "rate \u03bb in our learnable merging technique. X FT can con\u2212",
        "page_index": 7,
        "bbox": [
            306.142,
            484.56807840000005,
            526.06471974,
            494.74985560000005
        ]
    },
    {
        "content": "sistently outperform the normal SFT baseline regardless",
        "page_index": 7,
        "bbox": [
            306.142,
            472.6130784,
            524.4114078880001,
            482.5756784
        ]
    },
    {
        "content": "of the shared expert rate, while \u03bb = 0.75 is the optimal",
        "page_index": 7,
        "bbox": [
            306.142,
            460.65707840000005,
            524.4096845068001,
            470.83885560000004
        ]
    },
    {
        "content": "setting in our experiments.",
        "page_index": 7,
        "bbox": [
            306.142,
            448.7020784,
            412.25365260000007,
            458.66467839999996
        ]
    },
    {
        "content": "MoE model. As shown in Table 6, there are mainly",
        "page_index": 7,
        "bbox": [
            306.142,
            426.9786344,
            524.792654936,
            437.8877344
        ]
    },
    {
        "content": "three interesting observations:",
        "page_index": 7,
        "bbox": [
            306.142,
            413.4296344,
            437.3675639,
            424.3387344
        ]
    },
    {
        "content": "\u2022 The performance of the final merged dense",
        "page_index": 7,
        "bbox": [
            308.251,
            387.2066344,
            524.4148458120001,
            398.1157344
        ]
    },
    {
        "content": "model improves gradually when the shared ex\u2212",
        "page_index": 7,
        "bbox": [
            317.051,
            373.6576344,
            526.221647036,
            384.56673440000003
        ]
    },
    {
        "content": "pert rate grows from 0.00 to 0.75, indicating",
        "page_index": 7,
        "bbox": [
            317.051,
            360.10863439999997,
            524.4079000700001,
            371.0177344
        ]
    },
    {
        "content": "that general knowledge learned by the shared",
        "page_index": 7,
        "bbox": [
            317.051,
            346.5596344,
            524.4059570139999,
            357.5450981
        ]
    },
    {
        "content": "expert is important for better performance.",
        "page_index": 7,
        "bbox": [
            317.051,
            333.0106344,
            518.866275,
            343.99609810000004
        ]
    },
    {
        "content": "\u2022 The performance of the final merged dense",
        "page_index": 7,
        "bbox": [
            308.251,
            317.4686344,
            524.4148458120001,
            328.37773440000007
        ]
    },
    {
        "content": "model drops significantly when the shared ex\u2212",
        "page_index": 7,
        "bbox": [
            317.051,
            303.9196344,
            526.2216470359999,
            314.82873440000003
        ]
    },
    {
        "content": "pert rate grows from 0.75 to 1.00, showing that",
        "page_index": 7,
        "bbox": [
            317.051,
            290.3696344,
            524.4044527944001,
            301.2787344
        ]
    },
    {
        "content": "specific knowledge learned by other experts is",
        "page_index": 7,
        "bbox": [
            317.051,
            276.8969981,
            524.4120455279999,
            287.80609810000004
        ]
    },
    {
        "content": "also irreplaceable and ignoring them will lead",
        "page_index": 7,
        "bbox": [
            317.051,
            263.2716344,
            524.4064397046,
            274.2570981
        ]
    },
    {
        "content": "to a significant performance drop.",
        "page_index": 7,
        "bbox": [
            317.051,
            249.7226344,
            464.5965775,
            260.6317344
        ]
    },
    {
        "content": "\u2022 All the final merged dense models consistently",
        "page_index": 7,
        "bbox": [
            308.251,
            234.1806344,
            524.7924095,
            245.1660981
        ]
    },
    {
        "content": "outperform the normal SFT baseline regard\u2212",
        "page_index": 7,
        "bbox": [
            317.051,
            220.7079981,
            526.2154070308,
            231.6170981
        ]
    },
    {
        "content": "less of their shared expert rate, further demon\u2212",
        "page_index": 7,
        "bbox": [
            317.051,
            207.0826344,
            526.2172983902,
            218.0680981
        ]
    },
    {
        "content": "strating the effectiveness of X FT.",
        "page_index": 7,
        "bbox": [
            317.051,
            193.53363439999998,
            464.17719470000003,
            204.68273459999997
        ]
    },
    {
        "content": "5.3 Effect of Base Code LLM",
        "page_index": 7,
        "bbox": [
            306.142,
            167.38699809999997,
            449.4548467,
            178.29609809999997
        ]
    },
    {
        "content": "0.75 is the default shared expert rate used in our",
        "page_index": 7,
        "bbox": [
            70.866,
            147.9946344,
            289.3168002240001,
            158.9037344
        ]
    },
    {
        "content": "In this section, we demonstrate that the effective\u2212",
        "page_index": 7,
        "bbox": [
            306.142,
            147.9946344,
            526.217383396,
            158.9037344
        ]
    },
    {
        "content": "main experiments. If the shared expert rate is 0.00,",
        "page_index": 7,
        "bbox": [
            70.866,
            134.4456344,
            290.49970666430005,
            145.35473439999998
        ]
    },
    {
        "content": "ness of X FT is not dependent on the choice of base",
        "page_index": 7,
        "bbox": [
            306.142,
            134.4456344,
            524.40996536,
            145.59473459999998
        ]
    },
    {
        "content": "it means that the shared expert is ignored when",
        "page_index": 7,
        "bbox": [
            70.866,
            120.89563439999999,
            289.138763712,
            131.8047344
        ]
    },
    {
        "content": "code LLMs. To show this, we conduct an ablation",
        "page_index": 7,
        "bbox": [
            306.142,
            120.89563439999999,
            524.4112728000001,
            131.8047344
        ]
    },
    {
        "content": "constructing the merged dense model from the up\u2212",
        "page_index": 7,
        "bbox": [
            70.866,
            107.3466344,
            290.94618340000005,
            118.25573440000001
        ]
    },
    {
        "content": "experiment by applying X FT to STABLE\u2212CODE",
        "page_index": 7,
        "bbox": [
            306.142,
            107.3466344,
            524.132666322,
            118.4957346
        ]
    },
    {
        "content": "cycled MoE model; if the shared expert rate is 1.00,",
        "page_index": 7,
        "bbox": [
            70.866,
            93.79763439999999,
            290.5002193920001,
            104.7067344
        ]
    },
    {
        "content": "3B (Pinnaparaju et al., 2024), whose architecture",
        "page_index": 7,
        "bbox": [
            306.142,
            93.79763439999999,
            524.414763712,
            104.7067344
        ]
    },
    {
        "content": "it means that the final dense model is built by sim\u2212",
        "page_index": 7,
        "bbox": [
            70.866,
            80.2486344,
            290.9491070388001,
            91.15773440000001
        ]
    },
    {
        "content": "is different from DeepSeek\u2212Coder\u2212Base 1.3B (Guo",
        "page_index": 7,
        "bbox": [
            306.142,
            80.2486344,
            524.4110764362,
            91.15773440000001
        ]
    },
    {
        "content": "ply extracting the shared expert from the upcycled",
        "page_index": 7,
        "bbox": [
            70.866,
            66.6996344,
            289.1314327968,
            77.6087344
        ]
    },
    {
        "content": "et al., 2024), and see whether it can still improve",
        "page_index": 7,
        "bbox": [
            306.142,
            66.6996344,
            524.414763712,
            77.6087344
        ]
    },
    {
        "content": "8",
        "page_index": 7,
        "bbox": [
            294.911,
            36.8116344,
            300.36555,
            47.7207344
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 8,
        "bbox": [
            82.956,
            756.3176344,
            271.06161130000004,
            767.2267343999999
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 8,
        "bbox": [
            311.973,
            756.3176344,
            512.6022581000001,
            767.2267343999999
        ]
    },
    {
        "content": "SFTSTABLE 62.2 56.1",
        "page_index": 8,
        "bbox": [
            82.956,
            737.2364584000001,
            277.046261,
            749.1467344
        ]
    },
    {
        "content": "MoESTABLE 64.0 59.1",
        "page_index": 8,
        "bbox": [
            82.956,
            719.1564584,
            277.046261,
            731.0677344
        ]
    },
    {
        "content": "X FTSTABLE 68.3 62.2",
        "page_index": 8,
        "bbox": [
            82.95600000000002,
            705.6074584,
            277.046261,
            717.7577346
        ]
    },
    {
        "content": "Table 7: Ablation over the effect of the base model",
        "page_index": 8,
        "bbox": [
            70.557,
            689.0240784,
            289.13843652000014,
            698.9866784000001
        ]
    },
    {
        "content": "by replacing DeepSeek\u2212Coder\u2212Base 1.3B with STABLE\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            677.0690784,
            290.7944177046,
            687.0316784
        ]
    },
    {
        "content": "CODE 3B. X FT can consistently improve the instruction",
        "page_index": 8,
        "bbox": [
            71.115,
            665.1140783999999,
            289.13040956400005,
            675.2958556
        ]
    },
    {
        "content": "tuning performance of different base code LLMs.",
        "page_index": 8,
        "bbox": [
            70.866,
            653.1590784,
            267.8764150000001,
            663.1216784000001
        ]
    },
    {
        "content": "its performance. Hyperparameter settings are de\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            631.4346344,
            290.94138339600005,
            642.3437344
        ]
    },
    {
        "content": "tailed in Appendix A.6. As is shown in Table",
        "page_index": 8,
        "bbox": [
            70.866,
            617.8856344,
            289.13876371200007,
            628.7947343999999
        ]
    },
    {
        "content": "7, X FTSTABLE significantly improves SFTSTABLE",
        "page_index": 8,
        "bbox": [
            70.593,
            603.3344584,
            288.4363435,
            615.4857346
        ]
    },
    {
        "content": "by 10% on HumanEval and 11% on HumanEval+",
        "page_index": 8,
        "bbox": [
            70.866,
            590.7876344,
            290.67258317200003,
            601.6967344
        ]
    },
    {
        "content": "respectively. Furthermore, X FTSTABLE consis\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            576.2364584,
            290.94558990600007,
            588.3867346000001
        ]
    },
    {
        "content": "tently boosts the performance of MoESTABLE while",
        "page_index": 8,
        "bbox": [
            70.866,
            562.6874584,
            289.134219796,
            574.5977343999999
        ]
    },
    {
        "content": "only using 1/4\u00d7 parameters and 1/2\u00d7 computations.",
        "page_index": 8,
        "bbox": [
            70.866,
            550.1396344,
            291.044086602,
            561.2887346000001
        ]
    },
    {
        "content": "These results show that the effectiveness of X FT",
        "page_index": 8,
        "bbox": [
            70.528,
            536.5906344,
            289.472538094,
            547.7397346
        ]
    },
    {
        "content": "does not depend on any specific choice of base",
        "page_index": 8,
        "bbox": [
            70.866,
            523.0416344,
            289.13876371200007,
            533.9507344
        ]
    },
    {
        "content": "code LLMs, demonstrating the generalizability of",
        "page_index": 8,
        "bbox": [
            70.866,
            509.4916344,
            289.1361782553,
            520.4007344
        ]
    },
    {
        "content": "X FT across different model architectures.",
        "page_index": 8,
        "bbox": [
            70.866,
            495.9426344,
            254.1687813,
            507.0917346
        ]
    },
    {
        "content": "6 Discussion",
        "page_index": 8,
        "bbox": [
            70.866,
            471.5053632,
            142.6091552,
            483.46056319999997
        ]
    },
    {
        "content": "6.1 Training Overhead Analysis",
        "page_index": 8,
        "bbox": [
            70.866,
            450.4319981,
            226.6370389,
            461.3410981
        ]
    },
    {
        "content": "SFTDS",
        "page_index": 8,
        "bbox": [
            311.973,
            737.2364584000001,
            341.55229280000003,
            749.1467344
        ]
    },
    {
        "content": "w/ same steps 61.6 57.3",
        "page_index": 8,
        "bbox": [
            317.427,
            724.92427096,
            518.581261,
            742.3547344
        ]
    },
    {
        "content": "SFTDS",
        "page_index": 8,
        "bbox": [
            311.97299999999996,
            710.1384584,
            341.5522928,
            722.0487344
        ]
    },
    {
        "content": "w/ same budget 62.2 57.3",
        "page_index": 8,
        "bbox": [
            317.427,
            697.82627096,
            518.581261,
            715.2567343999999
        ]
    },
    {
        "content": "X FTDS 67.1 64.6",
        "page_index": 8,
        "bbox": [
            311.973,
            678.5094584,
            518.5812610000002,
            690.6597346
        ]
    },
    {
        "content": "Table 8: Experiments on the effect of training overhead.",
        "page_index": 8,
        "bbox": [
            305.833,
            661.9260783999999,
            526.1527109680002,
            671.8886784
        ]
    },
    {
        "content": "For our two SFT baselines, \"w/ same steps\" refers to",
        "page_index": 8,
        "bbox": [
            306.142,
            649.9710784,
            524.4084191080001,
            659.9336784000001
        ]
    },
    {
        "content": "one SFT baseline using the same training steps as X FT",
        "page_index": 8,
        "bbox": [
            306.142,
            638.0150784,
            524.7187170122,
            648.1968556
        ]
    },
    {
        "content": "while \"w/ same budget\" refers to the other SFT base\u2212",
        "page_index": 8,
        "bbox": [
            305.783,
            626.0600784,
            526.0614658040001,
            636.0226784
        ]
    },
    {
        "content": "line using the same training budget as X FT. X FT can",
        "page_index": 8,
        "bbox": [
            306.142,
            614.1050783999999,
            524.410671156,
            624.2868556
        ]
    },
    {
        "content": "consistently outperform both SFT baselines to a large",
        "page_index": 8,
        "bbox": [
            306.142,
            602.1500784,
            524.4084191080001,
            612.1126784
        ]
    },
    {
        "content": "extent, further demonstrating the ability of X FT to un\u2212",
        "page_index": 8,
        "bbox": [
            306.142,
            590.1950783999999,
            526.0612232106,
            600.3768556
        ]
    },
    {
        "content": "lock the power of code instruction tuning.",
        "page_index": 8,
        "bbox": [
            306.142,
            578.2400784,
            472.74655980000006,
            588.2026784000001
        ]
    },
    {
        "content": "in Table 9, overall, X FTTL improves SFTTL by 5%",
        "page_index": 8,
        "bbox": [
            306.142,
            555.5144584,
            525.315732014,
            567.6647346
        ]
    },
    {
        "content": "on MMLU, demonstrating the generalizable effec\u2212",
        "page_index": 8,
        "bbox": [
            306.142,
            542.9666344,
            526.2200670346,
            553.8757343999999
        ]
    },
    {
        "content": "tiveness of X FT for general instruction tuning.",
        "page_index": 8,
        "bbox": [
            306.142,
            529.4176344,
            510.52057600000006,
            540.5667346
        ]
    },
    {
        "content": "6.3 Preliminary Theoretical Explanation",
        "page_index": 8,
        "bbox": [
            306.14200000000005,
            504.5319981,
            502.4185272000001,
            515.4410981
        ]
    },
    {
        "content": "We provide a preliminary theoretical explanation of",
        "page_index": 8,
        "bbox": [
            305.629,
            485.5016344,
            524.4079459520001,
            496.4107344
        ]
    },
    {
        "content": "X FT by considering a simplified variant of it. Let\u2019s",
        "page_index": 8,
        "bbox": [
            306.142,
            471.9526344,
            524.4108468019999,
            483.10173460000004
        ]
    },
    {
        "content": "start by analyzing the two major steps of X FT:",
        "page_index": 8,
        "bbox": [
            306.142,
            458.4036344,
            511.15064950000004,
            469.5527346
        ]
    },
    {
        "content": "Compared with SFT, X FT will inevitably introduce",
        "page_index": 8,
        "bbox": [
            70.866,
            432.5756344,
            289.134516948,
            443.72473460000003
        ]
    },
    {
        "content": "\u2022 Step 1: Upcycling. According to the scaling",
        "page_index": 8,
        "bbox": [
            308.251,
            432.9126344,
            524.414609598,
            443.8980981
        ]
    },
    {
        "content": "additional overhead in the training process because",
        "page_index": 8,
        "bbox": [
            70.866,
            419.0266344,
            289.1317818880001,
            429.9357344
        ]
    },
    {
        "content": "law (Kaplan et al., 2020), the upcycled MoE",
        "page_index": 8,
        "bbox": [
            317.051,
            419.3636344,
            524.40790007,
            430.27273440000005
        ]
    },
    {
        "content": "X FT needs to fine\u2212tune the upcycled MoE model",
        "page_index": 8,
        "bbox": [
            70.866,
            405.4766344,
            289.13877407800004,
            416.62573460000004
        ]
    },
    {
        "content": "model performs better than the normal SFT",
        "page_index": 8,
        "bbox": [
            317.051,
            405.8146344,
            524.752845812,
            416.7237344
        ]
    },
    {
        "content": "while the normal SFT technique only needs to fine\u2212",
        "page_index": 8,
        "bbox": [
            70.473,
            391.9276344,
            290.94134008710006,
            402.8367344
        ]
    },
    {
        "content": "dense model due to more trainable parameters.",
        "page_index": 8,
        "bbox": [
            317.051,
            392.2646344,
            520.9420789999999,
            403.1737344
        ]
    },
    {
        "content": "tune the original dense model. To better understand",
        "page_index": 8,
        "bbox": [
            70.866,
            378.3786344,
            289.13178188800003,
            389.28773440000003
        ]
    },
    {
        "content": "the effect of such overhead, we conduct an experi\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            364.8296344,
            290.94991431220006,
            375.7387344
        ]
    },
    {
        "content": "ment where we use the same training budget (i.e.,",
        "page_index": 8,
        "bbox": [
            70.866,
            351.2806344,
            290.4938048412,
            362.1897344
        ]
    },
    {
        "content": "the same GPU hours) instead of the same training",
        "page_index": 8,
        "bbox": [
            70.866,
            337.7306344,
            289.1361782553,
            348.6397344
        ]
    },
    {
        "content": "steps for the normal SFT baseline. As shown in",
        "page_index": 8,
        "bbox": [
            70.866,
            324.1816344,
            289.138763712,
            335.09073440000003
        ]
    },
    {
        "content": "Table 8, although sharing the same training budget",
        "page_index": 8,
        "bbox": [
            70.528,
            310.6326344,
            289.13098216900005,
            321.5417344
        ]
    },
    {
        "content": "as X FTDS, the performance of SFTDS is still signif\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            296.0814584,
            290.94615543599997,
            308.2327346
        ]
    },
    {
        "content": "icantly worse than that of X FTDS, demonstrating",
        "page_index": 8,
        "bbox": [
            70.866,
            282.5324584,
            289.1313739464,
            294.68373460000004
        ]
    },
    {
        "content": "the ability of X FT to unlock the power of code",
        "page_index": 8,
        "bbox": [
            70.866,
            269.9846344,
            289.134976352,
            281.1337346
        ]
    },
    {
        "content": "instruction tuning using the same training budget.",
        "page_index": 8,
        "bbox": [
            70.866,
            256.43563439999997,
            287.31345310000006,
            267.3447344
        ]
    },
    {
        "content": "6.2 Generalizability for General Tasks",
        "page_index": 8,
        "bbox": [
            70.866,
            233.59699809999998,
            256.2552454,
            244.50609809999997
        ]
    },
    {
        "content": "In this section, we demonstrate that X FT can",
        "page_index": 8,
        "bbox": [
            70.866,
            215.7406344,
            289.136264358,
            226.8897346
        ]
    },
    {
        "content": "improve the performance of LLMs on general",
        "page_index": 8,
        "bbox": [
            70.866,
            202.1916344,
            289.138763712,
            213.1007344
        ]
    },
    {
        "content": "tasks across different domains by applying X FT",
        "page_index": 8,
        "bbox": [
            70.866,
            188.6416344,
            289.472538094,
            199.79073459999998
        ]
    },
    {
        "content": "to general instruction tuning. We use TinyLlama",
        "page_index": 8,
        "bbox": [
            70.866,
            175.0926344,
            289.138763712,
            186.0017344
        ]
    },
    {
        "content": "1.1B (Zhang et al., 2024) as the base model and",
        "page_index": 8,
        "bbox": [
            70.048,
            161.5436344,
            289.1330552980001,
            172.4527344
        ]
    },
    {
        "content": "use evol\u2212instruct\u221270k (Xu et al., 2023) as the",
        "page_index": 8,
        "bbox": [
            70.866,
            147.9946344,
            289.13335952000006,
            159.3619166
        ]
    },
    {
        "content": "\u2022 Step 2: Merging. We consider a simplified vari\u2212",
        "page_index": 8,
        "bbox": [
            308.251,
            376.7236344,
            526.2228104974,
            387.7090981
        ]
    },
    {
        "content": "ant of X FT, where the upcycled MoE model",
        "page_index": 8,
        "bbox": [
            317.051,
            363.1746344,
            524.411993692,
            374.3237346
        ]
    },
    {
        "content": "(e.g., MoEDS) can be viewed as the ensembling",
        "page_index": 8,
        "bbox": [
            316.691,
            348.6234584,
            524.4146046154,
            360.5337344
        ]
    },
    {
        "content": "of two dense models and the merged dense",
        "page_index": 8,
        "bbox": [
            317.051,
            336.0756344,
            524.40790007,
            346.98473440000004
        ]
    },
    {
        "content": "model (e.g., X FTDS) can be viewed as the merg\u2212",
        "page_index": 8,
        "bbox": [
            317.051,
            321.5254584,
            526.224119986,
            333.6757346
        ]
    },
    {
        "content": "ing of the same two dense models; see Appendix",
        "page_index": 8,
        "bbox": [
            317.051,
            308.9776344,
            524.679318478,
            319.8867344
        ]
    },
    {
        "content": "A.8 for more details. As such, we can directly ap\u2212",
        "page_index": 8,
        "bbox": [
            316.658,
            295.4286344,
            526.2213746360001,
            306.33773440000004
        ]
    },
    {
        "content": "ply the theoretical analyzing process in Section",
        "page_index": 8,
        "bbox": [
            317.051,
            281.8786344,
            524.4126128012,
            292.78773440000003
        ]
    },
    {
        "content": "4 of (Wortsman et al., 2022) to analyze the per\u2212",
        "page_index": 8,
        "bbox": [
            316.669,
            268.3296344,
            526.2229491736,
            279.2387344
        ]
    },
    {
        "content": "formance difference between the upcycled MoE",
        "page_index": 8,
        "bbox": [
            317.051,
            254.7806344,
            524.4067109781,
            265.6897344
        ]
    },
    {
        "content": "model and the merged dense model, which is",
        "page_index": 8,
        "bbox": [
            317.051,
            241.2316344,
            524.4079000700001,
            252.14073439999999
        ]
    },
    {
        "content": "initially designed to analyze the performance dif\u2212",
        "page_index": 8,
        "bbox": [
            317.051,
            227.68263439999998,
            526.21881067,
            238.59173439999998
        ]
    },
    {
        "content": "ference between model ensembling and model",
        "page_index": 8,
        "bbox": [
            317.051,
            214.1326344,
            524.4079000700001,
            225.0417344
        ]
    },
    {
        "content": "merging. According to (Wortsman et al., 2022),",
        "page_index": 8,
        "bbox": [
            317.051,
            200.5836344,
            525.7772975748001,
            211.4927344
        ]
    },
    {
        "content": "the convexity of the loss can help the merged",
        "page_index": 8,
        "bbox": [
            317.051,
            187.0346344,
            524.40790007,
            197.94373439999998
        ]
    },
    {
        "content": "dense model achieve a similar expected loss as",
        "page_index": 8,
        "bbox": [
            317.051,
            173.4856344,
            524.4047037037,
            184.3947344
        ]
    },
    {
        "content": "that of the upcycled MoE model.",
        "page_index": 8,
        "bbox": [
            317.051,
            159.9366344,
            460.50566499999996,
            170.8457344
        ]
    },
    {
        "content": "training dataset for general instruction tuning. Fol\u2212",
        "page_index": 8,
        "bbox": [
            70.866,
            134.4456344,
            290.94862703840005,
            145.35473439999998
        ]
    },
    {
        "content": "Overall, the Upcycling step improves the per\u2212",
        "page_index": 8,
        "bbox": [
            317.051,
            134.4456344,
            526.21772056,
            145.43109809999999
        ]
    },
    {
        "content": "lowing existing work (Zhang et al., 2024), we use",
        "page_index": 8,
        "bbox": [
            70.866,
            120.89563439999999,
            289.13300370720003,
            131.8047344
        ]
    },
    {
        "content": "formance with more trainable parameters, while",
        "page_index": 8,
        "bbox": [
            306.142,
            120.89563439999999,
            524.414763712,
            131.8047344
        ]
    },
    {
        "content": "MMLU (Hendrycks et al., 2021) with the 5\u2212shot",
        "page_index": 8,
        "bbox": [
            70.866,
            107.3466344,
            289.13876371200007,
            118.25573440000001
        ]
    },
    {
        "content": "the Merging step maintains the upcycled MoE\u2212",
        "page_index": 8,
        "bbox": [
            306.142,
            107.3466344,
            526.2190402999998,
            118.3320981
        ]
    },
    {
        "content": "setting as our evaluation benchmark to showcase",
        "page_index": 8,
        "bbox": [
            70.866,
            93.79763439999999,
            289.13876371200007,
            104.7067344
        ]
    },
    {
        "content": "level performance with only dense\u2212model compute.",
        "page_index": 8,
        "bbox": [
            306.142,
            93.79763439999999,
            526.32145621,
            104.7067344
        ]
    },
    {
        "content": "the general performance of LLMs. Hyperparameter",
        "page_index": 8,
        "bbox": [
            70.866,
            80.2486344,
            289.32421841199994,
            91.15773440000001
        ]
    },
    {
        "content": "Consequently, we provide a preliminary theoretical",
        "page_index": 8,
        "bbox": [
            306.142,
            80.2486344,
            524.407781888,
            91.15773440000001
        ]
    },
    {
        "content": "settings are detailed in Appendix A.7. As shown",
        "page_index": 8,
        "bbox": [
            70.866,
            66.6996344,
            289.138763712,
            77.6087344
        ]
    },
    {
        "content": "explanation for the effectiveness of X FT.",
        "page_index": 8,
        "bbox": [
            306.142,
            66.6996344,
            486.72619470000006,
            77.8487346
        ]
    },
    {
        "content": "9",
        "page_index": 8,
        "bbox": [
            294.91100000000006,
            36.81163439999999,
            300.36555000000004,
            47.72073439999999
        ]
    },
    {
        "content": "Model Discipline Overall",
        "page_index": 9,
        "bbox": [
            146.589,
            749.5426344,
            442.71002920000006,
            767.2267343999999
        ]
    },
    {
        "content": "Humanities Social Science STEM Other",
        "page_index": 9,
        "bbox": [
            191.688,
            738.4556344,
            395.70998819999994,
            749.3647344
        ]
    },
    {
        "content": "SFTTL 25.38 23.30 24.20 26.78 24.97",
        "page_index": 9,
        "bbox": [
            146.589,
            719.3744584,
            440.45695720000003,
            731.3620980999999
        ]
    },
    {
        "content": "MoETL 23.85 26.32 27.40 28.03 26.11",
        "page_index": 9,
        "bbox": [
            146.589,
            701.2954584,
            440.4534693000001,
            713.2057344
        ]
    },
    {
        "content": "X FTTL 23.91 26.49 27.72 28.29 26.30",
        "page_index": 9,
        "bbox": [
            146.589,
            687.7464584,
            440.45695720000003,
            699.8967346000001
        ]
    },
    {
        "content": "Table 9: Experiments on the generalizable effectiveness of X FT for general tasks in MMLU benchmark (Hendrycks",
        "page_index": 9,
        "bbox": [
            70.557,
            671.1630784,
            524.411924372,
            681.3448556000001
        ]
    },
    {
        "content": "et al., 2021). It shows that X FT can improve the general instruction tuning performance of LLMs.",
        "page_index": 9,
        "bbox": [
            70.866,
            659.2070784,
            462.15341180000013,
            669.3888556
        ]
    },
    {
        "content": "7 Conclusion",
        "page_index": 9,
        "bbox": [
            70.866,
            637.3413632,
            145.9327008,
            649.2965632
        ]
    },
    {
        "content": "References",
        "page_index": 9,
        "bbox": [
            306.142,
            637.3413632,
            361.68585920000004,
            649.2965632
        ]
    },
    {
        "content": "This paper introduces X FT to unlock the power",
        "page_index": 9,
        "bbox": [
            70.528,
            615.4306344,
            289.322198786,
            626.5797346
        ]
    },
    {
        "content": "of code instruction tuning by simply merging up\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            601.8816344,
            290.94138339600005,
            612.7907344
        ]
    },
    {
        "content": "cycled MoE. Similar to SFT, X FT starts with a",
        "page_index": 9,
        "bbox": [
            70.866,
            588.3326344,
            289.12982438200004,
            599.4817346
        ]
    },
    {
        "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten",
        "page_index": 9,
        "bbox": [
            306.142,
            619.4560783999999,
            524.4059085328001,
            629.4186784
        ]
    },
    {
        "content": "Bosma, Henryk Michalewski, David Dohan, Ellen",
        "page_index": 9,
        "bbox": [
            317.051,
            608.4970784,
            524.4137519120001,
            618.4596784
        ]
    },
    {
        "content": "Jiang, Carrie Cai, Michael Terry, Quoc Le, and",
        "page_index": 9,
        "bbox": [
            316.862,
            597.5380784,
            524.4076652480001,
            607.5006784000001
        ]
    },
    {
        "content": "Charles Sutton. 2021. Program synthesis with large",
        "page_index": 9,
        "bbox": [
            317.051,
            586.5790784,
            524.4090894152001,
            596.5416784
        ]
    },
    {
        "content": "dense LLM and produces a fine\u2212tuned dense LLM",
        "page_index": 9,
        "bbox": [
            70.866,
            574.7836344,
            289.1314327968001,
            585.6927344
        ]
    },
    {
        "content": "language models.",
        "page_index": 9,
        "bbox": [
            317.051,
            575.6200784,
            386.7792374,
            585.5826784000001
        ]
    },
    {
        "content": "with the exact size and model structure. Yet, X FT",
        "page_index": 9,
        "bbox": [
            70.473,
            561.2346344,
            289.47211245909995,
            572.3837346
        ]
    },
    {
        "content": "improves SFT by upcycling the pre\u2212trained dense",
        "page_index": 9,
        "bbox": [
            70.866,
            547.6846344,
            289.1317818879999,
            558.5937344
        ]
    },
    {
        "content": "LLM to an MoE model for fine\u2212tuning, after which",
        "page_index": 9,
        "bbox": [
            70.866,
            534.1356344,
            289.13178188800003,
            545.0447343999999
        ]
    },
    {
        "content": "we compile the MoE model back to an efficient",
        "page_index": 9,
        "bbox": [
            70.473,
            520.5866344,
            289.135218582,
            531.4957344
        ]
    },
    {
        "content": "dense LLM with a learnable merging mechanism.",
        "page_index": 9,
        "bbox": [
            70.866,
            507.0376344,
            291.04261984400006,
            517.9467344
        ]
    },
    {
        "content": "As such, we unleash the performance limit of in\u2212",
        "page_index": 9,
        "bbox": [
            70.473,
            493.4886344,
            290.94896554800005,
            504.39773440000005
        ]
    },
    {
        "content": "struction tuning without any additional inference",
        "page_index": 9,
        "bbox": [
            70.866,
            479.9386344,
            289.138763712,
            490.84773440000004
        ]
    },
    {
        "content": "overhead. Using the same dataset, X FT improves",
        "page_index": 9,
        "bbox": [
            70.866,
            466.3896344,
            289.1361813814,
            477.5387346
        ]
    },
    {
        "content": "SFT on a variety of benchmarks, including Hu\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            452.8406344,
            290.94138339600005,
            463.7497344
        ]
    },
    {
        "content": "manEval(+), MBPP(+), MultiPL\u2212E, and DS\u22121000,",
        "page_index": 9,
        "bbox": [
            70.866,
            439.2916344,
            290.5007103015,
            450.20073440000004
        ]
    },
    {
        "content": "from 2% to 13%. By applying X FT to DeepSeek\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            425.7426344,
            290.9477461436,
            436.8917346
        ]
    },
    {
        "content": "Loubna Ben Allal, Niklas Muennighoff, Lo\u2212",
        "page_index": 9,
        "bbox": [
            306.142,
            554.7610784,
            526.064800984,
            564.7236784
        ]
    },
    {
        "content": "gesh Kumar Umapathi, Ben Lipkin, and",
        "page_index": 9,
        "bbox": [
            317.051,
            543.8020783999999,
            524.413751912,
            553.7646784
        ]
    },
    {
        "content": "Leandro von Werra. 2022. A framework",
        "page_index": 9,
        "bbox": [
            317.051,
            532.8430784,
            524.6576363600001,
            542.8056784
        ]
    },
    {
        "content": "for the evaluation of code generation mod\u2212",
        "page_index": 9,
        "bbox": [
            317.051,
            521.8840783999999,
            526.0599719360001,
            531.8466784
        ]
    },
    {
        "content": "els. https://github.com/bigcode\u2212project/",
        "page_index": 9,
        "bbox": [
            317.051,
            510.9250784,
            525.4046300000001,
            521.3061076
        ]
    },
    {
        "content": "bigcode\u2212evaluation\u2212harness.",
        "page_index": 9,
        "bbox": [
            317.051,
            499.9660784,
            450.05265,
            510.3471076
        ]
    },
    {
        "content": "Federico Cassano, John Gouwar, Daniel Nguyen, Syd\u2212",
        "page_index": 9,
        "bbox": [
            306.142,
            479.10707840000003,
            526.062011456,
            489.06967840000004
        ]
    },
    {
        "content": "ney Nguyen, Luna Phipps\u2212Costin, Donald Pinckney,",
        "page_index": 9,
        "bbox": [
            317.051,
            468.14807840000003,
            525.6558291044,
            478.1106784
        ]
    },
    {
        "content": "Ming\u2212Ho Yee, Yangtian Zi, Carolyn Jane Anderson,",
        "page_index": 9,
        "bbox": [
            317.051,
            457.1900784,
            525.6567257384,
            467.1526784
        ]
    },
    {
        "content": "Molly Q Feldman, Arjun Guha, Michael Greenberg,",
        "page_index": 9,
        "bbox": [
            317.051,
            446.2310784,
            525.6558291044001,
            456.19367839999995
        ]
    },
    {
        "content": "and Abhinav Jangda. 2022. Multipl\u2212e: A scalable",
        "page_index": 9,
        "bbox": [
            317.051,
            435.2720784,
            524.4137519120001,
            445.2346784
        ]
    },
    {
        "content": "and extensible approach to benchmarking neural code",
        "page_index": 9,
        "bbox": [
            317.051,
            424.3130784,
            524.4049848240002,
            434.27567839999995
        ]
    },
    {
        "content": "Coder\u2212Base 1.3B, we create the next state\u2212of\u2212the\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            412.1926344,
            290.94138339600005,
            423.1017344
        ]
    },
    {
        "content": "generation.",
        "page_index": 9,
        "bbox": [
            317.051,
            413.3540784,
            361.5937846,
            423.3166784
        ]
    },
    {
        "content": "art small (<3B) LLM for code. The ultimate dense",
        "page_index": 9,
        "bbox": [
            70.866,
            398.6436344,
            289.12949097700005,
            409.5527344
        ]
    },
    {
        "content": "LLM produced by X FT preserves or even outper\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            385.0946344,
            290.94356029,
            396.24373460000004
        ]
    },
    {
        "content": "forms the full upcycled MoE which uses 8\u00d7 param\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            371.5456344,
            290.944679776,
            382.6947346
        ]
    },
    {
        "content": "eters as much as our final dense LLM. X FT is fully",
        "page_index": 9,
        "bbox": [
            70.866,
            357.9966344,
            289.512564584,
            369.1457346
        ]
    },
    {
        "content": "orthogonal to the existing instruction tuners such",
        "page_index": 9,
        "bbox": [
            70.866,
            344.44763439999997,
            289.13876371200007,
            355.3567344
        ]
    },
    {
        "content": "as Evol\u2212Instruct and OSS\u2212INSTRUCT, opening a",
        "page_index": 9,
        "bbox": [
            70.866,
            330.8976344,
            289.13040541000004,
            341.80673440000004
        ]
    },
    {
        "content": "new dimension to maximal code instruction tuning.",
        "page_index": 9,
        "bbox": [
            70.866,
            317.3486344,
            290.97061978400006,
            328.2577344
        ]
    },
    {
        "content": "Limitations",
        "page_index": 9,
        "bbox": [
            70.866,
            291.8473632,
            129.984464,
            303.8025632
        ]
    },
    {
        "content": "While X FT has proven to be effective through ex\u2212",
        "page_index": 9,
        "bbox": [
            70.353,
            269.9376344,
            290.94409679200004,
            281.0867346
        ]
    },
    {
        "content": "tensive experiments in the paper, we apply our tech\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            256.3876344,
            290.94923794799996,
            267.29673440000005
        ]
    },
    {
        "content": "nique to LLMs with no more than 3B parameters",
        "page_index": 9,
        "bbox": [
            70.866,
            242.8386344,
            289.13876371200007,
            253.74773439999998
        ]
    },
    {
        "content": "due to resource constraints. This limitation hin\u2212",
        "page_index": 9,
        "bbox": [
            70.866,
            229.28963439999998,
            290.9413833960001,
            240.19873439999998
        ]
    },
    {
        "content": "ders our ability to showcase the impact of X FT on",
        "page_index": 9,
        "bbox": [
            70.866,
            215.7406344,
            289.133621758,
            226.8897346
        ]
    },
    {
        "content": "larger models. In addition, to balance the general",
        "page_index": 9,
        "bbox": [
            70.866,
            202.1916344,
            289.138763712,
            213.1007344
        ]
    },
    {
        "content": "knowledge in the shared expert and the specific",
        "page_index": 9,
        "bbox": [
            70.866,
            188.6416344,
            289.138763712,
            199.55073439999998
        ]
    },
    {
        "content": "knowledge in other normal experts, we introduce a",
        "page_index": 9,
        "bbox": [
            70.866,
            175.0926344,
            289.1386000755,
            186.0017344
        ]
    },
    {
        "content": "hyperparameter \u03bb in the merging process of X FT,",
        "page_index": 9,
        "bbox": [
            70.866,
            161.5436344,
            290.49794386819997,
            172.6927346
        ]
    },
    {
        "content": "which might slightly increase the efforts for hyper\u2212",
        "page_index": 9,
        "bbox": [
            70.473,
            147.9946344,
            290.94608554560006,
            158.9037344
        ]
    },
    {
        "content": "parameter search. It would be interesting to explore",
        "page_index": 9,
        "bbox": [
            70.866,
            134.4456344,
            289.13178188800003,
            145.35473439999998
        ]
    },
    {
        "content": "other hyperparameter\u2212free techniques to tackle this",
        "page_index": 9,
        "bbox": [
            70.866,
            120.89563439999999,
            289.1350764362001,
            131.8047344
        ]
    },
    {
        "content": "challenge in the future. Furthermore, while X FT",
        "page_index": 9,
        "bbox": [
            70.866,
            107.3466344,
            289.472538094,
            118.4957346
        ]
    },
    {
        "content": "has been empirically proven powerful, it would be",
        "page_index": 9,
        "bbox": [
            70.866,
            93.79763439999999,
            289.12938188600003,
            104.7067344
        ]
    },
    {
        "content": "interesting to provide a theoretical explanation for",
        "page_index": 9,
        "bbox": [
            70.866,
            80.2486344,
            289.32072750000003,
            91.15773440000001
        ]
    },
    {
        "content": "its strong performance.",
        "page_index": 9,
        "bbox": [
            70.866,
            66.69963440000001,
            171.75335679999998,
            77.60873440000002
        ]
    },
    {
        "content": "Sahil Chaudhary. 2023. Code alpaca: An instruction\u2212",
        "page_index": 9,
        "bbox": [
            306.142,
            392.4950784,
            526.064800984,
            402.45767839999996
        ]
    },
    {
        "content": "following llama model for code generation. https:",
        "page_index": 9,
        "bbox": [
            317.051,
            381.5360784,
            526.8998,
            391.9171076
        ]
    },
    {
        "content": "//github.com/sahil280114/codealpaca.",
        "page_index": 9,
        "bbox": [
            316.553,
            370.5770784,
            493.38965,
            380.9581076
        ]
    },
    {
        "content": "Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and",
        "page_index": 9,
        "bbox": [
            306.142,
            349.7180784,
            524.4084191080001,
            359.68067840000003
        ]
    },
    {
        "content": "Shangsong Liang. 2022. Revisiting parameter\u2212",
        "page_index": 9,
        "bbox": [
            317.051,
            338.7590784,
            526.0599719360001,
            348.7216784
        ]
    },
    {
        "content": "efficient tuning: Are we really there yet?",
        "page_index": 9,
        "bbox": [
            317.051,
            327.8000784,
            478.9731378,
            337.7626784
        ]
    },
    {
        "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming",
        "page_index": 9,
        "bbox": [
            306.142,
            306.94107840000004,
            524.4084191080001,
            316.9036784
        ]
    },
    {
        "content": "Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka\u2212",
        "page_index": 9,
        "bbox": [
            316.473,
            295.98207840000003,
            526.0611975,
            305.94467840000004
        ]
    },
    {
        "content": "plan, Harri Edwards, Yuri Burda, Nicholas Joseph,",
        "page_index": 9,
        "bbox": [
            317.051,
            285.02307840000003,
            525.6534978560001,
            294.9856784
        ]
    },
    {
        "content": "Greg Brockman, Alex Ray, Raul Puri, Gretchen",
        "page_index": 9,
        "bbox": [
            317.051,
            274.0640784,
            524.4137519120001,
            284.02667840000004
        ]
    },
    {
        "content": "Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas\u2212",
        "page_index": 9,
        "bbox": [
            317.051,
            263.1050784,
            526.0602110384001,
            273.0676784
        ]
    },
    {
        "content": "try, Pamela Mishkin, Brooke Chan, Scott Gray,",
        "page_index": 9,
        "bbox": [
            317.051,
            252.1460784,
            525.6534978560002,
            262.10867840000003
        ]
    },
    {
        "content": "Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz",
        "page_index": 9,
        "bbox": [
            317.051,
            241.1870784,
            524.406678466,
            251.1496784
        ]
    },
    {
        "content": "Kaiser, Mohammad Bavarian, Clemens Winter,",
        "page_index": 9,
        "bbox": [
            317.051,
            230.2290784,
            525.6534978560002,
            240.1916784
        ]
    },
    {
        "content": "Philippe Tillet, Felipe Petroski Such, Dave Cum\u2212",
        "page_index": 9,
        "bbox": [
            317.051,
            219.2700784,
            526.0599719360001,
            229.2326784
        ]
    },
    {
        "content": "mings, Matthias Plappert, Fotios Chantzis, Eliza\u2212",
        "page_index": 9,
        "bbox": [
            317.051,
            208.31107839999999,
            526.059971936,
            218.2736784
        ]
    },
    {
        "content": "beth Barnes, Ariel Herbert\u2212Voss, William Hebgen",
        "page_index": 9,
        "bbox": [
            317.051,
            197.35207839999998,
            524.4137519120001,
            207.3146784
        ]
    },
    {
        "content": "Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie",
        "page_index": 9,
        "bbox": [
            317.051,
            186.39307839999998,
            524.4137519120001,
            196.3556784
        ]
    },
    {
        "content": "Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,",
        "page_index": 9,
        "bbox": [
            316.742,
            175.4340784,
            525.6496522940001,
            185.3966784
        ]
    },
    {
        "content": "William Saunders, Christopher Hesse, Andrew N.",
        "page_index": 9,
        "bbox": [
            316.583,
            164.4750784,
            526.1508737960003,
            174.4376784
        ]
    },
    {
        "content": "Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan",
        "page_index": 9,
        "bbox": [
            317.051,
            153.5160784,
            524.4137519120001,
            163.4786784
        ]
    },
    {
        "content": "Morikawa, Alec Radford, Matthew Knight, Miles",
        "page_index": 9,
        "bbox": [
            317.051,
            142.5570784,
            524.4137519120001,
            152.5196784
        ]
    },
    {
        "content": "Brundage, Mira Murati, Katie Mayer, Peter Welinder,",
        "page_index": 9,
        "bbox": [
            317.051,
            131.5980784,
            525.654693368,
            141.5606784
        ]
    },
    {
        "content": "Bob McGrew, Dario Amodei, Sam McCandlish, Ilya",
        "page_index": 9,
        "bbox": [
            317.051,
            120.64007840000001,
            524.4129050910001,
            130.6026784
        ]
    },
    {
        "content": "Sutskever, and Wojciech Zaremba. 2021. Evaluating",
        "page_index": 9,
        "bbox": [
            317.051,
            109.6810784,
            524.4076249130001,
            119.6436784
        ]
    },
    {
        "content": "large language models trained on code.",
        "page_index": 9,
        "bbox": [
            317.051,
            98.7220784,
            472.9158770000001,
            108.6846784
        ]
    },
    {
        "content": "Damai Dai, Chengqi Deng, Chenggang Zhao, R. X.",
        "page_index": 9,
        "bbox": [
            306.142,
            77.8630784,
            526.1562576520001,
            87.8256784
        ]
    },
    {
        "content": "Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding",
        "page_index": 9,
        "bbox": [
            316.692,
            66.9040784,
            524.410416732,
            76.8666784
        ]
    },
    {
        "content": "10",
        "page_index": 9,
        "bbox": [
            292.183,
            36.8116344,
            303.09209999999996,
            47.7207344
        ]
    },
    {
        "content": "Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li,",
        "page_index": 10,
        "bbox": [
            81.775,
            757.9130784,
            290.37749785600005,
            767.8756784000001
        ]
    },
    {
        "content": "Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,",
        "page_index": 10,
        "bbox": [
            81.775,
            746.9540784,
            290.383624855,
            756.9166784
        ]
    },
    {
        "content": "and Wenfeng Liang. 2024. Deepseekmoe: Towards",
        "page_index": 10,
        "bbox": [
            81.775,
            735.9950784,
            289.1302201864001,
            745.9576784000001
        ]
    },
    {
        "content": "ultimate expert specialization in mixture\u2212of\u2212experts",
        "page_index": 10,
        "bbox": [
            81.775,
            725.0360784,
            289.1324717340001,
            734.9986784
        ]
    },
    {
        "content": "language models.",
        "page_index": 10,
        "bbox": [
            81.775,
            714.0770784,
            151.5032374,
            724.0396784000001
        ]
    },
    {
        "content": "Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun",
        "page_index": 10,
        "bbox": [
            70.866,
            694.5280783999999,
            289.1358063920001,
            704.4906784
        ]
    },
    {
        "content": "Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao",
        "page_index": 10,
        "bbox": [
            81.775,
            683.5690784,
            289.1377519120001,
            693.5316784
        ]
    },
    {
        "content": "Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui",
        "page_index": 10,
        "bbox": [
            81.307,
            672.6100783999999,
            289.137197104,
            682.5726784
        ]
    },
    {
        "content": "Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang.",
        "page_index": 10,
        "bbox": [
            81.775,
            661.6510784,
            290.8754286040001,
            671.6136784
        ]
    },
    {
        "content": "2023. Loramoe: Revolutionizing mixture of experts",
        "page_index": 10,
        "bbox": [
            81.775,
            650.6930784,
            289.13201345440007,
            660.6556784
        ]
    },
    {
        "content": "for maintaining world knowledge in language model",
        "page_index": 10,
        "bbox": [
            81.775,
            639.7340783999999,
            289.1341653760001,
            649.6966784
        ]
    },
    {
        "content": "alignment.",
        "page_index": 10,
        "bbox": [
            81.775,
            628.7750784,
            124.11605000000002,
            638.7376784
        ]
    },
    {
        "content": "Rotem Dror, Gili Baumer, Segev Shlomov, and Roi",
        "page_index": 10,
        "bbox": [
            70.866,
            609.2260784,
            289.1324191080001,
            619.1886784000001
        ]
    },
    {
        "content": "Reichart. 2018. The hitchhiker\u2019s guide to testing sta\u2212",
        "page_index": 10,
        "bbox": [
            81.775,
            598.2670784,
            290.79114500800017,
            608.2296784
        ]
    },
    {
        "content": "tistical significance in natural language processing.",
        "page_index": 10,
        "bbox": [
            81.775,
            587.3080784,
            290.8754286040001,
            597.2706784000001
        ]
    },
    {
        "content": "In Proceedings of the 56th Annual Meeting of the",
        "page_index": 10,
        "bbox": [
            81.775,
            576.3490783999999,
            289.12970877600003,
            586.5408182
        ]
    },
    {
        "content": "Association for Computational Linguistics (Volume",
        "page_index": 10,
        "bbox": [
            81.168,
            565.6192182,
            289.1294543590001,
            575.5818182
        ]
    },
    {
        "content": "1: Long Papers), pages 1383\u20131392, Melbourne, Aus\u2212",
        "page_index": 10,
        "bbox": [
            81.028,
            554.4310783999999,
            290.78693517560004,
            564.6228182
        ]
    },
    {
        "content": "tralia. Association for Computational Linguistics.",
        "page_index": 10,
        "bbox": [
            81.775,
            543.4720784,
            279.36324580000013,
            553.4346784
        ]
    },
    {
        "content": "Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong,",
        "page_index": 10,
        "bbox": [
            70.866,
            523.9230784,
            290.3833430892,
            533.8856784000001
        ]
    },
    {
        "content": "Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,",
        "page_index": 10,
        "bbox": [
            81.775,
            512.9650783999999,
            290.37749785600005,
            522.9276784
        ]
    },
    {
        "content": "Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret",
        "page_index": 10,
        "bbox": [
            81.197,
            502.00607840000004,
            289.128815624,
            511.96867840000004
        ]
    },
    {
        "content": "Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou,",
        "page_index": 10,
        "bbox": [
            81.775,
            491.04707840000003,
            290.37817531280007,
            501.0096784
        ]
    },
    {
        "content": "Tao Wang, Yu Emma Wang, Kellie Webster, Marie",
        "page_index": 10,
        "bbox": [
            81.466,
            480.08807840000003,
            289.13360747200005,
            490.05067840000004
        ]
    },
    {
        "content": "Pellat, Kevin Robinson, Kathleen Meier\u2212Hellstern,",
        "page_index": 10,
        "bbox": [
            81.775,
            469.1290784,
            290.3774978560001,
            479.0916784
        ]
    },
    {
        "content": "Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le,",
        "page_index": 10,
        "bbox": [
            81.466,
            458.1700784,
            290.38351526800005,
            468.13267840000003
        ]
    },
    {
        "content": "Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022.",
        "page_index": 10,
        "bbox": [
            81.197,
            447.2110784,
            290.8766541680001,
            457.1736784
        ]
    },
    {
        "content": "Glam: Efficient scaling of language models with",
        "page_index": 10,
        "bbox": [
            81.775,
            436.2520784,
            289.1377519120001,
            446.2146784
        ]
    },
    {
        "content": "mixture\u2212of\u2212experts.",
        "page_index": 10,
        "bbox": [
            81.775,
            425.2930784,
            158.8157858,
            435.25567839999997
        ]
    },
    {
        "content": "William Fedus, Barret Zoph, and Noam Shazeer. 2022.",
        "page_index": 10,
        "bbox": [
            70.866,
            405.74407840000003,
            290.87876326200006,
            415.7066784
        ]
    },
    {
        "content": "Switch transformers: Scaling to trillion parameter",
        "page_index": 10,
        "bbox": [
            81.775,
            394.78507840000003,
            289.3003415440001,
            404.74767840000004
        ]
    },
    {
        "content": "models with simple and efficient sparsity.",
        "page_index": 10,
        "bbox": [
            81.775,
            383.8270784,
            247.56262660000007,
            393.78967839999996
        ]
    },
    {
        "content": "Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang",
        "page_index": 10,
        "bbox": [
            70.866,
            364.2780784,
            289.13540788800003,
            374.2406784
        ]
    },
    {
        "content": "Xu, Aoxue Li, Dit\u2212Yan Yeung, James T. Kwok, and",
        "page_index": 10,
        "bbox": [
            81.417,
            353.3190784,
            289.1294790224001,
            363.28167840000003
        ]
    },
    {
        "content": "Yu Zhang. 2024. Mixture of cluster\u2212conditional lora",
        "page_index": 10,
        "bbox": [
            81.197,
            342.3600784,
            289.1363872000001,
            352.3226784
        ]
    },
    {
        "content": "experts for vision\u2212language instruction tuning.",
        "page_index": 10,
        "bbox": [
            81.775,
            331.4010784,
            266.1926886000001,
            341.3636784
        ]
    },
    {
        "content": "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai",
        "page_index": 10,
        "bbox": [
            70.866,
            311.85207840000004,
            289.1324191080001,
            321.81467840000005
        ]
    },
    {
        "content": "Dong, Wentao Zhang, Guanting Chen, Xiao Bi,",
        "page_index": 10,
        "bbox": [
            81.775,
            300.89307840000004,
            290.37749785600005,
            310.8556784
        ]
    },
    {
        "content": "Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen\u2212",
        "page_index": 10,
        "bbox": [
            81.197,
            289.93407840000003,
            290.7831651296001,
            299.89667840000004
        ]
    },
    {
        "content": "feng Liang. 2024. Deepseek\u2212coder: When the large",
        "page_index": 10,
        "bbox": [
            81.775,
            278.97507840000003,
            289.1346037304001,
            288.9376784
        ]
    },
    {
        "content": "language model meets programming \u2013 the rise of",
        "page_index": 10,
        "bbox": [
            81.775,
            268.0160784,
            289.1377519120001,
            277.97867840000004
        ]
    },
    {
        "content": "code intelligence.",
        "page_index": 10,
        "bbox": [
            81.775,
            257.0570784,
            152.0511804,
            267.0196784
        ]
    },
    {
        "content": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,",
        "page_index": 10,
        "bbox": [
            70.866,
            237.5080784,
            290.37535308400004,
            247.4706784
        ]
    },
    {
        "content": "Mantas Mazeika, Dawn Song, and Jacob Steinhardt.",
        "page_index": 10,
        "bbox": [
            81.775,
            226.5500784,
            290.87423309200005,
            236.5126784
        ]
    },
    {
        "content": "2021. Measuring massive multitask language under\u2212",
        "page_index": 10,
        "bbox": [
            81.775,
            215.5910784,
            290.7903380374001,
            225.5536784
        ]
    },
    {
        "content": "standing.",
        "page_index": 10,
        "bbox": [
            81.775,
            204.63207839999998,
            118.02890140000001,
            214.5946784
        ]
    },
    {
        "content": "Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li,",
        "page_index": 10,
        "bbox": [
            70.866,
            185.0830784,
            290.3823269040001,
            195.0456784
        ]
    },
    {
        "content": "Tao Chen, Tong He, and Wanli Ouyang. 2023. Ex\u2212",
        "page_index": 10,
        "bbox": [
            81.466,
            174.1240784,
            290.78998934800006,
            184.0866784
        ]
    },
    {
        "content": "perts weights averaging: A new general training",
        "page_index": 10,
        "bbox": [
            81.775,
            163.1650784,
            289.1377519120001,
            173.1276784
        ]
    },
    {
        "content": "scheme for vision transformers.",
        "page_index": 10,
        "bbox": [
            81.775,
            152.2060784,
            207.94136640000005,
            162.1686784
        ]
    },
    {
        "content": "Albert Q. Jiang, Alexandre Sablayrolles, Antoine",
        "page_index": 10,
        "bbox": [
            70.866,
            132.6570784,
            289.1324191080001,
            142.6196784
        ]
    },
    {
        "content": "Roux, Arthur Mensch, Blanche Savary, Chris",
        "page_index": 10,
        "bbox": [
            81.775,
            121.6980784,
            289.1377519120001,
            131.6606784
        ]
    },
    {
        "content": "Bamford, Devendra Singh Chaplot, Diego de las",
        "page_index": 10,
        "bbox": [
            81.775,
            110.73907840000001,
            289.13775191200006,
            120.7016784
        ]
    },
    {
        "content": "Casas, Emma Bou Hanna, Florian Bressand, Gi\u2212",
        "page_index": 10,
        "bbox": [
            81.775,
            99.78007840000001,
            290.78397193600006,
            109.7426784
        ]
    },
    {
        "content": "Sophia Yang, Szymon Antoniak, Teven Le Scao,",
        "page_index": 10,
        "bbox": [
            317.051,
            757.9130784,
            525.6534978560001,
            767.8756784000001
        ]
    },
    {
        "content": "Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,",
        "page_index": 10,
        "bbox": [
            316.742,
            746.9540784,
            525.6493534160002,
            756.9166784
        ]
    },
    {
        "content": "Timoth\u00e9e Lacroix, and William El Sayed. 2024. Mix\u2212",
        "page_index": 10,
        "bbox": [
            316.742,
            735.9950784,
            526.0584177720001,
            745.9576784000001
        ]
    },
    {
        "content": "tral of experts.",
        "page_index": 10,
        "bbox": [
            317.051,
            725.0360784,
            374.7244914,
            734.9986784
        ]
    },
    {
        "content": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.",
        "page_index": 10,
        "bbox": [
            306.142,
            704.4130784,
            526.1492838320002,
            714.3756784000001
        ]
    },
    {
        "content": "Brown, Benjamin Chess, Rewon Child, Scott Gray,",
        "page_index": 10,
        "bbox": [
            317.051,
            693.4540784,
            525.6534978560002,
            703.4166784
        ]
    },
    {
        "content": "Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",
        "page_index": 10,
        "bbox": [
            316.692,
            682.4950784,
            526.1553262716001,
            692.4576784000001
        ]
    },
    {
        "content": "Scaling laws for neural language models.",
        "page_index": 10,
        "bbox": [
            317.051,
            671.5370783999999,
            481.2545732000001,
            681.4996784
        ]
    },
    {
        "content": "Aran Komatsuzaki, Joan Puigcerver, James Lee\u2212Thorp,",
        "page_index": 10,
        "bbox": [
            306.142,
            650.9140784,
            525.6563742344001,
            660.8766784000001
        ]
    },
    {
        "content": "Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,",
        "page_index": 10,
        "bbox": [
            317.051,
            639.9550783999999,
            525.654693368,
            649.9176784
        ]
    },
    {
        "content": "Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023.",
        "page_index": 10,
        "bbox": [
            316.473,
            628.9960784,
            526.1502631440002,
            638.9586784
        ]
    },
    {
        "content": "Sparse upcycling: Training mixture\u2212of\u2212experts from",
        "page_index": 10,
        "bbox": [
            317.051,
            618.0370783999999,
            524.4080134544,
            627.9996784
        ]
    },
    {
        "content": "dense checkpoints.",
        "page_index": 10,
        "bbox": [
            317.051,
            607.0780784,
            392.3084804,
            617.0406784
        ]
    },
    {
        "content": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,",
        "page_index": 10,
        "bbox": [
            306.142,
            586.4560783999999,
            525.6494601900001,
            596.4186784
        ]
    },
    {
        "content": "Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih,",
        "page_index": 10,
        "bbox": [
            317.051,
            575.4970784,
            525.6510072060001,
            585.4596784
        ]
    },
    {
        "content": "Daniel Fried, Sida Wang, and Tao Yu. 2022. Ds\u22121000:",
        "page_index": 10,
        "bbox": [
            317.051,
            564.5380784,
            525.7913802400001,
            574.5006784000001
        ]
    },
    {
        "content": "A natural and reliable benchmark for data science",
        "page_index": 10,
        "bbox": [
            316.692,
            553.5790784,
            524.410416732,
            563.5416784
        ]
    },
    {
        "content": "code generation.",
        "page_index": 10,
        "bbox": [
            317.051,
            542.6200784,
            382.8938234,
            552.5826784000001
        ]
    },
    {
        "content": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,",
        "page_index": 10,
        "bbox": [
            306.142,
            521.9970784,
            525.6583269040001,
            531.9596784
        ]
    },
    {
        "content": "Dehao Chen, Orhan Firat, Yanping Huang, Maxim",
        "page_index": 10,
        "bbox": [
            317.051,
            511.0380784000001,
            524.4137519120001,
            521.0006784000001
        ]
    },
    {
        "content": "Krikun, Noam Shazeer, and Zhifeng Chen. 2020.",
        "page_index": 10,
        "bbox": [
            317.051,
            500.0790784,
            526.1514286040001,
            510.0416784
        ]
    },
    {
        "content": "Gshard: Scaling giant models with conditional com\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            489.12107840000004,
            526.0651126376001,
            499.08367840000005
        ]
    },
    {
        "content": "putation and automatic sharding.",
        "page_index": 10,
        "bbox": [
            317.051,
            478.16207840000004,
            447.66068600000006,
            488.1246784
        ]
    },
    {
        "content": "Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling\u2212",
        "page_index": 10,
        "bbox": [
            306.142,
            457.5390784,
            526.0614137000001,
            467.50167839999995
        ]
    },
    {
        "content": "ming Zhang. 2023. Is your code generated by chat\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            446.58007840000005,
            526.0599719360001,
            456.5426784
        ]
    },
    {
        "content": "GPT really correct? rigorous evaluation of large lan\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            435.62107840000004,
            526.0659096456001,
            445.58367840000005
        ]
    },
    {
        "content": "guage models for code generation. In Thirty\u2212seventh",
        "page_index": 10,
        "bbox": [
            317.051,
            424.66207840000004,
            524.411456188,
            434.85381820000003
        ]
    },
    {
        "content": "Conference on Neural Information Processing Sys\u2212",
        "page_index": 10,
        "bbox": [
            316.722,
            413.9322182,
            526.066313052,
            423.89481820000003
        ]
    },
    {
        "content": "tems.",
        "page_index": 10,
        "bbox": [
            317.051,
            402.74407840000003,
            337.80265,
            412.9358182
        ]
    },
    {
        "content": "LLaMA\u2212MoE Team. 2023. Llama\u2212moe: Building",
        "page_index": 10,
        "bbox": [
            306.142,
            382.1220784,
            524.4084191080001,
            392.08467840000003
        ]
    },
    {
        "content": "mixture\u2212of\u2212experts from llama with continual pre\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            371.1630784,
            526.0599719360001,
            381.12567839999997
        ]
    },
    {
        "content": "training.",
        "page_index": 10,
        "bbox": [
            317.051,
            360.2040784,
            350.53529860000003,
            370.1666784
        ]
    },
    {
        "content": "Anton Lozhkov, Raymond Li, Loubna Ben Allal, Fed\u2212",
        "page_index": 10,
        "bbox": [
            306.142,
            339.5810784,
            526.0664149252,
            349.5436784
        ]
    },
    {
        "content": "erico Cassano, Joel Lamy\u2212Poirier, Nouamane Tazi,",
        "page_index": 10,
        "bbox": [
            317.051,
            328.6220784,
            525.6534978560002,
            338.58467840000003
        ]
    },
    {
        "content": "Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,",
        "page_index": 10,
        "bbox": [
            316.692,
            317.6630784,
            525.657535,
            327.62567839999997
        ]
    },
    {
        "content": "Tianyang Liu, Max Tian, Denis Kocetkov, Arthur",
        "page_index": 10,
        "bbox": [
            316.742,
            306.7040784,
            524.5823589560001,
            316.6666784
        ]
    },
    {
        "content": "Zucker, Younes Belkada, Zijian Wang, Qian Liu,",
        "page_index": 10,
        "bbox": [
            317.051,
            295.74607840000004,
            525.6534978560001,
            305.70867840000005
        ]
    },
    {
        "content": "Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            284.78707840000004,
            526.064753984,
            294.7496784
        ]
    },
    {
        "content": "Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue",
        "page_index": 10,
        "bbox": [
            317.051,
            273.82807840000004,
            524.4137519120001,
            283.79067840000005
        ]
    },
    {
        "content": "Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade,",
        "page_index": 10,
        "bbox": [
            317.051,
            262.86907840000003,
            525.6521030920001,
            272.8316784
        ]
    },
    {
        "content": "Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su,",
        "page_index": 10,
        "bbox": [
            316.583,
            251.9100784,
            525.6529430480001,
            261.87267840000004
        ]
    },
    {
        "content": "Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai,",
        "page_index": 10,
        "bbox": [
            316.692,
            240.9510784,
            525.6535499600002,
            250.9136784
        ]
    },
    {
        "content": "Niklas Muennighoff, Xiangru Tang, Muhtasham",
        "page_index": 10,
        "bbox": [
            317.051,
            229.9920784,
            524.4137519120001,
            239.9546784
        ]
    },
    {
        "content": "Oblokulov, Christopher Akiki, Marc Marone, Cheng\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            219.0330784,
            526.0630204916,
            228.9956784
        ]
    },
    {
        "content": "hao Mou, Mayank Mishra, Alex Gu, Binyuan Hui,",
        "page_index": 10,
        "bbox": [
            317.051,
            208.0740784,
            525.6534978560001,
            218.0366784
        ]
    },
    {
        "content": "Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas",
        "page_index": 10,
        "bbox": [
            316.742,
            197.1150784,
            524.4096074720002,
            207.0776784
        ]
    },
    {
        "content": "Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten",
        "page_index": 10,
        "bbox": [
            317.051,
            186.1570784,
            524.404984824,
            196.1196784
        ]
    },
    {
        "content": "Scholak, Sebastien Paquet, Jennifer Robinson, Car\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            175.19807839999999,
            526.059971936,
            185.1606784
        ]
    },
    {
        "content": "olyn Jane Anderson, Nicolas Chapados, Mostofa Pat\u2212",
        "page_index": 10,
        "bbox": [
            317.051,
            164.23907839999998,
            526.0630204916,
            174.2016784
        ]
    },
    {
        "content": "wary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz",
        "page_index": 10,
        "bbox": [
            316.692,
            153.28007839999998,
            524.4072287000001,
            163.2426784
        ]
    },
    {
        "content": "Ferrandis, Lingming Zhang, Sean Hughes, Thomas",
        "page_index": 10,
        "bbox": [
            317.051,
            142.3210784,
            524.4103247776001,
            152.2836784
        ]
    },
    {
        "content": "Wolf, Arjun Guha, Leandro von Werra, and Harm",
        "page_index": 10,
        "bbox": [
            316.583,
            131.3620784,
            524.4131971040001,
            141.3246784
        ]
    },
    {
        "content": "de Vries. 2024. Starcoder 2 and the stack v2: The",
        "page_index": 10,
        "bbox": [
            317.051,
            120.40307840000001,
            524.4137519120001,
            130.3656784
        ]
    },
    {
        "content": "next generation.",
        "page_index": 10,
        "bbox": [
            317.051,
            109.44407840000001,
            381.0905928,
            119.4066784
        ]
    },
    {
        "content": "anna Lengyel, Guillaume Bour, Guillaume Lam\u2212",
        "page_index": 10,
        "bbox": [
            81.775,
            88.82207840000001,
            290.78397193600006,
            98.7846784
        ]
    },
    {
        "content": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi\u2212",
        "page_index": 10,
        "bbox": [
            306.142,
            88.82207840000001,
            526.064800984,
            98.7846784
        ]
    },
    {
        "content": "ple, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie\u2212",
        "page_index": 10,
        "bbox": [
            81.775,
            77.8630784,
            290.78397193600006,
            87.8256784
        ]
    },
    {
        "content": "Anne Lachaux, Pierre Stock, Sandeep Subramanian,",
        "page_index": 10,
        "bbox": [
            81.417,
            66.9040784,
            290.3812000116001,
            76.8666784
        ]
    },
    {
        "content": "ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,",
        "page_index": 10,
        "bbox": [
            317.051,
            77.8630784,
            525.651007206,
            87.8256784
        ]
    },
    {
        "content": "Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:",
        "page_index": 10,
        "bbox": [
            317.051,
            66.9040784,
            525.7941498428002,
            76.8666784
        ]
    },
    {
        "content": "11",
        "page_index": 10,
        "bbox": [
            292.183,
            36.8116344,
            303.09209999999996,
            47.7207344
        ]
    },
    {
        "content": "Empowering code large language models with evol\u2212",
        "page_index": 11,
        "bbox": [
            81.775,
            757.9130784,
            290.79171287620005,
            767.8756784000001
        ]
    },
    {
        "content": "Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zang\u2212",
        "page_index": 11,
        "bbox": [
            306.142,
            757.9130784,
            526.064800984,
            767.8756784000001
        ]
    },
    {
        "content": "instruct.",
        "page_index": 11,
        "bbox": [
            81.775,
            746.9540784,
            114.15345,
            756.9166784
        ]
    },
    {
        "content": "Zohar Manna and Richard J Waldinger. 1971. Toward",
        "page_index": 11,
        "bbox": [
            70.866,
            725.8330784,
            289.13436181500003,
            735.7956784
        ]
    },
    {
        "content": "automatic program synthesis. Communications of",
        "page_index": 11,
        "bbox": [
            81.775,
            714.8740783999999,
            289.137041168,
            725.0658182
        ]
    },
    {
        "content": "the ACM, 14(3):151\u2013165.",
        "page_index": 11,
        "bbox": [
            81.775,
            703.9150784,
            183.86164439999996,
            714.1068182
        ]
    },
    {
        "content": "Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung,",
        "page_index": 11,
        "bbox": [
            70.866,
            682.7940784,
            290.3823269040001,
            692.7566784
        ]
    },
    {
        "content": "Jonathan Tow, James Baicoianu, , and Nathan Cooper.",
        "page_index": 11,
        "bbox": [
            81.586,
            671.8350783999999,
            290.8731277280001,
            681.7976784
        ]
    },
    {
        "content": "2024. Stable code 3b.",
        "page_index": 11,
        "bbox": [
            81.775,
            660.8770784,
            168.92782480000002,
            670.8396784
        ]
    },
    {
        "content": "wei Zheng, Wangchunshu Zhou, and Yang You.",
        "page_index": 11,
        "bbox": [
            316.692,
            746.9540784,
            526.1480934240002,
            756.9166784
        ]
    },
    {
        "content": "2024. Openmoe: An early effort on open",
        "page_index": 11,
        "bbox": [
            317.051,
            735.9950784,
            524.4137519120001,
            745.9576784000001
        ]
    },
    {
        "content": "mixture\u2212of\u2212experts language models. arXiv preprint",
        "page_index": 11,
        "bbox": [
            317.051,
            725.0360784,
            524.4052964,
            735.2278182
        ]
    },
    {
        "content": "arXiv:2402.01739.",
        "page_index": 11,
        "bbox": [
            317.051,
            714.0770784,
            392.31865,
            724.2688182
        ]
    },
    {
        "content": "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and",
        "page_index": 11,
        "bbox": [
            306.142,
            688.9500783999999,
            524.4084191080001,
            698.9126784
        ]
    },
    {
        "content": "Wei Lu. 2024. Tinyllama: An open\u2212source small",
        "page_index": 11,
        "bbox": [
            316.583,
            677.9910784,
            524.4131971040001,
            687.9536784000001
        ]
    },
    {
        "content": "language model.",
        "page_index": 11,
        "bbox": [
            317.051,
            667.0320783999999,
            382.903786,
            676.9946784
        ]
    },
    {
        "content": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,",
        "page_index": 11,
        "bbox": [
            70.866,
            639.7560784,
            290.3745959264001,
            649.7186784
        ]
    },
    {
        "content": "Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff",
        "page_index": 11,
        "bbox": [
            81.417,
            628.7970783999999,
            289.13541673200007,
            638.7596784
        ]
    },
    {
        "content": "Dean. 2017. Outrageously large neural networks:",
        "page_index": 11,
        "bbox": [
            81.775,
            617.8380784,
            290.5197637840001,
            627.8006784
        ]
    },
    {
        "content": "The sparsely\u2212gated mixture\u2212of\u2212experts layer.",
        "page_index": 11,
        "bbox": [
            81.466,
            606.8790783999999,
            258.8799808000001,
            616.8416784
        ]
    },
    {
        "content": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,",
        "page_index": 11,
        "bbox": [
            306.142,
            641.9050784,
            525.6580479512,
            651.8676784
        ]
    },
    {
        "content": "Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian\u2212",
        "page_index": 11,
        "bbox": [
            316.692,
            630.9470784,
            526.0645271352001,
            640.9096784000001
        ]
    },
    {
        "content": "wei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruc\u2212",
        "page_index": 11,
        "bbox": [
            316.692,
            619.9880784,
            526.0669978600001,
            629.9506784
        ]
    },
    {
        "content": "tion tuning for large language models: A survey.",
        "page_index": 11,
        "bbox": [
            317.051,
            609.0290784,
            510.8634204000001,
            618.9916784000001
        ]
    },
    {
        "content": "Anders S\u00f8gaard, Anders Johannsen, Barbara Plank,",
        "page_index": 11,
        "bbox": [
            70.866,
            585.7580783999999,
            290.382326904,
            595.7206784
        ]
    },
    {
        "content": "Dirk Hovy, and Hector Mart\u00ednez Alonso. 2014.",
        "page_index": 11,
        "bbox": [
            81.775,
            574.7990784,
            290.8754286040001,
            584.7616784
        ]
    },
    {
        "content": "What\u2019s in a p\u2212value in NLP? In Proceedings of the",
        "page_index": 11,
        "bbox": [
            81.307,
            563.8400783999999,
            289.137519996,
            574.0318182
        ]
    },
    {
        "content": "Eighteenth Conference on Computational Natural",
        "page_index": 11,
        "bbox": [
            81.466,
            553.1102182,
            289.1336074720001,
            563.0728182
        ]
    },
    {
        "content": "Language Learning, pages 1\u201310, Ann Arbor, Michi\u2212",
        "page_index": 11,
        "bbox": [
            81.496,
            541.9230784,
            290.78962183880003,
            552.1148182000001
        ]
    },
    {
        "content": "gan. Association for Computational Linguistics.",
        "page_index": 11,
        "bbox": [
            81.775,
            530.9640784,
            273.22628420000007,
            540.9266784
        ]
    },
    {
        "content": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa",
        "page_index": 11,
        "bbox": [
            70.866,
            509.8430784,
            289.13101438140006,
            519.8056784
        ]
    },
    {
        "content": "Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh",
        "page_index": 11,
        "bbox": [
            81.775,
            498.8840784,
            289.12898482400004,
            508.8466784
        ]
    },
    {
        "content": "Hajishirzi. 2023. Self\u2212instruct: Aligning language",
        "page_index": 11,
        "bbox": [
            81.775,
            487.9250784,
            289.1377519120001,
            497.8876784
        ]
    },
    {
        "content": "models with self\u2212generated instructions.",
        "page_index": 11,
        "bbox": [
            81.775,
            476.9660784,
            241.15667480000008,
            486.92867839999997
        ]
    },
    {
        "content": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin",
        "page_index": 11,
        "bbox": [
            70.866,
            455.84507840000003,
            289.1324191080001,
            465.8076784
        ]
    },
    {
        "content": "Guu, Adams Wei Yu, Brian Lester, Nan Du, An\u2212",
        "page_index": 11,
        "bbox": [
            81.775,
            444.88607840000003,
            290.78397193600006,
            454.84867840000004
        ]
    },
    {
        "content": "drew M. Dai, and Quoc V. Le. 2022. Finetuned",
        "page_index": 11,
        "bbox": [
            81.775,
            433.9280784,
            289.1377519120001,
            443.89067839999996
        ]
    },
    {
        "content": "language models are zero\u2212shot learners.",
        "page_index": 11,
        "bbox": [
            81.775,
            422.9690784,
            239.74198560000008,
            432.9316784
        ]
    },
    {
        "content": "Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and",
        "page_index": 11,
        "bbox": [
            70.866,
            401.8480784,
            289.1339134980001,
            411.81067840000003
        ]
    },
    {
        "content": "Lingming Zhang. 2023. Magicoder: Source code is",
        "page_index": 11,
        "bbox": [
            81.775,
            390.8890784,
            289.13114670820005,
            400.85167839999997
        ]
    },
    {
        "content": "all you need. arXiv preprint arXiv:2312.02120.",
        "page_index": 11,
        "bbox": [
            81.775,
            379.9300784,
            271.22365,
            390.1218182
        ]
    },
    {
        "content": "Frank. Wilcoxon. 1945. Individual comparisons by",
        "page_index": 11,
        "bbox": [
            70.866,
            358.80907840000003,
            289.4779220760001,
            368.77167840000004
        ]
    },
    {
        "content": "ranking methods. Biometrics, 1:196\u2013202.",
        "page_index": 11,
        "bbox": [
            81.775,
            347.85007840000003,
            247.80095279999998,
            358.0418182
        ]
    },
    {
        "content": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak",
        "page_index": 11,
        "bbox": [
            70.866,
            326.7290784,
            289.38646540800005,
            336.6916784
        ]
    },
    {
        "content": "Gadre, Rebecca Roelofs, Raphael Gontijo\u2212Lopes,",
        "page_index": 11,
        "bbox": [
            81.775,
            315.7710784,
            290.37749785600016,
            325.73367840000003
        ]
    },
    {
        "content": "Ari S. Morcos, Hongseok Namkoong, Ali Farhadi,",
        "page_index": 11,
        "bbox": [
            81.417,
            304.8120784,
            290.37516267600006,
            314.77467839999997
        ]
    },
    {
        "content": "Yair Carmon, Simon Kornblith, and Ludwig Schmidt.",
        "page_index": 11,
        "bbox": [
            81.197,
            293.8530784,
            290.87466164800003,
            303.8156784
        ]
    },
    {
        "content": "2022. Model soups: averaging weights of multiple",
        "page_index": 11,
        "bbox": [
            81.775,
            282.8940784,
            289.13775191200006,
            292.85667839999996
        ]
    },
    {
        "content": "fine\u2212tuned models improves accuracy without increas\u2212",
        "page_index": 11,
        "bbox": [
            81.775,
            271.9350784,
            290.7887539840001,
            281.8976784
        ]
    },
    {
        "content": "ing inference time.",
        "page_index": 11,
        "bbox": [
            81.775,
            260.9760784,
            156.7535276,
            270.93867839999996
        ]
    },
    {
        "content": "Haoyuan Wu, Haisheng Zheng, and Bei Yu. 2024.",
        "page_index": 11,
        "bbox": [
            70.866,
            239.8550784,
            290.880257652,
            249.8176784
        ]
    },
    {
        "content": "Parameter\u2212efficient sparsity crafting from dense to",
        "page_index": 11,
        "bbox": [
            81.775,
            228.8960784,
            289.1377519120001,
            238.8586784
        ]
    },
    {
        "content": "mixture\u2212of\u2212experts for instruction tuning on general",
        "page_index": 11,
        "bbox": [
            81.775,
            217.9370784,
            289.1333982558001,
            227.8996784
        ]
    },
    {
        "content": "tasks.",
        "page_index": 11,
        "bbox": [
            81.775,
            206.9780784,
            104.19085000000001,
            216.9406784
        ]
    },
    {
        "content": "Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong",
        "page_index": 11,
        "bbox": [
            70.866,
            185.85807839999998,
            289.1376395104001,
            195.8206784
        ]
    },
    {
        "content": "Chen, Xiyang Dai, and Lu Yuan. 2022. Residual",
        "page_index": 11,
        "bbox": [
            81.775,
            174.89907839999998,
            289.137751912,
            184.8616784
        ]
    },
    {
        "content": "mixture of experts.",
        "page_index": 11,
        "bbox": [
            81.775,
            163.9400784,
            157.1619942,
            173.9026784
        ]
    },
    {
        "content": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,",
        "page_index": 11,
        "bbox": [
            70.866,
            142.8190784,
            290.3823269040001,
            152.7816784
        ]
    },
    {
        "content": "Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin",
        "page_index": 11,
        "bbox": [
            81.775,
            131.8600784,
            289.1346037304001,
            141.8226784
        ]
    },
    {
        "content": "Jiang. 2023. Wizardlm: Empowering large language",
        "page_index": 11,
        "bbox": [
            81.586,
            120.9010784,
            289.1348732052001,
            130.8636784
        ]
    },
    {
        "content": "models to follow complex instructions.",
        "page_index": 11,
        "bbox": [
            81.775,
            109.9420784,
            237.18159740000004,
            119.9046784
        ]
    },
    {
        "content": "Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou,",
        "page_index": 11,
        "bbox": [
            70.866,
            88.82207840000001,
            290.3823269040001,
            98.7846784
        ]
    },
    {
        "content": "and Yang You. 2022. One student knows all experts",
        "page_index": 11,
        "bbox": [
            81.775,
            77.8630784,
            289.1357992424001,
            87.8256784
        ]
    },
    {
        "content": "know: From sparse to dense.",
        "page_index": 11,
        "bbox": [
            81.775,
            66.9040784,
            196.40467560000005,
            76.8666784
        ]
    },
    {
        "content": "A Appendix for \"X FT: Unlocking the",
        "page_index": 11,
        "bbox": [
            306.142,
            578.9203632000001,
            507.77403680000003,
            591.0548912
        ]
    },
    {
        "content": "Power of Code Instruction Tuning by",
        "page_index": 11,
        "bbox": [
            326.72900000000004,
            564.9723632000001,
            516.2308752000001,
            576.9275632000001
        ]
    },
    {
        "content": "Simply Merging Upcycled",
        "page_index": 11,
        "bbox": [
            326.72900000000004,
            551.0243632000002,
            459.44367520000003,
            562.9795632000001
        ]
    },
    {
        "content": "Mixture\u2212of\u2212Experts\"",
        "page_index": 11,
        "bbox": [
            326.72900000000004,
            537.0763632000002,
            432.74771360000005,
            549.0315632000002
        ]
    },
    {
        "content": "A.1 Hyperparameter Settings",
        "page_index": 11,
        "bbox": [
            306.14200000000005,
            513.1599981000002,
            451.46212110000005,
            524.0690981000001
        ]
    },
    {
        "content": "We use a batch size of 64 and a learning rare of",
        "page_index": 11,
        "bbox": [
            305.629,
            493.02463439999997,
            524.413618684,
            503.9337344
        ]
    },
    {
        "content": "5e\u22125 with a linear scheduler to fine\u2212tune MoEDS",
        "page_index": 11,
        "bbox": [
            306.142,
            478.41324910000003,
            523.7122928000001,
            490.4610981
        ]
    },
    {
        "content": "for 4 epochs with 500 warmup steps, following",
        "page_index": 11,
        "bbox": [
            306.142,
            465.9266344,
            524.414763712,
            476.83573440000004
        ]
    },
    {
        "content": "the implementation of previous work (Wei et al.,",
        "page_index": 11,
        "bbox": [
            306.142,
            452.3776344,
            525.7722921159999,
            463.2867344
        ]
    },
    {
        "content": "2023). We further use a batch size of 64, a shared",
        "page_index": 11,
        "bbox": [
            306.142,
            438.8286344,
            524.4140218932,
            449.7377344
        ]
    },
    {
        "content": "expert rate \u03bb of 0.75, and a learning rare of 1e\u22125",
        "page_index": 11,
        "bbox": [
            306.142,
            425.2786344,
            524.4076407079999,
            436.4277346
        ]
    },
    {
        "content": "with a linear schedule to fine\u2212tune the learnable",
        "page_index": 11,
        "bbox": [
            305.749,
            411.7296344,
            524.411218582,
            422.63873440000003
        ]
    },
    {
        "content": "mixing coefficients for each of the experts in the",
        "page_index": 11,
        "bbox": [
            306.142,
            398.1806344,
            524.4147637120001,
            409.0897344
        ]
    },
    {
        "content": "instruction\u2212tuned MoEDS on the instruction dataset",
        "page_index": 11,
        "bbox": [
            306.142,
            383.5682491,
            524.4116341,
            395.6170981
        ]
    },
    {
        "content": "for 1 epoch with 125 warmup steps. Detailedly, we",
        "page_index": 11,
        "bbox": [
            306.142,
            371.0826344,
            524.4077818879999,
            381.99173440000004
        ]
    },
    {
        "content": "use Softmax to keep the sum of the mixing coef\u2212",
        "page_index": 11,
        "bbox": [
            306.142,
            357.5326344,
            526.217383396,
            368.44173440000003
        ]
    },
    {
        "content": "ficients of the other 7 normal experts as 0.25. For",
        "page_index": 11,
        "bbox": [
            306.142,
            343.98363439999997,
            524.593782043,
            354.8927344
        ]
    },
    {
        "content": "SFTDS and EWADS, we use the same hyperparam\u2212",
        "page_index": 11,
        "bbox": [
            306.142,
            329.3712491,
            526.22447402,
            341.4200981
        ]
    },
    {
        "content": "eter setting as X FT, where the batch size is 64 and",
        "page_index": 11,
        "bbox": [
            306.142,
            316.8856344,
            524.4141916167999,
            328.03473460000004
        ]
    },
    {
        "content": "the learning rate is 5e\u22125 with a linear scheduler. Be\u2212",
        "page_index": 11,
        "bbox": [
            306.142,
            303.3366344,
            526.2252379480001,
            314.2457344
        ]
    },
    {
        "content": "cause X FT is trained for 4 epochs during upcycling",
        "page_index": 11,
        "bbox": [
            306.142,
            289.7876344,
            524.4050796360001,
            300.9367346
        ]
    },
    {
        "content": "and 1 epoch during merging, for a fair comparison,",
        "page_index": 11,
        "bbox": [
            306.142,
            276.2376344,
            525.7685284764999,
            287.1467344
        ]
    },
    {
        "content": "we train SFTDS and EWADS for 5 (= 4 + 1) epochs",
        "page_index": 11,
        "bbox": [
            305.749,
            261.68745839999997,
            524.412859648,
            273.59773440000004
        ]
    },
    {
        "content": "with 625 warmup steps.",
        "page_index": 11,
        "bbox": [
            305.749,
            249.1396344,
            409.87635950000004,
            260.0487344
        ]
    },
    {
        "content": "A.2 Implementation details of EWA",
        "page_index": 11,
        "bbox": [
            306.142,
            222.3259981,
            479.9785085,
            233.2350981
        ]
    },
    {
        "content": "Because EWA (Huang et al., 2023) does not release",
        "page_index": 11,
        "bbox": [
            306.142,
            202.1916344,
            524.407781888,
            213.1007344
        ]
    },
    {
        "content": "their implementation, we implemented EWA by",
        "page_index": 11,
        "bbox": [
            306.142,
            188.6416344,
            524.7930913,
            199.55073439999998
        ]
    },
    {
        "content": "ourselves, including constant schedule and linear",
        "page_index": 11,
        "bbox": [
            306.142,
            175.0926344,
            524.5928002239999,
            186.0017344
        ]
    },
    {
        "content": "schedule. We use a share rate \u03b2 of 0.3, following",
        "page_index": 11,
        "bbox": [
            306.142,
            161.5436344,
            524.4078076559999,
            172.6927346
        ]
    },
    {
        "content": "the original setting of EWA. While EWA with the",
        "page_index": 11,
        "bbox": [
            306.142,
            147.9946344,
            524.4053927950999,
            158.9037344
        ]
    },
    {
        "content": "constant schedule achieves reasonable performance",
        "page_index": 11,
        "bbox": [
            306.142,
            134.4456344,
            524.407781888,
            145.35473439999998
        ]
    },
    {
        "content": "in our evaluation, the training loss of EWA with the",
        "page_index": 11,
        "bbox": [
            306.142,
            120.89563439999999,
            524.4077818879999,
            131.8047344
        ]
    },
    {
        "content": "linear schedule becomes very unstable, as is shown",
        "page_index": 11,
        "bbox": [
            306.142,
            107.3466344,
            524.407781888,
            118.25573440000001
        ]
    },
    {
        "content": "in Figure 3, and thus cannot achieve reasonable",
        "page_index": 11,
        "bbox": [
            306.142,
            93.79763439999999,
            524.414763712,
            104.7067344
        ]
    },
    {
        "content": "performance. As a result, we report the results of",
        "page_index": 11,
        "bbox": [
            306.142,
            80.2486344,
            524.414763712,
            91.15773440000001
        ]
    },
    {
        "content": "EWA with the constant schedule in Section 4.",
        "page_index": 11,
        "bbox": [
            306.142,
            66.6996344,
            505.4185297,
            77.6087344
        ]
    },
    {
        "content": "12",
        "page_index": 11,
        "bbox": [
            292.183,
            36.81163439999999,
            303.09209999999996,
            47.72073439999999
        ]
    },
    {
        "content": "Figure 3: Training loss curve of EWA with constant",
        "page_index": 12,
        "bbox": [
            70.866,
            589.6290783999999,
            289.13241910800014,
            599.5916784
        ]
    },
    {
        "content": "schedule and linear schedule.",
        "page_index": 12,
        "bbox": [
            70.866,
            577.6740784,
            187.61770940000005,
            587.6366784
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 12,
        "bbox": [
            306.205,
            756.3176344,
            518.3651768000001,
            767.2267343999999
        ]
    },
    {
        "content": "X FTDS vs. EWADS 2.6e\u221218 8.0e\u221223",
        "page_index": 12,
        "bbox": [
            306.205,
            737.57221256,
            524.3491817,
            749.1467344
        ]
    },
    {
        "content": "X FTDS vs. SFTDS 9.6e\u221230 3.7e\u221233",
        "page_index": 12,
        "bbox": [
            306.205,
            724.0232125599999,
            524.3491817,
            735.5977343999999
        ]
    },
    {
        "content": "Table 11: p\u2212values for X FTDS vs. EWADS and X FTDS",
        "page_index": 12,
        "bbox": [
            305.833,
            706.2546592,
            523.7372064,
            717.2858556
        ]
    },
    {
        "content": "vs. SFTDS in 200 experiments on HumanEval (+) com\u2212",
        "page_index": 12,
        "bbox": [
            305.893,
            694.2996592000001,
            526.066392419,
            705.1116784000001
        ]
    },
    {
        "content": "puted with sampling. Results show that improvements",
        "page_index": 12,
        "bbox": [
            306.142,
            683.1930784,
            524.4118063920001,
            693.1556784
        ]
    },
    {
        "content": "brought by X FT are statistically significant.",
        "page_index": 12,
        "bbox": [
            306.142,
            671.2380784,
            481.30204860000003,
            681.4198556
        ]
    },
    {
        "content": "Following prior work (Liu et al., 2023), we re\u2212",
        "page_index": 12,
        "bbox": [
            317.051,
            649.5146344,
            526.2216470359999,
            660.4237344
        ]
    },
    {
        "content": "peat this experiment 200 times for three techniques:",
        "page_index": 12,
        "bbox": [
            306.142,
            635.9656344,
            525.925892244,
            646.8747344
        ]
    },
    {
        "content": "X FTDS, EWADS, and SFTDS. EWADS is included",
        "page_index": 12,
        "bbox": [
            306.142,
            621.4144584,
            524.406239276,
            633.5657346
        ]
    },
    {
        "content": "because it is the best\u2212performing baseline in our",
        "page_index": 12,
        "bbox": [
            306.142,
            608.8666344,
            524.592800224,
            619.7757343999999
        ]
    },
    {
        "content": "main experiment. We first compute their average",
        "page_index": 12,
        "bbox": [
            306.142,
            595.3176344,
            524.414763712,
            606.2267343999999
        ]
    },
    {
        "content": "pass@1 performance in these 200 experiments. As",
        "page_index": 12,
        "bbox": [
            306.142,
            581.7686344,
            524.4110764362001,
            592.6777344
        ]
    },
    {
        "content": "is shown in Table 10, X FTDS outperforms both",
        "page_index": 12,
        "bbox": [
            306.142,
            567.2174583999999,
            524.406119538,
            579.3687346
        ]
    },
    {
        "content": "Model HumanEval HumanEval+",
        "page_index": 12,
        "bbox": [
            92.634,
            554.5596344,
            261.38686789999997,
            565.4687344
        ]
    },
    {
        "content": "EWADS and SFTDS clearly.",
        "page_index": 12,
        "bbox": [
            306.142,
            553.6684584000001,
            427.0668446000001,
            565.5797344
        ]
    },
    {
        "content": "SFTDS 61.6 57.2",
        "page_index": 12,
        "bbox": [
            92.634,
            535.4784584,
            267.368261,
            547.3887344
        ]
    },
    {
        "content": "EWADS 62.7 58.8",
        "page_index": 12,
        "bbox": [
            92.634,
            521.9284584000001,
            267.368261,
            533.8397344
        ]
    },
    {
        "content": "X FTDS 64.5 60.9",
        "page_index": 12,
        "bbox": [
            92.634,
            508.37945840000003,
            267.368261,
            520.5307346000001
        ]
    },
    {
        "content": "Table 10: Average pass@1 results of 200 experiments",
        "page_index": 12,
        "bbox": [
            70.557,
            491.7960784,
            289.1315523634001,
            501.7586784
        ]
    },
    {
        "content": "on HumanEval (+) computed with sampling. X FT",
        "page_index": 12,
        "bbox": [
            70.866,
            479.8410784,
            289.442881284,
            490.0228556
        ]
    },
    {
        "content": "clearly outperforms both EWADS and SFTDS.",
        "page_index": 12,
        "bbox": [
            70.866,
            467.0376592,
            252.58565,
            477.84867840000004
        ]
    },
    {
        "content": "Furthermore, we use the Wilcoxon signed\u2212rank",
        "page_index": 12,
        "bbox": [
            317.051,
            540.9726344,
            524.6848384826001,
            551.8817343999999
        ]
    },
    {
        "content": "test (Wilcoxon, 1945; Dror et al., 2018), a widely",
        "page_index": 12,
        "bbox": [
            306.142,
            527.4226344,
            524.7934840275999,
            538.3317344
        ]
    },
    {
        "content": "used statistical test, to check if the improvements",
        "page_index": 12,
        "bbox": [
            306.142,
            513.8736344,
            524.4147637120001,
            524.7827344
        ]
    },
    {
        "content": "brought by X FT are statistically significant. As",
        "page_index": 12,
        "bbox": [
            306.142,
            500.3246344,
            524.4057116920001,
            511.4737346
        ]
    },
    {
        "content": "shown in Table 11, the p\u2212values for both X FTDS",
        "page_index": 12,
        "bbox": [
            306.142,
            485.7734584,
            523.7122928000001,
            497.9247346
        ]
    },
    {
        "content": "vs. EWADS and X FTDS vs. SFTDS are much",
        "page_index": 12,
        "bbox": [
            305.869,
            472.2244584,
            524.4045083159999,
            484.37573460000004
        ]
    },
    {
        "content": "smaller than both 0.0025 (the significance level",
        "page_index": 12,
        "bbox": [
            306.142,
            459.6766344,
            524.4147637120001,
            470.58573440000004
        ]
    },
    {
        "content": "A.3 Details of HumanEval and MBPP",
        "page_index": 12,
        "bbox": [
            70.86599999999999,
            446.2389981,
            254.70615319999996,
            457.1480981
        ]
    },
    {
        "content": "recommended for NLP work by (S\u00f8gaard et al.,",
        "page_index": 12,
        "bbox": [
            306.142,
            446.1276344,
            525.772292116,
            457.0367344
        ]
    },
    {
        "content": "In these benchmarks, each task consists of a task",
        "page_index": 12,
        "bbox": [
            70.866,
            425.8836344,
            289.40581848000005,
            436.79273440000003
        ]
    },
    {
        "content": "description in English, which is sent to LLMs as",
        "page_index": 12,
        "bbox": [
            70.866,
            412.33463439999997,
            289.138763712,
            423.2437344
        ]
    },
    {
        "content": "the prompt, and LLMs are expected to generate the",
        "page_index": 12,
        "bbox": [
            70.866,
            398.7856344,
            289.13178188800003,
            409.6947344
        ]
    },
    {
        "content": "corresponding code to satisfy the requirements in",
        "page_index": 12,
        "bbox": [
            70.866,
            385.2356344,
            289.1362328008,
            396.1447344
        ]
    },
    {
        "content": "the description. While these benchmarks provide",
        "page_index": 12,
        "bbox": [
            70.866,
            371.6866344,
            289.1387637120001,
            382.5957344
        ]
    },
    {
        "content": "a handful of test cases to validate the correctness",
        "page_index": 12,
        "bbox": [
            70.866,
            358.1376344,
            289.138763712,
            369.04673440000005
        ]
    },
    {
        "content": "of the generated code, these tests are often insuf\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            344.5886344,
            290.94138339600005,
            355.4977344
        ]
    },
    {
        "content": "ficient for more rigorous evaluation. As such, Hu\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            331.0396344,
            290.9406633954,
            341.94873440000003
        ]
    },
    {
        "content": "manEval+ and MBPP+ proposed by EvalPlus (Liu",
        "page_index": 12,
        "bbox": [
            70.866,
            317.4896344,
            289.13418189000004,
            328.3987344
        ]
    },
    {
        "content": "et al., 2023) are usually used to evaluate the correct\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            303.9406344,
            290.94923794799996,
            314.84973440000005
        ]
    },
    {
        "content": "ness of the generated code, which provides 80\u00d7/35\u00d7",
        "page_index": 12,
        "bbox": [
            70.866,
            290.3916344,
            289.744677752,
            301.3007344
        ]
    },
    {
        "content": "more tests compared with the original benchmarks.",
        "page_index": 12,
        "bbox": [
            70.866,
            276.8426344,
            291.040547115,
            287.75173440000003
        ]
    },
    {
        "content": "A.4 Statistical Significance Analysis",
        "page_index": 12,
        "bbox": [
            70.866,
            249.6449981,
            244.51705379999999,
            260.55409810000003
        ]
    },
    {
        "content": "2014)) and 0.05 (the most common significance",
        "page_index": 12,
        "bbox": [
            306.142,
            432.5786344,
            524.4147637120001,
            443.4877344
        ]
    },
    {
        "content": "level), demonstrating the statistical significance of",
        "page_index": 12,
        "bbox": [
            306.142,
            419.0296344,
            524.4090473435999,
            429.93873440000004
        ]
    },
    {
        "content": "the improvements brought by X FT.",
        "page_index": 12,
        "bbox": [
            306.142,
            405.4806344,
            462.1591947,
            416.6297346
        ]
    },
    {
        "content": "A.5 Analysis on Expert Specialization",
        "page_index": 12,
        "bbox": [
            306.14199999999994,
            382.6549981,
            489.1857889,
            393.5640981
        ]
    },
    {
        "content": "Inspired by recent works (Jiang et al., 2024; Xue",
        "page_index": 12,
        "bbox": [
            306.142,
            364.8066344,
            524.4147637120001,
            375.71573440000003
        ]
    },
    {
        "content": "et al., 2024), we analyze whether each expert in",
        "page_index": 12,
        "bbox": [
            306.142,
            351.2576344,
            524.414763712,
            362.1667344
        ]
    },
    {
        "content": "MoEDS has different specializations in different",
        "page_index": 12,
        "bbox": [
            306.142,
            336.70745839999995,
            524.409295988,
            348.6177344
        ]
    },
    {
        "content": "programming languages by visualizing the routing",
        "page_index": 12,
        "bbox": [
            306.142,
            324.1596344,
            524.4101818900001,
            335.06873440000004
        ]
    },
    {
        "content": "decision of the tokens from different programming",
        "page_index": 12,
        "bbox": [
            306.142,
            310.6106344,
            524.4146000755,
            321.5197344
        ]
    },
    {
        "content": "languages in the MultiPL\u2212E benchmark (including",
        "page_index": 12,
        "bbox": [
            306.142,
            297.0616344,
            524.4108037087001,
            307.9707344
        ]
    },
    {
        "content": "Python). For the MultiPL\u2212E benchmark, we collect",
        "page_index": 12,
        "bbox": [
            306.142,
            283.5116344,
            524.407781888,
            294.4207344
        ]
    },
    {
        "content": "the routing decision when conducting experiments",
        "page_index": 12,
        "bbox": [
            306.142,
            269.9626344,
            524.414436439,
            280.87173440000004
        ]
    },
    {
        "content": "in Section 4.3. For Python, we collect the routing",
        "page_index": 12,
        "bbox": [
            306.142,
            256.4136344,
            524.4077818879999,
            267.3227344
        ]
    },
    {
        "content": "decision by reruning HumanEval experiment fol\u2212",
        "page_index": 12,
        "bbox": [
            306.142,
            242.8646344,
            526.217383396,
            253.7737344
        ]
    },
    {
        "content": "In this section, we show that improvements brought",
        "page_index": 12,
        "bbox": [
            70.866,
            229.28963439999998,
            289.1317818880001,
            240.19873439999998
        ]
    },
    {
        "content": "lowing the same setting as Section 4.3. Following",
        "page_index": 12,
        "bbox": [
            306.142,
            229.3156344,
            524.406254614,
            240.2247344
        ]
    },
    {
        "content": "by X FT are statistically significant. In our main ex\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            215.7406344,
            290.944782804,
            226.8897346
        ]
    },
    {
        "content": "Mixtral (Jiang et al., 2024), we get the visualiza\u2212",
        "page_index": 12,
        "bbox": [
            306.142,
            215.7656344,
            526.2173833960001,
            226.6747344
        ]
    },
    {
        "content": "periments, we follow prior works (Wei et al., 2023;",
        "page_index": 12,
        "bbox": [
            70.866,
            202.1916344,
            290.04050991800005,
            213.1007344
        ]
    },
    {
        "content": "tion results from layers 0, 11, and 23 in MoEDS,",
        "page_index": 12,
        "bbox": [
            306.142,
            201.21545840000002,
            525.7728205,
            213.1257344
        ]
    },
    {
        "content": "Lozhkov et al., 2024) to conduct experiments on",
        "page_index": 12,
        "bbox": [
            70.866,
            188.6416344,
            289.13876371200007,
            199.55073439999998
        ]
    },
    {
        "content": "where layer 0 and layer 23 are the first and the last",
        "page_index": 12,
        "bbox": [
            305.749,
            188.6676344,
            524.4148404032001,
            199.5767344
        ]
    },
    {
        "content": "HumanEval (+) using greedy decoding. To demon\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            175.0926344,
            290.9413833960001,
            186.0017344
        ]
    },
    {
        "content": "layers of MoEDS. As is shown in Figure 4, we do",
        "page_index": 12,
        "bbox": [
            306.142,
            174.1164584,
            524.414792292,
            186.02773439999999
        ]
    },
    {
        "content": "strate the statistical significance of our improve\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            161.5436344,
            290.94138339600005,
            172.4527344
        ]
    },
    {
        "content": "not observe obvious patterns in the assignment of",
        "page_index": 12,
        "bbox": [
            306.142,
            161.56963439999998,
            524.4086873433,
            172.47873439999998
        ]
    },
    {
        "content": "ments, we change our setting from greedy decoding",
        "page_index": 12,
        "bbox": [
            70.866,
            147.9946344,
            289.1317818880001,
            158.9037344
        ]
    },
    {
        "content": "experts based on the programming language, which",
        "page_index": 12,
        "bbox": [
            306.142,
            148.0196344,
            524.407781888,
            158.9287344
        ]
    },
    {
        "content": "to sampling. In detail, to conduct one experiment",
        "page_index": 12,
        "bbox": [
            70.866,
            134.4456344,
            289.1327855252,
            145.35473439999998
        ]
    },
    {
        "content": "is in line with the observation reported by recent",
        "page_index": 12,
        "bbox": [
            306.142,
            134.4706344,
            524.4147637120001,
            145.3797344
        ]
    },
    {
        "content": "on HumanEval (+), the model will sample one so\u2212",
        "page_index": 12,
        "bbox": [
            70.866,
            120.89563439999999,
            290.94559430860005,
            131.8047344
        ]
    },
    {
        "content": "works (Jiang et al., 2024; Xue et al., 2024). 1681",
        "page_index": 12,
        "bbox": [
            305.749,
            120.9216344,
            519.6000873000002,
            131.8307344
        ]
    },
    {
        "content": "lution for each problem in HumanEval (+) with top",
        "page_index": 12,
        "bbox": [
            70.866,
            107.3466344,
            289.131781888,
            118.25573440000001
        ]
    },
    {
        "content": "p = 0.95 and temperature = 0.8, which is the same",
        "page_index": 12,
        "bbox": [
            70.866,
            93.79763439999999,
            289.13037503850006,
            104.9467346
        ]
    },
    {
        "content": "A.6 Training Settings for STABLE\u2212CODE 3B",
        "page_index": 12,
        "bbox": [
            306.142,
            98.09699810000001,
            516.2269196999999,
            109.0060981
        ]
    },
    {
        "content": "setting used in prior works (Liu et al., 2023; Chen",
        "page_index": 12,
        "bbox": [
            70.866,
            80.2486344,
            289.1335709804,
            91.15773440000001
        ]
    },
    {
        "content": "We use evol\u2212codealpaca\u2212v1 as the training",
        "page_index": 12,
        "bbox": [
            305.629,
            80.2486344,
            524.4118045959999,
            91.6159166
        ]
    },
    {
        "content": "et al., 2021).",
        "page_index": 12,
        "bbox": [
            70.866,
            66.6996344,
            125.70604569999999,
            77.6087344
        ]
    },
    {
        "content": "dataset. Since STABLE\u2212CODE 3B is the base model,",
        "page_index": 12,
        "bbox": [
            306.142,
            66.6996344,
            525.7752395239999,
            77.6087344
        ]
    },
    {
        "content": "13",
        "page_index": 12,
        "bbox": [
            292.183,
            36.8116344,
            303.09209999999996,
            47.7207344
        ]
    },
    {
        "content": "Figure 4: Proportion of tokens assigned to each expert on different programming languages from MultiPL\u2212E",
        "page_index": 13,
        "bbox": [
            70.866,
            472.9060784,
            524.4097784639996,
            482.8686784
        ]
    },
    {
        "content": "(including Python) for layers 0, 11, and 23. The shared expert 0 is excluded from the chart because all the tokens are",
        "page_index": 13,
        "bbox": [
            70.537,
            460.95107840000003,
            524.4057584759997,
            470.9136784
        ]
    },
    {
        "content": "always assigned to it. The gray vertical line marks 1",
        "page_index": 13,
        "bbox": [
            70.866,
            448.99607840000004,
            277.91957909999996,
            460.69188280000003
        ]
    },
    {
        "content": "7 , which is the proportion expected with the uniform sampling.",
        "page_index": 13,
        "bbox": [
            273.948,
            446.3590828,
            524.3970015280003,
            458.95867840000005
        ]
    },
    {
        "content": "we upcycle a new MoE model from the base model,",
        "page_index": 13,
        "bbox": [
            70.473,
            427.2726344,
            290.4920924400001,
            438.18173440000004
        ]
    },
    {
        "content": "namely MoESTABLE. Due to limited computa\u2212",
        "page_index": 13,
        "bbox": [
            70.866,
            412.66024910000004,
            290.947032036,
            424.70809810000003
        ]
    },
    {
        "content": "of parameters for MoETL can be written as 8\u00d71.1B.",
        "page_index": 13,
        "bbox": [
            306.142,
            426.2704584,
            526.318219306,
            438.42173460000004
        ]
    },
    {
        "content": "We use a batch size of 64 and a learning rate of 5e\u22125",
        "page_index": 13,
        "bbox": [
            305.629,
            413.7226344,
            524.407945952,
            424.6317344
        ]
    },
    {
        "content": "tional resources, we construct MoESTABLE with 4",
        "page_index": 13,
        "bbox": [
            70.866,
            399.1724584,
            289.12838679749996,
            411.0827344
        ]
    },
    {
        "content": "with a linear scheduler to fine\u2212tune MoETL for 4",
        "page_index": 13,
        "bbox": [
            305.749,
            399.1724584,
            524.408090874,
            411.0827344
        ]
    },
    {
        "content": "experts in one expert layer, where the top 2 experts",
        "page_index": 13,
        "bbox": [
            70.866,
            386.6246344,
            289.1297637045001,
            397.5337344
        ]
    },
    {
        "content": "epochs with 240 warmup steps. To obtain X FTTL,",
        "page_index": 13,
        "bbox": [
            306.142,
            385.5612491,
            525.7726386,
            397.7737346
        ]
    },
    {
        "content": "are activated for each token, including one shared",
        "page_index": 13,
        "bbox": [
            70.866,
            373.0756344,
            289.13636371000007,
            383.98473440000004
        ]
    },
    {
        "content": "we learn mixing coefficients to merge MoE layers",
        "page_index": 13,
        "bbox": [
            305.749,
            373.0756344,
            524.4104767632,
            383.98473440000004
        ]
    },
    {
        "content": "expert. Consequently, the size of MoESTABLE can",
        "page_index": 13,
        "bbox": [
            70.866,
            358.52445839999996,
            289.133539954,
            370.4357344
        ]
    },
    {
        "content": "inside MoETL by fine\u2212tuning them with a batch",
        "page_index": 13,
        "bbox": [
            306.142,
            358.52445839999996,
            524.4135073059999,
            370.4357344
        ]
    },
    {
        "content": "be described as 4\u00d73B. We use a batch size of 64",
        "page_index": 13,
        "bbox": [
            70.866,
            345.9766344,
            289.132492516,
            357.12573460000004
        ]
    },
    {
        "content": "size of 64, a shared expert rate \u03bb of 0.85, and a",
        "page_index": 13,
        "bbox": [
            306.142,
            345.9766344,
            524.40768945,
            357.12573460000004
        ]
    },
    {
        "content": "and a learning rate of 5e\u22125 with a linear sched\u2212",
        "page_index": 13,
        "bbox": [
            70.866,
            332.4276344,
            290.9413833960001,
            343.3367344
        ]
    },
    {
        "content": "learning rate of 2e\u22125 with a linear schedule for 1",
        "page_index": 13,
        "bbox": [
            306.142,
            332.4276344,
            525.227055298,
            343.3367344
        ]
    },
    {
        "content": "uler to fine\u2212tune MoESTABLE for 4 epochs with",
        "page_index": 13,
        "bbox": [
            70.866,
            317.87745839999997,
            289.13912259800003,
            329.78773440000003
        ]
    },
    {
        "content": "epoch with 60 warmup steps. For a fair compari\u2212",
        "page_index": 13,
        "bbox": [
            306.142,
            318.8786344,
            526.217383396,
            329.78773440000003
        ]
    },
    {
        "content": "500 warmup steps. Similar to X FTDS, we ob\u2212",
        "page_index": 13,
        "bbox": [
            70.866,
            304.32745839999995,
            290.949960636,
            316.4787346
        ]
    },
    {
        "content": "son, we fine\u2212tune a baseline model SFTTL for 5 (=",
        "page_index": 13,
        "bbox": [
            306.142,
            304.26624910000004,
            524.405100134,
            316.3150981
        ]
    },
    {
        "content": "tain X FTSTABLE by learning mixing coefficients",
        "page_index": 13,
        "bbox": [
            70.866,
            290.7172491,
            289.13118828399996,
            302.9297346
        ]
    },
    {
        "content": "to merge MoE layers inside MoESTABLE as normal",
        "page_index": 13,
        "bbox": [
            70.866,
            277.2294584,
            289.129107544,
            289.1397344
        ]
    },
    {
        "content": "FFN layers, which is fine\u2212tuned with a batch size",
        "page_index": 13,
        "bbox": [
            70.866,
            264.6816344,
            289.138763712,
            275.59073440000003
        ]
    },
    {
        "content": "of 64, a shared expert rate \u03bb of 0.85, and a learning",
        "page_index": 13,
        "bbox": [
            70.866,
            251.1326344,
            289.138825476,
            262.2817346
        ]
    },
    {
        "content": "rate of 1e\u22125 with a linear schedule for 1 epoch with",
        "page_index": 13,
        "bbox": [
            70.866,
            237.5836344,
            289.13178188800003,
            248.4927344
        ]
    },
    {
        "content": "125 warmup steps. Our baseline model, namely",
        "page_index": 13,
        "bbox": [
            70.048,
            224.0346344,
            289.5113828860001,
            234.94373439999998
        ]
    },
    {
        "content": "SFTSTABLE, is fine\u2212tuned for 5 (= 4 + 1) epochs",
        "page_index": 13,
        "bbox": [
            70.866,
            209.4222491,
            289.13368232,
            221.4700981
        ]
    },
    {
        "content": "with a batch size of 64, a learning rate of 5e\u22125, and",
        "page_index": 13,
        "bbox": [
            70.473,
            196.9356344,
            289.1324367615,
            207.8447344
        ]
    },
    {
        "content": "625 warmup steps for a fair comparison.",
        "page_index": 13,
        "bbox": [
            70.866,
            183.3866344,
            247.27705610000007,
            194.2957344
        ]
    },
    {
        "content": "A.7 Training Settings for TinyLlama 1.1B",
        "page_index": 13,
        "bbox": [
            70.866,
            155.30599809999998,
            272.6407136,
            166.21509809999998
        ]
    },
    {
        "content": "Using TinyLlama 1.1B as the base model, we up\u2212",
        "page_index": 13,
        "bbox": [
            70.866,
            134.4456344,
            290.9413833960001,
            145.35473439999998
        ]
    },
    {
        "content": "cycle a new MoE model, namely MoETL, from the",
        "page_index": 13,
        "bbox": [
            70.866,
            119.83324909999999,
            289.134743774,
            131.8810981
        ]
    },
    {
        "content": "base model. Following the setting for MoEDS, we",
        "page_index": 13,
        "bbox": [
            70.866,
            106.3454584,
            289.1342588424,
            118.25573440000001
        ]
    },
    {
        "content": "construct MoETL with 8 experts in one expert layer,",
        "page_index": 13,
        "bbox": [
            70.866,
            92.7964584,
            290.4933012,
            104.7067344
        ]
    },
    {
        "content": "4 + 1) epochs with a batch size of 64, a learning",
        "page_index": 13,
        "bbox": [
            305.76,
            291.7806344,
            524.4110913,
            302.6897344
        ]
    },
    {
        "content": "rate of 5e\u22125, and 300 warmup steps.",
        "page_index": 13,
        "bbox": [
            306.142,
            278.2306344,
            462.360312,
            289.1397344
        ]
    },
    {
        "content": "A.8 Theoratical Explanation Details",
        "page_index": 13,
        "bbox": [
            306.142,
            256.5589981,
            481.6039644,
            267.4680981
        ]
    },
    {
        "content": "We consider a simplified variant of X FT as below:",
        "page_index": 13,
        "bbox": [
            305.629,
            239.0066344,
            525.9248288455,
            250.1557346
        ]
    },
    {
        "content": "\u2022 The original dense model is a one\u2212layer trans\u2212",
        "page_index": 13,
        "bbox": [
            308.251,
            219.7256344,
            526.217465496,
            230.63473439999999
        ]
    },
    {
        "content": "former model, which contains one attention layer",
        "page_index": 13,
        "bbox": [
            317.051,
            206.17663439999998,
            524.5937911340001,
            217.08573439999998
        ]
    },
    {
        "content": "connected with one feed\u2212forward network (FFN)",
        "page_index": 13,
        "bbox": [
            317.051,
            192.6276344,
            525.139027952,
            203.5367344
        ]
    },
    {
        "content": "layer. As such, the upcycled MoE model is also",
        "page_index": 13,
        "bbox": [
            317.051,
            179.0776344,
            524.4101691628,
            189.9867344
        ]
    },
    {
        "content": "a one\u2212layer transformer model, containing one",
        "page_index": 13,
        "bbox": [
            317.051,
            165.5286344,
            524.40790007,
            176.43773439999998
        ]
    },
    {
        "content": "attention layer connected with an MoE layer.",
        "page_index": 13,
        "bbox": [
            317.051,
            151.9796344,
            513.0766179,
            162.8887344
        ]
    },
    {
        "content": "\u2022 The upcycled MoE model only has two experts",
        "page_index": 13,
        "bbox": [
            308.251,
            136.4376344,
            524.40590035,
            147.3467344
        ]
    },
    {
        "content": "(e1 and e2), both of which are always selected",
        "page_index": 13,
        "bbox": [
            316.691,
            122.0628006,
            524.4079170979999,
            133.8740981
        ]
    },
    {
        "content": "for processing the input tokens.",
        "page_index": 13,
        "bbox": [
            317.051,
            109.3396344,
            454.20020519999997,
            120.2487344
        ]
    },
    {
        "content": "\u2022 The router in the MoE model assigns constant",
        "page_index": 13,
        "bbox": [
            308.251,
            93.79763439999999,
            524.4148458120002,
            104.7067344
        ]
    },
    {
        "content": "where the top 6 experts are activated for each token,",
        "page_index": 13,
        "bbox": [
            70.473,
            80.2486344,
            290.49209244,
            91.15773440000001
        ]
    },
    {
        "content": "weights to each expert, regardless of the input",
        "page_index": 13,
        "bbox": [
            316.658,
            80.2486344,
            524.4043549400001,
            91.15773440000001
        ]
    },
    {
        "content": "including one shared expert. As such, the number",
        "page_index": 13,
        "bbox": [
            70.866,
            66.6996344,
            289.32367295700004,
            77.6087344
        ]
    },
    {
        "content": "token. Consequently, the output of the MoE",
        "page_index": 13,
        "bbox": [
            317.051,
            66.6996344,
            524.4079000700001,
            77.6087344
        ]
    },
    {
        "content": "14",
        "page_index": 13,
        "bbox": [
            292.183,
            36.8116344,
            303.09209999999996,
            47.7207344
        ]
    },
    {
        "content": "layer for the t\u2212th token ht can be represented",
        "page_index": 14,
        "bbox": [
            81.775,
            756.8818006000001,
            289.130416224,
            768.8577346000001
        ]
    },
    {
        "content": "as (1 \u2212 \u03b1)e1(ut) + \u03b1e2(ut), where 1 \u2212 \u03b1 is",
        "page_index": 14,
        "bbox": [
            81.775,
            743.3328006,
            289.13389709399996,
            755.3087346
        ]
    },
    {
        "content": "the router weight assigned to e1, \u03b1 is the router",
        "page_index": 14,
        "bbox": [
            81.775,
            729.7838006000001,
            289.3188947773,
            741.7587346
        ]
    },
    {
        "content": "weight assigned to e2, and ut is the input of the",
        "page_index": 14,
        "bbox": [
            81.382,
            716.2348006000001,
            289.13415509,
            728.0460981
        ]
    },
    {
        "content": "MoE layer for the t\u2212th token.",
        "page_index": 14,
        "bbox": [
            81.775,
            703.5116344,
            208.00448930000002,
            714.6607346000001
        ]
    },
    {
        "content": "\u2022 We simplify the process of merging the MoE",
        "page_index": 14,
        "bbox": [
            72.97600000000001,
            687.9696344,
            289.13075504200003,
            698.8787344
        ]
    },
    {
        "content": "model back to a dense model as We\u03b1 = (1 \u2212",
        "page_index": 14,
        "bbox": [
            81.775,
            672.9843456000001,
            289.13409798,
            685.5697346000002
        ]
    },
    {
        "content": "\u03b1)We1 + \u03b1We2, where We refers to the weight",
        "page_index": 14,
        "bbox": [
            81.775,
            659.3253456000002,
            289.133470752,
            672.0207346000003
        ]
    },
    {
        "content": "of e and e\u03b1 refers to the weight of the FFN in",
        "page_index": 14,
        "bbox": [
            81.775,
            646.4958006,
            289.134145756,
            658.3080980999999
        ]
    },
    {
        "content": "the merged dense model.",
        "page_index": 14,
        "bbox": [
            81.775,
            633.7726344,
            190.647818,
            644.6817344
        ]
    },
    {
        "content": "In this simplified scenario, if we denote f (x; \u03b8)",
        "page_index": 14,
        "bbox": [
            81.775,
            611.2576344,
            290.41054899000005,
            622.4067346
        ]
    },
    {
        "content": "as the output of the model \u03b8 for the input x, the",
        "page_index": 14,
        "bbox": [
            70.866,
            597.7086344,
            289.13574401,
            608.8577346000001
        ]
    },
    {
        "content": "output of this simplified MoE model for input to\u2212",
        "page_index": 14,
        "bbox": [
            70.866,
            584.1586344,
            290.94138339600005,
            595.0677344
        ]
    },
    {
        "content": "ken x can be represented as f (x; \u03b8MoE). Interest\u2212",
        "page_index": 14,
        "bbox": [
            70.866,
            569.5662490999999,
            290.940964206,
            581.7587346
        ]
    },
    {
        "content": "ingly, if we define two new dense models \u03b81 and",
        "page_index": 14,
        "bbox": [
            70.866,
            556.2348006000001,
            289.133795208,
            568.2097346
        ]
    },
    {
        "content": "\u03b82, where \u03b81 and \u03b82 use the same attention layer as",
        "page_index": 14,
        "bbox": [
            70.866,
            542.6858006000001,
            289.1346253349,
            554.6607346000001
        ]
    },
    {
        "content": "this MoE model while using e1 and e2 as the FFN",
        "page_index": 14,
        "bbox": [
            70.866,
            529.1358006,
            289.135476454,
            540.9480980999999
        ]
    },
    {
        "content": "layer separately, f (x; \u03b8MoE) can be represented as",
        "page_index": 14,
        "bbox": [
            70.866,
            515.3702491,
            289.1311359052,
            527.5617346
        ]
    },
    {
        "content": "(1 \u2212 \u03b1)f (x; \u03b81) + \u03b1f (x; \u03b82)! Consequently, the",
        "page_index": 14,
        "bbox": [
            69.59,
            502.0378006,
            289.13329353200004,
            514.0127346
        ]
    },
    {
        "content": "computation process of this simplified MoE model",
        "page_index": 14,
        "bbox": [
            70.866,
            489.3146344,
            289.1343127992001,
            500.2237344
        ]
    },
    {
        "content": "can be viewed as ensembling the outputs of two",
        "page_index": 14,
        "bbox": [
            70.866,
            475.7656344,
            289.13876371200007,
            486.67473440000003
        ]
    },
    {
        "content": "dense models \u03b81 and \u03b82. Meanwhile, the process of",
        "page_index": 14,
        "bbox": [
            70.866,
            461.3898006,
            289.1316439,
            473.3657346
        ]
    },
    {
        "content": "merging the upcycled MoE model back to a dense",
        "page_index": 14,
        "bbox": [
            70.866,
            448.6666344,
            289.1349673452001,
            459.57573440000004
        ]
    },
    {
        "content": "model in this simplified X FT can be represented",
        "page_index": 14,
        "bbox": [
            70.866,
            435.1176344,
            289.13690108,
            446.2667346
        ]
    },
    {
        "content": "as \u03b8\u03b1 = (1 \u2212 \u03b1)\u03b81 + \u03b1\u03b82, which is the merging of",
        "page_index": 14,
        "bbox": [
            70.866,
            420.7428006,
            289.13718,
            432.71773460000003
        ]
    },
    {
        "content": "the same two dense models \u03b81 and \u03b82.",
        "page_index": 14,
        "bbox": [
            70.866,
            407.1938006,
            236.502275,
            419.1687346
        ]
    },
    {
        "content": "15",
        "page_index": 14,
        "bbox": [
            292.183,
            36.811634399999946,
            303.09209999999996,
            47.72073439999995
        ]
    }
]