\section{Main Evaluation}\label{sec:experiment}
\input{src/table/python-to-text}

\subsection{Experimental Setup}
\textbf{Training.} 
We use \dscoderbase 1.3B~\cite{guo2024deepseekcoder} as the main base code \llm. 
\evolcode, an open-source \evolinstruct~\cite{luo2023wizardcoder} dataset containing 110K samples, is used as our instruction dataset.
\textbf{\oursmoe}, our \moe model upcycled from the base model, is implemented following Llama-MoE~\cite{llama-moe-2023}. 
It is constructed with 8 experts in one expert layer and the top 6 experts\footnote{
6 is the best-performing number of activated experts per our \humanevalp{} experiments using top $\{2,4,6\}$ experts.} are activated for each token, including one shared expert. 
As such, we denote the model size of \textbf{\oursmoe} as 8$\times$1.3B.
Other hyperparameter settings are detailed in Appendix \ref{sec:hyperparameter}. 
We finally obtain \textbf{\oursmerge} by using the learned mixing coefficients to merge \moe layers inside \oursmoe as normal FFN layers. 
Note that \textbf{\oursmerge} is the final instruction-tuned \llm we produce, while \textbf{\oursmoe} is only an intermediate product of \ours framework.


\textbf{Baselines.} 
To study the effectiveness of \ours, we build a baseline model, namely \textbf{\baselineds}, by directly performing SFT for \dscoderbase 1.3B on \evolcode. 
To compare \ours with \ewa~\cite{huang2023experts}, we also implement a baseline \textbf{\ewads} and instruction-tune it using the same hyperparameter setting as \baselineds, which is described in Appendix \ref{sec:hyperparameter}. More implementation details of \ewads can be seen in Appendix \ref{sec:ewa_details}. 
Furthermore, we incorporate multiple small open-source models (<3B) as our baselines, including \dscoderbase 1.3B, \dscoderinst 1.3B~\cite{guo2024deepseekcoder}, Phi-2 2.7B, and \stablecoder 3B~\cite{stable-code-3b}.


\input{src/table/multipl-e}
\input{src/table/ds1000}
\subsection{Python Text-to-Code Generation}

\humaneval~\cite{chen2021evaluating} and \mbpp~\cite{austin2021program} benchmarks are the two most widely-used collections of Python code generation tasks. 
We further employ \humanevalp and \mbppp, which use more tests automatically generated by \evalplus~\cite{evalplus} for more rigorous evaluation. 
We leave the details in Appendix \ref{sec:benchmarks}.

Table~\ref{tab:python-text2code} shows the \passat{1} results of different \llm{s}.
\ours achieves 67.1 \passat{1} on \humaneval and 64.6 \passat{1} on \humanevalp, which makes it the new state-of-the-art small code \llm{} (<3B). 
We can also observe that \oursmerge has a clear improvement over the \baselineds on both benchmarks, with 13\% and 2\% improvement on \humanevalp and \mbppp respectively, while \ewads even performs worse than \baselineds on \mbpp{(+)}.
\oursmerge also outperforms \ewads on both benchmarks. 
Surprisingly, \oursmerge even surpasses \oursmoe on \humaneval and \humanevalp, despite only using around $\sfrac{1}{8}\times$ parameters and around $\sfrac{1}{6}\times$ computations, which showcases the effectiveness of our simple learnable merging technique. Appendix~\ref{sec:statistic} further demonstrates the statistical significance of the improvements brought by \ours.

\subsection{Multilingual Code Generation}\label{sec:multiple}

We use \multiple~\cite{cassano2022multiple}, a multi-programming benchmark that supports 18 programming languages in addition to Python, to evaluate the multilingual ability and generalizability of \ours{}. 
Among these, we choose 6 representative programming for their distinct language features: Java, JavaScript, C++, PHP, Swift, and Rust, following~\citet{wei2023magicoder}.
Table \ref{tab:multilang} shows, among all 1.3B models, \oursmerge achieves the best average multilingual performance and performs the best on 5 (out of 6) individual programming languages, 
overall largely improving \baselineds{} which uses standard SFT.
Notably, the overall performance of \ewads is on par with \baselineds, indicating that \ewads{} may not improve SFT on multilingual coding.
Appendix~\ref{sec:expert_analysis} further studies whether each expert in \oursmoe specializes differently in these programming languages.

\subsection{Code Generation for Data Science}

The \dsonek dataset~\cite{lai2022ds1000} is a collection of 1000 realistic data science coding problems ranging from 7 popular data science libraries in Python, including Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), and TensorFlow (tf). 
We evaluate \ours on \dsonek{} to understand its effectiveness for practical data science engineering. 
We follow the evaluation setting of prior works~\cite{guo2024deepseekcoder, wei2023magicoder}. 
In Table \ref{tab:ds1000}, \oursmerge achieves the best overall performance among all the evaluated 1.3B models. 
Specifically, \oursmerge consistently surpasses \baselineds among all the seven studied libraries and also outperforms \ewads in general.


