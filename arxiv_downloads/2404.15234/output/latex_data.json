{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "Massively Annotated Datasets for Assessment of Synthetic and Real Data in Face Recognition",
            "leftover": "Massively Annotated Datasets for Assessment of Synthetic and Real Data in Face Recognition",
            "matches": []
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{\\parbox{16cm}{ {\\large Pedro C. Neto^1,2, Rafael M. Mamede^1,2, Carolina Albuquerque^1,2, Tiago Gonçalves^1,2, Ana F. Sequeira^1,2} {\\normalsize ^1 Faculty of Engineering of the University of Porto, Porto, Portugal ^2 Institute for Systems and Computer Engineering, Technology and Science, Porto, Portugal}} }",
            "leftover": "{\\parbox{16cm}{ {\\large Pedro C. Neto^1,2, Rafael M. Mamede^1,2, Carolina Albuquerque^1,2, Tiago Gonçalves^1,2, Ana F. Sequeira^1,2} {\\normalsize ^1 Faculty of Engineering of the University of Porto, Porto, Portugal ^2 Institute for Systems and Computer Engineering, Technology and Science, Porto, Portugal}} }",
            "matches": []
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power. However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access. Privacy and ethical concerns are relevant topics within these domains. Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems. Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the stateoftheart models trained on real data. To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic. From these annotations, we conduct studies on the distribution of each attribute within all four datasets. Additionally, we further inspect the differences between real and synthetic datasets on the attribute set. When comparing through the Kullback–Leibler divergence we have found differences between real and synthetic samples. Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true.",
            "leftover": "Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power. However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access. Privacy and ethical concerns are relevant topics within these domains. Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems. Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the stateoftheart models trained on real data. To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic. From these annotations, we conduct studies on the distribution of each attribute within all four datasets. Additionally, we further inspect the differences between real and synthetic datasets on the attribute set. When comparing through the Kullback–Leibler divergence we have found differences between real and synthetic samples. Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true.",
            "matches": []
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "key": "doc/body/sec0",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 3,
                            "key": "doc/body/sec0/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "Introduction",
                            "matches": []
                        },
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec0/txl0",
                            "block type": "txl",
                            "content": "Complex Face Recognition systems have matched and surpassed humanlevel performance. Recent advances led to deep learningbased neural networks that can learn to distinguish between the most variate identities from a single image. The resourcefulness of such models has led to a continuous focus on improving the bestperforming models. Over the years, this improvement was supported by three strong pillars. 1) Exponential increase in computing power; 2) Novel architectures and more expressive deep learning models; 3) Very large datasets.",
                            "leftover": "Complex Face Recognition systems have matched and surpassed humanlevel performance. Recent advances led to deep learningbased neural networks that can learn to distinguish between the most variate identities from a single image. The resourcefulness of such models has led to a continuous focus on improving the bestperforming models. Over the years, this improvement was supported by three strong pillars. 1) Exponential increase in computing power; 2) Novel architectures and more expressive deep learning models; 3) Very large datasets.",
                            "matches": []
                        },
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec0/txl1",
                            "block type": "txl",
                            "content": "As mentioned, one of the approaches to further enhance these models relied on the collection and curation of large datasets. These datasets vary in the number of identities, from 10k to 672k, and in the number of images, from 500k to 17M. However, the collection of these datasets has raised privacy and ethical concerns regarding the consent of the individuals present in the data. This led to the retraction of several of these previously publicly available datasets. Moreover, a dataset that is composed of real images with proper curation and consent is not static, since according to the European Union (EU) General Data Protection Regulation (GDPR), consent can be removed at any time. Additionally, individual anonymization of the sample faces is not feasible and further removes the utility of the face to train a face recognition system. This poses a problem for current face recognition research, which requires the use of largescale datasets that are being limited and removed from public access as previously mentioned.",
                            "leftover": "As mentioned, one of the approaches to further enhance these models relied on the collection and curation of large datasets. These datasets vary in the number of identities, from 10k to 672k, and in the number of images, from 500k to 17M. However, the collection of these datasets has raised privacy and ethical concerns regarding the consent of the individuals present in the data. This led to the retraction of several of these previously publicly available datasets. Moreover, a dataset that is composed of real images with proper curation and consent is not static, since according to the European Union (EU) General Data Protection Regulation (GDPR), consent can be removed at any time. Additionally, individual anonymization of the sample faces is not feasible and further removes the utility of the face to train a face recognition system. This poses a problem for current face recognition research, which requires the use of largescale datasets that are being limited and removed from public access as previously mentioned.",
                            "matches": []
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec0/txl2",
                            "block type": "txl",
                            "content": "Recently, there has been significant growth in generative artificial intelligence approaches, leading to stateoftheart methods that can synthesise images that closely resemble real images. Since the initial generative adversarial neural network (GAN) and their improved versions, there have been several advances that led to the development of diffusion models. These models are easier to train and lead to betterquality images. Recently, some generative models have been proposed for face data, allowing researchers to condition the identity or other attributes. Following the improvements in generative artificial intelligence, researchers have redirected their efforts into how to synthesise new datasets for face recognition that could remove the dependency on the previously used real datasets.",
                            "leftover": "Recently, there has been significant growth in generative artificial intelligence approaches, leading to stateoftheart methods that can synthesise images that closely resemble real images. Since the initial generative adversarial neural network (GAN) and their improved versions, there have been several advances that led to the development of diffusion models. These models are easier to train and lead to betterquality images. Recently, some generative models have been proposed for face data, allowing researchers to condition the identity or other attributes. Following the improvements in generative artificial intelligence, researchers have redirected their efforts into how to synthesise new datasets for face recognition that could remove the dependency on the previously used real datasets.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec0/figure3",
                            "block_type": "figure",
                            "children": [
                                {
                                    "leaf id": 7,
                                    "key": "doc/body/sec0/figure3/cpt0",
                                    "block type": "cpt",
                                    "content": "An example of an annotated synthetic image. It is possible to observe some of the well defined attributes.",
                                    "leftover": "An example of an annotated synthetic image. It is possible to observe some of the well defined attributes.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec0/txl4",
                            "block type": "txl",
                            "content": "Despite efforts to develop synthetic data that faithfully represents real data, the performance of models trained on these new synthetic datasets is yet to achieve a performance similar to models trained on real data, even when the data is constructed through different synthesising approaches. These former models seem to perform considerably worse on certain ethnicities and other variations of the traditional face verification setting, such as crosspose or crossage. This behaviour can be sustained by several factors, and Huber~et al. has already explored the diversity of synthetic datasets with regards to gender, ethnicity, age and head position. We firmly believe that the diversity of face datasets can be further described by other attributes. The work of Terhörst et al.} aims to create datasets that are annotated for 47 distinct attributes, which can be leveraged to highlight differences between synthetic and real datasets. Previous research has noted a potential domain gap between real and synthetic data",
                            "leftover": "Despite efforts to develop synthetic data that faithfully represents real data, the performance of models trained on these new synthetic datasets is yet to achieve a performance similar to models trained on real data, even when the data is constructed through different synthesising approaches. These former models seem to perform considerably worse on certain ethnicities and other variations of the traditional face verification setting, such as crosspose or crossage. This behaviour can be sustained by several factors, and Huber~et al. has already explored the diversity of synthetic datasets with regards to gender, ethnicity, age and head position. We firmly believe that the diversity of face datasets can be further described by other attributes. The work of Terhörst et al.} aims to create datasets that are annotated for 47 distinct attributes, which can be leveraged to highlight differences between synthetic and real datasets. Previous research has noted a potential domain gap between real and synthetic data",
                            "matches": []
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec0/txl5",
                            "block type": "txl",
                            "content": "As seen in Figure, the performance of an FR system trained on synthetic samples might be restricted by the fact that those samples do not capture the complete variation and full spectrum of real samples. In this paper, we aim to understand how closely the synthetic data mimics the distribution of the real data. For this, we have leveraged two real datasets BUPTBalancedFace and BUPTGlobalFace in addition to two synthetic datasets generated, one generated through diffusion (IDiffFace) and the other with a GAN. Using Terhörst~et al. MAC method, we have computed annotations for all the samples in the four datasets. Following this, we have conducted several studies on the distribution of these annotations in order to extract information regarding the diversity of each dataset.",
                            "leftover": "As seen in Figure, the performance of an FR system trained on synthetic samples might be restricted by the fact that those samples do not capture the complete variation and full spectrum of real samples. In this paper, we aim to understand how closely the synthetic data mimics the distribution of the real data. For this, we have leveraged two real datasets BUPTBalancedFace and BUPTGlobalFace in addition to two synthetic datasets generated, one generated through diffusion (IDiffFace) and the other with a GAN. Using Terhörst~et al. MAC method, we have computed annotations for all the samples in the four datasets. Following this, we have conducted several studies on the distribution of these annotations in order to extract information regarding the diversity of each dataset.",
                            "matches": []
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec0/txl6",
                            "block type": "txl",
                            "content": "This paper is organized into four main sections. The first of these, presents related work regarding automatic annotation strategies and synthetic data generation. Afterwards, in the Methods section, the fundamental details of the experiment's design and setup are discussed in detail. This latter section is followed by a Results section, which aims to present the main findings. Finally, we conclude with a discussion of future work and a summary of the most important elements of this research. The contributions of this work are the following:",
                            "leftover": "This paper is organized into four main sections. The first of these, presents related work regarding automatic annotation strategies and synthetic data generation. Afterwards, in the Methods section, the fundamental details of the experiment's design and setup are discussed in detail. This latter section is followed by a Results section, which aims to present the main findings. Finally, we conclude with a discussion of future work and a summary of the most important elements of this research. The contributions of this work are the following:",
                            "matches": []
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec0/itemize7",
                            "block type": "itemize",
                            "content": "Created annotations, which will be publicy available, for two real datasets. One of the datasets is balanced for ethnicity, whereas the other follows the world ethnicity distribution; Replicated the annotation process on two synthetic datasets, enabling future research on the softbiometrics of these datasets using the released annotations; Performed a statistical analysis of the diversity of these datasets through the study of their annotations.",
                            "leftover": "Created annotations, which will be publicy available, for two real datasets. One of the datasets is balanced for ethnicity, whereas the other follows the world ethnicity distribution; Replicated the annotation process on two synthetic datasets, enabling future research on the softbiometrics of these datasets using the released annotations; Performed a statistical analysis of the diversity of these datasets through the study of their annotations.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Related Work",
                            "leftover": "Related Work",
                            "matches": []
                        },
                        {
                            "leaf id": 13,
                            "key": "doc/body/sec1/txl0",
                            "block type": "txl",
                            "content": "Face recognition from synthetic data has grown in popularity in recent years. It presented some challenges to researchers as it was not possible to generate several samples from the same identity, nor generate sufficiently realistic samples. Over the years, these problems have been mitigated. In 2022, Boutros~et al. proposed SFace, a generative adversarial networkbased approach to create new samples, and in 2024 proposed SFace2 achieving stateoftheart results when compared to other GANbased methods. To mitigate the existence of a single image per identity, some works have also explored unsupervised approaches to train face recognition systems. IDiffFace comprised a novel diffusionbased technique that conditions the model on a desired identity, leading to a dataset that achieves, in some datasets, a performance comparable to a model trained on real datasets. Kim~et al. leveraged the conditioning of diffusion models to generate samples from a specific identity and with a specific style. For instance, it is possible to generate an image of a certain person using glasses.",
                            "leftover": "Face recognition from synthetic data has grown in popularity in recent years. It presented some challenges to researchers as it was not possible to generate several samples from the same identity, nor generate sufficiently realistic samples. Over the years, these problems have been mitigated. In 2022, Boutros~et al. proposed SFace, a generative adversarial networkbased approach to create new samples, and in 2024 proposed SFace2 achieving stateoftheart results when compared to other GANbased methods. To mitigate the existence of a single image per identity, some works have also explored unsupervised approaches to train face recognition systems. IDiffFace comprised a novel diffusionbased technique that conditions the model on a desired identity, leading to a dataset that achieves, in some datasets, a performance comparable to a model trained on real datasets. Kim~et al. leveraged the conditioning of diffusion models to generate samples from a specific identity and with a specific style. For instance, it is possible to generate an image of a certain person using glasses.",
                            "matches": []
                        },
                        {
                            "leaf id": 14,
                            "key": "doc/body/sec1/txl1",
                            "block type": "txl",
                            "content": "Softbiometric annotations for face images provide contextual information not dependent on a specific identity (such as gender, age, or ethnicity) and are essential for exploring the variability of the data. Low variability of some characteristics can make models trained on that dataset less robust to applications on realworld data, where inference on examples with such characteristics is necessary. With this information, it is also possible to explore, disclose, and correct demographic biases, addressing fairness concerns. The process of annotating manually is labourintensive, which can be unfeasible for large datasets. Some works have proposed classificationbased estimation of softbiometric characteristics which can aid the annotation of large datasets: Karkkainen et al. } (ethnicity, gender, age), GonzalezSosa et al. } (gender, age, craniofacial features, skin colour, subjective annotation),Mirabet~et al. (hidden face attributes), Merler et al. } (gender, age, glasses, beard, and moustache), Terhörst et al.} (47 attribute annotations covering gender, ethnicity, age, accessories, facial hair, hairstyles, subjective annotation, and other facial characteristics). Following the completeness of the 47 attributes generated by the model proposed by Terhörst et al., it presents itself as the ideal approach to studying the broad diversity of synthetic datasets. A similar annotation approach has been followed for DeepFake datasets, and through the analysis of such annotations it was possible to understand and detect certain biases on DeepFake detection systems. This further highlights the importance of having this information available.",
                            "leftover": "Softbiometric annotations for face images provide contextual information not dependent on a specific identity (such as gender, age, or ethnicity) and are essential for exploring the variability of the data. Low variability of some characteristics can make models trained on that dataset less robust to applications on realworld data, where inference on examples with such characteristics is necessary. With this information, it is also possible to explore, disclose, and correct demographic biases, addressing fairness concerns. The process of annotating manually is labourintensive, which can be unfeasible for large datasets. Some works have proposed classificationbased estimation of softbiometric characteristics which can aid the annotation of large datasets: Karkkainen et al. } (ethnicity, gender, age), GonzalezSosa et al. } (gender, age, craniofacial features, skin colour, subjective annotation),Mirabet~et al. (hidden face attributes), Merler et al. } (gender, age, glasses, beard, and moustache), Terhörst et al.} (47 attribute annotations covering gender, ethnicity, age, accessories, facial hair, hairstyles, subjective annotation, and other facial characteristics). Following the completeness of the 47 attributes generated by the model proposed by Terhörst et al., it presents itself as the ideal approach to studying the broad diversity of synthetic datasets. A similar annotation approach has been followed for DeepFake datasets, and through the analysis of such annotations it was possible to understand and detect certain biases on DeepFake detection systems. This further highlights the importance of having this information available.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 15,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Methods",
                            "leftover": "Methods",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec2/sub0",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 16,
                                    "key": "doc/body/sec2/sub0/tit",
                                    "block type": "title",
                                    "content": "Datasets",
                                    "leftover": "Datasets",
                                    "matches": []
                                },
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec2/sub0/txl0",
                                    "block type": "txl",
                                    "content": "In this section we present the four datasets used in our experiments. We provide details on their composition and, in the case of synthetic datasets, the generative approach.",
                                    "leftover": "In this section we present the four datasets used in our experiments. We provide details on their composition and, in the case of synthetic datasets, the generative approach.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/sub0/ssb1",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 18,
                                            "key": "doc/body/sec2/sub0/ssb1/tit",
                                            "block type": "title",
                                            "content": "Real datasets",
                                            "leftover": "Real datasets",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 19,
                                            "key": "doc/body/sec2/sub0/ssb1/txl0",
                                            "block type": "txl",
                                            "content": "BUPTBalanced and BUPTGlobalFace have been proposed by Wang~et al. and were intended to create a framework to study the biases of face recognition models. Each identity on the dataset has been labelled according to its skin tone into one of the following ethnicities: African, Asian, Caucasian and Indian. BUPTBalanced balances the number of identities that belong to each of these four categories and is composed of 1.3 million images with 28k identities, which means that there are 7k identities per ethnicity. On the other hand, BUPTGlobalface contains two million images from 38k identities, and the ethnicity distribution of the identities follows the same distribution seen in the world's population.",
                                            "leftover": "BUPTBalanced and BUPTGlobalFace have been proposed by Wang~et al. and were intended to create a framework to study the biases of face recognition models. Each identity on the dataset has been labelled according to its skin tone into one of the following ethnicities: African, Asian, Caucasian and Indian. BUPTBalanced balances the number of identities that belong to each of these four categories and is composed of 1.3 million images with 28k identities, which means that there are 7k identities per ethnicity. On the other hand, BUPTGlobalface contains two million images from 38k identities, and the ethnicity distribution of the identities follows the same distribution seen in the world's population.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 20,
                                            "key": "doc/body/sec2/sub0/ssb1/txl1",
                                            "block type": "txl",
                                            "content": "Comprising two distinct ethnicity distributions these datasets allow for a comparison of the Real vs. Real to be used as a baseline of the differences that can be expected from datasets that are known to have different distributions on certain attributes.",
                                            "leftover": "Comprising two distinct ethnicity distributions these datasets allow for a comparison of the Real vs. Real to be used as a baseline of the differences that can be expected from datasets that are known to have different distributions on certain attributes.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/sub0/ssb2",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 21,
                                            "key": "doc/body/sec2/sub0/ssb2/tit",
                                            "block type": "title",
                                            "content": "Synthetic datasets",
                                            "leftover": "Synthetic datasets",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 22,
                                            "key": "doc/body/sec2/sub0/ssb2/txl0",
                                            "block type": "txl",
                                            "content": "For the study of synthetic data, we have selected two fundamentally different datasets. The first dataset, referred to throughout the rest of this paper as SynGAN, was introduced as a tool to be used in quantisation scenarios. It is comprised of 500k images that have no information regarding their identity. They were generated with a generative adversarial network. Using noise sampled from a Gaussian distribution, a pretrained generatorhttps://github.com/NVlabs/stylegan2ada of StyleGAN2AD is used to create novel face samples. Neto~et al. explored the effect of the quantisation of deep neural networks with this synthetic dataset on the bias of the final face recognition system. The higher robustness shown highlights a possibility that this data comprises samples that slightly deviate from the real data distribution.",
                                            "leftover": "For the study of synthetic data, we have selected two fundamentally different datasets. The first dataset, referred to throughout the rest of this paper as SynGAN, was introduced as a tool to be used in quantisation scenarios. It is comprised of 500k images that have no information regarding their identity. They were generated with a generative adversarial network. Using noise sampled from a Gaussian distribution, a pretrained generatorhttps://github.com/NVlabs/stylegan2ada of StyleGAN2AD is used to create novel face samples. Neto~et al. explored the effect of the quantisation of deep neural networks with this synthetic dataset on the bias of the final face recognition system. The higher robustness shown highlights a possibility that this data comprises samples that slightly deviate from the real data distribution.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 23,
                                            "key": "doc/body/sec2/sub0/ssb2/txl1",
                                            "block type": "txl",
                                            "content": "The second dataset exploited diffusion models to generate identityconditioned samples. IDiff.Face was used to create a dataset called CPD25 (TwoStage), which comprises 10k identities and 50 images for each identity. This dataset is significantly more realistic and has been shown to have a performance that reduces the gap between models trained on real and synthetic datasets. Besides the identity, no other attribute is conditioned.",
                                            "leftover": "The second dataset exploited diffusion models to generate identityconditioned samples. IDiff.Face was used to create a dataset called CPD25 (TwoStage), which comprises 10k identities and 50 images for each identity. This dataset is significantly more realistic and has been shown to have a performance that reduces the gap between models trained on real and synthetic datasets. Besides the identity, no other attribute is conditioned.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/sub0/ssb3",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 24,
                                            "key": "doc/body/sec2/sub0/ssb3/tit",
                                            "block type": "title",
                                            "content": "Annotations",
                                            "leftover": "Annotations",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 25,
                                            "key": "doc/body/sec2/sub0/ssb3/txl0",
                                            "block type": "txl",
                                            "content": "None of the datasets includes annotations beyond identity, with an exception for the skin tonebased labels on the two real datasets. Hence, studies on the diversity of these datasets from the point of view of softbiometrics was limited. Additionally, fairness assessments were not trivial. Knowing the impact that these annotations might have no future research, we released, for each image in each dataset, 45 different attributeshttps://github.com/NetoPedro/SynthMAADFace. For ethical reasons, we have decided to exclude annotations for the fields ''Chubby'' and ''Attractive'' seen in the original paper. In total we provide roughly 189M annotations, which is slightly larger than the annotations given by Terhörst et al. at 124M.",
                                            "leftover": "None of the datasets includes annotations beyond identity, with an exception for the skin tonebased labels on the two real datasets. Hence, studies on the diversity of these datasets from the point of view of softbiometrics was limited. Additionally, fairness assessments were not trivial. Knowing the impact that these annotations might have no future research, we released, for each image in each dataset, 45 different attributeshttps://github.com/NetoPedro/SynthMAADFace. For ethical reasons, we have decided to exclude annotations for the fields ''Chubby'' and ''Attractive'' seen in the original paper. In total we provide roughly 189M annotations, which is slightly larger than the annotations given by Terhörst et al. at 124M.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 26,
                                    "key": "doc/body/sec2/sub1/tit",
                                    "block type": "title",
                                    "content": "Experimental Design",
                                    "leftover": "Experimental Design",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec2/sub1/ssb0",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 27,
                                            "key": "doc/body/sec2/sub1/ssb0/tit",
                                            "block type": "title",
                                            "content": "Annotation process",
                                            "leftover": "Annotation process",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 28,
                                            "key": "doc/body/sec2/sub1/ssb0/txl0",
                                            "block type": "txl",
                                            "content": "The generation of annotations for each of the aforementioned datasets used the methodology proposed by Terhörst et al.}. Given an image x aligned with MTCNN, we map the image to face template space given by FaceNet, which is further mapped to 47 different softbiometric attributes via the Massive Attribute Classifier (MAC), a multiobjective NNbased classifier. Each attribute is considered to be an individual classification task, and the majority of the attributes can be ''Positive'', ''Negative'' or ''Undefined''.",
                                            "leftover": "The generation of annotations for each of the aforementioned datasets used the methodology proposed by Terhörst et al.}. Given an image x aligned with MTCNN, we map the image to face template space given by FaceNet, which is further mapped to 47 different softbiometric attributes via the Massive Attribute Classifier (MAC), a multiobjective NNbased classifier. Each attribute is considered to be an individual classification task, and the majority of the attributes can be ''Positive'', ''Negative'' or ''Undefined''.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 29,
                                            "key": "doc/body/sec2/sub1/ssb0/txl1",
                                            "block type": "txl",
                                            "content": "The proposed implementation strategy leveraged the idea of reliability as a metric for the confidence of a prediction of each attribute . Here, we consider an ensemble of m MAC classifiers where dropout is applied individually with a given probability (pdrop=0.5) during test. This results in m estimators with slightly different architectures due to the zeroed connections. This dropout process mimics the behaviour of a Gaussian process and the final prediction for each of the A attributes is computed through a majority vote of the classifiers in the ensemble. Considering each element of the ensemble as a classifier fi:R^N → [0,1]^A, where i∈1,2,...,m, the reliability of the prediction for the attribute a, rel(f(x)^(a))=rel(y^(a)), for a∈1,2,...,A, is given by:",
                                            "leftover": "The proposed implementation strategy leveraged the idea of reliability as a metric for the confidence of a prediction of each attribute . Here, we consider an ensemble of m MAC classifiers where dropout is applied individually with a given probability (pdrop=0.5) during test. This results in m estimators with slightly different architectures due to the zeroed connections. This dropout process mimics the behaviour of a Gaussian process and the final prediction for each of the A attributes is computed through a majority vote of the classifiers in the ensemble. Considering each element of the ensemble as a classifier fi:R^N → [0,1]^A, where i∈1,2,...,m, the reliability of the prediction for the attribute a, rel(f(x)^(a))=rel(y^(a)), for a∈1,2,...,A, is given by:",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 30,
                                            "key": "doc/body/sec2/sub1/ssb0/equation2",
                                            "block type": "equation",
                                            "content": "rel(y^(a)) = {(1 α)m ∑i=1^m yi^(a)  {αm^2 ∑i=1^m ∑j=1^m |yi^(a)yj^(a)|",
                                            "leftover": "rel(y^(a)) = {(1 α)m ∑i=1^m yi^(a)  {αm^2 ∑i=1^m ∑j=1^m |yi^(a)yj^(a)|",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 31,
                                            "key": "doc/body/sec2/sub1/ssb0/txl3",
                                            "block type": "txl",
                                            "content": "The parameter α balances the impact of the centrality measure versus the impact of the dispersion metric and, for equal impact of these factors, we chose this value to be 0.5. Similarly to the original paper, the number of estimators was set as m=100.",
                                            "leftover": "The parameter α balances the impact of the centrality measure versus the impact of the dispersion metric and, for equal impact of these factors, we chose this value to be 0.5. Similarly to the original paper, the number of estimators was set as m=100.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec2/sub1/ssb0/txl4",
                                            "block type": "txl",
                                            "content": "The original implementation of MAC creates a new ensemble of models for each image, and that ensemble is then used to estimate the softbiometric attributes. Hence, each image is passed, one by one, 100 times through the network. Our implementation creates a batch of images during the inference step. Each image in the batch will be evaluated by the same ensemble of models. However, since the same image is still seen by 100 distinct architectures, we retain the advantages of the Gaussian Process while being able to run experiments hundreds of times faster. This allows the usage of the MAAD methodology to be more easily applicable to label large datasets. We used a labelling batch size of 1024.",
                                            "leftover": "The original implementation of MAC creates a new ensemble of models for each image, and that ensemble is then used to estimate the softbiometric attributes. Hence, each image is passed, one by one, 100 times through the network. Our implementation creates a batch of images during the inference step. Each image in the batch will be evaluated by the same ensemble of models. However, since the same image is still seen by 100 distinct architectures, we retain the advantages of the Gaussian Process while being able to run experiments hundreds of times faster. This allows the usage of the MAAD methodology to be more easily applicable to label large datasets. We used a labelling batch size of 1024.",
                                            "matches": []
                                        },
                                        {
                                            "key": "doc/body/sec2/sub1/ssb0/figure*5",
                                            "block_type": "figure*",
                                            "children": [
                                                {
                                                    "leaf id": 33,
                                                    "key": "doc/body/sec2/sub1/ssb0/figure*5/cpt0",
                                                    "block type": "cpt",
                                                    "content": "Comparison of the real and the synthetic datasets on individual attribute distribution. From the left to the right we have a comparison between BalancedFace and CPD25, BalancedFace and SynGAN, GlobalFace and CPD25, and GlobalFace and SynGAN. The first seven entries of each plot represent the most similar attribute distributions, whereas the seven bottom attributes represent the less similar distributions.",
                                                    "leftover": "Comparison of the real and the synthetic datasets on individual attribute distribution. From the left to the right we have a comparison between BalancedFace and CPD25, BalancedFace and SynGAN, GlobalFace and CPD25, and GlobalFace and SynGAN. The first seven entries of each plot represent the most similar attribute distributions, whereas the seven bottom attributes represent the less similar distributions.",
                                                    "matches": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec2/sub1/ssb1",
                                    "block_type": "ssb",
                                    "children": [
                                        {
                                            "leaf id": 34,
                                            "key": "doc/body/sec2/sub1/ssb1/tit",
                                            "block type": "title",
                                            "content": "Comparison of the different datasets",
                                            "leftover": "Comparison of the different datasets",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 35,
                                            "key": "doc/body/sec2/sub1/ssb1/txl0",
                                            "block type": "txl",
                                            "content": "Comparing two datasets at the image level is a nontrivial task. Hence, dataset comparison is easier if done at the level of the latent space of the deep neural network. However, with regards to faces, it might not encapsulate the variety of attributes that might be present in an image, since two individuals with a very similar set of attributes will have some distance between them. Additionally, it is not direct to understand the ''differences'' in this space. However, in the MAAD attribute space, not only comparing different samples with clarity is easier, but individuals with similar characteristics share the space.",
                                            "leftover": "Comparing two datasets at the image level is a nontrivial task. Hence, dataset comparison is easier if done at the level of the latent space of the deep neural network. However, with regards to faces, it might not encapsulate the variety of attributes that might be present in an image, since two individuals with a very similar set of attributes will have some distance between them. Additionally, it is not direct to understand the ''differences'' in this space. However, in the MAAD attribute space, not only comparing different samples with clarity is easier, but individuals with similar characteristics share the space.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 36,
                                            "key": "doc/body/sec2/sub1/ssb1/txl1",
                                            "block type": "txl",
                                            "content": "Taking into consideration the advantages of this attributespace, we have devised several strategies to measure the discrepancy between real and synthetic data. One of the first approaches was to measure the relative frequency of ''Undefined'' predictions on each of the four datasets, which indicated very similar results on average. Afterwards, we focused on the comparison of individual attributes and how much different was the prevalence of positives and negatives for datasets of different sources. This brought some interesting perspectives on the distribution of each attribute.",
                                            "leftover": "Taking into consideration the advantages of this attributespace, we have devised several strategies to measure the discrepancy between real and synthetic data. One of the first approaches was to measure the relative frequency of ''Undefined'' predictions on each of the four datasets, which indicated very similar results on average. Afterwards, we focused on the comparison of individual attributes and how much different was the prevalence of positives and negatives for datasets of different sources. This brought some interesting perspectives on the distribution of each attribute.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 37,
                                            "key": "doc/body/sec2/sub1/ssb1/txl2",
                                            "block type": "txl",
                                            "content": "Additionally, and considering that learnt models might be useful to detect the distinct patterns of data sources, we attempted to measure the relative classifiability of ''Real vs. Synthetic''. This was done with two strategies: using a classifier; creating two clusters with KMeans and validating how many samples of each source fall within each of the learnt clusters.",
                                            "leftover": "Additionally, and considering that learnt models might be useful to detect the distinct patterns of data sources, we attempted to measure the relative classifiability of ''Real vs. Synthetic''. This was done with two strategies: using a classifier; creating two clusters with KMeans and validating how many samples of each source fall within each of the learnt clusters.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 38,
                                            "key": "doc/body/sec2/sub1/ssb1/txl3",
                                            "block type": "txl",
                                            "content": "Considering the fact that the individual analysis of the distribution of a single dataset is a poor metric, we propose to model the prediction of each dataset with a Kernel Density Estimation approach on the attribute space. This information takes into consideration the several configurations that each individual might take. Before computing the distribution we take the mode of all the attributesets of an identity. Finally, having learnt the distribution of each dataset, we can compute the Kullback–Leibler divergence (Eq.), on both sides, between real and synthetic datasets.",
                                            "leftover": "Considering the fact that the individual analysis of the distribution of a single dataset is a poor metric, we propose to model the prediction of each dataset with a Kernel Density Estimation approach on the attribute space. This information takes into consideration the several configurations that each individual might take. Before computing the distribution we take the mode of all the attributesets of an identity. Finally, having learnt the distribution of each dataset, we can compute the Kullback–Leibler divergence (Eq.), on both sides, between real and synthetic datasets.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 39,
                                            "key": "doc/body/sec2/sub1/ssb1/equation4",
                                            "block type": "equation",
                                            "content": "DKL(P||Q) = ∑x∈X P(x) log({P(x)Q(x))",
                                            "leftover": "DKL(P||Q) = ∑x∈X P(x) log({P(x)Q(x))",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 40,
                                            "key": "doc/body/sec2/sub1/ssb1/txl5",
                                            "block type": "txl",
                                            "content": "This distance function has the particularity of not being a metric, since it does not respect the symmetry property. Hence results differ if we swap P and Q. A possible interpretation of this distance is the quantification of the information lost when using Q to approximate the distribution P. This is particularly useful in our scenario as it can tell us the information lost when we approximate the distribution, at the attribute space, of real data using synthetic data. Ideally, this distance should be close to zero as the information contained in one distribution would be reflected in the other distribution.",
                                            "leftover": "This distance function has the particularity of not being a metric, since it does not respect the symmetry property. Hence results differ if we swap P and Q. A possible interpretation of this distance is the quantification of the information lost when using Q to approximate the distribution P. This is particularly useful in our scenario as it can tell us the information lost when we approximate the distribution, at the attribute space, of real data using synthetic data. Ideally, this distance should be close to zero as the information contained in one distribution would be reflected in the other distribution.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 41,
                                    "key": "doc/body/sec2/sub2/tit",
                                    "block type": "title",
                                    "content": "Experimental Setup",
                                    "leftover": "Experimental Setup",
                                    "matches": []
                                },
                                {
                                    "leaf id": 42,
                                    "key": "doc/body/sec2/sub2/txl0",
                                    "block type": "txl",
                                    "content": "All the annotation experiments were conducted in a GPU cluster, leveraging a NVIDIA A100 GPU with 80GB of VRAM. The batch size for the inference was set at 1024, and we have conducted several tests to find the optimal batch size for our configuration. Additionally, we have measured the impact of having different batch sizes on the predictions and did not find statistically significant differences between different runs. We separated the face template extraction and the attribute computation steps so that it could be possible to create batches on the latter stage without affecting the approximation to a Gaussian Process.",
                                    "leftover": "All the annotation experiments were conducted in a GPU cluster, leveraging a NVIDIA A100 GPU with 80GB of VRAM. The batch size for the inference was set at 1024, and we have conducted several tests to find the optimal batch size for our configuration. Additionally, we have measured the impact of having different batch sizes on the predictions and did not find statistically significant differences between different runs. We separated the face template extraction and the attribute computation steps so that it could be possible to create batches on the latter stage without affecting the approximation to a Gaussian Process.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 43,
                                    "key": "doc/body/sec2/sub2/txl1",
                                    "block type": "txl",
                                    "content": "The remaining experiments were conducted in a consumer grade laptop without GPU. Annotations were saved for later use and release.",
                                    "leftover": "The remaining experiments were conducted in a consumer grade laptop without GPU. Annotations were saved for later use and release.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 44,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Results",
                            "leftover": "Results",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/ssb0",
                            "block_type": "ssb",
                            "children": [
                                {
                                    "leaf id": 45,
                                    "key": "doc/body/sec3/ssb0/tit",
                                    "block type": "title",
                                    "content": "A study on the attributes",
                                    "leftover": "A study on the attributes",
                                    "matches": []
                                },
                                {
                                    "leaf id": 46,
                                    "key": "doc/body/sec3/ssb0/txl0",
                                    "block type": "txl",
                                    "content": "Following the previously described methodology, we aimed to understand how the different datasets behaved with respect to their attributes. In the initial stage, we aimed to understand the attributes of each dataset individually, hence, for the different combinations of real datasets with synthetic datasets we calculated the seven most similar attribute distributions and the seven most nonsimilar.",
                                    "leftover": "Following the previously described methodology, we aimed to understand how the different datasets behaved with respect to their attributes. In the initial stage, we aimed to understand the attributes of each dataset individually, hence, for the different combinations of real datasets with synthetic datasets we calculated the seven most similar attribute distributions and the seven most nonsimilar.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/ssb0/figure1",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "leaf id": 47,
                                            "key": "doc/body/sec3/ssb0/figure1/cpt0",
                                            "block type": "cpt",
                                            "content": "Distributions Prop0d1^(a)Prop0d2^(a), where Prop0d'^(a') represents the proportion of the Undefined prediction of MAC for attribute a' in dataset d'. The title of each plot represents the d2 set, and the label of each KDE plot represents the d1 set.",
                                            "leftover": "Distributions Prop0d1^(a)Prop0d2^(a), where Prop0d'^(a') represents the proportion of the Undefined prediction of MAC for attribute a' in dataset d'. The title of each plot represents the d2 set, and the label of each KDE plot represents the d1 set.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 48,
                                    "key": "doc/body/sec3/ssb0/txl2",
                                    "block type": "txl",
                                    "content": "Looking at Figure it is possible to observe the four different combinations. For each plot, the top seven attributes are the ones that are more in line when the two datasets are compared, whereas the bottom seven are the most dissimilar. Some attributes are frequently displayed in the dissimilar set: Square Face, Male, No Beard and Smiling. In the synthetic data, samples are generally more skewed towards being female, whereas in real data the opposite happens. Probably related to this prevalence, there are significantly fewer samples with beards on synthetic datasets. Although it seems a minor issue, the beard represents one of the most natural and common forms of face occlusions. One other relevant aspect is the inability to properly detect smiles on synthetic data, as the majority of the annotations are ''Undefined''. This might impact the variability of the samples as the emotions/face reactions are one of the elements that most affect the perception of a face. One additional relevant element is the presence of White as one of the most dissimilar attribute distributions when the real dataset is BalancedFace. While this dataset focuses on balancing the different skin tones, synthetic datasets contain mostly white individuals.",
                                    "leftover": "Looking at Figure it is possible to observe the four different combinations. For each plot, the top seven attributes are the ones that are more in line when the two datasets are compared, whereas the bottom seven are the most dissimilar. Some attributes are frequently displayed in the dissimilar set: Square Face, Male, No Beard and Smiling. In the synthetic data, samples are generally more skewed towards being female, whereas in real data the opposite happens. Probably related to this prevalence, there are significantly fewer samples with beards on synthetic datasets. Although it seems a minor issue, the beard represents one of the most natural and common forms of face occlusions. One other relevant aspect is the inability to properly detect smiles on synthetic data, as the majority of the annotations are ''Undefined''. This might impact the variability of the samples as the emotions/face reactions are one of the elements that most affect the perception of a face. One additional relevant element is the presence of White as one of the most dissimilar attribute distributions when the real dataset is BalancedFace. While this dataset focuses on balancing the different skin tones, synthetic datasets contain mostly white individuals.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 49,
                                    "key": "doc/body/sec3/ssb0/txl3",
                                    "block type": "txl",
                                    "content": "On the other hand, a few attributes are consistently displayed as ones with the most similar distribution. Agerelated attributes such as Senior, Young and Middleaged are considerably similar in both types of datasets. Although this seems to be a good indicator, we have not measured if the age distribution within each identity is similar. For instance, we have 100 images uniformly spread between 18 and 70 years old, but having only one age group within each identity. Or we could have images of different ages within each identity. It is also visible that on synthetic datasets it is extremely rare to have a sample face wearing heavy makeup.",
                                    "leftover": "On the other hand, a few attributes are consistently displayed as ones with the most similar distribution. Agerelated attributes such as Senior, Young and Middleaged are considerably similar in both types of datasets. Although this seems to be a good indicator, we have not measured if the age distribution within each identity is similar. For instance, we have 100 images uniformly spread between 18 and 70 years old, but having only one age group within each identity. Or we could have images of different ages within each identity. It is also visible that on synthetic datasets it is extremely rare to have a sample face wearing heavy makeup.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/ssb1",
                            "block_type": "ssb",
                            "children": [
                                {
                                    "leaf id": 50,
                                    "key": "doc/body/sec3/ssb1/tit",
                                    "block type": "title",
                                    "content": "Dispersion of Undefined Samples",
                                    "leftover": "Dispersion of Undefined Samples",
                                    "matches": []
                                },
                                {
                                    "leaf id": 51,
                                    "key": "doc/body/sec3/ssb1/txl0",
                                    "block type": "txl",
                                    "content": "Another relevant question we can raise when discussing the labeling of the synthetic data with MAC is whether we get an abnormal amount of Undefined predictions. Abnormal behavior of this outcome could indicate underlying problems of the labeling method, and its adequacy when applied to our synthetic datasets. If we noticed that in general there is a higher amount of this label across all attributes in the synthetic datasets, we could infer that the MAC is having trouble defining meaningful predictions for each class, either because the synthetic data has a poor representation of that attribute, or it is not represented.",
                                    "leftover": "Another relevant question we can raise when discussing the labeling of the synthetic data with MAC is whether we get an abnormal amount of Undefined predictions. Abnormal behavior of this outcome could indicate underlying problems of the labeling method, and its adequacy when applied to our synthetic datasets. If we noticed that in general there is a higher amount of this label across all attributes in the synthetic datasets, we could infer that the MAC is having trouble defining meaningful predictions for each class, either because the synthetic data has a poor representation of that attribute, or it is not represented.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 52,
                                    "key": "doc/body/sec3/ssb1/txl1",
                                    "block type": "txl",
                                    "content": "To verify underlying trends in the Undefined prediction, we analyse the differences between the proportions of this label, given two datasets, for each attribute. We take notice of changes in the behavior of these distributions when analysing RealReal or SyntheticReal differences. The results, Figure, show no significant difference in the mean of these differences, meaning we do not observe generalised trends for Undefined labeling across all attributes. What we can verify is the difference in the dispersion of these differences, meaning that some attributes have more extreme fluctuations. We also noted that the attributes that have the highest decrease in the proportion of Undefined when compared to a Real dataset were consistently attributes related to facial hair (such as 'oclockshadow', 'nobeard', 'sideburns', 'goatee' ). This is consistent with a balancing of the gender attribute in the synthetic dataset, since prediction for these attributes tends to be easier in the 'female' class. On the other hand attributes that consistently have an increase in unpredictability include 'smiling' and accessory use such as 'wearing lipstick' and 'wearing earrings'. Emotions and their expressions might be particularly difficult to model with a generative system, as well as artifacts such as accessories. It might be the case that since these share no link to the identity information, they might trigger the generative model to ignore or smooth them.",
                                    "leftover": "To verify underlying trends in the Undefined prediction, we analyse the differences between the proportions of this label, given two datasets, for each attribute. We take notice of changes in the behavior of these distributions when analysing RealReal or SyntheticReal differences. The results, Figure, show no significant difference in the mean of these differences, meaning we do not observe generalised trends for Undefined labeling across all attributes. What we can verify is the difference in the dispersion of these differences, meaning that some attributes have more extreme fluctuations. We also noted that the attributes that have the highest decrease in the proportion of Undefined when compared to a Real dataset were consistently attributes related to facial hair (such as 'oclockshadow', 'nobeard', 'sideburns', 'goatee' ). This is consistent with a balancing of the gender attribute in the synthetic dataset, since prediction for these attributes tends to be easier in the 'female' class. On the other hand attributes that consistently have an increase in unpredictability include 'smiling' and accessory use such as 'wearing lipstick' and 'wearing earrings'. Emotions and their expressions might be particularly difficult to model with a generative system, as well as artifacts such as accessories. It might be the case that since these share no link to the identity information, they might trigger the generative model to ignore or smooth them.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/ssb1/figure2",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "leaf id": 53,
                                            "key": "doc/body/sec3/ssb1/figure2/cpt0",
                                            "block type": "cpt",
                                            "content": "Cluster aggregation of the different identities on each of three datasets (BalancedFace, GlobalFace and CPD25). Clusters calculated with KMeans. SynGAN was removed from the comparison, since it consists of 500k images of distinct identities. Synthetic data has higher presence in cluster 2 (>60) and lower in cluster 3 (<10).",
                                            "leftover": "Cluster aggregation of the different identities on each of three datasets (BalancedFace, GlobalFace and CPD25). Clusters calculated with KMeans. SynGAN was removed from the comparison, since it consists of 500k images of distinct identities. Synthetic data has higher presence in cluster 2 (>60) and lower in cluster 3 (<10).",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/ssb2",
                            "block_type": "ssb",
                            "children": [
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec3/ssb2/tit",
                                    "block type": "title",
                                    "content": "Clustering Synthetic vs. Real",
                                    "leftover": "Clustering Synthetic vs. Real",
                                    "matches": []
                                },
                                {
                                    "leaf id": 55,
                                    "key": "doc/body/sec3/ssb2/txl0",
                                    "block type": "txl",
                                    "content": "In addition to the comparison of the different attributes with respect to their real and synthetic distribution, we have attempted to fit a kmeans (k=3) model to three out of the four datasets. Figure shows the distribution of the samples of each dataset within one of the three clusters. As expected, BalancedFace and GlobalFace share very similar distributions across clusters. Surprisingly, the presence of CPD25 in the third cluster is rather small, and the dataset is heavily inserted in the second cluster. This evidence already highlights some potential differences between the attribute space of synthetic and real samples. Especially if we consider that even on the most prevalent cluster for synthetic data, real data is significantly present, whereas the opposite is not verifiable.",
                                    "leftover": "In addition to the comparison of the different attributes with respect to their real and synthetic distribution, we have attempted to fit a kmeans (k=3) model to three out of the four datasets. Figure shows the distribution of the samples of each dataset within one of the three clusters. As expected, BalancedFace and GlobalFace share very similar distributions across clusters. Surprisingly, the presence of CPD25 in the third cluster is rather small, and the dataset is heavily inserted in the second cluster. This evidence already highlights some potential differences between the attribute space of synthetic and real samples. Especially if we consider that even on the most prevalent cluster for synthetic data, real data is significantly present, whereas the opposite is not verifiable.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/ssb2/table1",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/ssb2/table1/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 56,
                                                    "key": "doc/body/sec3/ssb2/table1/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "KL divergence between all the distribution learnt by KDE for all four datasets. Interesting to denote the lower information loss when real data is used to approximate synthetic data in comparison with the higher information loss when we swap P and Q.",
                                                    "leftover": "KL divergence between all the distribution learnt by KDE for all four datasets. Interesting to denote the lower information loss when real data is used to approximate synthetic data in comparison with the higher information loss when we swap P and Q.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 57,
                                            "key": "doc/body/sec3/ssb2/table1/tabular1",
                                            "block type": "tabular",
                                            "content": "{|c|c|c|c|c|} \\diagboxPQ & GlobalFace & BalancedFace & SynGAN & CPD25 GlobalFace &  & 0.708& 3.007 & 2.223 BalancedFace & 0.409 &  & 1.984 & 1.223 SynGAN & 1.010 & 0.613 &  & 0.249 CPD25 & 0.507 & 0.247 & 0.371 & ",
                                            "leftover": "{|c|c|c|c|c|} \\diagboxPQ & GlobalFace & BalancedFace & SynGAN & CPD25 GlobalFace &  & 0.708& 3.007 & 2.223 BalancedFace & 0.409 &  & 1.984 & 1.223 SynGAN & 1.010 & 0.613 &  & 0.249 CPD25 & 0.507 & 0.247 & 0.371 & ",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/ssb3",
                            "block_type": "ssb",
                            "children": [
                                {
                                    "leaf id": 58,
                                    "key": "doc/body/sec3/ssb3/tit",
                                    "block type": "title",
                                    "content": "Synthetic vs. Real Divergence",
                                    "leftover": "Synthetic vs. Real Divergence",
                                    "matches": []
                                },
                                {
                                    "leaf id": 59,
                                    "key": "doc/body/sec3/ssb3/txl0",
                                    "block type": "txl",
                                    "content": "Following the potential difference in the distribution of the attribute set of real and synthetic datasets, we have used a KDE model to estimate and approximate the distribution for each one of the four datasets. Afterwards, as seen in Table, we have measured the Kullback–Leibler divergence for all combinations of datasets. When P is set to the distribution of GlobalFace, we noticed that both SynGAN and CPD25 as Q lead to significant information loss. On the other hand, BalancedFace as Q allows for diminished information loss. Setting BalancedFace as P leads to very similar results with a synthetic Q, but considerably better than the previous comparison. On the other hand, GlobalFace is quite accurate at approximating the other real datasets. Both synthetic datasets can be easily explained by the other datasets, leading to very small values of information loss. This discrepancy in the KL values highlights the lack of diversity shown in synthetic datasets for face recognition and how they must improve to replace real data.",
                                    "leftover": "Following the potential difference in the distribution of the attribute set of real and synthetic datasets, we have used a KDE model to estimate and approximate the distribution for each one of the four datasets. Afterwards, as seen in Table, we have measured the Kullback–Leibler divergence for all combinations of datasets. When P is set to the distribution of GlobalFace, we noticed that both SynGAN and CPD25 as Q lead to significant information loss. On the other hand, BalancedFace as Q allows for diminished information loss. Setting BalancedFace as P leads to very similar results with a synthetic Q, but considerably better than the previous comparison. On the other hand, GlobalFace is quite accurate at approximating the other real datasets. Both synthetic datasets can be easily explained by the other datasets, leading to very small values of information loss. This discrepancy in the KL values highlights the lack of diversity shown in synthetic datasets for face recognition and how they must improve to replace real data.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 60,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Conclusion",
                            "leftover": "Conclusion",
                            "matches": []
                        },
                        {
                            "leaf id": 61,
                            "key": "doc/body/sec4/txl0",
                            "block type": "txl",
                            "content": "Throughout this paper we have covered several strategies to uncover the reason behind the inferior performance of models trained on synthetic data when compared to models trained on real data. Acknowledging the difficulties of comparing these datasets at image level, we have proposed to use MAC to create massive annotations for each of four datasets. This process allowed for a few studies on the diversity and gap between real and synthetic datasets.",
                            "leftover": "Throughout this paper we have covered several strategies to uncover the reason behind the inferior performance of models trained on synthetic data when compared to models trained on real data. Acknowledging the difficulties of comparing these datasets at image level, we have proposed to use MAC to create massive annotations for each of four datasets. This process allowed for a few studies on the diversity and gap between real and synthetic datasets.",
                            "matches": []
                        },
                        {
                            "leaf id": 62,
                            "key": "doc/body/sec4/txl1",
                            "block type": "txl",
                            "content": "Leveraging the annotations, it was possible to immediately inspect some attributes and their difference in distribution across all datasets. It was also possible to measure the undefined dispersion on synthetic datasets and uncovering that attributes such as smiling are difficult to measure on synthetic data, as shown by the quantity of undefined samples for this attribute.",
                            "leftover": "Leveraging the annotations, it was possible to immediately inspect some attributes and their difference in distribution across all datasets. It was also possible to measure the undefined dispersion on synthetic datasets and uncovering that attributes such as smiling are difficult to measure on synthetic data, as shown by the quantity of undefined samples for this attribute.",
                            "matches": []
                        },
                        {
                            "leaf id": 63,
                            "key": "doc/body/sec4/txl2",
                            "block type": "txl",
                            "content": "Considering the attribute set as a whole combination of attributes, it was possible to place the samples of each dataset on one of two cluster and extract hints regarding the lower variability of synthetic data. Additionally, after modelling this distributions, we manage to user the Kullback–Leibler divergence to measure the information difference between the four datasets. As expected, synthetic datasets shown a poor capability to approximate real data.",
                            "leftover": "Considering the attribute set as a whole combination of attributes, it was possible to place the samples of each dataset on one of two cluster and extract hints regarding the lower variability of synthetic data. Additionally, after modelling this distributions, we manage to user the Kullback–Leibler divergence to measure the information difference between the four datasets. As expected, synthetic datasets shown a poor capability to approximate real data.",
                            "matches": []
                        },
                        {
                            "leaf id": 64,
                            "key": "doc/body/sec4/txl3",
                            "block type": "txl",
                            "content": "In summary, we have not yet find a clear answer to the reason behind the performance difference of models trained on these datasets. Yet, we have made contributions on the gaps between both types of datasets and we have released the annotations. Future researchers can leverage these annotations to further condition diffusion models, to find correlations between a set of attributes and the performance, or to build better automatic annotation tools. There are several directions in which this research might lead.",
                            "leftover": "In summary, we have not yet find a clear answer to the reason behind the performance difference of models trained on these datasets. Yet, we have made contributions on the gaps between both types of datasets and we have released the annotations. Future researchers can leverage these annotations to further condition diffusion models, to find correlations between a set of attributes and the performance, or to build better automatic annotation tools. There are several directions in which this research might lead.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 65,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Acknowledgments",
                            "leftover": "Acknowledgments",
                            "matches": []
                        },
                        {
                            "leaf id": 66,
                            "key": "doc/body/sec5/txl0",
                            "block type": "txl",
                            "content": "This work was financed by National Funds through the Portuguese funding agency, FCT  Fundação para a Ciência e a Tecnologia, within the PhD grants with the references ''2020.06434.BD'' and ''2021.06872.BD'', and within project UIDB/50014/2020. DOI 10.54499/UIDB/50014/2020 | https://doi.org/10.54499/uidb/50014/2020.",
                            "leftover": "This work was financed by National Funds through the Portuguese funding agency, FCT  Fundação para a Ciência e a Tecnologia, within the PhD grants with the references ''2020.06434.BD'' and ''2021.06872.BD'', and within project UIDB/50014/2020. DOI 10.54499/UIDB/50014/2020 | https://doi.org/10.54499/uidb/50014/2020.",
                            "matches": []
                        },
                        {
                            "leaf id": 67,
                            "key": "doc/body/sec5/txl1",
                            "block type": "txl",
                            "content": "{\\small \\bibliographystyleieee \\bibliographyegbib",
                            "leftover": "{\\small \\bibliographystyleieee \\bibliographyegbib",
                            "matches": []
                        },
                        {
                            "leaf id": 68,
                            "key": "doc/body/sec5/txl2",
                            "block type": "txl",
                            "content": "}",
                            "leftover": "}",
                            "matches": []
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 69,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "J.B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds, et~al. Flamingo: a visual language model for fewshot learning. {\\em Advances in neural information processing systems}, 35:2371623736, 2022.",
            "leftover": "J.B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds, et~al. Flamingo: a visual language model for fewshot learning. {\\em Advances in neural information processing systems}, 35:2371623736, 2022.",
            "matches": []
        },
        {
            "leaf id": 70,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "X.~An, X.~Zhu, Y.~Gao, Y.~Xiao, Y.~Zhao, Z.~Feng, L.~Wu, B.~Qin, M.~Zhang, D.~Zhang, et~al. Partial fc: Training 10 million identities on a single machine. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 14451449, 2021.",
            "leftover": "X.~An, X.~Zhu, Y.~Gao, Y.~Xiao, Y.~Zhao, Z.~Feng, L.~Wu, B.~Qin, M.~Zhang, D.~Zhang, et~al. Partial fc: Training 10 million identities on a single machine. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 14451449, 2021.",
            "matches": []
        },
        {
            "leaf id": 71,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "G.~Bae, M.~de~La~Gorce, T.~Baltru{\\vs}aitis, C.~Hewitt, D.~Chen, J.~Valentin, R.~Cipolla, and J.~Shen. Digiface1m: 1 million digital face images for face recognition. In {\\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 35263535, 2023.",
            "leftover": "G.~Bae, M.~de~La~Gorce, T.~Baltru{\\vs}aitis, C.~Hewitt, D.~Chen, J.~Valentin, R.~Cipolla, and J.~Shen. Digiface1m: 1 million digital face images for face recognition. In {\\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 35263535, 2023.",
            "matches": []
        },
        {
            "leaf id": 72,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "F.~Boutros, N.~Damer, and A.~Kuijper. Quantface: Towards lightweight face recognition by synthetic data lowbit quantization. In {\\em 2022 26th International Conference on Pattern Recognition (ICPR)}, pages 855862. IEEE, 2022.",
            "leftover": "F.~Boutros, N.~Damer, and A.~Kuijper. Quantface: Towards lightweight face recognition by synthetic data lowbit quantization. In {\\em 2022 26th International Conference on Pattern Recognition (ICPR)}, pages 855862. IEEE, 2022.",
            "matches": []
        },
        {
            "leaf id": 73,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "F.~Boutros, J.~H. Grebe, A.~Kuijper, and N.~Damer. Idiffface: Syntheticbased face recognition through fizzy identityconditioned diffusion model. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1965019661, 2023.",
            "leftover": "F.~Boutros, J.~H. Grebe, A.~Kuijper, and N.~Damer. Idiffface: Syntheticbased face recognition through fizzy identityconditioned diffusion model. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1965019661, 2023.",
            "matches": []
        },
        {
            "leaf id": 74,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "F.~Boutros, M.~Huber, A.~T. Luu, P.~Siebke, and N.~Damer. Sface2: Syntheticbased face recognition with wspace identitydriven sampling. {\\em IEEE Transactions on Biometrics, Behavior, and Identity Science}, 2024.",
            "leftover": "F.~Boutros, M.~Huber, A.~T. Luu, P.~Siebke, and N.~Damer. Sface2: Syntheticbased face recognition with wspace identitydriven sampling. {\\em IEEE Transactions on Biometrics, Behavior, and Identity Science}, 2024.",
            "matches": []
        },
        {
            "leaf id": 75,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "F.~Boutros, M.~Huber, P.~Siebke, T.~Rieber, and N.~Damer. Sface: Privacyfriendly and accurate face recognition using synthetic data. In {\\em 2022 IEEE International Joint Conference on Biometrics (IJCB)}, pages 111. IEEE, 2022.",
            "leftover": "F.~Boutros, M.~Huber, P.~Siebke, T.~Rieber, and N.~Damer. Sface: Privacyfriendly and accurate face recognition using synthetic data. In {\\em 2022 IEEE International Joint Conference on Biometrics (IJCB)}, pages 111. IEEE, 2022.",
            "matches": []
        },
        {
            "leaf id": 76,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "F.~Boutros, M.~Klemt, M.~Fang, A.~Kuijper, and N.~Damer. Unsupervised face recognition using unlabeled synthetic data. In {\\em 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)}, pages 18. IEEE, 2023.",
            "leftover": "F.~Boutros, M.~Klemt, M.~Fang, A.~Kuijper, and N.~Damer. Unsupervised face recognition using unlabeled synthetic data. In {\\em 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)}, pages 18. IEEE, 2023.",
            "matches": []
        },
        {
            "leaf id": 77,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Q.~Cao, L.~Shen, W.~Xie, O.~M. Parkhi, and A.~Zisserman. Vggface2: A dataset for recognising faces across pose and age. In {\\em 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)}, pages 6774. IEEE, 2018.",
            "leftover": "Q.~Cao, L.~Shen, W.~Xie, O.~M. Parkhi, and A.~Zisserman. Vggface2: A dataset for recognising faces across pose and age. In {\\em 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)}, pages 6774. IEEE, 2018.",
            "matches": []
        },
        {
            "leaf id": 78,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "R.~A. Davis, K.S. Lii, and D.~N. Politis. Remarks on some nonparametric estimates of a density function. {\\em Selected Works of Murray Rosenblatt}, pages 95100, 2011.",
            "leftover": "R.~A. Davis, K.S. Lii, and D.~N. Politis. Remarks on some nonparametric estimates of a density function. {\\em Selected Works of Murray Rosenblatt}, pages 95100, 2011.",
            "matches": []
        },
        {
            "leaf id": 79,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "I.~DeAndresTame, R.~Tolosana, P.~Melzi, R.~VeraRodriguez, M.~Kim, C.~Rathgeb, X.~Liu, A.~Morales, J.~Fierrez, J.~OrtegaGarcia, et~al. Frcsyn challenge at cvpr 2024: Face recognition challenge in the era of synthetic data. {\\em arXiv preprint arXiv:2404.10378}, 2024.",
            "leftover": "I.~DeAndresTame, R.~Tolosana, P.~Melzi, R.~VeraRodriguez, M.~Kim, C.~Rathgeb, X.~Liu, A.~Morales, J.~Fierrez, J.~OrtegaGarcia, et~al. Frcsyn challenge at cvpr 2024: Face recognition challenge in the era of synthetic data. {\\em arXiv preprint arXiv:2404.10378}, 2024.",
            "matches": []
        },
        {
            "leaf id": 80,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "{European Parliament} and {Council of the European Union}. Regulation (EU) 2016/679 of the European Parliament and of the Council.",
            "leftover": "{European Parliament} and {Council of the European Union}. Regulation (EU) 2016/679 of the European Parliament and of the Council.",
            "matches": []
        },
        {
            "leaf id": 81,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "E.~GonzalezSosa, J.~Fierrez, R.~VeraRodriguez, and F.~AlonsoFernandez. Facial Soft Biometrics for Recognition in the Wild: Recent Works, Annotation, and COTS Evaluation. {\\em IEEE Transactions on Information Forensics and Security}, 13(8):20012014, 2018.",
            "leftover": "E.~GonzalezSosa, J.~Fierrez, R.~VeraRodriguez, and F.~AlonsoFernandez. Facial Soft Biometrics for Recognition in the Wild: Recent Works, Annotation, and COTS Evaluation. {\\em IEEE Transactions on Information Forensics and Security}, 13(8):20012014, 2018.",
            "matches": []
        },
        {
            "leaf id": 82,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "I.~Goodfellow, J.~PougetAbadie, M.~Mirza, B.~Xu, D.~WardeFarley, S.~Ozair, A.~Courville, and Y.~Bengio. Generative adversarial nets. {\\em Advances in neural information processing systems}, 27, 2014.",
            "leftover": "I.~Goodfellow, J.~PougetAbadie, M.~Mirza, B.~Xu, D.~WardeFarley, S.~Ozair, A.~Courville, and Y.~Bengio. Generative adversarial nets. {\\em Advances in neural information processing systems}, 27, 2014.",
            "matches": []
        },
        {
            "leaf id": 83,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "M.~Huber, A.~T. Luu, F.~Boutros, A.~Kuijper, and N.~Damer. Bias and diversity in syntheticbased face recognition. In {\\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 62156226, 2024.",
            "leftover": "M.~Huber, A.~T. Luu, F.~Boutros, A.~Kuijper, and N.~Damer. Bias and diversity in syntheticbased face recognition. In {\\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 62156226, 2024.",
            "matches": []
        },
        {
            "leaf id": 84,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "K.~Karkkainen and J.~Joo. Fairface: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation. In {\\em 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}. IEEE, 2021.",
            "leftover": "K.~Karkkainen and J.~Joo. Fairface: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation. In {\\em 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}. IEEE, 2021.",
            "matches": []
        },
        {
            "leaf id": 85,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "T.~Karras, T.~Aila, S.~Laine, and J.~Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In {\\em 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30  May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.",
            "leftover": "T.~Karras, T.~Aila, S.~Laine, and J.~Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In {\\em 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30  May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.",
            "matches": []
        },
        {
            "leaf id": 86,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "M.~Kim, F.~Liu, A.~Jain, and X.~Liu. Dcface: Synthetic face generation with dual condition diffusion model. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1271512725, 2023.",
            "leftover": "M.~Kim, F.~Liu, A.~Jain, and X.~Liu. Dcface: Synthetic face generation with dual condition diffusion model. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1271512725, 2023.",
            "matches": []
        },
        {
            "leaf id": 87,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "J.~N. Kolf, T.~Rieber, J.~Elliesen, F.~Boutros, A.~Kuijper, and N.~Damer. Identitydriven threeplayer generative adversarial network for syntheticbased face recognition. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 806816, 2023.",
            "leftover": "J.~N. Kolf, T.~Rieber, J.~Elliesen, F.~Boutros, A.~Kuijper, and N.~Damer. Identitydriven threeplayer generative adversarial network for syntheticbased face recognition. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 806816, 2023.",
            "matches": []
        },
        {
            "leaf id": 88,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "S.~Kullback and R.~A. Leibler. On information and sufficiency. {\\em The annals of mathematical statistics}, 22(1):7986, 1951.",
            "leftover": "S.~Kullback and R.~A. Leibler. On information and sufficiency. {\\em The annals of mathematical statistics}, 22(1):7986, 1951.",
            "matches": []
        },
        {
            "leaf id": 89,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "S.~Lee, E.~Park, H.~Yi, and S.~H. Lee. Strdan: Synthetictoreal domain adaptation network for vehicle reidentification. In {\\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 1419, 2020}, pages 25902597. Computer Vision Foundation / IEEE, 2020.",
            "leftover": "S.~Lee, E.~Park, H.~Yi, and S.~H. Lee. Strdan: Synthetictoreal domain adaptation network for vehicle reidentification. In {\\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 1419, 2020}, pages 25902597. Computer Vision Foundation / IEEE, 2020.",
            "matches": []
        },
        {
            "leaf id": 90,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "J.~MacQueen et~al. Some methods for classification and analysis of multivariate observations. In {\\em Proceedings of the fifth Berkeley symposium on mathematical statistics and probability}, number~14 in 1, pages 281297. Oakland, CA, USA, 1967.",
            "leftover": "J.~MacQueen et~al. Some methods for classification and analysis of multivariate observations. In {\\em Proceedings of the fifth Berkeley symposium on mathematical statistics and probability}, number~14 in 1, pages 281297. Oakland, CA, USA, 1967.",
            "matches": []
        },
        {
            "leaf id": 91,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "B.~Meden, P.~Rot, P.~Terh{''o}rst, N.~Damer, A.~Kuijper, W.~J. Scheirer, A.~Ross, P.~Peer, and V.~{\\vS}truc. Privacyenhancing face biometrics: A comprehensive survey. {\\em IEEE Transactions on Information Forensics and Security}, 16:41474183, 2021.",
            "leftover": "B.~Meden, P.~Rot, P.~Terh{''o}rst, N.~Damer, A.~Kuijper, W.~J. Scheirer, A.~Ross, P.~Peer, and V.~{\\vS}truc. Privacyenhancing face biometrics: A comprehensive survey. {\\em IEEE Transactions on Information Forensics and Security}, 16:41474183, 2021.",
            "matches": []
        },
        {
            "leaf id": 92,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "M.~Merler, N.~K. Ratha, R.~S. Feris, and J.~R. Smith. Diversity in Faces. {\\em ArXiv}, abs/1901.10436, 2019.",
            "leftover": "M.~Merler, N.~K. Ratha, R.~S. Feris, and J.~R. Smith. Diversity in Faces. {\\em ArXiv}, abs/1901.10436, 2019.",
            "matches": []
        },
        {
            "leaf id": 93,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "N.~MirabetHerranz and J.L. Dugelay. Lvt face database: A benchmark database for visible and hidden face biometrics. In {\\em 2023 International Conference of the Biometrics Special Interest Group (BIOSIG)}, pages 16. IEEE, 2023.",
            "leftover": "N.~MirabetHerranz and J.L. Dugelay. Lvt face database: A benchmark database for visible and hidden face biometrics. In {\\em 2023 International Conference of the Biometrics Special Interest Group (BIOSIG)}, pages 16. IEEE, 2023.",
            "matches": []
        },
        {
            "leaf id": 94,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "A.~Nech and I.~Kemelmacher{}Shlizerman. Level playing field for million scale face recognition. In {\\em 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 2126, 2017}, pages 34063415. IEEE Computer Society, 2017.",
            "leftover": "A.~Nech and I.~Kemelmacher{}Shlizerman. Level playing field for million scale face recognition. In {\\em 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 2126, 2017}, pages 34063415. IEEE Computer Society, 2017.",
            "matches": []
        },
        {
            "leaf id": 95,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "P.~C. Neto, E.~Caldeira, J.~S. Cardoso, and A.~F. Sequeira. Compressed models decompress race biases: What quantized models forget for fair face recognition. In {\\em 2023 International Conference of the Biometrics Special Interest Group (BIOSIG)}, pages 15. IEEE, 2023.",
            "leftover": "P.~C. Neto, E.~Caldeira, J.~S. Cardoso, and A.~F. Sequeira. Compressed models decompress race biases: What quantized models forget for fair face recognition. In {\\em 2023 International Conference of the Biometrics Special Interest Group (BIOSIG)}, pages 15. IEEE, 2023.",
            "matches": []
        },
        {
            "leaf id": 96,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "E.~Parzen. On estimation of a probability density function and mode. {\\em The annals of mathematical statistics}, 33(3):10651076, 1962.",
            "leftover": "E.~Parzen. On estimation of a probability density function and mode. {\\em The annals of mathematical statistics}, 33(3):10651076, 1962.",
            "matches": []
        },
        {
            "leaf id": 97,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "H.~Qiu, B.~Yu, D.~Gong, Z.~Li, W.~Liu, and D.~Tao. Synface: Face recognition with synthetic data. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1088010890, 2021.",
            "leftover": "H.~Qiu, B.~Yu, D.~Gong, Z.~Li, W.~Liu, and D.~Tao. Synface: Face recognition with synthetic data. In {\\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1088010890, 2021.",
            "matches": []
        },
        {
            "leaf id": 98,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "S.~Sankaranarayanan, Y.~Balaji, A.~Jain, S.~Lim, and R.~Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. In {\\em 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 1822, 2018}, pages 37523761. Computer Vision Foundation / IEEE Computer Society, 2018.",
            "leftover": "S.~Sankaranarayanan, Y.~Balaji, A.~Jain, S.~Lim, and R.~Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. In {\\em 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 1822, 2018}, pages 37523761. Computer Vision Foundation / IEEE Computer Society, 2018.",
            "matches": []
        },
        {
            "leaf id": 99,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "F.~Schroff, D.~Kalenichenko, and J.~Philbin. Facenet: A unified embedding for face recognition and clustering. In {\\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 815823, 2015.",
            "leftover": "F.~Schroff, D.~Kalenichenko, and J.~Philbin. Facenet: A unified embedding for face recognition and clustering. In {\\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 815823, 2015.",
            "matches": []
        },
        {
            "leaf id": 100,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Y.~Taigman, M.~Yang, M.~Ranzato, and L.~Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In {\\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 17011708, 2014.",
            "leftover": "Y.~Taigman, M.~Yang, M.~Ranzato, and L.~Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In {\\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 17011708, 2014.",
            "matches": []
        },
        {
            "leaf id": 101,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "P.~Terh{'' o}rst, D.~F{'' a}hrmann, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. Beyond Identity: What Information Is Stored in Biometric Face Templates? In {\\em International Joint Conference on Biometrics (IJCB)}, pages 110, 2020.",
            "leftover": "P.~Terh{'' o}rst, D.~F{'' a}hrmann, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. Beyond Identity: What Information Is Stored in Biometric Face Templates? In {\\em International Joint Conference on Biometrics (IJCB)}, pages 110, 2020.",
            "matches": []
        },
        {
            "leaf id": 102,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "P.~Terhorst, D.~Fahrmann, J.~N. Kolf, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. MaadFace: A Massively Annotated Attribute Dataset for Face Images. {\\em IEEE Transactions on Information Forensics and Security}, 16:39423957, 2021.",
            "leftover": "P.~Terhorst, D.~Fahrmann, J.~N. Kolf, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. MaadFace: A Massively Annotated Attribute Dataset for Face Images. {\\em IEEE Transactions on Information Forensics and Security}, 16:39423957, 2021.",
            "matches": []
        },
        {
            "leaf id": 103,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "P.~Terhorst, M.~Huber, J.~N. Kolf, I.~Zelch, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. Reliable Age and Gender Estimation from Face Images: Stating the Confidence of Model Predictions. In {\\em 2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)}. IEEE, 2019.",
            "leftover": "P.~Terhorst, M.~Huber, J.~N. Kolf, I.~Zelch, N.~Damer, F.~Kirchbuchner, and A.~Kuijper. Reliable Age and Gender Estimation from Face Images: Stating the Confidence of Model Predictions. In {\\em 2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)}. IEEE, 2019.",
            "matches": []
        },
        {
            "leaf id": 104,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "M.~Wang, Y.~Zhang, and W.~Deng. Meta balanced network for fair face recognition. {\\em IEEE transactions on pattern analysis and machine intelligence}, 44(11):84338448, 2021.",
            "leftover": "M.~Wang, Y.~Zhang, and W.~Deng. Meta balanced network for fair face recognition. {\\em IEEE transactions on pattern analysis and machine intelligence}, 44(11):84338448, 2021.",
            "matches": []
        },
        {
            "leaf id": 105,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "M.~Xu, J.~Zhang, B.~Ni, T.~Li, C.~Wang, Q.~Tian, and W.~Zhang. Adversarial domain adaptation with domain mixup. In {\\em The ThirtyFourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 712, 2020}, pages 65026509. AAAI Press, 2020.",
            "leftover": "M.~Xu, J.~Zhang, B.~Ni, T.~Li, C.~Wang, Q.~Tian, and W.~Zhang. Adversarial domain adaptation with domain mixup. In {\\em The ThirtyFourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 712, 2020}, pages 65026509. AAAI Press, 2020.",
            "matches": []
        },
        {
            "leaf id": 106,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Y.~Xu, P.~Terh{''o}rst, K.~Raja, and M.~Pedersen. A comprehensive analysis of ai biases in deepfake detection with massively annotated databases. {\\em arXiv preprint arXiv:2208.05845}, 2022.",
            "leftover": "Y.~Xu, P.~Terh{''o}rst, K.~Raja, and M.~Pedersen. A comprehensive analysis of ai biases in deepfake detection with massively annotated databases. {\\em arXiv preprint arXiv:2208.05845}, 2022.",
            "matches": []
        },
        {
            "leaf id": 107,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "D.~Yi, Z.~Lei, S.~Liao, and S.~Z. Li. Learning face representation from scratch. {\\em arXiv preprint arXiv:1411.7923}, 2014.",
            "leftover": "D.~Yi, Z.~Lei, S.~Liao, and S.~Z. Li. Learning face representation from scratch. {\\em arXiv preprint arXiv:1411.7923}, 2014.",
            "matches": []
        },
        {
            "leaf id": 108,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "K.~Zhang, Z.~Zhang, Z.~Li, and Y.~Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. {\\em IEEE signal processing letters}, 23(10):14991503, 2016.",
            "leftover": "K.~Zhang, Z.~Zhang, Z.~Li, and Y.~Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. {\\em IEEE signal processing letters}, 23(10):14991503, 2016.",
            "matches": []
        },
        {
            "leaf id": 109,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Z.~Zhu, G.~Huang, J.~Deng, Y.~Ye, J.~Huang, X.~Chen, J.~Zhu, T.~Yang, J.~Lu, D.~Du, et~al. Webface260m: A benchmark unveiling the power of millionscale deep face recognition. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1049210502, 2021.",
            "leftover": "Z.~Zhu, G.~Huang, J.~Deng, Y.~Ye, J.~Huang, X.~Chen, J.~Zhu, T.~Yang, J.~Lu, D.~Du, et~al. Webface260m: A benchmark unveiling the power of millionscale deep face recognition. In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1049210502, 2021.",
            "matches": []
        }
    ]
}