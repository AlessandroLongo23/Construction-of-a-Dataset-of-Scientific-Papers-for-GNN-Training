\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{barron2021mipnerf}
Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In: Proceedings of the International Conference on Computer Vision (ICCV) (2021)

\bibitem{bian2022nopenerf}
Bian, W., Wang, Z., Li, K., Bian, J., Prisacariu, V.A.: Nope-nerf: Optimising neural radiance field with no pose prior (2023)

\bibitem{bloesch2018codeslam}
Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., Davison, A.J.: Codeslam—learning a compact, optimisable representation for dense visual slam. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2560--2568 (2018)

\bibitem{bowen2022dimensions}
Bowen, R.S., Tucker, R., Zabih, R., Snavely, N.: Dimensions of motion: Monocular prediction through flow subspaces. In: Proceedings of the International Conference on 3D Vision (3DV). pp. 454--464. IEEE (2022)

\bibitem{campos_orb3}
Campos, C., Elvira, R., Rodríguez, J.J.G., M.~Montiel, J.M., D.~Tardós, J.: Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam. Transactions on Robotics (6),  1874--1890 (2021)

\bibitem{chan2023generative}
Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., De~Mello, S., Karras, T., Wetzstein, G.: Generative novel view synthesis with 3d-aware diffusion models. Proceedings of the International Conference on 3D Vision (3DV)  (2023)

\bibitem{charatan23pixelsplat}
Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2023)

\bibitem{Chen_2023_CVPR}
Chen, Y., Lee, G.H.: Dbarf: Deep bundle-adjusting generalizable neural radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 24--34 (June 2023)

\bibitem{cheng2023lu}
Cheng, Z., Esteves, C., Jampani, V., Kar, A., Maji, S., Makadia, A.: Lu-nerf: Scene and pose estimation by synchronizing local unposed nerfs. arXiv preprint arXiv:2306.05410  (2023)

\bibitem{chng2022garf}
Chng, S.F., Ramasinghe, S., Sherrah, J., Lucey, S.: Garf: gaussian activated radiance fields for high fidelity reconstruction and pose estimation. arXiv e-prints pp. arXiv--2204 (2022)

\bibitem{choy2020deep}
Choy, C., Dong, W., Koltun, V.: Deep global registration. In: Proc. CVPR (2020)

\bibitem{choy2016universal}
Choy, C.B., Gwak, J., Savarese, S., Chandraker, M.: Universal correspondence network. Advances in neural information processing systems  \textbf{29} (2016)

\bibitem{clark2018learning}
Clark, R., Bloesch, M., Czarnowski, J., Leutenegger, S., Davison, A.J.: Learning to solve nonlinear least squares for monocular stereo. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 284--299 (2018)

\bibitem{czarnowski2020deepfactors}
Czarnowski, J., Laidlow, T., Clark, R., Davison, A.J.: Deepfactors: Real-time probabilistic dense monocular {SLAM}. Computing Research Repository (CoRR)  (2020)

\bibitem{dsnerf}
Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised {NeRF}: Fewer views and faster training for free. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2022)

\bibitem{detone2018superpoint}
DeTone, D., Malisiewicz, T., Rabinovich, A.: Superpoint: Self-supervised interest point detection and description. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 224--236 (2018)

\bibitem{doersch2023tapir}
Doersch, C., Yang, Y., Vecerik, M., Gokay, D., Gupta, A., Aytar, Y., Carreira, J., Zisserman, A.: Tapir: Tracking any point with per-frame initialization and temporal refinement. arXiv preprint arXiv:2306.08637  (2023)

\bibitem{du2023cross}
Du, Y., Smith, C., Tewari, A., Sitzmann, V.: Learning to render novel views from wide-baseline stereo pairs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2023)

\bibitem{engel2017direct}
Engel, J., Koltun, V., Cremers, D.: Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence  \textbf{40}(3),  611--625 (2017)

\bibitem{engel2014lsd}
Engel, J., Sch{\"o}ps, T., Cremers, D.: Lsd-slam: Large-scale direct monocular slam. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 834--849. Springer (2014)

\bibitem{fu2023cbarf}
Fu, H., Yu, X., Li, L., Zhang, L.: Cbarf: Cascaded bundle-adjusting neural radiance fields from imperfect camera poses (2023)

\bibitem{fu2023colmapfree}
Fu, Y., Liu, S., Kulkarni, A., Kautz, J., Efros, A.A., Wang, X.: Colmap-free 3d gaussian splatting  (2023)

\bibitem{fu2022mononerf}
Fu, Y., Misra, I., Wang, X.: Mononerf: Learning generalizable nerfs from monocular videos without camera poses (2023)

\bibitem{harley2022particle}
Harley, A.W., Fang, Z., Fragkiadaki, K.: Particle video revisited: {T}racking through occlusions using point trajectories. In: Proceedings of the European Conference on Computer Vision (ECCV) (2022)

\bibitem{jeong2021self}
Jeong, Y., Ahn, S., Choy, C., Anandkumar, A., Cho, M., Park, J.: Self-calibrating neural radiance fields. In: Proceedings of the International Conference on Computer Vision (ICCV). pp. 5846--5854 (2021)

\bibitem{karaev2023cotracker}
Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.: {CoTracker}: It is better to track together (2023)

\bibitem{keetha2023splatam}
Keetha, N., Karhade, J., Jatavallabhula, K.M., Yang, G., Scherer, S., Ramanan, D., Luiten, J.: Splatam: Splat, track \& map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126  (2023)

\bibitem{kerbl20233d}
Kerbl, B., Kopanas, G., Leimk{\"u}hler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG)  \textbf{42}(4),  1--14 (2023)

\bibitem{kingma2014adam}
Kingma, D.P., Ba, J.: Adam: {A} method for stochastic optimization. arXiv:1412.6980  (2014)

\bibitem{Knapitsch2017tanks}
Knapitsch, A., Park, J., Zhou, Q.Y., Koltun, V.: Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG)  \textbf{36}(4) (2017)

\bibitem{lai2021video}
Lai, Z., Liu, S., Efros, A.A., Wang, X.: Video autoencoder: self-supervised disentanglement of static 3d structure and motion. In: Proceedings of the International Conference on Computer Vision (ICCV). pp. 9730--9740 (2021)

\bibitem{li2023neuralangelo}
Li, Z., M\"uller, T., Evans, A., Taylor, R.H., Unberath, M., Liu, M.Y., Lin, C.H.: Neuralangelo: High-fidelity neural surface reconstruction. In: IEEE Conference on Computer Vision and Pattern Recognition ({CVPR}) (2023)

\bibitem{lin2021barf}
Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radiance fields. In: Proceedings of the International Conference on Computer Vision (ICCV). pp. 5741--5751 (2021)

\bibitem{lindenberger2023lightglue}
Lindenberger, P., Sarlin, P.E., Pollefeys, M.: {LightGlue: Local Feature Matching at Light Speed}. In: Proceedings of the International Conference on Computer Vision (ICCV) (2023)

\bibitem{liu2019neural}
Liu, C., Gu, J., Kim, K., Narasimhan, S.G., Kautz, J.: Neural rgb (r) d sensing: Depth and uncertainty from a video camera. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10986--10995 (2019)

\bibitem{liu2023robust}
Liu, Y.L., Gao, C., Meuleman, A., Tseng, H.Y., Saraf, A., Kim, C., Chuang, Y.Y., Kopf, J., Huang, J.B.: Robust dynamic radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13--23 (2023)

\bibitem{sift}
Lowe, D.: Object recognition from local scale-invariant features. In: Proceedings of the Seventh IEEE International Conference on Computer Vision. pp. 1150--1157 vol.2 (1999)

\bibitem{luo2018geodesc}
Luo, Z., Shen, T., Zhou, L., Zhu, S., Zhang, R., Yao, Y., Fang, T., Quan, L.: Geodesc: Learning local descriptors by integrating geometry constraints. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 168--183 (2018)

\bibitem{Matsuki:Murai:etal:CVPR2024}
Matsuki, H., Murai, R., Kelly, P.H.J., Davison, A.J.: {G}aussian {S}platting {SLAM}. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2024)

\bibitem{meuleman2023localrf}
Meuleman, A., Liu, Y.L., Gao, C., Huang, J.B., Kim, C., Kim, M.H., Kopf, J.: Progressively optimized local radiance fields for robust view synthesis. In: CVPR (2023)

\bibitem{mildenhall2019local}
Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG)  \textbf{38}(4),  1--14 (2019)

\bibitem{mildenhall2020nerf}
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: {NeRF}: {R}epresenting scenes as neural radiance fields for view synthesis. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 405--421 (2020)

\bibitem{mishchuk2017working}
Mishchuk, A., Mishkin, D., Radenovic, F., Matas, J.: Working hard to know your neighbor's margins: Local descriptor learning loss. Advances in Neural Information Processing Systems (NeurIPS)  \textbf{30} (2017)

\bibitem{mueller2022instant}
M\"uller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (TOG)  \textbf{41}(4),  102:1--102:15 (2022)

\bibitem{mur2015orb}
Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate monocular slam system. Transactions on Robotics (5),  1147--1163 (2015)

\bibitem{mur2017orb}
Mur-Artal, R., Tard{\'o}s, J.D.: Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. Transactions on Robotics  \textbf{33}(5),  1255--1262 (2017)

\bibitem{niemeyer2020differentiable}
Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3504--3515 (2020)

\bibitem{ono2018lf}
Ono, Y., Trulls, E., Fua, P., Yi, K.M.: Lf-net: Learning local features from images. Advances in Neural Information Processing Systems (NeurIPS)  \textbf{31} (2018)

\bibitem{park2023camp}
Park, K., Henzler, P., Mildenhall, B., Barron, J.T., Martin-Brualla, R.: Camp: Camera preconditioning for neural radiance fields. ACM Transactions on Graphics (TOG)  \textbf{42}(6),  1--11 (2023)

\bibitem{ranftl2020towards}
Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards robust monocular depth estimation: {M}ixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence  (2020)

\bibitem{reizenstein2021common}
Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny, D.: Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In: Proceedings of the International Conference on Computer Vision (ICCV). pp. 10901--10911 (2021)

\bibitem{rosinol2020kimera}
Rosinol, A., Abate, M., Chang, Y., Carlone, L.: Kimera: an open-source library for real-time metric-semantic localization and mapping. In: Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). pp. 1689--1696. IEEE (2020)

\bibitem{sarlin2020superglue}
Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: Superglue: Learning feature matching with graph neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4938--4947 (2020)

\bibitem{sarlin20superglue}
Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: {SuperGlue}: {L}earning feature matching with graph neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)

\bibitem{sarlin2021pixloc}
Sarlin, P.E., Unagar, A., Larsson, M., Germain, H., Toft, C., Larsson, V., Pollefeys, M., Lepetit, V., Hammarstrand, L., Kahl, F., Sattler, T.: {Back to the Feature: Learning Robust Camera Localization from Pixels to Pose}. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)

\bibitem{schonberger2016structure}
Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4104--4113 (2016)

\bibitem{sitzmann2019deepvoxels}
Sitzmann, V., Thies, J., Heide, F., Nie{\ss}ner, M., Wetzstein, G., Zollh{\"o}fer, M.: Deepvoxels: Learning persistent 3d feature embeddings. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)

\bibitem{sitzmann2019scene}
Sitzmann, V., Zollh{\"o}fer, M., Wetzstein, G.: Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems (NeurIPS)  (2019)

\bibitem{smith2023flowcam}
Smith, C., Du, Y., Tewari, A., Sitzmann, V.: Flowcam: Training generalizable 3d radiance fields without camera poses via pixel-aligned scene flow. Advances in Neural Information Processing Systems (NeurIPS)  (2023)

\bibitem{suhail2022generalizable}
Suhail, M., Esteves, C., Sigal, L., Makadia, A.: Generalizable patch-based neural rendering. In: Proceedings of the European Conference on Computer Vision (ECCV). Springer (2022)

\bibitem{nerfstudio}
Tancik, M., Weber, E., Ng, E., Li, R., Yi, B., Kerr, J., Wang, T., Kristoffersen, A., Austin, J., Salahi, K., Ahuja, A., McAllister, D., Kanazawa, A.: Nerfstudio: A modular framework for neural radiance field development. In: ACM Transactions on Graphics (TOG) (2023)

\bibitem{tang2018ba}
Tang, C., Tan, P.: {BA-Net}: {D}ense bundle adjustment network. arXiv preprint arXiv:1806.04807  (2018)

\bibitem{teed2018deepv2d}
Teed, Z., Deng, J.: Deepv2d: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605  (2018)

\bibitem{raft}
Teed, Z., Deng, J.: {RAFT}: {R}ecurrent all-pairs field transforms for optical flow. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020)

\bibitem{teed2021droid}
Teed, Z., Deng, J.: Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in Neural Information Processing Systems (NeurIPS)  \textbf{34} (2021)

\bibitem{tewari2023diffusion}
Tewari, A., Yin, T., Cazenavette, G., Rezchikov, S., Tenenbaum, J.B., Durand, F., Freeman, W.T., Sitzmann, V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. Advances in Neural Information Processing Systems (NeurIPS)  (2023)

\bibitem{ummenhofer2017demon}
Ummenhofer, B., Zhou, H., Uhrig, J., Mayer, N., Ilg, E., Dosovitskiy, A., Brox, T.: Demon: Depth and motion network for learning monocular stereo. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5038--5047 (2017)

\bibitem{wang2023visual}
Wang, J., Karaev, N., Rupprecht, C., Novotny, D.: Visual geometry grounded deep structure from motion. arXiv preprint arXiv:2312.04563  (2023)

\bibitem{wang2021ibrnet}
Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T., Martin-Brualla, R., Snavely, N., Funkhouser, T.: Ibrnet: Learning multi-view image-based rendering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)

\bibitem{wang2021tartanvo}
Wang, W., Hu, Y., Scherer, S.: Tartanvo: A generalizable learning-based vo. In: Conference on Robot Learning. pp. 1761--1772. PMLR (2021)

\bibitem{nerf--}
Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Ne{RF}$--$: Neural radiance fields without known camera parameters. arXiv:2102.07064  (2021)

\bibitem{wang2021nerf}
Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Nerf--: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064  (2021)

\bibitem{wewer24latentsplat}
Wewer, C., Raj, K., Ilg, E., Schiele, B., Lenssen, J.E.: latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction. In: arXiv (2024)

\bibitem{wu2023scanerf}
Wu, X., Xu, J., Zhang, X., Bao, H., Huang, Q., Shen, Y., Tompkin, J., Xu, W.: Scanerf: Scalable bundle-adjusting neural radiance fields for large-scale scene rendering. ACM Transactions on Graphics (TOG)  (2023)

\bibitem{xia2022sinerf}
Xia, Y., Tang, H., Timofte, R., Van~Gool, L.: Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. arXiv preprint arXiv:2210.04553  (2022)

\bibitem{yen2021inerf}
Yen-Chen, L., Florence, P., Barron, J.T., Rodriguez, A., Isola, P., Lin, T.Y.: inerf: Inverting neural radiance fields for pose estimation. In: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 1323--1330. IEEE (2021)

\bibitem{pixelnerf}
Yu, A., Ye, V., Tancik, M., Kanazawa, A.: {pixelNeRF}: {N}eural radiance fields from one or few images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)

\bibitem{yugay2023gaussian}
Yugay, V., Li, Y., Gevers, T., Oswald, M.R.: Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070  (2023)

\bibitem{zhao2022particlesfm}
Zhao, W., Liu, S., Guo, H., Wang, W., Liu, Y.J.: Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 523--542. Springer (2022)

\bibitem{zhou2018deeptam}
Zhou, H., Ummenhofer, B., Brox, T.: Deeptam: Deep tracking and mapping. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 822--838 (2018)

\end{thebibliography}
