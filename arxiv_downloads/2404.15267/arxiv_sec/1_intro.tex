\section{Introduction}

Controllable human image generation aims to synthesize human images that align with specific textual descriptions, structural signals or more precise appearance conditions. It emerges as a significant technology within the realm of digital content creation, providing users with a portrait customization solution. However, due to the complexity of the control conditions, this task presents significant challenges, especially when it comes to multi-type condition input and control of various aspects of human appearance.

As diffusion models~\cite{ho2020ddpm,rombach2022ldm,ramesh2022dalle2,saharia2022imagen,nichol2022glide,dhariwal2021diffusionbeatgans} have brought great success in image generation, the task of controllable human image generation has experienced rapid development. Several works~\cite{jiang2022text2human} utilize languages as condition, generating human images by providing attributes about the textures of clothes. Due to the rough control of texts, it struggles to accurately guide the generation of human appearance. Another group of works~\cite{zhang2023controlnet, mou2023t2iadapter, zhao2024unicontrolnet, liu2023hyperhuman} focuses on introducing structural signals to control human posture. Although these methods have achieved impressive results, they do not consider appearance as a condition, which is crucial for portrait customization.

Recently, several works~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, kumari2023customdiffusion, liu2023cones, shi2023instantbooth, ye2023ipadapter, chen2023anydoor, li2023photomaker, wang2024instantid, zhang2024ssrencoder} have emerged that use appearance conditions to guide human image generation. They learn human representation from reference images and generate images aligning with the specific face identity. One prominent approach involves test-time fine-tuning~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, kumari2023customdiffusion, liu2023cones}. It requires substantial computational resources to learn each new individual, which costs about half an hour to achieve satisfactory results. Another approach~\cite{shi2023instantbooth, ye2023ipadapter, chen2023anydoor, li2023photomaker, wang2024instantid, zhang2024ssrencoder} investigates the zero-shot setting to bypass the fine-tuning cost. It encodes the reference image into one or several tokens and injects them into the generation process along with text tokens. These zero-shot methods make human image customization practical with faster speed. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details. And they lack the design to obtain specified information from the images, but instead utilize all the information, resulting in ambiguous subject representation.

In this paper, we target generating human images from multi-part images of human appearance, along with specific pose maps or optionally text descriptions. The above-mentioned generation methods conditioned on structural signals~\cite{zhang2023controlnet, mou2023t2iadapter} or face identity~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, ye2023ipadapter, wang2024instantid, zhang2024ssrencoder} have their limitations on this task (results shown in \cref{fig:comparison} and \cref{fig:ablation}). It is attributed to the spatial misalignment of the input multi-body parts with the target image, and the lack of specific design in existing methods to \textbf{address the variation in spatial positions during feature injection}. Methods like IP-Adapter~\cite{ye2023ipadapter} and SSR-Encoder~\cite{zhang2024ssrencoder} incorporate features into the denoising U-Net through cross-attention mechanisms. They encode reference images into other modal features (e.g., semantic features) and utilize the cross-attention keys and values from them rather than from \textbf{image dimensional} feature maps. As a result, the spatial relationship in the original image dimensions between the reference images and the target image is lost, resulting in a mixture of attributes from different subjects. Although methods like ControlNet~\cite{zhang2023controlnet} encode the reference images into image-dimensional features, they add the features to the feature maps in the U-Net decoder. It is suitable for tasks where the condition maps and the target map have the same structure, such as guiding generation using line drawings. However, in the case of the spatial misalignment of the conditional images with the target image, it is difficult to model the correlation of spatial information by \textbf{directly adding or concat features} on the channel dimension.

To address the above issues, we present Parts2Whole, a unified reference framework for portrait customization from multiple reference images, including various parts of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Inspired by the effective reference mechanism used in image-to-video tasks~\cite{hu2023animateanyone, xu2023magicanimate}, we develop a semantic-aware appearance encoder based on the Reference U-Net architecture. It encodes each image with its textual label into \textbf{a series of multi-scale feature maps in image dimension}, preserving appearance details and spatial information of multiple reference images. The additional semantic condition represents a category instruction, which helps retain richer shapes and detailed attributes of each aspect. Furthermore, to preserve the positional relationship when injecting reference features into the image generation process, we employ a \textbf{shared self-attention} operation across reference and target features during the diffusion process. We also build a tiny convolution network to extract the pose features and inject them into the generation. To precisely select the specified part from each reference image, we enhance the vanilla self-attention mechanism by \textbf{incorporating masks} of the subjects in the reference images.

Equipped with these techniques, Parts2Whole demonstrates superior quality and controllability for human image generation. Our contributions are summarized as follows:

\begin{itemize}
\item We construct a novel framework, Parts2Whole, which supports the controllable generation of human images conditioned on texts, pose signals, and multiple aspects of human appearance.
\item We propose an advanced multi-reference mechanism consisting of a semantic-aware image encoder and the shared attention operation, which retains details of the specific key elements and achieves precise subject selection with the help of our proposed mask-guided approach.
\item Experiments show that our Parts2Whole generates high-quality human images from multiple conditions and maintains high consistency with the given conditions.
\end{itemize}