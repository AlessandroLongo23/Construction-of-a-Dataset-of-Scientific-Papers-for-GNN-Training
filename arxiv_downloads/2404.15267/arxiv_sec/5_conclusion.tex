\section{Conclusion}

In this work, we propose Parts2Whole, a novel framework for controllable human image generation conditioned on multiple reference images, including various aspects of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Based on a dual U-Net design, we develop a semantic-aware appearance encoder to process each condition image with its label into multi-scale feature maps and inject those detail-rich reference features into the generation via a shared self-attention mechanism. This design retains details from multiple references and looks very good. We also enhance vanilla self-attention by incorporating subject masks, enabling Parts2Whole to synthesize human images from specified parts from condition images. Extensive experiments demonstrate that our Parts2Whole performs well in terms of image quality and condition alignment.

\cparagraph{Future Works.} Our Parts2Whole is currently trained at the resolution of 512, which may cause artifacts in some generated results. This could be improved by using higher resolutions and larger diffusion models like SD-XL~\cite{podell2023sdxl} as our backbone. Furthermore, it will be valuable to achieve the try-on of layer-wise clothing based on our Parts2Whole.
