\section{Related Work}

\cparagraph{Text-to-Image Generation.}
In recent years, text-to-image generation has made remarkable progress, particularly with the development of diffusion models~\cite{ramesh2022dalle2, nichol2022glide, rombach2022ldm, saharia2022imagen, dhariwal2021diffusionbeatgans, ho2020ddpm, podell2023sdxl} and auto-regressive
models~\cite{chang2023muse, yu2022scalingauto, tian2024var}, which have propelled text-to-image generation to large-scale commercialization. Since DALLE2~\cite{ramesh2022dalle2}, Stable Diffusion~\cite{rombach2022ldm} and Imagen~\cite{saharia2022imagen} employ diffusion models as generative models and train the models on large datasets, text-to-image synthesis ability has been significantly enhanced. More recently, Stable Diffusion XL~\cite{podell2023sdxl}, a two-stage cascade diffusion model, has greatly improved the generation of high-frequency details and overall image color, taking aesthetic appeal to a higher level. However, these existing methods are limited to generating images solely from text prompts, and they do not meet the demand for producing customized images with the preservation of appearance.

\cparagraph{Controllable Image Generation.}
Given the robust generative capabilities of image diffusion models, a series of research~\cite{zhang2023controlnet, mou2023t2iadapter, qin2023unicontrol, ruiz2023dreambooth, hu2021lora, ye2023ipadapter, chen2023anydoor, zhang2024ssrencoder} attempts to explore the controllability of image generation, enabling image synthesis guided by multimodal image information. Some work~\cite{zhang2023controlnet, mou2023t2iadapter, jiang2023scedit, qin2023unicontrol, zhao2024unicontrolnet, hu2023cocktail} focuses on introducing structural signals such as edges, depth maps, and segmentation maps, to control spatial structure of images. Another group of work~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, ye2023ipadapter, chen2023anydoor} use appearance conditions to guide image generation, aiming to generate images aligning with specific concepts like identity and style, known as subject-driven image generation. The methods generally fall into two categories: those requiring test-time fine-tuning and those that do not. Test-time fine-tuning methods~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, kumari2023customdiffusion, liu2023cones} often optimizes additional text embedding, parameter residuals or direct fine-tune the model to fit the specified subject. Although these methods have achieved impressive results, they costs about half an hour to achieve satisfactory results. Fine-tuning-free methods~\cite{shi2023instantbooth, ye2023ipadapter, chen2023anydoor, zhang2024ssrencoder, ma2023subjectdiffusion, gal2023encoderdiff, wei2023elite} typically train an additional network to encode the reference image into embeddings or image prompts. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details.

\cparagraph{Controllable Human Image Generation.}
In this paper, we mainly focus on controllable human image generation, and aims to synthesize human images aligning with specific text prompts, pose signals and various parts of human appearance. Text2Human~\cite{jiang2022text2human} generates full-body human images using detailed descriptions about the textures of clothes, but is limited by the coarse-grained textual condition. Test-time fine-tuning methods~\cite{ruiz2023dreambooth,hu2021lora,kumari2023customdiffusion} produce satisfactory results, but when it comes to customize portraits using multiple parts of human appearance, they take much more time to fit each aspect. Recently, methods like IP-Adapter-FaceID~\cite{ye2023ipadapter}, FastComposer~\cite{xiao2023fastcomposer}, PhotoMaker~\cite{li2023photomaker}, and InstantID~\cite{wang2024instantid} show promising results on zero-shot human image personalization. They encode the reference face to one or several tokens as conditions to generate customized images. With the addition of adaptable structural control networks~\cite{zhang2023controlnet, mou2023t2iadapter}, these methods can generate portraits aligned with specified poses and human identities. However, they usually fail to maintain the details of human identities and utilize all the information from a single image, resulting in ambiguous subject representation. These make it difficult to apply these schemes to precisely generation conditioned on multiple parts of the human appearance. In contrast, our Parts2Whole is both generalizable and efficient, and precisely retains details in multiple parts of human appearance.