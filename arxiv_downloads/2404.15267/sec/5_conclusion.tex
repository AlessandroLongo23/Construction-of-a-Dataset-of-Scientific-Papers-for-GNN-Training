\section{Conclusion}

In this work, we propose Parts2Whole, a novel framework for controllable human image generation conditioned on multiple reference images, including various aspects of human appearance (e.g. hair, face, clothes, shoes, etc.) and pose maps. Based on a dual U-Net design, we develop a semantic-aware appearance encoder to process each condition image with its label into multi-scale feature maps, and injects those detail-rich reference features into the generation via a shared self-attention mechanism. This design retains details from multiple reference looks very well. We also enhance the vanilla self-attention by incorporating subject masks, enabling Parts2Whole to synthesize human images from specified part from condition images. Extensive experiments demonstrate that our Parts2Whole performs well in terms of image quality and condition alignment.

\cparagraph{Future Works.} Our Parts2Whole is currently trained at the resolution of 512, which may cause artifacts in some generated results. This could be improved by using higher resolutions and larger diffusion models like SD-XL~\cite{podell2023sdxl} as our backbone. Furthermore, it will be more controllable and valuable to realize the try-on of multi-layer clothing based on our Parts2Whole.
