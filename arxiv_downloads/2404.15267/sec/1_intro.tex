\section{Introduction}

Controllable human image generation aims to synthesize human images aligning with specific textual descriptions, structural signals or more precise appearance conditions. It emerges as a significant technology within the realm of digital content creation, providing users with a portrait customization solution. However, due to the complexity of the control conditions, this task presents significant challenges, especially when it comes to multi-type condition input and control of various aspects of human appearance.

As diffusion models~\cite{ho2020ddpm,rombach2022ldm,ramesh2022dalle2,saharia2022imagen,nichol2022glide,dhariwal2021diffusionbeatgans} have brought great success in image generation, the task of controllable human image generation has experienced rapid development. Several works~\cite{jiang2022text2human} utilize languages as condition, generating full-body human images by providing more attributes about the textures of clothes. However, due to the rough control of text conditions, these methods demonstrate poor consistency with expectations. Another group of works~\cite{zhang2023controlnet, mou2023t2iadapter, zhao2024unicontrolnet, liu2023hyperhuman} focuses on introducing structural signals into image generation to control human posture. Although these methods have achieved impressive results, they do not consider the appearance as a condition, which is crucial for portrait customization.

Recently, several works~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, kumari2023customdiffusion, liu2023cones, shi2023instantbooth, ye2023ipadapter, chen2023anydoor, li2023photomaker, wang2024instantid, zhang2024ssrencoder} have emerged that use appearance conditions to guide human image generation. They can learn human representation from reference images and generate images aligning with the specific face identity. One prominent approach involves test-time fine-tuning~\cite{ruiz2023dreambooth, hu2021lora, gal2022textualinversion, kumari2023customdiffusion, liu2023cones}. It requires substantial computational resources to learn each new individual, which costs about half an hour to achieve satisfactory results. For learning of multiple concepts like multiple parts of human appearance, these methods requires prohibitive computational resources. Another approach~\cite{shi2023instantbooth, ye2023ipadapter, chen2023anydoor, li2023photomaker, wang2024instantid, zhang2024ssrencoder} investigates the zero-shot setting to bypass the fine-tuning cost. It encodes the reference image into one or several tokens and inject them into the generation process along with text tokens. These zero-shot methods make human image customization practical with significantly faster speed. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details. And they lack the design to obtain specified information from the image, but instead utilize all the information, resulting in ambiguous subject representation. These make it hard for the schemes to be applied to precisely generation conditioned on multiple parts of the human appearance.

In this paper, we present Parts2Whole, a unified reference framework designed for portrait customization from multiple reference images, including pose images and various aspects of human appearance (e.g. hair, face, clothes, shoes, etc.). Inspired by the effective reference mechanism used in image-to-video tasks~\cite{hu2023animateanyone, xu2023magicanimate}, we develop a semantic-aware appearance encoder based on the Reference UNet architecture. It encodes each image with its textual label into a series of multi-scale feature maps rather than one or several tokens, preserving details of different human appearance's key elements. The additional semantic condition represents a category instruction, which helps retain richer shape and detailed attributes of each aspect. Furthermore, in order to use the appearance information of multiple images to guide the human image generation, we employ a shared self-attention operation across reference and target features during the diffusion process. We also construct a tiny convolution network to extract the pose features and inject them into the generation. To precisely select the specified part from each reference image, we enhance the vanilla self-attention mechanism by incorporating mask information from the subject in the reference images.

Equipped with all these techniques, Parts2Whole demonstrates superior quality and controllability for human image generation. In general, our contributions are summarized as follows:

\begin{itemize}
\item We construct a novel framework, Parts2Whole, which supports the controllable generation of human images conditioned on texts, pose signals, and multiple aspects of human appearance.
\item We propose an advanced multi-reference mechanism consisting of a semantic-aware image encoder and the shared attention operation, which retains details of the specific key elements and achieves precise subject selection with the help of our proposed mask-guided approach.
\item Experiments show that our Parts2Whole generates high-quality human images from multiple conditions and maintains high consistency with the given conditions.
\end{itemize}