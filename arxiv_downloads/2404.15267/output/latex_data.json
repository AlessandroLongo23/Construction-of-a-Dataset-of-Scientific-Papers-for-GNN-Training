{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation",
            "leftover": "From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation",
            "matches": []
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{ { Zehuan Huang\\footnotemark[1] \\quad Hongxing Fan\\footnotemark[1] \\quad Lipeng Wang\\footnotemark[1] \\quad Lu Sheng\\footnotemark[2] } {Beihang University} {\\tt\\small {huangzehuan, fanhongxing, wanglipeng, lsheng}@buaa.edu.cn} {\\smallhttps://huanngzh.github.io/Parts2Whole/} }",
            "leftover": "{ { Zehuan Huang\\footnotemark[1] \\quad Hongxing Fan\\footnotemark[1] \\quad Lipeng Wang\\footnotemark[1] \\quad Lu Sheng\\footnotemark[2] } {Beihang University} {\\tt\\small {huangzehuan, fanhongxing, wanglipeng, lsheng}@buaa.edu.cn} {\\smallhttps://huanngzh.github.io/Parts2Whole/} }",
            "matches": []
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Recent advancements in controllable human image generation have led to zeroshot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semanticaware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multiscale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multiimage conditioned generation through a shared selfattention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multipart controllable human image customization.",
            "leftover": "Recent advancements in controllable human image generation have led to zeroshot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semanticaware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multiscale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multiimage conditioned generation through a shared selfattention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multipart controllable human image customization.",
            "matches": []
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "leaf id": 3,
                    "key": "doc/body/center0",
                    "block type": "center",
                    "content": "\\includegraphics[width=]{figure/teaser.pdf} \\captionoffigure{We propose Parts2Whole, which can generate realistic and highquality human figures in various postures from referential human part images of any quantity and different origins. Our method maintains the high alignment with the corresponding conditional semantic regions, while ensuring diversity and harmony among the whole body.}",
                    "leftover": "\\includegraphics[width=]{figure/teaser.pdf} \\captionoffigure{We propose Parts2Whole, which can generate realistic and highquality human figures in various postures from referential human part images of any quantity and different origins. Our method maintains the high alignment with the corresponding conditional semantic regions, while ensuring diversity and harmony among the whole body.}",
                    "matches": []
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "Introduction",
                            "matches": []
                        },
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec1/txl0",
                            "block type": "txl",
                            "content": "Controllable human image generation aims to synthesize human images that align with specific textual descriptions, structural signals or more precise appearance conditions. It emerges as a significant technology within the realm of digital content creation, providing users with a portrait customization solution. However, due to the complexity of the control conditions, this task presents significant challenges, especially when it comes to multitype condition input and control of various aspects of human appearance.",
                            "leftover": "Controllable human image generation aims to synthesize human images that align with specific textual descriptions, structural signals or more precise appearance conditions. It emerges as a significant technology within the realm of digital content creation, providing users with a portrait customization solution. However, due to the complexity of the control conditions, this task presents significant challenges, especially when it comes to multitype condition input and control of various aspects of human appearance.",
                            "matches": []
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec1/txl1",
                            "block type": "txl",
                            "content": "As diffusion models have brought great success in image generation, the task of controllable human image generation has experienced rapid development. Several works utilize languages as condition, generating human images by providing attributes about the textures of clothes. Due to the rough control of texts, it struggles to accurately guide the generation of human appearance. Another group of works focuses on introducing structural signals to control human posture. Although these methods have achieved impressive results, they do not consider appearance as a condition, which is crucial for portrait customization.",
                            "leftover": "As diffusion models have brought great success in image generation, the task of controllable human image generation has experienced rapid development. Several works utilize languages as condition, generating human images by providing attributes about the textures of clothes. Due to the rough control of texts, it struggles to accurately guide the generation of human appearance. Another group of works focuses on introducing structural signals to control human posture. Although these methods have achieved impressive results, they do not consider appearance as a condition, which is crucial for portrait customization.",
                            "matches": []
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec1/txl2",
                            "block type": "txl",
                            "content": "Recently, several works have emerged that use appearance conditions to guide human image generation. They learn human representation from reference images and generate images aligning with the specific face identity. One prominent approach involves testtime finetuning. It requires substantial computational resources to learn each new individual, which costs about half an hour to achieve satisfactory results. Another approach investigates the zeroshot setting to bypass the finetuning cost. It encodes the reference image into one or several tokens and injects them into the generation process along with text tokens. These zeroshot methods make human image customization practical with faster speed. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details. And they lack the design to obtain specified information from the images, but instead utilize all the information, resulting in ambiguous subject representation.",
                            "leftover": "Recently, several works have emerged that use appearance conditions to guide human image generation. They learn human representation from reference images and generate images aligning with the specific face identity. One prominent approach involves testtime finetuning. It requires substantial computational resources to learn each new individual, which costs about half an hour to achieve satisfactory results. Another approach investigates the zeroshot setting to bypass the finetuning cost. It encodes the reference image into one or several tokens and injects them into the generation process along with text tokens. These zeroshot methods make human image customization practical with faster speed. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details. And they lack the design to obtain specified information from the images, but instead utilize all the information, resulting in ambiguous subject representation.",
                            "matches": []
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec1/txl3",
                            "block type": "txl",
                            "content": "In this paper, we target generating human images from multipart images of human appearance, along with specific pose maps or optionally text descriptions. The abovementioned generation methods conditioned on structural signals or face identity have their limitations on this task (results shown in and ). It is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. Methods like IPAdapter and SSREncoder incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g., semantic features) and utilize the crossattention keys and values from them rather than from image dimensional}feature maps. As a result, the spatial relationship in the original image dimensions between the reference images and the target image is lost, resulting in a mixture of attributes from different subjects. Although methods like ControlNet encode the reference images into imagedimensional features, they add the features to the feature maps in the UNet decoder. It is suitable for tasks where the condition maps and the target map have the same structure, such as guiding generation using line drawings. However, in the case of the spatial misalignment of the conditional images with the target image, it is difficult to model the correlation of spatial information by directly adding or concat features}on the channel dimension.",
                            "leftover": "In this paper, we target generating human images from multipart images of human appearance, along with specific pose maps or optionally text descriptions. The abovementioned generation methods conditioned on structural signals or face identity have their limitations on this task (results shown in and ). It is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. Methods like IPAdapter and SSREncoder incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g., semantic features) and utilize the crossattention keys and values from them rather than from image dimensional}feature maps. As a result, the spatial relationship in the original image dimensions between the reference images and the target image is lost, resulting in a mixture of attributes from different subjects. Although methods like ControlNet encode the reference images into imagedimensional features, they add the features to the feature maps in the UNet decoder. It is suitable for tasks where the condition maps and the target map have the same structure, such as guiding generation using line drawings. However, in the case of the spatial misalignment of the conditional images with the target image, it is difficult to model the correlation of spatial information by directly adding or concat features}on the channel dimension.",
                            "matches": []
                        },
                        {
                            "leaf id": 9,
                            "key": "doc/body/sec1/txl4",
                            "block type": "txl",
                            "content": "To address the above issues, we present Parts2Whole, a unified reference framework for portrait customization from multiple reference images, including various parts of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Inspired by the effective reference mechanism used in imagetovideo tasks, we develop a semanticaware appearance encoder based on the Reference UNet architecture. It encodes each image with its textual label into a series of multiscale feature maps in image dimension, preserving appearance details and spatial information of multiple reference images. The additional semantic condition represents a category instruction, which helps retain richer shapes and detailed attributes of each aspect. Furthermore, to preserve the positional relationship when injecting reference features into the image generation process, we employ a shared selfattention}operation across reference and target features during the diffusion process. We also build a tiny convolution network to extract the pose features and inject them into the generation. To precisely select the specified part from each reference image, we enhance the vanilla selfattention mechanism by incorporating masks}of the subjects in the reference images.",
                            "leftover": "To address the above issues, we present Parts2Whole, a unified reference framework for portrait customization from multiple reference images, including various parts of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Inspired by the effective reference mechanism used in imagetovideo tasks, we develop a semanticaware appearance encoder based on the Reference UNet architecture. It encodes each image with its textual label into a series of multiscale feature maps in image dimension, preserving appearance details and spatial information of multiple reference images. The additional semantic condition represents a category instruction, which helps retain richer shapes and detailed attributes of each aspect. Furthermore, to preserve the positional relationship when injecting reference features into the image generation process, we employ a shared selfattention}operation across reference and target features during the diffusion process. We also build a tiny convolution network to extract the pose features and inject them into the generation. To precisely select the specified part from each reference image, we enhance the vanilla selfattention mechanism by incorporating masks}of the subjects in the reference images.",
                            "matches": []
                        },
                        {
                            "leaf id": 10,
                            "key": "doc/body/sec1/txl5",
                            "block type": "txl",
                            "content": "Equipped with these techniques, Parts2Whole demonstrates superior quality and controllability for human image generation. Our contributions are summarized as follows:",
                            "leftover": "Equipped with these techniques, Parts2Whole demonstrates superior quality and controllability for human image generation. Our contributions are summarized as follows:",
                            "matches": []
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec1/itemize6",
                            "block type": "itemize",
                            "content": "We construct a novel framework, Parts2Whole, which supports the controllable generation of human images conditioned on texts, pose signals, and multiple aspects of human appearance. We propose an advanced multireference mechanism consisting of a semanticaware image encoder and the shared attention operation, which retains details of the specific key elements and achieves precise subject selection with the help of our proposed maskguided approach. Experiments show that our Parts2Whole generates highquality human images from multiple conditions and maintains high consistency with the given conditions.",
                            "leftover": "We construct a novel framework, Parts2Whole, which supports the controllable generation of human images conditioned on texts, pose signals, and multiple aspects of human appearance. We propose an advanced multireference mechanism consisting of a semanticaware image encoder and the shared attention operation, which retains details of the specific key elements and achieves precise subject selection with the help of our proposed maskguided approach. Experiments show that our Parts2Whole generates highquality human images from multiple conditions and maintains high consistency with the given conditions.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Related Work",
                            "leftover": "Related Work",
                            "matches": []
                        },
                        {
                            "leaf id": 13,
                            "key": "doc/body/sec2/txl0",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In recent years, texttoimage generation has made remarkable progress, particularly with the development of diffusion models and autoregressive models, which have propelled texttoimage generation to largescale commercialization. Since DALLE2, Stable Diffusion and Imagen employ diffusion models as generative models and train the models on large datasets, texttoimage synthesis ability has been significantly enhanced. More recently, Stable Diffusion XL, a twostage cascade diffusion model, has greatly improved the generation of highfrequency details and overall image color, taking aesthetic appeal to a higher level. However, these existing methods are limited to generating images solely from text prompts, and they do not meet the demand for producing customized images with the preservation of appearance.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In recent years, texttoimage generation has made remarkable progress, particularly with the development of diffusion models and autoregressive models, which have propelled texttoimage generation to largescale commercialization. Since DALLE2, Stable Diffusion and Imagen employ diffusion models as generative models and train the models on large datasets, texttoimage synthesis ability has been significantly enhanced. More recently, Stable Diffusion XL, a twostage cascade diffusion model, has greatly improved the generation of highfrequency details and overall image color, taking aesthetic appeal to a higher level. However, these existing methods are limited to generating images solely from text prompts, and they do not meet the demand for producing customized images with the preservation of appearance.",
                            "matches": []
                        },
                        {
                            "leaf id": 14,
                            "key": "doc/body/sec2/txl1",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Given the robust generative capabilities of image diffusion models, a series of research attempts to explore the controllability of image generation, enabling image synthesis guided by multimodal conditions. Some work focuses on introducing structural signals such as edges, depth maps, and segmentation maps, to control the spatial structure of generated images. Another group of work uses appearance conditions to guide image generation, aiming to generate images aligning with specific concepts like identity and style, known as subjectdriven image generation. The methods generally fall into two categories: those requiring testtime finetuning and those that do not. Testtime finetuning methods often optimizes additional text embedding, parameter residuals or direct finetune the whole model to fit the specified subject. Although these methods have achieved impressive results, they cost about half an hour to achieve satisfactory results. Finetuningfree methods typically train an additional encoding network to encode the reference image into embeddings or image prompts. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Given the robust generative capabilities of image diffusion models, a series of research attempts to explore the controllability of image generation, enabling image synthesis guided by multimodal conditions. Some work focuses on introducing structural signals such as edges, depth maps, and segmentation maps, to control the spatial structure of generated images. Another group of work uses appearance conditions to guide image generation, aiming to generate images aligning with specific concepts like identity and style, known as subjectdriven image generation. The methods generally fall into two categories: those requiring testtime finetuning and those that do not. Testtime finetuning methods often optimizes additional text embedding, parameter residuals or direct finetune the whole model to fit the specified subject. Although these methods have achieved impressive results, they cost about half an hour to achieve satisfactory results. Finetuningfree methods typically train an additional encoding network to encode the reference image into embeddings or image prompts. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details.",
                            "matches": []
                        },
                        {
                            "leaf id": 15,
                            "key": "doc/body/sec2/txl2",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In this paper, we mainly focus on controllable human image generation and aim to synthesize human images aligning with specific text prompts, pose signals, and various parts of human appearance. Text2Human generates fullbody human images using detailed descriptions about the textures of clothes, but is limited by the coarsegrained textual condition. Testtime finetuning methods produce satisfactory results, but when it comes to customizing portraits using multiple parts of human appearance, they take much more time to fit each aspect. Recently, methods like IPAdapterFaceID, FastComposer, PhotoMaker, and InstantID show promising results on zeroshot human image personalization. They encode the reference face to one or several tokens as conditions to generate customized images. With the addition of adaptable structural control networks, these methods can generate portraits aligned with specified poses and human identities. However, they usually fail to maintain the details of human identities and utilize all the information from a single image, resulting in ambiguous subject representation. These make it difficult to apply these schemes to precisely generation conditioned on multiple parts of the human appearance. In contrast, our Parts2Whole is both generalizable and efficient, and precisely retains details in multiple parts of human appearance.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In this paper, we mainly focus on controllable human image generation and aim to synthesize human images aligning with specific text prompts, pose signals, and various parts of human appearance. Text2Human generates fullbody human images using detailed descriptions about the textures of clothes, but is limited by the coarsegrained textual condition. Testtime finetuning methods produce satisfactory results, but when it comes to customizing portraits using multiple parts of human appearance, they take much more time to fit each aspect. Recently, methods like IPAdapterFaceID, FastComposer, PhotoMaker, and InstantID show promising results on zeroshot human image personalization. They encode the reference face to one or several tokens as conditions to generate customized images. With the addition of adaptable structural control networks, these methods can generate portraits aligned with specified poses and human identities. However, they usually fail to maintain the details of human identities and utilize all the information from a single image, resulting in ambiguous subject representation. These make it difficult to apply these schemes to precisely generation conditioned on multiple parts of the human appearance. In contrast, our Parts2Whole is both generalizable and efficient, and precisely retains details in multiple parts of human appearance.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 16,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Method",
                            "leftover": "Method",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec3/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Overview of Parts2Whole. Based on the texttoimage diffusion model, our method designs an appearance encoder for encoding various parts of human appearance into multiscale feature maps. We build this encoder by copying the network structure and pretrained weights from denoising UNet. Features obtained from reference images with their textual labels are injected into the generation process by shared attention mechanism layer by layer. To precisely select the specified parts from reference images, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. An illustration of one block in UNet is shown on the right part.",
                                    "leftover": "Overview of Parts2Whole. Based on the texttoimage diffusion model, our method designs an appearance encoder for encoding various parts of human appearance into multiscale feature maps. We build this encoder by copying the network structure and pretrained weights from denoising UNet. Features obtained from reference images with their textual labels are injected into the generation process by shared attention mechanism layer by layer. To precisely select the specified parts from reference images, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. An illustration of one block in UNet is shown on the right part.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 18,
                            "key": "doc/body/sec3/txl1",
                            "block type": "txl",
                            "content": "We target controllable human image generation guided by multiple reference images. Given N images that capture distinct parts of human appearance x^1:N and a pose map p, and optionally text inputs, our objective is to synthesize a human image x̂ aligning with the specified appearances and posture. To achieve this goal, we propose Parts2Whole, a specialized framework designed to interpolate various reference images and generate highquality portraits.",
                            "leftover": "We target controllable human image generation guided by multiple reference images. Given N images that capture distinct parts of human appearance x^1:N and a pose map p, and optionally text inputs, our objective is to synthesize a human image x̂ aligning with the specified appearances and posture. To achieve this goal, we propose Parts2Whole, a specialized framework designed to interpolate various reference images and generate highquality portraits.",
                            "matches": []
                        },
                        {
                            "leaf id": 19,
                            "key": "doc/body/sec3/txl2",
                            "block type": "txl",
                            "content": "In general, Parts2Whole is built on texttoimage diffusion models. In the following sections, we start with an overview of T2I diffusion models, and in particular, the selfattention mechanism in Sec.. We continue by presenting our unified reference framework in Sec., which consists of a semanticaware appearance encoder, a shared selfattention that queries referential features within the self–attention layers, and the enhanced maskguided subject selection. These methods enable Parts2Whole to accurately obtain the specific subject information from multiple reference images while preserving appearance details.",
                            "leftover": "In general, Parts2Whole is built on texttoimage diffusion models. In the following sections, we start with an overview of T2I diffusion models, and in particular, the selfattention mechanism in Sec.. We continue by presenting our unified reference framework in Sec., which consists of a semanticaware appearance encoder, a shared selfattention that queries referential features within the self–attention layers, and the enhanced maskguided subject selection. These methods enable Parts2Whole to accurately obtain the specific subject information from multiple reference images while preserving appearance details.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 20,
                                    "key": "doc/body/sec3/sub3/tit",
                                    "block type": "title",
                                    "content": "Preliminaries",
                                    "leftover": "Preliminaries",
                                    "matches": []
                                },
                                {
                                    "leaf id": 21,
                                    "key": "doc/body/sec3/sub3/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Diffusion models exhibit promising capabilities in image generation. In this study, we select the widely adopted Stable Diffusion as our foundational model, which is also known as Latent Diffusion Models (LDM). The model operates the denoising process in the latent space of an autoencoder, namely ℰ(·) and 𝒟(·). During the training phase, an input image x0 is initially mapped to the latent space using a frozen encoder, yielding z0=ℰ(x0), then perturbed by a predefined Markov process:",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Diffusion models exhibit promising capabilities in image generation. In this study, we select the widely adopted Stable Diffusion as our foundational model, which is also known as Latent Diffusion Models (LDM). The model operates the denoising process in the latent space of an autoencoder, namely ℰ(·) and 𝒟(·). During the training phase, an input image x0 is initially mapped to the latent space using a frozen encoder, yielding z0=ℰ(x0), then perturbed by a predefined Markov process:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 22,
                                    "key": "doc/body/sec3/sub3/equation1",
                                    "block type": "equation",
                                    "content": "q(zt|zt1)=𝒩(zt; √(1βt)zt1, βtI)",
                                    "leftover": "q(zt|zt1)=𝒩(zt; √(1βt)zt1, βtI)",
                                    "matches": []
                                },
                                {
                                    "leaf id": 23,
                                    "key": "doc/body/sec3/sub3/txl2",
                                    "block type": "txl",
                                    "content": "For t=1,⋯, T, where T represents the number of steps in the forward diffusion process. The sequence of hyperparameters βt determines the noise strength at each step. The denoising UNet ϵθ is trained to approximate the reverse process q(zt1|zt). The training objective is expressed as:",
                                    "leftover": "For t=1,⋯, T, where T represents the number of steps in the forward diffusion process. The sequence of hyperparameters βt determines the noise strength at each step. The denoising UNet ϵθ is trained to approximate the reverse process q(zt1|zt). The training objective is expressed as:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec3/sub3/equation3",
                                    "block type": "equation",
                                    "content": "ℒ=𝔼ℰ(x0),ϵ∼𝒩(0, I), c, t[‖ϵϵθ(zt,c,t) ‖2^2]",
                                    "leftover": "ℒ=𝔼ℰ(x0),ϵ∼𝒩(0, I), c, t[‖ϵϵθ(zt,c,t) ‖2^2]",
                                    "matches": []
                                },
                                {
                                    "leaf id": 25,
                                    "key": "doc/body/sec3/sub3/txl4",
                                    "block type": "txl",
                                    "content": "Here, c denotes the conditioning texts. At the inference stage, Stable Diffusion effectively reconstructs an image from Gaussian noise step by step, predicting the noise added at each stage. The denoised results are then fed into a latent decoder 𝒟(·) to regenerate colored images from the latent representations, denoted as x̂0 = 𝒟(ẑ0).",
                                    "leftover": "Here, c denotes the conditioning texts. At the inference stage, Stable Diffusion effectively reconstructs an image from Gaussian noise step by step, predicting the noise added at each stage. The denoised results are then fed into a latent decoder 𝒟(·) to regenerate colored images from the latent representations, denoted as x̂0 = 𝒟(ẑ0).",
                                    "matches": []
                                },
                                {
                                    "leaf id": 26,
                                    "key": "doc/body/sec3/sub3/txl5",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Stable Diffusion employs a UNet architecture that consists of convolution layers and transformer attention blocks. In these attention mechanisms, selfattention layers are used to aggregate the spatial features of the image itself and crossattention layers are designed to query information from text embedding. The main difference is that the crossattention layer uses text features as keys and values, while in selfattention layers, image features with spatial dimensions serve as query, key, and value by themselves, preserving more freedom to represent spatially varying visual elements. The selfattention layer takes a feature map F of the image as input and computes the attention of the feature in location s with the entire feature map:",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Stable Diffusion employs a UNet architecture that consists of convolution layers and transformer attention blocks. In these attention mechanisms, selfattention layers are used to aggregate the spatial features of the image itself and crossattention layers are designed to query information from text embedding. The main difference is that the crossattention layer uses text features as keys and values, while in selfattention layers, image features with spatial dimensions serve as query, key, and value by themselves, preserving more freedom to represent spatially varying visual elements. The selfattention layer takes a feature map F of the image as input and computes the attention of the feature in location s with the entire feature map:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec3/sub3/equation6",
                                    "block type": "equation",
                                    "content": "F̃s = SoftMax( { Q(Fs)·K(F)^⊤ √(d) )·V(F)",
                                    "leftover": "F̃s = SoftMax( { Q(Fs)·K(F)^⊤ √(d) )·V(F)",
                                    "matches": []
                                },
                                {
                                    "leaf id": 28,
                                    "key": "doc/body/sec3/sub3/txl7",
                                    "block type": "txl",
                                    "content": "where Q,K,V are linear projection layers, F∈ℝ^(hw)× d is a flattened feature map obtained from the denoiser ϵθ, where d is the feature dimension, and h,w are intermediate spatial dimensions. Fs, F̃s is the input and output feature for location s respectively.",
                                    "leftover": "where Q,K,V are linear projection layers, F∈ℝ^(hw)× d is a flattened feature map obtained from the denoiser ϵθ, where d is the feature dimension, and h,w are intermediate spatial dimensions. Fs, F̃s is the input and output feature for location s respectively.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec3/sub3/txl8",
                                    "block type": "txl",
                                    "content": "Several works extend the selfattention layer to inject the reference image features, or generate stylealigned or subjectconsistent images, and demonstrate the effectiveness of this mechanism. Inspired by them, we extend the keys and values of the selfattention layer to multiple reference images and preserve the details of the referential appearance successfully.",
                                    "leftover": "Several works extend the selfattention layer to inject the reference image features, or generate stylealigned or subjectconsistent images, and demonstrate the effectiveness of this mechanism. Inspired by them, we extend the keys and values of the selfattention layer to multiple reference images and preserve the details of the referential appearance successfully.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 30,
                                    "key": "doc/body/sec3/sub4/tit",
                                    "block type": "title",
                                    "content": "Unified Reference Framework",
                                    "leftover": "Unified Reference Framework",
                                    "matches": []
                                },
                                {
                                    "leaf id": 31,
                                    "key": "doc/body/sec3/sub4/txl0",
                                    "block type": "txl",
                                    "content": "As demonstrated in Fig., our Parts2Whole consists of two branches: the reference branch used to encode multiple parts of human appearance, and the denoising branch, to gradually denoise the randomly sampled noise to finally obtain the image. The two branches utilize the same network architecture UNet, initialized with the pretrained weights of Stable Diffusion. In detail, our framework mainly consists of three crucial components: 1) SemanticAware Appearance Encoder, encoding the multiscale features of various human parts from reference images; 2) Shared SelfAttention, which obtains detailed information and spatial information by sharing keys and values in selfattention layers between denoising UNet and appearance encoder, and supports pose control by utilizing a lightweight pose encoder; 3) Enhanced MaskGuided Subject Selection, achieving precisely subject selection by explicitly introducing subject masks into the selfattention mechanism.",
                                    "leftover": "As demonstrated in Fig., our Parts2Whole consists of two branches: the reference branch used to encode multiple parts of human appearance, and the denoising branch, to gradually denoise the randomly sampled noise to finally obtain the image. The two branches utilize the same network architecture UNet, initialized with the pretrained weights of Stable Diffusion. In detail, our framework mainly consists of three crucial components: 1) SemanticAware Appearance Encoder, encoding the multiscale features of various human parts from reference images; 2) Shared SelfAttention, which obtains detailed information and spatial information by sharing keys and values in selfattention layers between denoising UNet and appearance encoder, and supports pose control by utilizing a lightweight pose encoder; 3) Enhanced MaskGuided Subject Selection, achieving precisely subject selection by explicitly introducing subject masks into the selfattention mechanism.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub4/figure1",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec3/sub4/figure1/cpt0",
                                            "block type": "cpt",
                                            "content": "Illustration of our MaskGuided Attention.",
                                            "leftover": "Illustration of our MaskGuided Attention.",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 33,
                                            "key": "doc/body/sec3/sub4/figure1/txl1",
                                            "block type": "txl",
                                            "content": "For each patch s (red point) on the feature map F^0, given subject masks M^1:N on the N reference images, we only attend patch s to features in these masks along with the patches on itself.}",
                                            "leftover": "For each patch s (red point) on the feature map F^0, given subject masks M^1:N on the N reference images, we only attend patch s to features in these masks along with the patches on itself.}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 34,
                                    "key": "doc/body/sec3/sub4/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In image conditioned generation tasks, previous work employs CLIP image encoder, or combined with some simple linear layers to encode reference images, thereby replacing the original text encoder in Stable Diffusion. However, such methods struggle to preserve appearance details due to the loss of spatial representations when encoding the reference images into semanticlevel features.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In image conditioned generation tasks, previous work employs CLIP image encoder, or combined with some simple linear layers to encode reference images, thereby replacing the original text encoder in Stable Diffusion. However, such methods struggle to preserve appearance details due to the loss of spatial representations when encoding the reference images into semanticlevel features.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 35,
                                    "key": "doc/body/sec3/sub4/txl3",
                                    "block type": "txl",
                                    "content": "Inspired by recent works on dense reference image conditioning, we propose a semanticaware appearance encoder with improved identity and details preservation. Specifically, we adopt a framework identical to the denoising UNet for the appearance encoder. Unlike the denoising branch, we do not add any noise to the reference images. Given N images capturing various parts of human appearance, we first compress them into latent features and then input them into the copied trainable UNet. Instead of simply piecing multiple reference images together, we pass the latent features of different parts through the appearance encoder one by one and provide a textual class label for each part. These text labels, such as face, hair, upper body clothes, etc., are converted into feature representations by CLIP text encoder and then injected into the appearance encoder through crossattention. This simple yet effective external condition provides a classifierlike guidance, which enables the encoder to have semantic awareness of different parts of the human appearance rather than simply performing operations such as image downsampling and upsampling. This helps produce results that are not only rich in detail, but also flexible and realistic.",
                                    "leftover": "Inspired by recent works on dense reference image conditioning, we propose a semanticaware appearance encoder with improved identity and details preservation. Specifically, we adopt a framework identical to the denoising UNet for the appearance encoder. Unlike the denoising branch, we do not add any noise to the reference images. Given N images capturing various parts of human appearance, we first compress them into latent features and then input them into the copied trainable UNet. Instead of simply piecing multiple reference images together, we pass the latent features of different parts through the appearance encoder one by one and provide a textual class label for each part. These text labels, such as face, hair, upper body clothes, etc., are converted into feature representations by CLIP text encoder and then injected into the appearance encoder through crossattention. This simple yet effective external condition provides a classifierlike guidance, which enables the encoder to have semantic awareness of different parts of the human appearance rather than simply performing operations such as image downsampling and upsampling. This helps produce results that are not only rich in detail, but also flexible and realistic.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 36,
                                    "key": "doc/body/sec3/sub4/txl4",
                                    "block type": "txl",
                                    "content": "In the encoding process, we set the timestep to 0 and only perform one processing instead of iterating successively, so it will not cause time burden at the inference stage. We cache the features before each selfattention layer for the next multiimage conditioned generation.",
                                    "leftover": "In the encoding process, we set the timestep to 0 and only perform one processing instead of iterating successively, so it will not cause time burden at the inference stage. We cache the features before each selfattention layer for the next multiimage conditioned generation.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 37,
                                    "key": "doc/body/sec3/sub4/txl5",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} After obtaining the multilayer feature maps of N reference images, we do not directly add them to the features in denoising UNet, but use shared keys and values in selfattention to achieve feature injection. This is because our reference and target images are not structurally aligned.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} After obtaining the multilayer feature maps of N reference images, we do not directly add them to the features in denoising UNet, but use shared keys and values in selfattention to achieve feature injection. This is because our reference and target images are not structurally aligned.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 38,
                                    "key": "doc/body/sec3/sub4/txl6",
                                    "block type": "txl",
                                    "content": "Take one certain selfattention layer as an example. Given the features of N reference images F^1:N and the feature maps F^0 in the denoising UNet, we concatenate the feature maps of them sidebyside as input to the selfattention layer, denoted as [F^0|F^1|⋯|F^N]. This allows each location s on F^0 to attend to all locations on itself and reference feature maps, calculated as:",
                                    "leftover": "Take one certain selfattention layer as an example. Given the features of N reference images F^1:N and the feature maps F^0 in the denoising UNet, we concatenate the feature maps of them sidebyside as input to the selfattention layer, denoted as [F^0|F^1|⋯|F^N]. This allows each location s on F^0 to attend to all locations on itself and reference feature maps, calculated as:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 39,
                                    "key": "doc/body/sec3/sub4/equation7",
                                    "block type": "equation",
                                    "content": "F̃s^0 = SoftMax( { Q(Fs^0)·K([F^0|F^1|⋯|F^N])^⊤ √(d) ) ·V([F^0|F^1|⋯|F^N])",
                                    "leftover": "F̃s^0 = SoftMax( { Q(Fs^0)·K([F^0|F^1|⋯|F^N])^⊤ √(d) ) ·V([F^0|F^1|⋯|F^N])",
                                    "matches": []
                                },
                                {
                                    "leaf id": 40,
                                    "key": "doc/body/sec3/sub4/txl8",
                                    "block type": "txl",
                                    "content": "We retain the crossattention layers in Stable Diffusion for injecting CLIP features of reference images and optional text input. We use the decoupled crossattention proposed by IPAdapter to support both images and text input. Specifically, feature maps F̃^0 obtained from shared selfattention serve as the origin of the query, and the reference image features and text features are each used as the key and value of the two crossattention. The final feature maps are the sum of the two crossattention outputs.",
                                    "leftover": "We retain the crossattention layers in Stable Diffusion for injecting CLIP features of reference images and optional text input. We use the decoupled crossattention proposed by IPAdapter to support both images and text input. Specifically, feature maps F̃^0 obtained from shared selfattention serve as the origin of the query, and the reference image features and text features are each used as the key and value of the two crossattention. The final feature maps are the sum of the two crossattention outputs.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 41,
                                    "key": "doc/body/sec3/sub4/txl9",
                                    "block type": "txl",
                                    "content": "To further enhance the controllability of the human image generation, we add the pose map as an additional control. We construct a tiny convolution network, which is similar to the condition embedding network in ControlNet, to extract the features of the pose map. The features are then added to the initial feature maps in the denoising UNet.",
                                    "leftover": "To further enhance the controllability of the human image generation, we add the pose map as an additional control. We construct a tiny convolution network, which is similar to the condition embedding network in ControlNet, to extract the features of the pose map. The features are then added to the initial feature maps in the denoising UNet.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 42,
                                    "key": "doc/body/sec3/sub4/txl10",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We find that the vanilla shared selfattention leads to interference from irrelevant subjects in the reference images (shown in the 6th column in Fig.), resulting in an unnatural appearance and background. To synthesize human images conditioned on specified parts from each reference image, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. Fig. presents this mechanism. Starting with a patch s on a feature map F^0 in the denoising UNet, and subject masks M^1:N on the N reference images. When computing the attention map between the one in the denoising UNet and those from the appearance encoder, patches that do not lie in these masks are ignored. Hence, the target patch s only has access to features of the subjects specified by masks in the reference images, thereby avoiding interference from other elements such as the background. The final formulation of the maskguided attention is defined as follows:",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} We find that the vanilla shared selfattention leads to interference from irrelevant subjects in the reference images (shown in the 6th column in Fig.), resulting in an unnatural appearance and background. To synthesize human images conditioned on specified parts from each reference image, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. Fig. presents this mechanism. Starting with a patch s on a feature map F^0 in the denoising UNet, and subject masks M^1:N on the N reference images. When computing the attention map between the one in the denoising UNet and those from the appearance encoder, patches that do not lie in these masks are ignored. Hence, the target patch s only has access to features of the subjects specified by masks in the reference images, thereby avoiding interference from other elements such as the background. The final formulation of the maskguided attention is defined as follows:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 43,
                                    "key": "doc/body/sec3/sub4/equation11",
                                    "block type": "equation",
                                    "content": "F̃s^0 = SoftMax( { Q(Fs^0)·K([F^0|⋯|F^NM^N])^⊤ √(d) ) ·V([F^0|⋯|F^NM^N])",
                                    "leftover": "F̃s^0 = SoftMax( { Q(Fs^0)·K([F^0|⋯|F^NM^N])^⊤ √(d) ) ·V([F^0|⋯|F^NM^N])",
                                    "matches": []
                                },
                                {
                                    "leaf id": 44,
                                    "key": "doc/body/sec3/sub4/txl12",
                                    "block type": "txl",
                                    "content": "In practice, to avoid misalignment between masks and original images caused by downsampling, a fullones convolutional kernel is applied to the mask before each attention layer, ensuring that the mask preserves critical regions. Overall, the maskguided attention enhances the ability of Parts2Whole to precisely extract the appearance of specified subjects in reference images.",
                                    "leftover": "In practice, to avoid misalignment between masks and original images caused by downsampling, a fullones convolutional kernel is applied to the mask before each attention layer, ensuring that the mask preserves critical regions. Overall, the maskguided attention enhances the ability of Parts2Whole to precisely extract the appearance of specified subjects in reference images.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 45,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Experiments",
                            "leftover": "Experiments",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec4/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 46,
                                    "key": "doc/body/sec4/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Qualitative results generated by Parts2Whole and existing alternatives on our partitioned test set. We do not show the text condition in the figure, but notably, when we input the reference images to our proposed appearance encoder, we will pass in short labels such as face, hair or headwear, upper body clothes, lower body clothes, whole body clothes, shoes, etc.",
                                    "leftover": "Qualitative results generated by Parts2Whole and existing alternatives on our partitioned test set. We do not show the text condition in the figure, but notably, when we input the reference images to our proposed appearance encoder, we will pass in short labels such as face, hair or headwear, upper body clothes, lower body clothes, whole body clothes, shoes, etc.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 47,
                                    "key": "doc/body/sec4/sub1/tit",
                                    "block type": "title",
                                    "content": "Implementation Details",
                                    "leftover": "Implementation Details",
                                    "matches": []
                                },
                                {
                                    "leaf id": 48,
                                    "key": "doc/body/sec4/sub1/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} To train the Parts2Whole model, we build a multimodal dataset comprising about 41,500 referencetarget pairs from the opensource DeepFashionMultiModal dataset. Each pair in this newly constructed dataset includes multiple reference images, which encompass human pose images (e.g., OpenPose, Human Parsing, DensePose), various aspects of human appearance (e.g., hair, face, clothes, shoes) with their short textual labels, and a target image featuring the same individual (ID) in the same outfit but in a different pose, along with textual captions.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} To train the Parts2Whole model, we build a multimodal dataset comprising about 41,500 referencetarget pairs from the opensource DeepFashionMultiModal dataset. Each pair in this newly constructed dataset includes multiple reference images, which encompass human pose images (e.g., OpenPose, Human Parsing, DensePose), various aspects of human appearance (e.g., hair, face, clothes, shoes) with their short textual labels, and a target image featuring the same individual (ID) in the same outfit but in a different pose, along with textual captions.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 49,
                                    "key": "doc/body/sec4/sub1/txl1",
                                    "block type": "txl",
                                    "content": "The DeepFashionMultiModal dataset exhibits noise in its ID data. For example, different images are tagged with the same ID but depict different individuals. To address this issue, we first cleanse the IDs by extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs to distinguish between different ID images within the same ID group. Subsequently, we utilize DWPose to generate pose images corresponding to each image. Guided by human parsing files, we crop human images into various parts. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images. Textual descriptions of the original dataset are used as captions. For constructing pairs, we select images with cleaned IDs that feature the same clothes and individual but in different poses. Specifically, a pair contains multiple parts from one human image as reference images, and an image of the person in another pose as the target. Finally, we build a total of about 41,500 pairs, of which the training set is about 40,000 and the test set is about 1,500 pairs.",
                                    "leftover": "The DeepFashionMultiModal dataset exhibits noise in its ID data. For example, different images are tagged with the same ID but depict different individuals. To address this issue, we first cleanse the IDs by extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs to distinguish between different ID images within the same ID group. Subsequently, we utilize DWPose to generate pose images corresponding to each image. Guided by human parsing files, we crop human images into various parts. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images. Textual descriptions of the original dataset are used as captions. For constructing pairs, we select images with cleaned IDs that feature the same clothes and individual but in different poses. Specifically, a pair contains multiple parts from one human image as reference images, and an image of the person in another pose as the target. Finally, we build a total of about 41,500 pairs, of which the training set is about 40,000 and the test set is about 1,500 pairs.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 50,
                                    "key": "doc/body/sec4/sub1/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In this work, the denoising UNet and the appearance encoder both leverage the pretrained weights from Stable Diffusion1.5. We use CLIP Vision Model with projection layers as our image encoder, initialized with Stable Diffusion Image Variations. During training, we set the initial learning rate 1e5 with a batch size of 64. The model is trained using 8 A800 GPUs, for a total of 30000 iterations. To maintain the capability of image generation, we randomly drop all of the reference image features and the pose condition with a probability of 0.2. At the same time, to improve the flexibility of generation, we randomly drop each appearance condition with a probability of 0.2, so that the human images can be generating from indefinite reference images. At the inference stage, we adopt DDIM sampler with 50 steps, and set the guidance scale to 7.5.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In this work, the denoising UNet and the appearance encoder both leverage the pretrained weights from Stable Diffusion1.5. We use CLIP Vision Model with projection layers as our image encoder, initialized with Stable Diffusion Image Variations. During training, we set the initial learning rate 1e5 with a batch size of 64. The model is trained using 8 A800 GPUs, for a total of 30000 iterations. To maintain the capability of image generation, we randomly drop all of the reference image features and the pose condition with a probability of 0.2. At the same time, to improve the flexibility of generation, we randomly drop each appearance condition with a probability of 0.2, so that the human images can be generating from indefinite reference images. At the inference stage, we adopt DDIM sampler with 50 steps, and set the guidance scale to 7.5.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 51,
                                    "key": "doc/body/sec4/sub2/tit",
                                    "block type": "title",
                                    "content": "Comparison with Existing Alternatives",
                                    "leftover": "Comparison with Existing Alternatives",
                                    "matches": []
                                },
                                {
                                    "leaf id": 52,
                                    "key": "doc/body/sec4/sub2/txl0",
                                    "block type": "txl",
                                    "content": "Our Parts2Whole targets at controllable human image generation conditioned on multiple parts of human appearance. To evaluate the performance of our proposed framework, we compare our Parts2Whole with existing subjectdriven solutions. For fairness, we make some improvements to the methods, to make them more suitable for generating human images from multiple conditions.",
                                    "leftover": "Our Parts2Whole targets at controllable human image generation conditioned on multiple parts of human appearance. To evaluate the performance of our proposed framework, we compare our Parts2Whole with existing subjectdriven solutions. For fairness, we make some improvements to the methods, to make them more suitable for generating human images from multiple conditions.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 53,
                                    "key": "doc/body/sec4/sub2/txl1",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningbased methods, we adopt DreamBooth LoRA and Custom Diffusion as baseline methods for comparison, as these methods are relatively robust and effective. DreamBooth LoRA inserts a smaller number of new weights into Stable Diffusion and only trains these parameters on just a few images of a subject or style, thereby associating a special word in the prompt with the example images. Custom Diffusion finetunes only key and value projection matrices in the crossattention layers to customize texttoimage models. Given as input several aspects of human appearance, we use these two methods to finetune Stable Diffusion, such that it learns to bind identifiers with specific human parts. As shown in, when it comes to multiaspect composition, the attributes of different parts in images generated by these tuningbase methods mix together, resulting in unrealistic human images. In contrast, Parts2Whole generates highfidelity results without the need for parameter tuning.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningbased methods, we adopt DreamBooth LoRA and Custom Diffusion as baseline methods for comparison, as these methods are relatively robust and effective. DreamBooth LoRA inserts a smaller number of new weights into Stable Diffusion and only trains these parameters on just a few images of a subject or style, thereby associating a special word in the prompt with the example images. Custom Diffusion finetunes only key and value projection matrices in the crossattention layers to customize texttoimage models. Given as input several aspects of human appearance, we use these two methods to finetune Stable Diffusion, such that it learns to bind identifiers with specific human parts. As shown in, when it comes to multiaspect composition, the attributes of different parts in images generated by these tuningbase methods mix together, resulting in unrealistic human images. In contrast, Parts2Whole generates highfidelity results without the need for parameter tuning.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec4/sub2/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningfree methods, we adopt IPAdapter and SSREncoder for comparison. IPAdapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting, and can be combined with other adapters like ControlNet. We firstly use IPAdapter FaceID and ControlNet to generate human images from facial appearance and pose maps. Then we repaint hair, clothes, shoes and other areas using the specific image step by step, thereby achieving multiimage conditioned generation of portraits in a multistep way. SSREncoder is an effective encoder designed for selectively capturing any subject from single or multiple reference images by the text query or mask query. For fairness, we finetune it in our human dataset to enhance its ability for human images.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningfree methods, we adopt IPAdapter and SSREncoder for comparison. IPAdapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting, and can be combined with other adapters like ControlNet. We firstly use IPAdapter FaceID and ControlNet to generate human images from facial appearance and pose maps. Then we repaint hair, clothes, shoes and other areas using the specific image step by step, thereby achieving multiimage conditioned generation of portraits in a multistep way. SSREncoder is an effective encoder designed for selectively capturing any subject from single or multiple reference images by the text query or mask query. For fairness, we finetune it in our human dataset to enhance its ability for human images.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 55,
                                    "key": "doc/body/sec4/sub2/txl3",
                                    "block type": "txl",
                                    "content": "We compare our Parts2Whole with the above two referencebased alternatives in the test set. For quantitative comparison, we compute the commonly used CLIP score and DINO score to evaluate the similarity between the generated image and the specified human parts. For further alignment evaluation, we use DreamSim, a new metric for perceptual image similarity that bridges the gap between ''lowlevel'' metrics (e.g., LPIPS, PSNR, SSIM) and ''highlevel'' measures (e.g., CLIP). Since the generated image is conditioned on multiple parts of human appearance, it is difficult to evaluate the degree of alignment by calculating the average metric with these multiple images. Therefore, we calculate the above three indicators between the output image and the original reference portrait from which these different parts come. We present the quantitative results in and the qualitative results in . Both IPAdapter and SSREncoder fail to maintain alignment with the specified appearance images and often produce unrealistic results when multipart combinations are involved. In comparison, our method achieves the best results in terms of image quality and appearance alignment.",
                                    "leftover": "We compare our Parts2Whole with the above two referencebased alternatives in the test set. For quantitative comparison, we compute the commonly used CLIP score and DINO score to evaluate the similarity between the generated image and the specified human parts. For further alignment evaluation, we use DreamSim, a new metric for perceptual image similarity that bridges the gap between ''lowlevel'' metrics (e.g., LPIPS, PSNR, SSIM) and ''highlevel'' measures (e.g., CLIP). Since the generated image is conditioned on multiple parts of human appearance, it is difficult to evaluate the degree of alignment by calculating the average metric with these multiple images. Therefore, we calculate the above three indicators between the output image and the original reference portrait from which these different parts come. We present the quantitative results in and the qualitative results in . Both IPAdapter and SSREncoder fail to maintain alignment with the specified appearance images and often produce unrealistic results when multipart combinations are involved. In comparison, our method achieves the best results in terms of image quality and appearance alignment.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 56,
                                                    "key": "doc/body/sec4/sub2/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Quantitative comparison between our Parts2Whole and existing referencebased alternatives.",
                                                    "leftover": "Quantitative comparison between our Parts2Whole and existing referencebased alternatives.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 57,
                                            "key": "doc/body/sec4/sub2/table4/tabular1",
                                            "block type": "tabular",
                                            "content": "{@{}lccc@{}} Method & CLIP↑ & DINO↑ & DreamSim↓ IPAdapter + Inpaint & 80.1 & 69.8 & 0.445 SSREncoder & 86.9 & 75.1 & 0.346 Parts2Whole (Ours) & 91.2}& 93.7}& 0.221}",
                                            "leftover": "{@{}lccc@{}} Method & CLIP↑ & DINO↑ & DreamSim↓ IPAdapter + Inpaint & 80.1 & 69.8 & 0.445 SSREncoder & 86.9 & 75.1 & 0.346 Parts2Whole (Ours) & 91.2}& 93.7}& 0.221}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 58,
                                    "key": "doc/body/sec4/sub2/txl5",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We conduct a user study to further evaluate the referencebased methods IPAdapter, SSREncoder and our Parts2Whole. We randomly select 20 pairs of referencetarget pairs from the test set. For each pair, we provide multiple referential appearance images, pose images, textual captions, and the generated human images. We evaluate the performance from two main aspects: first, the quality}of the generated images, which primarily refers to the realism, rationality, and clarity of the images; and second, the similarity}between the generated images and the reference images. The similarity assessment includes consistency in ID, pose, texture, and color between the generated images and the reference images. We involve 20 users in the user study, who are required to score the three methods based on these two evaluative aspects. The final experimental results are shown in, from which we observe that our model owns obvious superiorities for alignment with given appearance conditions.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} We conduct a user study to further evaluate the referencebased methods IPAdapter, SSREncoder and our Parts2Whole. We randomly select 20 pairs of referencetarget pairs from the test set. For each pair, we provide multiple referential appearance images, pose images, textual captions, and the generated human images. We evaluate the performance from two main aspects: first, the quality}of the generated images, which primarily refers to the realism, rationality, and clarity of the images; and second, the similarity}between the generated images and the reference images. The similarity assessment includes consistency in ID, pose, texture, and color between the generated images and the reference images. We involve 20 users in the user study, who are required to score the three methods based on these two evaluative aspects. The final experimental results are shown in, from which we observe that our model owns obvious superiorities for alignment with given appearance conditions.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table6",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table6/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 59,
                                                    "key": "doc/body/sec4/sub2/table6/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "User study on the comparison with existing referencebased alternatives. ''Quality'' and ''Similarity'' measures synthesis quality and appearance preservation. Each metric is rated from 1 (worst) to 5 (best).",
                                                    "leftover": "User study on the comparison with existing referencebased alternatives. ''Quality'' and ''Similarity'' measures synthesis quality and appearance preservation. Each metric is rated from 1 (worst) to 5 (best).",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 60,
                                            "key": "doc/body/sec4/sub2/table6/tabular1",
                                            "block type": "tabular",
                                            "content": "{@{}lcccc@{}} Method & Quality↑ & Similarity↑ IPAdapter + Inpaint & 3.78 & 3.58 SSREncoder & 3.64 & 3.14 Parts2Whole (Ours) & 4.52}& 4.55}",
                                            "leftover": "{@{}lcccc@{}} Method & Quality↑ & Similarity↑ IPAdapter + Inpaint & 3.78 & 3.58 SSREncoder & 3.64 & 3.14 Parts2Whole (Ours) & 4.52}& 4.55}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 61,
                                    "key": "doc/body/sec4/sub3/tit",
                                    "block type": "title",
                                    "content": "Ablation Studies",
                                    "leftover": "Ablation Studies",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub3/figure*0",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 62,
                                            "key": "doc/body/sec4/sub3/figure*0/cpt0",
                                            "block type": "cpt",
                                            "content": "Qualitative analysis of using different backbones for the appearance encoder, and our proposed methods.",
                                            "leftover": "Qualitative analysis of using different backbones for the appearance encoder, and our proposed methods.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 63,
                                    "key": "doc/body/sec4/sub3/txl1",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} As described in Sec., to extract detailed features from multiple reference images, our Parts2Whole designs an appearance encoder by copying the network structure and pretrained weights from the denoising UNet. Here, we compare it to the baseline with other image encoders. Specifically, we leverage the CLIP image encoder, DINOv2, and ControlNet as feature extractors and apply the same training settings for fair comparison. The qualitative results of generated human images are presented in . From the second and third columns of the figure, we observe that these semanticlevel feature extractors cannot preserve the appearance details of multiple reference images and only extract color and rough texture. ControlNet directly adds different image features with misaligned structures to the feature maps, resulting in unstable image quality. In contrast, our proposed appearance encoder provides finegrained details of multiple aspects of human appearance.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} As described in Sec., to extract detailed features from multiple reference images, our Parts2Whole designs an appearance encoder by copying the network structure and pretrained weights from the denoising UNet. Here, we compare it to the baseline with other image encoders. Specifically, we leverage the CLIP image encoder, DINOv2, and ControlNet as feature extractors and apply the same training settings for fair comparison. The qualitative results of generated human images are presented in . From the second and third columns of the figure, we observe that these semanticlevel feature extractors cannot preserve the appearance details of multiple reference images and only extract color and rough texture. ControlNet directly adds different image features with misaligned structures to the feature maps, resulting in unstable image quality. In contrast, our proposed appearance encoder provides finegrained details of multiple aspects of human appearance.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 64,
                                    "key": "doc/body/sec4/sub3/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In the process of encoding multiple reference image features, we provide a textual class label for each aspect of human appearance, thus providing a classifierlike guidance. To assess the effectiveness of the additional external condition, we compare it with directly concatenating multiple reference images in the dimension of width as input to the appearance encoder. As shown in the 5th column in, simply piecing reference images produces images relatively aligned with the given images, but leads to stifflooking and unrealistic results. This is because modeling only the image itself makes the model lack awareness of different types of appearances. Conversely, after injecting different semantic labels for each reference image, the model has an awareness of various parts of the human appearance, producing realistic and flexible portraits.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In the process of encoding multiple reference image features, we provide a textual class label for each aspect of human appearance, thus providing a classifierlike guidance. To assess the effectiveness of the additional external condition, we compare it with directly concatenating multiple reference images in the dimension of width as input to the appearance encoder. As shown in the 5th column in, simply piecing reference images produces images relatively aligned with the given images, but leads to stifflooking and unrealistic results. This is because modeling only the image itself makes the model lack awareness of different types of appearances. Conversely, after injecting different semantic labels for each reference image, the model has an awareness of various parts of the human appearance, producing realistic and flexible portraits.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 65,
                                    "key": "doc/body/sec4/sub3/txl3",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} To precisely select subjects from multiple reference images, we introduce the subject masks into the shared selfattention mechanism. To evaluate the effectiveness of our proposed maskguided attention, we compare it with that without masks. When not using subject masks, the image is generated with reference to all patches of the conditional images, including the unexpected background or other parts. As shown in, due to the generation being interfered with by irrelevant subjects, the model produces homogeneous colors or appears with unexpected backgrounds or subjects. On the contrary, with the support of maskguided attention, Parts2Whole accurately refers to the appearance of the specified parts to generate real human images.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} To precisely select subjects from multiple reference images, we introduce the subject masks into the shared selfattention mechanism. To evaluate the effectiveness of our proposed maskguided attention, we compare it with that without masks. When not using subject masks, the image is generated with reference to all patches of the conditional images, including the unexpected background or other parts. As shown in, due to the generation being interfered with by irrelevant subjects, the model produces homogeneous colors or appears with unexpected backgrounds or subjects. On the contrary, with the support of maskguided attention, Parts2Whole accurately refers to the appearance of the specified parts to generate real human images.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub3/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub3/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 66,
                                                    "key": "doc/body/sec4/sub3/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Quantitative analysis of using semanticaware encoder and maskguided subject selection.",
                                                    "leftover": "Quantitative analysis of using semanticaware encoder and maskguided subject selection.",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 67,
                                            "key": "doc/body/sec4/sub3/table4/tabular1",
                                            "block type": "tabular",
                                            "content": "{@{}lcccc@{}} Method & CLIP↑ & DINO↑ & DreamSim↓ & FID↓ w/o text labels & 90.1 & 91.9 & 0.248 & 23.95 w/o mask & 90.8 & 91.6 & 0.243 & 19.79 Parts2Whole & 91.2}& 93.7}& 0.221}& 17.29}",
                                            "leftover": "{@{}lcccc@{}} Method & CLIP↑ & DINO↑ & DreamSim↓ & FID↓ w/o text labels & 90.1 & 91.9 & 0.248 & 23.95 w/o mask & 90.8 & 91.6 & 0.243 & 19.79 Parts2Whole & 91.2}& 93.7}& 0.221}& 17.29}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/figure*5",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 68,
                                            "key": "doc/body/sec4/sub3/figure*5/cpt0",
                                            "block type": "cpt",
                                            "content": "The generated results from combinations of a different number of conditions.",
                                            "leftover": "The generated results from combinations of a different number of conditions.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 69,
                                    "key": "doc/body/sec4/sub4/tit",
                                    "block type": "title",
                                    "content": "More Results",
                                    "leftover": "More Results",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub4/figure*0",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 70,
                                            "key": "doc/body/sec4/sub4/figure*0/cpt0",
                                            "block type": "cpt",
                                            "content": "Results of image generation using selected parts from different individuals as control conditions.",
                                            "leftover": "Results of image generation using selected parts from different individuals as control conditions.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 71,
                                    "key": "doc/body/sec4/sub4/txl1",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is able to generate human images from varying numbers of condition images, such as single hair or face input, or arbitrary combinations like ''Face + Hair'', ''Face + Clothes'', and ''Upper body clothes + Lower body clothes''. The experimental results are presented in . The generated results under different control condition combinations still maintain high quality and realism. This flexibility enables our method to have broader application.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is able to generate human images from varying numbers of condition images, such as single hair or face input, or arbitrary combinations like ''Face + Hair'', ''Face + Clothes'', and ''Upper body clothes + Lower body clothes''. The experimental results are presented in . The generated results under different control condition combinations still maintain high quality and realism. This flexibility enables our method to have broader application.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 72,
                                    "key": "doc/body/sec4/sub4/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We select various parts from different human images to serve as conditional images. For example, the face from person A, the hair or headwear from person B, the upper clothes from person C, and the lower clothes from person D. These parts are collectively used as control conditions for generation. The experimental results, as shown in, demonstrate that our method not only accurately maps different parts of the reference image to the corresponding regions in the target image but also effectively preserves the details of the conditions, producing realistic images.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} We select various parts from different human images to serve as conditional images. For example, the face from person A, the hair or headwear from person B, the upper clothes from person C, and the lower clothes from person D. These parts are collectively used as control conditions for generation. The experimental results, as shown in, demonstrate that our method not only accurately maps different parts of the reference image to the corresponding regions in the target image but also effectively preserves the details of the conditions, producing realistic images.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 73,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Conclusion",
                            "leftover": "Conclusion",
                            "matches": []
                        },
                        {
                            "leaf id": 74,
                            "key": "doc/body/sec5/txl0",
                            "block type": "txl",
                            "content": "In this work, we propose Parts2Whole, a novel framework for controllable human image generation conditioned on multiple reference images, including various aspects of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Based on a dual UNet design, we develop a semanticaware appearance encoder to process each condition image with its label into multiscale feature maps and inject those detailrich reference features into the generation via a shared selfattention mechanism. This design retains details from multiple references and looks very good. We also enhance vanilla selfattention by incorporating subject masks, enabling Parts2Whole to synthesize human images from specified parts from condition images. Extensive experiments demonstrate that our Parts2Whole performs well in terms of image quality and condition alignment.",
                            "leftover": "In this work, we propose Parts2Whole, a novel framework for controllable human image generation conditioned on multiple reference images, including various aspects of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Based on a dual UNet design, we develop a semanticaware appearance encoder to process each condition image with its label into multiscale feature maps and inject those detailrich reference features into the generation via a shared selfattention mechanism. This design retains details from multiple references and looks very good. We also enhance vanilla selfattention by incorporating subject masks, enabling Parts2Whole to synthesize human images from specified parts from condition images. Extensive experiments demonstrate that our Parts2Whole performs well in terms of image quality and condition alignment.",
                            "matches": []
                        },
                        {
                            "leaf id": 75,
                            "key": "doc/body/sec5/txl1",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is currently trained at the resolution of 512, which may cause artifacts in some generated results. This could be improved by using higher resolutions and larger diffusion models like SDXL as our backbone. Furthermore, it will be valuable to achieve the tryon of layerwise clothing based on our Parts2Whole.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is currently trained at the resolution of 512, which may cause artifacts in some generated results. This could be improved by using higher resolutions and larger diffusion models like SDXL as our backbone. Furthermore, it will be valuable to achieve the tryon of layerwise clothing based on our Parts2Whole.",
                            "matches": []
                        },
                        {
                            "leaf id": 76,
                            "key": "doc/body/sec5/txl2",
                            "block type": "txl",
                            "content": "{ \\small \\bibliographystyle{ieeenatfullname} \\bibliographyarxiv } \\clearpage \\setcounterpage{1}",
                            "leftover": "{ \\small \\bibliographystyle{ieeenatfullname} \\bibliographyarxiv } \\clearpage \\setcounterpage{1}",
                            "matches": []
                        },
                        {
                            "leaf id": 77,
                            "key": "doc/body/sec5/txl3",
                            "block type": "txl",
                            "content": "supplementary",
                            "leftover": "supplementary",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 78,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Dataset",
                            "leftover": "Dataset",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec6/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 79,
                                    "key": "doc/body/sec6/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "A sample of referencetarget image pair in our dataset. Reference Human Part images are obtained from the reference image based on the human parsing image, while the pose and text description are descriptions of the target image.",
                                    "leftover": "A sample of referencetarget image pair in our dataset. Reference Human Part images are obtained from the reference image based on the human parsing image, while the pose and text description are descriptions of the target image.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/figure*1",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 80,
                                    "key": "doc/body/sec6/figure*1/cpt0",
                                    "block type": "cpt",
                                    "content": "Generated human images with OpenPose as pose condition by Parts2Whole. The upper row displays multiple reference human parts, while the lower row shows the results generated by our Parts2Whole method under the control of reference images and OpenPose image.",
                                    "leftover": "Generated human images with OpenPose as pose condition by Parts2Whole. The upper row displays multiple reference human parts, while the lower row shows the results generated by our Parts2Whole method under the control of reference images and OpenPose image.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 81,
                            "key": "doc/body/sec6/txl2",
                            "block type": "txl",
                            "content": "Generating the human body from multiple conditional parts is a significant undertaking, but lacking a directly available dataset. The datasets related to this task, such as those for Virtual Tryon, primarily suffer from a lack of multiple controllable conditions and are often limited to single control conditions (clothing). Issues with these datasets include a limited variety of clothing types, absence of facial data, low resolution, and lack of textual captions. The DeepFashionMultiModal dataset aligns more closely with our task as it includes a vast array of human body images, the same person and same clothes in different poses, and precise human parsing labels. However, this dataset cannot be used directly and requires data cleansing and further postprocessing.",
                            "leftover": "Generating the human body from multiple conditional parts is a significant undertaking, but lacking a directly available dataset. The datasets related to this task, such as those for Virtual Tryon, primarily suffer from a lack of multiple controllable conditions and are often limited to single control conditions (clothing). Issues with these datasets include a limited variety of clothing types, absence of facial data, low resolution, and lack of textual captions. The DeepFashionMultiModal dataset aligns more closely with our task as it includes a vast array of human body images, the same person and same clothes in different poses, and precise human parsing labels. However, this dataset cannot be used directly and requires data cleansing and further postprocessing.",
                            "matches": []
                        },
                        {
                            "leaf id": 82,
                            "key": "doc/body/sec6/txl3",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In the DeepFashionMultiModal dataset, there is some confusion with IDs where images of different individuals are mistakenly labeled under the same ID. We start by cleansing these IDs, extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs, allowing us to reclassify IDs within the same ID group. After cleansing, images from the same ID and the same clothes are selected if there are two or more images available.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} In the DeepFashionMultiModal dataset, there is some confusion with IDs where images of different individuals are mistakenly labeled under the same ID. We start by cleansing these IDs, extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs, allowing us to reclassify IDs within the same ID group. After cleansing, images from the same ID and the same clothes are selected if there are two or more images available.",
                            "matches": []
                        },
                        {
                            "leaf id": 83,
                            "key": "doc/body/sec6/txl4",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We use images with human parsing labels from the dataset as reference images. Target images are then selected from the same ID and clothing, creating pairs with the reference image.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} We use images with human parsing labels from the dataset as reference images. Target images are then selected from the same ID and clothing, creating pairs with the reference image.",
                            "matches": []
                        },
                        {
                            "leaf id": 84,
                            "key": "doc/body/sec6/txl5",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We crop the images according to the provided human parsing labels. Specifically, we divide the human image into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each part is cropped according to the human parsing labels to obtain the crop image and corresponding mask image. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} We crop the images according to the provided human parsing labels. Specifically, we divide the human image into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each part is cropped according to the human parsing labels to obtain the crop image and corresponding mask image. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images.",
                            "matches": []
                        },
                        {
                            "leaf id": 85,
                            "key": "doc/body/sec6/txl6",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Based on the reference human parts, we need to generate images that resemble the target image, requiring a description of the target image. The description is divided into two parts: one for the human body's pose and another for the target image's textual description. For pose information, we utilize DWPose to generate pose images corresponding to each image, and for DensePose, we use the provided DensePose files from the dataset. The textual description for each image is taken directly from the dataset's accompanying text description.",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Based on the reference human parts, we need to generate images that resemble the target image, requiring a description of the target image. The description is divided into two parts: one for the human body's pose and another for the target image's textual description. For pose information, we utilize DWPose to generate pose images corresponding to each image, and for DensePose, we use the provided DensePose files from the dataset. The textual description for each image is taken directly from the dataset's accompanying text description.",
                            "matches": []
                        },
                        {
                            "leaf id": 86,
                            "key": "doc/body/sec6/txl7",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Finally, we have constructed a multimodal dataset with approximately 41,500 referencetarget pairs derived from the opensource DeepFashionMultiModal dataset. The controllable conditions for each pair are categorized into two main types. The first type is the appearance reference image, which is subdivided into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each image is accompanied by a corresponding mask and has undergone superresolution processing. These data elements are sourced from the original reference image. The second type is the target description, primarily consisting of pose and text description. The pose is further divided into OpenPose and DensePose, all of which are derived from the target image. A sample of referencetarget image pair in our dataset is shown in Fig..",
                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} Finally, we have constructed a multimodal dataset with approximately 41,500 referencetarget pairs derived from the opensource DeepFashionMultiModal dataset. The controllable conditions for each pair are categorized into two main types. The first type is the appearance reference image, which is subdivided into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each image is accompanied by a corresponding mask and has undergone superresolution processing. These data elements are sourced from the original reference image. The second type is the target description, primarily consisting of pose and text description. The pose is further divided into OpenPose and DensePose, all of which are derived from the target image. A sample of referencetarget image pair in our dataset is shown in Fig..",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec7",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 87,
                            "key": "doc/body/sec7/tit",
                            "block type": "title",
                            "content": "Different Types of Pose Maps",
                            "leftover": "Different Types of Pose Maps",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec7/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 88,
                                    "key": "doc/body/sec7/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Results of singlecondition and multicondition generation. The top three rows labeled ''face'', ''upper body clothes'', ''lower body clothes'' represent separate conditions. The fourth row demonstrates joint control under multipart conditions.",
                                    "leftover": "Results of singlecondition and multicondition generation. The top three rows labeled ''face'', ''upper body clothes'', ''lower body clothes'' represent separate conditions. The fourth row demonstrates joint control under multipart conditions.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 89,
                            "key": "doc/body/sec7/txl1",
                            "block type": "txl",
                            "content": "To show the ability of Parts2Whole to generate human image conditions on different types of pose maps, we train a new Parts2Whole model but with OpenPose}as a condition. As shown in, the generated images strictly maintain consistency with the target pose, and each body part retains the appearance information from the reference images.",
                            "leftover": "To show the ability of Parts2Whole to generate human image conditions on different types of pose maps, we train a new Parts2Whole model but with OpenPose}as a condition. As shown in, the generated images strictly maintain consistency with the target pose, and each body part retains the appearance information from the reference images.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec8",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 90,
                            "key": "doc/body/sec8/tit",
                            "block type": "title",
                            "content": "Discussion about Existing Methods",
                            "leftover": "Discussion about Existing Methods",
                            "matches": []
                        },
                        {
                            "leaf id": 91,
                            "key": "doc/body/sec8/txl0",
                            "block type": "txl",
                            "content": "In our main text, we conduct experiments with both tuningbased methods DreamBooth LoRA and Custom Diffusion and tuningfree methods such as IPAdapter and SSREncoder. The results of these experiments are further illustrated in Fig, which showcases the performance of these methods conditioned on both a single image and multiple images.",
                            "leftover": "In our main text, we conduct experiments with both tuningbased methods DreamBooth LoRA and Custom Diffusion and tuningfree methods such as IPAdapter and SSREncoder. The results of these experiments are further illustrated in Fig, which showcases the performance of these methods conditioned on both a single image and multiple images.",
                            "matches": []
                        },
                        {
                            "leaf id": 92,
                            "key": "doc/body/sec8/txl1",
                            "block type": "txl",
                            "content": "The results demonstrate that while these methods generally perform well under a single control condition, they exhibit significant issues when multiple conditions are applied. For example, DreamBooth LoRA encountered cases where pants were omitted, and both Custom Diffusion and SSREncoder show alterations in facial ID. The phenomenon observed is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. For instance, methods like SSREncoder, Custom Diffusion, and IPAdapter incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g. semantic features) and utilize the crossattention keys (K) and values (V) from them rather than from image dimensional}feature maps. In this process, the correlation between the reference images and the target image loses the spatial relationship}of the original image dimensions. It is difficult for these methods to effectively model the attention from various conditional feature maps at different locations in the target image, resulting in a mixture of attributes from different subjects.",
                            "leftover": "The results demonstrate that while these methods generally perform well under a single control condition, they exhibit significant issues when multiple conditions are applied. For example, DreamBooth LoRA encountered cases where pants were omitted, and both Custom Diffusion and SSREncoder show alterations in facial ID. The phenomenon observed is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. For instance, methods like SSREncoder, Custom Diffusion, and IPAdapter incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g. semantic features) and utilize the crossattention keys (K) and values (V) from them rather than from image dimensional}feature maps. In this process, the correlation between the reference images and the target image loses the spatial relationship}of the original image dimensions. It is difficult for these methods to effectively model the attention from various conditional feature maps at different locations in the target image, resulting in a mixture of attributes from different subjects.",
                            "matches": []
                        },
                        {
                            "leaf id": 93,
                            "key": "doc/body/sec8/txl2",
                            "block type": "txl",
                            "content": "Conversely, our Parts2Whole model employs shared selfattention between the reference features and the feature maps in the Denoising UNet, executed on the image dimension. This allows our model to establish a more precise correlation between different condition images and distinct positions within the feature maps, thereby generating results that are consistent with the detailed attributes of multicondition images.",
                            "leftover": "Conversely, our Parts2Whole model employs shared selfattention between the reference features and the feature maps in the Denoising UNet, executed on the image dimension. This allows our model to establish a more precise correlation between different condition images and distinct positions within the feature maps, thereby generating results that are consistent with the detailed attributes of multicondition images.",
                            "matches": []
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 94,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuningfree mutual selfattention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023.",
            "leftover": "Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuningfree mutual selfattention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023.",
            "matches": []
        },
        {
            "leaf id": 95,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al. Muse: Texttoimage generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.",
            "leftover": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al. Muse: Texttoimage generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.",
            "matches": []
        },
        {
            "leaf id": 96,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot objectlevel image customization. arXiv preprint arXiv:2307.09481, 2023.",
            "leftover": "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot objectlevel image customization. arXiv preprint arXiv:2307.09481, 2023.",
            "matches": []
        },
        {
            "leaf id": 97,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Vitonhd: Highresolution virtual tryon via misalignmentaware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021.",
            "leftover": "Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Vitonhd: Highresolution virtual tryon via misalignmentaware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021.",
            "matches": []
        },
        {
            "leaf id": 98,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Jiankang Deng, Jia Guo, et~al. {InsightFace: 2D and 3D Face Analysis Project}. https://github.com/deepinsight/insightface, 2018. Accessed: 20240411.",
            "leftover": "Jiankang Deng, Jia Guo, et~al. {InsightFace: 2D and 3D Face Analysis Project}. https://github.com/deepinsight/insightface, 2018. Accessed: 20240411.",
            "matches": []
        },
        {
            "leaf id": 99,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019.",
            "leftover": "Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019.",
            "matches": []
        },
        {
            "leaf id": 100,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:\\penalty0 87808794, 2021.",
            "leftover": "Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:\\penalty0 87808794, 2021.",
            "matches": []
        },
        {
            "leaf id": 101,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data, 2023.",
            "leftover": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data, 2023.",
            "matches": []
        },
        {
            "leaf id": 102,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing texttoimage generation using textual inversion, 2022.",
            "leftover": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing texttoimage generation using textual inversion, 2022.",
            "matches": []
        },
        {
            "leaf id": 103,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Rinon Gal, Moab Arar, Yuval Atzmon, Amit~H Bermano, Gal Chechik, and Daniel CohenOr. Encoderbased domain tuning for fast personalization of texttoimage models. ACM Transactions on Graphics (TOG), 42\\penalty0 (4):\\penalty0 113, 2023.",
            "leftover": "Rinon Gal, Moab Arar, Yuval Atzmon, Amit~H Bermano, Gal Chechik, and Daniel CohenOr. Encoderbased domain tuning for fast personalization of texttoimage models. ACM Transactions on Graphics (TOG), 42\\penalty0 (4):\\penalty0 113, 2023.",
            "matches": []
        },
        {
            "leaf id": 104,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel CohenOr. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133, 2023.",
            "leftover": "Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel CohenOr. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133, 2023.",
            "matches": []
        },
        {
            "leaf id": 105,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:\\penalty0 68406851, 2020.",
            "leftover": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:\\penalty0 68406851, 2020.",
            "matches": []
        },
        {
            "leaf id": 106,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Lowrank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.",
            "leftover": "Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Lowrank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.",
            "matches": []
        },
        {
            "leaf id": 107,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable imagetovideo synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023{\\natexlaba}.",
            "leftover": "Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable imagetovideo synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 108,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and TatJen Cham. Cocktail: Mixing multimodality control for textconditional image generation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023{\\natexlabb}.",
            "leftover": "Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and TatJen Cham. Cocktail: Mixing multimodality control for textconditional image generation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 109,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, YanPei Cao, Ding Liang, Yu Qiao, Bo Dai, et~al. Epidiff: Enhancing multiview synthesis via localized epipolarconstrained diffusion. arXiv preprint arXiv:2312.06725, 2023.",
            "leftover": "Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, YanPei Cao, Ding Liang, Yu Qiao, Bo Dai, et~al. Epidiff: Enhancing multiview synthesis via localized epipolarconstrained diffusion. arXiv preprint arXiv:2312.06725, 2023.",
            "matches": []
        },
        {
            "leaf id": 110,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. arXiv preprint arXiv:2402.12974, 2024.",
            "leftover": "Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. arXiv preprint arXiv:2402.12974, 2024.",
            "matches": []
        },
        {
            "leaf id": 111,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen~Change Loy, and Ziwei Liu. Text2human: Textdriven controllable human image generation. ACM Transactions on Graphics (TOG), 41\\penalty0 (4):\\penalty0 111, 2022.",
            "leftover": "Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen~Change Loy, and Ziwei Liu. Text2human: Textdriven controllable human image generation. ACM Transactions on Graphics (TOG), 41\\penalty0 (4):\\penalty0 111, 2022.",
            "matches": []
        },
        {
            "leaf id": 112,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and Jingfeng Zhang. Scedit: Efficient and controllable image diffusion generation via skip connection editing. arXiv preprint arXiv:2312.11392, 2023.",
            "leftover": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and Jingfeng Zhang. Scedit: Efficient and controllable image diffusion generation via skip connection editing. arXiv preprint arXiv:2312.11392, 2023.",
            "matches": []
        },
        {
            "leaf id": 113,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Diederik~P Kingma and Max Welling. Autoencoding variational bayes, 2022.",
            "leftover": "Diederik~P Kingma and Max Welling. Autoencoding variational bayes, 2022.",
            "matches": []
        },
        {
            "leaf id": 114,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and JunYan Zhu. Multiconcept customization of texttoimage diffusion, 2023.",
            "leftover": "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and JunYan Zhu. Multiconcept customization of texttoimage diffusion, 2023.",
            "matches": []
        },
        {
            "leaf id": 115,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Lambda Labs. Stable diffusion image variations, 2022.",
            "leftover": "Lambda Labs. Stable diffusion image variations, 2022.",
            "matches": []
        },
        {
            "leaf id": 116,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
            "leftover": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
            "matches": []
        },
        {
            "leaf id": 117,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyperrealistic human generation with latent structural diffusion. arXiv preprint arXiv:2310.08579, 2023{\\natexlaba}.",
            "leftover": "Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyperrealistic human generation with latent structural diffusion. arXiv preprint arXiv:2310.08579, 2023{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 118,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "leftover": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "matches": []
        },
        {
            "leaf id": 119,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation, 2023{\\natexlabb}.",
            "leftover": "Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation, 2023{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 120,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized texttoimage generation without testtime finetuning. arXiv preprint arXiv:2307.11410, 2023.",
            "leftover": "Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized texttoimage generation without testtime finetuning. arXiv preprint arXiv:2307.11410, 2023.",
            "matches": []
        },
        {
            "leaf id": 121,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: highresolution multicategory virtual tryon. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22312235, 2022.",
            "leftover": "Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: highresolution multicategory virtual tryon. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22312235, 2022.",
            "matches": []
        },
        {
            "leaf id": 122,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for texttoimage diffusion models. arXiv preprint arXiv:2302.08453, 2023.",
            "leftover": "Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for texttoimage diffusion models. arXiv preprint arXiv:2302.08453, 2023.",
            "matches": []
        },
        {
            "leaf id": 123,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with textguided diffusion models, 2022.",
            "leftover": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with textguided diffusion models, 2022.",
            "matches": []
        },
        {
            "leaf id": 124,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, PoYao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024.",
            "leftover": "Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, PoYao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024.",
            "matches": []
        },
        {
            "leaf id": 125,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{''u}ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for highresolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.",
            "leftover": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{''u}ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for highresolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.",
            "matches": []
        },
        {
            "leaf id": 126,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan~Carlos Niebles, Caiming Xiong, Silvio Savarese, et~al. Unicontrol: A unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.",
            "leftover": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan~Carlos Niebles, Caiming Xiong, Silvio Savarese, et~al. Unicontrol: A unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.",
            "matches": []
        },
        {
            "leaf id": 127,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.",
            "leftover": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.",
            "matches": []
        },
        {
            "leaf id": 128,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1\\penalty0 (2):\\penalty0 3, 2022.",
            "leftover": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1\\penalty0 (2):\\penalty0 3, 2022.",
            "matches": []
        },
        {
            "leaf id": 129,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{''o}rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022.",
            "leftover": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{''o}rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022.",
            "matches": []
        },
        {
            "leaf id": 130,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computerassisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 59, 2015, proceedings, part III 18, pages 234241. Springer, 2015.",
            "leftover": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computerassisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 59, 2015, proceedings, part III 18, pages 234241. Springer, 2015.",
            "matches": []
        },
        {
            "leaf id": 131,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning texttoimage diffusion models for subjectdriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023.",
            "leftover": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning texttoimage diffusion models for subjectdriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023.",
            "matches": []
        },
        {
            "leaf id": 132,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al. Photorealistic texttoimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:\\penalty0 3647936494, 2022.",
            "leftover": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al. Photorealistic texttoimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:\\penalty0 3647936494, 2022.",
            "matches": []
        },
        {
            "leaf id": 133,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Jing Shi, Wei Xiong, Zhe Lin, and Hyun~Joon Jung. Instantbooth: Personalized texttoimage generation without testtime finetuning, 2023.",
            "leftover": "Jing Shi, Wei Xiong, Zhe Lin, and Hyun~Joon Jung. Instantbooth: Personalized texttoimage generation without testtime finetuning, 2023.",
            "matches": []
        },
        {
            "leaf id": 134,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.",
            "leftover": "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.",
            "matches": []
        },
        {
            "leaf id": 135,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Trainingfree consistent texttoimage generation. arXiv preprint arXiv:2402.03286, 2024.",
            "leftover": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Trainingfree consistent texttoimage generation. arXiv preprint arXiv:2402.03286, 2024.",
            "matches": []
        },
        {
            "leaf id": 136,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via nextscale prediction. 2024.",
            "leftover": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via nextscale prediction. 2024.",
            "matches": []
        },
        {
            "leaf id": 137,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\\L}ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
            "leftover": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\\L}ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
            "matches": []
        },
        {
            "leaf id": 138,
            "key": "doc/bib44",
            "block type": "bibliography",
            "content": "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zeroshot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.",
            "leftover": "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zeroshot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.",
            "matches": []
        },
        {
            "leaf id": 139,
            "key": "doc/bib45",
            "block type": "bibliography",
            "content": "Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Realesrgan: Training realworld blind superresolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW).",
            "leftover": "Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Realesrgan: Training realworld blind superresolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW).",
            "matches": []
        },
        {
            "leaf id": 140,
            "key": "doc/bib46",
            "block type": "bibliography",
            "content": "Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized texttoimage generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023.",
            "leftover": "Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized texttoimage generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023.",
            "matches": []
        },
        {
            "leaf id": 141,
            "key": "doc/bib47",
            "block type": "bibliography",
            "content": "Guangxuan Xiao, Tianwei Yin, William~T Freeman, Fr{\\'e}do Durand, and Song Han. Fastcomposer: Tuningfree multisubject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023.",
            "leftover": "Guangxuan Xiao, Tianwei Yin, William~T Freeman, Fr{\\'e}do Durand, and Song Han. Fastcomposer: Tuningfree multisubject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023.",
            "matches": []
        },
        {
            "leaf id": 142,
            "key": "doc/bib48",
            "block type": "bibliography",
            "content": "Zhongcong Xu, Jianfeng Zhang, Jun~Hao Liew, Hanshu Yan, JiaWei Liu, Chenxu Zhang, Jiashi Feng, and Mike~Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. 2024.",
            "leftover": "Zhongcong Xu, Jianfeng Zhang, Jun~Hao Liew, Hanshu Yan, JiaWei Liu, Chenxu Zhang, Jiashi Feng, and Mike~Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. 2024.",
            "matches": []
        },
        {
            "leaf id": 143,
            "key": "doc/bib49",
            "block type": "bibliography",
            "content": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplarbased image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023{\\natexlaba}.",
            "leftover": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplarbased image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023{\\natexlaba}.",
            "matches": []
        },
        {
            "leaf id": 144,
            "key": "doc/bib50",
            "block type": "bibliography",
            "content": "Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective wholebody pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023{\\natexlabb}.",
            "leftover": "Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective wholebody pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023{\\natexlabb}.",
            "matches": []
        },
        {
            "leaf id": 145,
            "key": "doc/bib51",
            "block type": "bibliography",
            "content": "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for texttoimage diffusion models. 2023.",
            "leftover": "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for texttoimage diffusion models. 2023.",
            "matches": []
        },
        {
            "leaf id": 146,
            "key": "doc/bib52",
            "block type": "bibliography",
            "content": "Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for contentrich texttoimage generation, 2022.",
            "leftover": "Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for contentrich texttoimage generation, 2022.",
            "matches": []
        },
        {
            "leaf id": 147,
            "key": "doc/bib53",
            "block type": "bibliography",
            "content": "Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warpbased network for poseguided human video generation. arXiv preprint arXiv:1910.09139, 2019.",
            "leftover": "Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warpbased network for poseguided human video generation. arXiv preprint arXiv:1910.09139, 2019.",
            "matches": []
        },
        {
            "leaf id": 148,
            "key": "doc/bib54",
            "block type": "bibliography",
            "content": "Lvmin Zhang. Referenceonly controlnet, 2023.",
            "leftover": "Lvmin Zhang. Referenceonly controlnet, 2023.",
            "matches": []
        },
        {
            "leaf id": 149,
            "key": "doc/bib55",
            "block type": "bibliography",
            "content": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to texttoimage diffusion models, 2023.",
            "leftover": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to texttoimage diffusion models, 2023.",
            "matches": []
        },
        {
            "leaf id": 150,
            "key": "doc/bib56",
            "block type": "bibliography",
            "content": "Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. Ssrencoder: Encoding selective subject representation for subjectdriven generation, 2024.",
            "leftover": "Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. Ssrencoder: Encoding selective subject representation for subjectdriven generation, 2024.",
            "matches": []
        },
        {
            "leaf id": 151,
            "key": "doc/bib57",
            "block type": "bibliography",
            "content": "Shihao Zhao, Dongdong Chen, YenChun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee~K Wong. Unicontrolnet: Allinone control to texttoimage diffusion models. Advances in Neural Information Processing Systems, 36, 2024.",
            "leftover": "Shihao Zhao, Dongdong Chen, YenChun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee~K Wong. Unicontrolnet: Allinone control to texttoimage diffusion models. Advances in Neural Information Processing Systems, 36, 2024.",
            "matches": []
        }
    ]
}