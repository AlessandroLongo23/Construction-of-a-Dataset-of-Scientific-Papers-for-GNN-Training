{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "0.32",
                    "matching_string": "From Parts to Whole: A Unified Reference Framework for Controllable Human "
                },
                {
                    "pdf_id": "0.33",
                    "matching_string": "Image Generation"
                }
            ]
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{ { Zehuan Huang\\footnotemark[1] \\quad Hongxing Fan\\footnotemark[1] \\quad Lipeng Wang\\footnotemark[1] \\quad Lu Sheng\\footnotemark[2] } {Beihang University} {\\tt\\small {huangzehuan, fanhongxing, wanglipeng, lsheng}@buaa.edu.cn} {\\smallhttps://huanngzh.github.io/Parts2Whole/} }",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "0.36",
                    "matching_string": "{huangzehuan, fanhongxing, wanglipeng, lsheng}@buaa.edu.c"
                },
                {
                    "pdf_id": "0.34",
                    "matching_string": "{ { Zehuan Huang\\footnotemark[1] \\quad Hongxing Fan\\footnotemark[1] \\quad Lipeng Wang\\footnotemark[1] \\quad Lu Sheng\\footnotemark[2] } {Beihang University} {\\tt\\small n} {\\smallhttps://huanngzh.github.io/Parts2Whole/} }"
                },
                {
                    "pdf_id": "0.35",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Recent advancements in controllable human image generation have led to zeroshot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semanticaware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multiscale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multiimage conditioned generation through a shared selfattention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multipart controllable human image customization.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "0.42",
                    "matching_string": "which processes each image based on its textual label to "
                },
                {
                    "pdf_id": "0.43",
                    "matching_string": "a series of multiscale feature maps rather than one image "
                },
                {
                    "pdf_id": "0.46",
                    "matching_string": "have led to zeroshot generation using structural "
                },
                {
                    "pdf_id": "0.47",
                    "matching_string": "supports multiimage conditioned generation through "
                },
                {
                    "pdf_id": "0.51",
                    "matching_string": "and target features during the diffusion process. We "
                },
                {
                    "pdf_id": "0.52",
                    "matching_string": "appearance remains challenging. Addressing this, we introduce "
                },
                {
                    "pdf_id": "0.55",
                    "matching_string": "mask information from the reference human images, allowing "
                },
                {
                    "pdf_id": "0.56",
                    "matching_string": "customized portraits from multiple reference images, including "
                },
                {
                    "pdf_id": "0.59",
                    "matching_string": "demonstrate the superiority of our approach over existing "
                },
                {
                    "pdf_id": "0.60",
                    "matching_string": "To achieve this, we first develop a semanticaware appearance "
                },
                {
                    "pdf_id": "0.63",
                    "matching_string": "controllable human image customization."
                },
                {
                    "pdf_id": "0.44",
                    "matching_string": "Recent advancements in controllable human image generation "
                },
                {
                    "pdf_id": "0.50",
                    "matching_string": "human images conditioned on multiple parts of human "
                },
                {
                    "pdf_id": "0.45",
                    "matching_string": "token, preserving the image dimension. Second, our framework "
                },
                {
                    "pdf_id": "0.49",
                    "matching_string": "a shared selfattention mechanism that operates across reference "
                },
                {
                    "pdf_id": "0.53",
                    "matching_string": "enhance the vanilla attention mechanism by incorporating "
                },
                {
                    "pdf_id": "0.54",
                    "matching_string": "Parts2Whole, a novel framework designed for generating "
                },
                {
                    "pdf_id": "0.57",
                    "matching_string": "for the precise selection of any part. Extensive experiments "
                },
                {
                    "pdf_id": "0.58",
                    "matching_string": "pose images and various aspects of human appearance. "
                },
                {
                    "pdf_id": "0.62",
                    "matching_string": "encoder to retain details of different human parts, "
                },
                {
                    "pdf_id": "0.48",
                    "matching_string": "signals (e.g., pose, depth) or facial appearance. Yet, generating "
                },
                {
                    "pdf_id": "0.61",
                    "matching_string": "alternatives, offering advanced capabilities for multipart "
                }
            ]
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "leaf id": 3,
                    "key": "doc/body/center0",
                    "block type": "center",
                    "content": "\\centering \\includegraphics[width=\\textwidth]{figure/teaser.pdf} \\captionof{figure}{We propose Parts2Whole, which can generate realistic and highquality human figures in various postures from referential human part images of any quantity and different origins. Our method maintains the high alignment with the corresponding conditional semantic regions, while ensuring diversity and harmony among the whole body.}",
                    "leftover": "\\centering \\includegraphics[width=\\textwidth]{figure/teaser.pdf} \\captionof{figure}{body.}",
                    "matches": [
                        {
                            "pdf_id": "0.39",
                            "matching_string": "human part images of any quantity and different origins. Our method maintains the high alignment with the corresponding conditional "
                        },
                        {
                            "pdf_id": "0.38",
                            "matching_string": "We propose Parts2Whole, which can generate realistic and highquality human figures in various postures from referential "
                        },
                        {
                            "pdf_id": "0.40",
                            "matching_string": "semantic regions, while ensuring diversity and harmony among the whole "
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "1.0",
                                    "matching_string": "Introduction"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 5,
                                    "key": "doc/body/sec1/txl0/txl0",
                                    "block type": "txl",
                                    "content": "Controllable human image generation aims to synthesize human images that align with specific textual descriptions, structural signals or more precise appearance conditions. It emerges as a significant technology within the realm of digital content creation, providing users with a portrait customization solution. However, due to the complexity of the control conditions, this task presents significant challenges, especially when it comes to multitype condition input and control of various aspects of human appearance.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.2",
                                            "matching_string": "human images that align with specific textual descriptions, "
                                        },
                                        {
                                            "pdf_id": "1.4",
                                            "matching_string": "It emerges as a significant technology within the realm of "
                                        },
                                        {
                                            "pdf_id": "1.6",
                                            "matching_string": "solution. However, due to the complexity of the "
                                        },
                                        {
                                            "pdf_id": "1.8",
                                            "matching_string": "especially when it comes to multitype condition input and "
                                        },
                                        {
                                            "pdf_id": "1.1",
                                            "matching_string": "Controllable human image generation aims to synthesize "
                                        },
                                        {
                                            "pdf_id": "1.3",
                                            "matching_string": "structural signals or more precise appearance conditions. "
                                        },
                                        {
                                            "pdf_id": "1.5",
                                            "matching_string": "digital content creation, providing users with a portrait customization "
                                        },
                                        {
                                            "pdf_id": "1.7",
                                            "matching_string": "control conditions, this task presents significant challenges, "
                                        },
                                        {
                                            "pdf_id": "1.9",
                                            "matching_string": "control of various aspects of human appearance."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 6,
                                    "key": "doc/body/sec1/txl1/txl0",
                                    "block type": "txl",
                                    "content": "As diffusion models have brought great success in image generation, the task of controllable human image generation has experienced rapid development. Several works utilize languages as condition, generating human images by providing attributes about the textures of clothes. Due to the rough control of texts, it struggles to accurately guide the generation of human appearance. Another group of works focuses on introducing structural signals to control human posture. Although these methods have achieved impressive results, they do not consider appearance as a condition, which is crucial for portrait customization.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.11",
                                            "matching_string": "great success in image generation, the task of controllable "
                                        },
                                        {
                                            "pdf_id": "1.14",
                                            "matching_string": "generating human images by providing attributes about the "
                                        },
                                        {
                                            "pdf_id": "1.16",
                                            "matching_string": "struggles to accurately guide the generation of human appearance. "
                                        },
                                        {
                                            "pdf_id": "1.18",
                                            "matching_string": "on introducing structural signals to control human posture. "
                                        },
                                        {
                                            "pdf_id": "1.20",
                                            "matching_string": "they do not consider appearance as a condition, which is "
                                        },
                                        {
                                            "pdf_id": "1.12",
                                            "matching_string": "human image generation has experienced rapid development. "
                                        },
                                        {
                                            "pdf_id": "1.15",
                                            "matching_string": "textures of clothes. Due to the rough control of texts, it "
                                        },
                                        {
                                            "pdf_id": "1.19",
                                            "matching_string": "Although these methods have achieved impressive results, "
                                        },
                                        {
                                            "pdf_id": "1.13",
                                            "matching_string": "Several works utilize languages as condition, "
                                        },
                                        {
                                            "pdf_id": "1.21",
                                            "matching_string": "crucial for portrait customization."
                                        },
                                        {
                                            "pdf_id": "1.10",
                                            "matching_string": "As diffusion models have brought Another group of works focuses "
                                        },
                                        {
                                            "pdf_id": "1.17",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl2",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 7,
                                    "key": "doc/body/sec1/txl2/txl0",
                                    "block type": "txl",
                                    "content": "Recently, several works have emerged that use appearance conditions to guide human image generation. They learn human representation from reference images and generate images aligning with the specific face identity. One prominent approach involves testtime finetuning. It requires substantial computational resources to learn each new individual, which costs about half an hour to achieve satisfactory results. Another approach investigates the zeroshot setting to bypass the finetuning cost. It encodes the reference image into one or several tokens and injects them into the generation process along with text tokens. These zeroshot methods make human image customization practical with faster speed. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details. And they lack the design to obtain specified information from the images, but instead utilize all the information, resulting in ambiguous subject representation.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.24",
                                            "matching_string": "to guide human image generation. They learn human representation "
                                        },
                                        {
                                            "pdf_id": "1.26",
                                            "matching_string": "with the specific face identity. One prominent approach "
                                        },
                                        {
                                            "pdf_id": "1.28",
                                            "matching_string": "substantial computational resources to learn each new individual, "
                                        },
                                        {
                                            "pdf_id": "1.32",
                                            "matching_string": "It encodes the reference image into one or several tokens "
                                        },
                                        {
                                            "pdf_id": "1.34",
                                            "matching_string": "tokens. These zeroshot methods make human image customization "
                                        },
                                        {
                                            "pdf_id": "1.36",
                                            "matching_string": "loss of spatial representations when encoding the reference "
                                        },
                                        {
                                            "pdf_id": "1.37",
                                            "matching_string": "images into one or a few tokens, they struggle to preserve "
                                        },
                                        {
                                            "pdf_id": "1.39",
                                            "matching_string": "information from the images, but instead utilize all the "
                                        },
                                        {
                                            "pdf_id": "1.25",
                                            "matching_string": "from reference images and generate images aligning "
                                        },
                                        {
                                            "pdf_id": "1.29",
                                            "matching_string": "which costs about half an hour to achieve satisfactory "
                                        },
                                        {
                                            "pdf_id": "1.31",
                                            "matching_string": "the zeroshot setting to bypass the finetuning cost. "
                                        },
                                        {
                                            "pdf_id": "1.33",
                                            "matching_string": "and injects them into the generation process along with text "
                                        },
                                        {
                                            "pdf_id": "1.38",
                                            "matching_string": "appearance details. And they lack the design to obtain specified "
                                        },
                                        {
                                            "pdf_id": "1.40",
                                            "matching_string": "information, resulting in ambiguous subject representation."
                                        },
                                        {
                                            "pdf_id": "1.23",
                                            "matching_string": "have emerged that use appearance conditions "
                                        },
                                        {
                                            "pdf_id": "1.35",
                                            "matching_string": "practical with faster speed. However, due to the "
                                        },
                                        {
                                            "pdf_id": "1.22",
                                            "matching_string": "Recently, several works involves testtime finetuning. It requires results. Another approach investigates "
                                        },
                                        {
                                            "pdf_id": "1.27",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "1.30",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "1.90",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl3",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 8,
                                    "key": "doc/body/sec1/txl3/txl0",
                                    "block type": "txl",
                                    "content": "In this paper, we target generating human images from multipart images of human appearance, along with specific pose maps or optionally text descriptions. The abovementioned generation methods conditioned on structural signals or face identity have their limitations on this task (results shown in and ). It is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. Methods like IPAdapter and SSREncoder incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g., semantic features) and utilize the crossattention keys and values from them rather than from image dimensional feature maps. As a result, the spatial relationship in the original image dimensions between the reference images and the target image is lost, resulting in a mixture of attributes from different subjects. Although methods like ControlNet encode the reference images into imagedimensional features, they add the features to the feature maps in the UNet decoder. It is suitable for tasks where the condition maps and the target map have the same structure, such as guiding generation using line drawings. However, in the case of the spatial misalignment of the conditional images with the target image, it is difficult to model the correlation of spatial information by directly adding or concat features on the channel dimension.",
                                    "leftover": "signals or face identity crossattention ",
                                    "matches": [
                                        {
                                            "pdf_id": "1.42",
                                            "matching_string": "other modal features (e.g., semantic features) and utilize the "
                                        },
                                        {
                                            "pdf_id": "1.44",
                                            "matching_string": "image dimensional feature maps. As a result, the spatial "
                                        },
                                        {
                                            "pdf_id": "1.46",
                                            "matching_string": "reference images and the target image is lost, resulting in "
                                        },
                                        {
                                            "pdf_id": "1.47",
                                            "matching_string": "a mixture of attributes from different subjects. Although "
                                        },
                                        {
                                            "pdf_id": "1.49",
                                            "matching_string": "into imagedimensional features, they add the features to the "
                                        },
                                        {
                                            "pdf_id": "1.50",
                                            "matching_string": "feature maps in the UNet decoder. It is suitable for tasks "
                                        },
                                        {
                                            "pdf_id": "1.52",
                                            "matching_string": "structure, such as guiding generation using line drawings. "
                                        },
                                        {
                                            "pdf_id": "1.54",
                                            "matching_string": "images with the target image, it is difficult to model "
                                        },
                                        {
                                            "pdf_id": "1.56",
                                            "matching_string": "concat features on the channel dimension."
                                        },
                                        {
                                            "pdf_id": "1.82",
                                            "matching_string": "In this paper, we target generating human images from "
                                        },
                                        {
                                            "pdf_id": "1.86",
                                            "matching_string": "pose maps or optionally text descriptions. The abovementioned "
                                        },
                                        {
                                            "pdf_id": "1.96",
                                            "matching_string": "the input multibody parts with the target image, and the "
                                        },
                                        {
                                            "pdf_id": "1.98",
                                            "matching_string": "lack of specific design in existing methods to address the "
                                        },
                                        {
                                            "pdf_id": "1.100",
                                            "matching_string": "variation in spatial positions during feature injection. "
                                        },
                                        {
                                            "pdf_id": "1.43",
                                            "matching_string": "crossattention keys and values from them rather than from "
                                        },
                                        {
                                            "pdf_id": "1.51",
                                            "matching_string": "where the condition maps and the target map have the same "
                                        },
                                        {
                                            "pdf_id": "1.53",
                                            "matching_string": "However, in the case of the spatial misalignment of the conditional "
                                        },
                                        {
                                            "pdf_id": "1.55",
                                            "matching_string": "the correlation of spatial information by directly adding or "
                                        },
                                        {
                                            "pdf_id": "1.84",
                                            "matching_string": "multipart images of human appearance, along with specific "
                                        },
                                        {
                                            "pdf_id": "1.88",
                                            "matching_string": "generation methods conditioned on structural "
                                        },
                                        {
                                            "pdf_id": "1.104",
                                            "matching_string": "features into the denoising UNet through "
                                        },
                                        {
                                            "pdf_id": "1.41",
                                            "matching_string": "mechanisms. They encode reference images into "
                                        },
                                        {
                                            "pdf_id": "1.48",
                                            "matching_string": "methods like ControlNet encode the reference images "
                                        },
                                        {
                                            "pdf_id": "1.92",
                                            "matching_string": "have their limitations on this task (results shown in "
                                        },
                                        {
                                            "pdf_id": "1.94",
                                            "matching_string": "and ). It is attributed to the spatial misalignment of "
                                        },
                                        {
                                            "pdf_id": "1.102",
                                            "matching_string": "Methods like IPAdapter and SSREncoder incorporate "
                                        },
                                        {
                                            "pdf_id": "1.45",
                                            "matching_string": "relationship in the original image dimensions between the "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl4",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 9,
                                    "key": "doc/body/sec1/txl4/txl0",
                                    "block type": "txl",
                                    "content": "To address the above issues, we present Parts2Whole, a unified reference framework for portrait customization from multiple reference images, including various parts of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Inspired by the effective reference mechanism used in imagetovideo tasks, we develop a semanticaware appearance encoder based on the Reference UNet architecture. It encodes each image with its textual label into a series of multiscale feature maps in image dimension, preserving appearance details and spatial information of multiple reference images. The additional semantic condition represents a category instruction, which helps retain richer shapes and detailed attributes of each aspect. Furthermore, to preserve the positional relationship when injecting reference features into the image generation process, we employ a shared selfattention operation across reference and target features during the diffusion process. We also build a tiny convolution network to extract the pose features and inject them into the generation. To precisely select the specified part from each reference image, we enhance the vanilla selfattention mechanism by incorporating masks of the subjects in the reference images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.57",
                                            "matching_string": "To address the above issues, we present Parts2Whole, a "
                                        },
                                        {
                                            "pdf_id": "1.59",
                                            "matching_string": "multiple reference images, including various parts of human "
                                        },
                                        {
                                            "pdf_id": "1.61",
                                            "matching_string": "maps. Inspired by the effective reference mechanism used "
                                        },
                                        {
                                            "pdf_id": "1.63",
                                            "matching_string": "appearance encoder based on the Reference UNet "
                                        },
                                        {
                                            "pdf_id": "1.65",
                                            "matching_string": "into a series of multiscale feature maps in image dimension, "
                                        },
                                        {
                                            "pdf_id": "1.67",
                                            "matching_string": "of multiple reference images. The additional semantic condition "
                                        },
                                        {
                                            "pdf_id": "1.69",
                                            "matching_string": "richer shapes and detailed attributes of each aspect. Furthermore, "
                                        },
                                        {
                                            "pdf_id": "1.71",
                                            "matching_string": "reference features into the image generation process, we "
                                        },
                                        {
                                            "pdf_id": "1.73",
                                            "matching_string": "and target features during the diffusion process. We also "
                                        },
                                        {
                                            "pdf_id": "1.75",
                                            "matching_string": "and inject them into the generation. To precisely select the "
                                        },
                                        {
                                            "pdf_id": "1.76",
                                            "matching_string": "specified part from each reference image, we enhance the "
                                        },
                                        {
                                            "pdf_id": "1.77",
                                            "matching_string": "vanilla selfattention mechanism by incorporating masks "
                                        },
                                        {
                                            "pdf_id": "1.58",
                                            "matching_string": "unified reference framework for portrait customization from "
                                        },
                                        {
                                            "pdf_id": "1.60",
                                            "matching_string": "appearance (e.g., hair, face, clothes, shoes, etc.) and pose "
                                        },
                                        {
                                            "pdf_id": "1.64",
                                            "matching_string": "architecture. It encodes each image with its textual label "
                                        },
                                        {
                                            "pdf_id": "1.70",
                                            "matching_string": "to preserve the positional relationship when injecting "
                                        },
                                        {
                                            "pdf_id": "1.72",
                                            "matching_string": "employ a shared selfattention operation across reference "
                                        },
                                        {
                                            "pdf_id": "1.74",
                                            "matching_string": "build a tiny convolution network to extract the pose features "
                                        },
                                        {
                                            "pdf_id": "1.78",
                                            "matching_string": "of the subjects in the reference images."
                                        },
                                        {
                                            "pdf_id": "1.62",
                                            "matching_string": "in imagetovideo tasks, we develop a semanticaware "
                                        },
                                        {
                                            "pdf_id": "1.66",
                                            "matching_string": "preserving appearance details and spatial information "
                                        },
                                        {
                                            "pdf_id": "1.68",
                                            "matching_string": "represents a category instruction, which helps retain "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/txl5",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 10,
                                    "key": "doc/body/sec1/txl5/txl0",
                                    "block type": "txl",
                                    "content": "Equipped with these techniques, Parts2Whole demonstrates superior quality and controllability for human image generation. Our contributions are summarized as follows:",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.79",
                                            "matching_string": "Equipped with these techniques, Parts2Whole demonstrates "
                                        },
                                        {
                                            "pdf_id": "1.81",
                                            "matching_string": "generation. Our contributions are summarized as follows:"
                                        },
                                        {
                                            "pdf_id": "1.80",
                                            "matching_string": "superior quality and controllability for human image "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/itemize6",
                            "block_type": "itemize",
                            "children": [
                                {
                                    "leaf id": 11,
                                    "key": "doc/body/sec1/itemize6/txl0",
                                    "block type": "txl",
                                    "content": "We construct a novel framework, Parts2Whole, which supports the controllable generation of human images conditioned on texts, pose signals, and multiple aspects of human appearance. We propose an advanced multireference mechanism consisting of a semanticaware image encoder and the shared attention operation, which retains details of the specific key elements and achieves precise subject selection with the help of our proposed maskguided approach. Experiments show that our Parts2Whole generates highquality human images from multiple conditions and maintains high consistency with the given conditions.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "1.85",
                                            "matching_string": "supports the controllable generation of human images "
                                        },
                                        {
                                            "pdf_id": "1.89",
                                            "matching_string": "of human appearance. "
                                        },
                                        {
                                            "pdf_id": "1.93",
                                            "matching_string": "of a semanticaware image encoder and the shared "
                                        },
                                        {
                                            "pdf_id": "1.99",
                                            "matching_string": "the help of our proposed maskguided approach. "
                                        },
                                        {
                                            "pdf_id": "1.103",
                                            "matching_string": "human images from multiple conditions and "
                                        },
                                        {
                                            "pdf_id": "1.83",
                                            "matching_string": "We construct a novel framework, Parts2Whole, which "
                                        },
                                        {
                                            "pdf_id": "1.87",
                                            "matching_string": "conditioned on texts, pose signals, and multiple aspects "
                                        },
                                        {
                                            "pdf_id": "1.91",
                                            "matching_string": "We propose an advanced multireference mechanism consisting "
                                        },
                                        {
                                            "pdf_id": "1.95",
                                            "matching_string": "attention operation, which retains details of the specific "
                                        },
                                        {
                                            "pdf_id": "1.101",
                                            "matching_string": "Experiments show that our Parts2Whole generates highquality "
                                        },
                                        {
                                            "pdf_id": "1.105",
                                            "matching_string": "maintains high consistency with the given conditions."
                                        },
                                        {
                                            "pdf_id": "1.97",
                                            "matching_string": "key elements and achieves precise subject selection with "
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Related Work",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "2.0",
                                    "matching_string": "Related Work"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 13,
                                    "key": "doc/body/sec2/txl0/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In recent years, texttoimage generation has made remarkable progress, particularly with the development of diffusion models and autoregressive models, which have propelled texttoimage generation to largescale commercialization. Since DALLE2, Stable Diffusion and Imagen employ diffusion models as generative models and train the models on large datasets, texttoimage synthesis ability has been significantly enhanced. More recently, Stable Diffusion XL, a twostage cascade diffusion model, has greatly improved the generation of highfrequency details and overall image color, taking aesthetic appeal to a higher level. However, these existing methods are limited to generating images solely from text prompts, and they do not meet the demand for producing customized images with the preservation of appearance.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.5",
                                            "matching_string": "propelled texttoimage generation to largescale commercialization. "
                                        },
                                        {
                                            "pdf_id": "2.8",
                                            "matching_string": "and train the models on large datasets, texttoimage "
                                        },
                                        {
                                            "pdf_id": "2.11",
                                            "matching_string": "model, has greatly improved the generation of highfrequency "
                                        },
                                        {
                                            "pdf_id": "2.13",
                                            "matching_string": "appeal to a higher level. However, these existing methods "
                                        },
                                        {
                                            "pdf_id": "2.15",
                                            "matching_string": "and they do not meet the demand for producing customized "
                                        },
                                        {
                                            "pdf_id": "2.1",
                                            "matching_string": "TexttoImage Generation.}} In recent years, texttoimage "
                                        },
                                        {
                                            "pdf_id": "2.7",
                                            "matching_string": "Imagen employ diffusion models as generative models "
                                        },
                                        {
                                            "pdf_id": "2.9",
                                            "matching_string": "synthesis ability has been significantly enhanced. More recently, "
                                        },
                                        {
                                            "pdf_id": "2.12",
                                            "matching_string": "details and overall image color, taking aesthetic "
                                        },
                                        {
                                            "pdf_id": "2.14",
                                            "matching_string": "are limited to generating images solely from text prompts, "
                                        },
                                        {
                                            "pdf_id": "2.16",
                                            "matching_string": "images with the preservation of appearance."
                                        },
                                        {
                                            "pdf_id": "2.2",
                                            "matching_string": "generation has made remarkable progress, particularly with "
                                        },
                                        {
                                            "pdf_id": "2.10",
                                            "matching_string": "Stable Diffusion and Stable Diffusion XL, a twostage cascade diffusion "
                                        },
                                        {
                                            "pdf_id": "2.3",
                                            "matching_string": "{\\vspace{+1mm}{the development of diffusion models and autoregressive models, which have Since DALLE2, "
                                        },
                                        {
                                            "pdf_id": "2.4",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.6",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 14,
                                    "key": "doc/body/sec2/txl1/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Given the robust generative capabilities of image diffusion models, a series of research attempts to explore the controllability of image generation, enabling image synthesis guided by multimodal conditions. Some work focuses on introducing structural signals such as edges, depth maps, and segmentation maps, to control the spatial structure of generated images. Another group of work uses appearance conditions to guide image generation, aiming to generate images aligning with specific concepts like identity and style, known as subjectdriven image generation. The methods generally fall into two categories: those requiring testtime finetuning and those that do not. Testtime finetuning methods often optimizes additional text embedding, parameter residuals or direct finetune the whole model to fit the specified subject. Although these methods have achieved impressive results, they cost about half an hour to achieve satisfactory results. Finetuningfree methods typically train an additional encoding network to encode the reference image into embeddings or image prompts. However, due to the loss of spatial representations when encoding the reference images into one or a few tokens, they struggle to preserve appearance details.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.18",
                                            "matching_string": "capabilities of image diffusion models, a series of "
                                        },
                                        {
                                            "pdf_id": "2.23",
                                            "matching_string": "signals such as edges, depth maps, and segmentation "
                                        },
                                        {
                                            "pdf_id": "2.26",
                                            "matching_string": "conditions to guide image generation, aiming to generate "
                                        },
                                        {
                                            "pdf_id": "2.28",
                                            "matching_string": "and style, known as subjectdriven image generation. The "
                                        },
                                        {
                                            "pdf_id": "2.30",
                                            "matching_string": "testtime finetuning and those that do not. Testtime "
                                        },
                                        {
                                            "pdf_id": "2.32",
                                            "matching_string": "text embedding, parameter residuals or direct finetune "
                                        },
                                        {
                                            "pdf_id": "2.34",
                                            "matching_string": "these methods have achieved impressive results, they cost "
                                        },
                                        {
                                            "pdf_id": "2.38",
                                            "matching_string": "image into embeddings or image prompts. However, "
                                        },
                                        {
                                            "pdf_id": "2.40",
                                            "matching_string": "reference images into one or a few tokens, they struggle to "
                                        },
                                        {
                                            "pdf_id": "2.20",
                                            "matching_string": "the controllability of image generation, enabling image "
                                        },
                                        {
                                            "pdf_id": "2.37",
                                            "matching_string": "train an additional encoding network to encode the reference "
                                        },
                                        {
                                            "pdf_id": "2.24",
                                            "matching_string": "maps, to control the spatial structure of generated images. "
                                        },
                                        {
                                            "pdf_id": "2.27",
                                            "matching_string": "images aligning with specific concepts like identity "
                                        },
                                        {
                                            "pdf_id": "2.29",
                                            "matching_string": "methods generally fall into two categories: those requiring "
                                        },
                                        {
                                            "pdf_id": "2.33",
                                            "matching_string": "the whole model to fit the specified subject. Although "
                                        },
                                        {
                                            "pdf_id": "2.35",
                                            "matching_string": "about half an hour to achieve satisfactory results. Finetuningfree "
                                        },
                                        {
                                            "pdf_id": "2.39",
                                            "matching_string": "due to the loss of spatial representations when encoding the "
                                        },
                                        {
                                            "pdf_id": "2.41",
                                            "matching_string": "preserve appearance details."
                                        },
                                        {
                                            "pdf_id": "2.21",
                                            "matching_string": "synthesis guided by multimodal conditions. Some "
                                        },
                                        {
                                            "pdf_id": "2.31",
                                            "matching_string": "finetuning methods often optimizes additional "
                                        },
                                        {
                                            "pdf_id": "2.17",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Given the robust generative research attempts to explore work focuses on introducing structural Another group of work uses appearance methods typically "
                                        },
                                        {
                                            "pdf_id": "2.19",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.22",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.25",
                                            "matching_string": ""
                                        },
                                        {
                                            "pdf_id": "2.36",
                                            "matching_string": ""
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 15,
                            "key": "doc/body/sec2/txl2",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In this paper, we mainly focus on controllable human image generation and aim to synthesize human images aligning with specific text prompts, pose signals, and various parts of human appearance. Text2Human generates fullbody human images using detailed descriptions about the textures of clothes, but is limited by the coarsegrained textual condition. Testtime finetuning methods produce satisfactory results, but when it comes to customizing portraits using multiple parts of human appearance, they take much more time to fit each aspect. Recently, methods like IPAdapterFaceID, FastComposer, PhotoMaker, and InstantID show promising results on zeroshot human image personalization. They encode the reference face to one or several tokens as conditions to generate customized images. With the addition of adaptable structural control networks, these methods can generate portraits aligned with specified poses and human identities. However, they usually fail to maintain the details of human identities and utilize all the information from a single image, resulting in ambiguous subject representation. These make it difficult to apply these schemes to precisely generation conditioned on multiple parts of the human appearance. In contrast, our Parts2Whole is both generalizable and efficient, and precisely retains details in multiple parts of human appearance.",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "2.43",
                                    "matching_string": "mainly focus on controllable human image generation and "
                                },
                                {
                                    "pdf_id": "2.48",
                                    "matching_string": "is limited by the coarsegrained textual condition. Testtime "
                                },
                                {
                                    "pdf_id": "2.50",
                                    "matching_string": "results, but when it comes to customizing portraits using "
                                },
                                {
                                    "pdf_id": "2.54",
                                    "matching_string": "image personalization. They encode the reference face to "
                                },
                                {
                                    "pdf_id": "2.55",
                                    "matching_string": "one or several tokens as conditions to generate customized "
                                },
                                {
                                    "pdf_id": "2.58",
                                    "matching_string": "aligned with specified poses and human identities. However, "
                                },
                                {
                                    "pdf_id": "2.60",
                                    "matching_string": "and utilize all the information from a single image, resulting "
                                },
                                {
                                    "pdf_id": "2.62",
                                    "matching_string": "difficult to apply these schemes to precisely generation conditioned "
                                },
                                {
                                    "pdf_id": "2.64",
                                    "matching_string": "our Parts2Whole is both generalizable and efficient, "
                                },
                                {
                                    "pdf_id": "2.99",
                                    "matching_string": "multiple parts of human appearance, they take much more "
                                },
                                {
                                    "pdf_id": "2.47",
                                    "matching_string": "using detailed descriptions about the textures of clothes, but "
                                },
                                {
                                    "pdf_id": "2.44",
                                    "matching_string": "aim to synthesize human images aligning with specific text "
                                },
                                {
                                    "pdf_id": "2.46",
                                    "matching_string": "Text2Human generates fullbody human images "
                                },
                                {
                                    "pdf_id": "2.51",
                                    "matching_string": "time to fit each aspect. Recently, methods like "
                                },
                                {
                                    "pdf_id": "2.53",
                                    "matching_string": "show promising results on zeroshot human "
                                },
                                {
                                    "pdf_id": "2.56",
                                    "matching_string": "images. With the addition of adaptable structural control "
                                },
                                {
                                    "pdf_id": "2.59",
                                    "matching_string": "they usually fail to maintain the details of human identities "
                                },
                                {
                                    "pdf_id": "2.63",
                                    "matching_string": "on multiple parts of the human appearance. In contrast, "
                                },
                                {
                                    "pdf_id": "2.65",
                                    "matching_string": "and precisely retains details in multiple parts of human appearance."
                                },
                                {
                                    "pdf_id": "2.45",
                                    "matching_string": "prompts, pose signals, and various parts of human appearance. "
                                },
                                {
                                    "pdf_id": "2.61",
                                    "matching_string": "in ambiguous subject representation. These make it "
                                },
                                {
                                    "pdf_id": "2.42",
                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} In this paper, we finetuning methods produce satisfactory IPAdapterFaceID, FastComposer, PhotoMaker, and InstantID networks, these methods can generate portraits "
                                },
                                {
                                    "pdf_id": "2.49",
                                    "matching_string": ""
                                },
                                {
                                    "pdf_id": "2.52",
                                    "matching_string": ""
                                },
                                {
                                    "pdf_id": "2.57",
                                    "matching_string": ""
                                },
                                {
                                    "pdf_id": "2.66",
                                    "matching_string": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 16,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Method",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "2.67",
                                    "matching_string": "Method"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec3/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Overview of Parts2Whole. Based on the texttoimage diffusion model, our method designs an appearance encoder for encoding various parts of human appearance into multiscale feature maps. We build this encoder by copying the network structure and pretrained weights from denoising UNet. Features obtained from reference images with their textual labels are injected into the generation process by shared attention mechanism layer by layer. To precisely select the specified parts from reference images, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. An illustration of one block in UNet is shown on the right part.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.0",
                                            "matching_string": "Overview of Parts2Whole. Based on the texttoimage diffusion model, our method designs an appearance encoder for encoding "
                                        },
                                        {
                                            "pdf_id": "3.1",
                                            "matching_string": "various parts of human appearance into multiscale feature maps. We build this encoder by copying the network structure and pretrained "
                                        },
                                        {
                                            "pdf_id": "3.2",
                                            "matching_string": "weights from denoising UNet. Features obtained from reference images with their textual labels are injected into the generation process "
                                        },
                                        {
                                            "pdf_id": "3.3",
                                            "matching_string": "by shared attention mechanism layer by layer. To precisely select the specified parts from reference images, we enhance the vanilla selfattention "
                                        },
                                        {
                                            "pdf_id": "3.4",
                                            "matching_string": "mechanism by incorporating subject masks in the reference images. An illustration of one block in UNet is shown on the right "
                                        },
                                        {
                                            "pdf_id": "3.5",
                                            "matching_string": "part."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 18,
                                    "key": "doc/body/sec3/txl1/txl0",
                                    "block type": "txl",
                                    "content": "We target controllable human image generation guided by multiple reference images. Given N images that capture distinct parts of human appearance x^1:N and a pose map p, and optionally text inputs, our objective is to synthesize a human image x aligning with the specified appearances and posture. To achieve this goal, we propose Parts2Whole, a specialized framework designed to interpolate various reference images and generate highquality portraits.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.68",
                                            "matching_string": "We target controllable human image generation guided by "
                                        },
                                        {
                                            "pdf_id": "2.69",
                                            "matching_string": "multiple reference images. Given N images that capture "
                                        },
                                        {
                                            "pdf_id": "2.71",
                                            "matching_string": "p, and optionally text inputs, our objective is to synthesize "
                                        },
                                        {
                                            "pdf_id": "2.73",
                                            "matching_string": "and posture. To achieve this goal, we propose Parts2Whole, "
                                        },
                                        {
                                            "pdf_id": "2.75",
                                            "matching_string": "images and generate highquality portraits."
                                        },
                                        {
                                            "pdf_id": "2.70",
                                            "matching_string": "distinct parts of human appearance x^1:N and a pose map "
                                        },
                                        {
                                            "pdf_id": "2.72",
                                            "matching_string": "a human image x aligning with the specified appearances "
                                        },
                                        {
                                            "pdf_id": "2.74",
                                            "matching_string": "a specialized framework designed to interpolate various reference "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/txl2",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 19,
                                    "key": "doc/body/sec3/txl2/txl0",
                                    "block type": "txl",
                                    "content": "In general, Parts2Whole is built on texttoimage diffusion models. In the following sections, we start with an overview of T2I diffusion models, and in particular, the selfattention mechanism in Sec.~. We continue by presenting our unified reference framework in Sec.~, which consists of a semanticaware appearance encoder, a shared selfattention that queries referential features within the selfattention layers, and the enhanced maskguided subject selection. These methods enable Parts2Whole to accurately obtain the specific subject information from multiple reference images while preserving appearance details.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.78",
                                            "matching_string": "with an overview of T2I diffusion models, and in particular, "
                                        },
                                        {
                                            "pdf_id": "2.83",
                                            "matching_string": "the selfattention layers, and the enhanced maskguided "
                                        },
                                        {
                                            "pdf_id": "2.85",
                                            "matching_string": "accurately obtain the specific subject information from multiple "
                                        },
                                        {
                                            "pdf_id": "2.76",
                                            "matching_string": "In general, Parts2Whole is built on texttoimage diffusion "
                                        },
                                        {
                                            "pdf_id": "2.80",
                                            "matching_string": "by presenting our unified reference framework in "
                                        },
                                        {
                                            "pdf_id": "2.81",
                                            "matching_string": "which consists of a semanticaware appearance encoder, a "
                                        },
                                        {
                                            "pdf_id": "2.82",
                                            "matching_string": "shared selfattention that queries referential features within "
                                        },
                                        {
                                            "pdf_id": "2.84",
                                            "matching_string": "subject selection. These methods enable Parts2Whole to "
                                        },
                                        {
                                            "pdf_id": "2.79",
                                            "matching_string": "the selfattention mechanism in Sec.~. We continue "
                                        },
                                        {
                                            "pdf_id": "2.86",
                                            "matching_string": "reference images while preserving appearance details."
                                        },
                                        {
                                            "pdf_id": "2.77",
                                            "matching_string": "models. In the following sections, we start Sec.~, "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 20,
                                    "key": "doc/body/sec3/sub3/tit",
                                    "block type": "title",
                                    "content": "Preliminaries",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.87",
                                            "matching_string": "Preliminaries"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 21,
                                            "key": "doc/body/sec3/sub3/txl0/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Diffusion models exhibit promising capabilities in image generation. In this study, we select the widely adopted Stable Diffusion as our foundational model, which is also known as Latent Diffusion Models (LDM). The model operates the denoising process in the latent space of an autoencoder, namely () and (). During the training phase, an input image x0 is initially mapped to the latent space using a frozen encoder, yielding z0=(x0), then perturbed by a predefined Markov process:",
                                            "leftover": "{\\vspace{+1mm}{() ",
                                            "matches": [
                                                {
                                                    "pdf_id": "2.91",
                                                    "matching_string": "our foundational model, which is also known as Latent Diffusion "
                                                },
                                                {
                                                    "pdf_id": "2.95",
                                                    "matching_string": "x0 is initially mapped to the latent space using a frozen encoder, "
                                                },
                                                {
                                                    "pdf_id": "2.97",
                                                    "matching_string": "Markov process:"
                                                },
                                                {
                                                    "pdf_id": "2.89",
                                                    "matching_string": "exhibit promising capabilities in image generation. In this "
                                                },
                                                {
                                                    "pdf_id": "2.92",
                                                    "matching_string": "Models (LDM). The model operates the denoising "
                                                },
                                                {
                                                    "pdf_id": "2.88",
                                                    "matching_string": "TexttoImage Generation.}} Diffusion models "
                                                },
                                                {
                                                    "pdf_id": "2.90",
                                                    "matching_string": "study, we select the widely adopted Stable Diffusion as "
                                                },
                                                {
                                                    "pdf_id": "2.93",
                                                    "matching_string": "process in the latent space of an autoencoder, namely "
                                                },
                                                {
                                                    "pdf_id": "2.94",
                                                    "matching_string": "and (). During the training phase, an input image "
                                                },
                                                {
                                                    "pdf_id": "2.96",
                                                    "matching_string": "yielding z0=(x0), then perturbed by a predefined "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 22,
                                    "key": "doc/body/sec3/sub3/equation1",
                                    "block type": "equation",
                                    "content": "q(zt|zt1)=(zt; (1t)zt1, t)",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "2.98",
                                            "matching_string": "q(zt|zt1)=(zt; (1t)zt1, t)"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 23,
                                            "key": "doc/body/sec3/sub3/txl2/txl0",
                                            "block type": "txl",
                                            "content": "For t=1,, T, where T represents the number of steps in the forward diffusion process. The sequence of hyperparameters t determines the noise strength at each step. The denoising UNet  is trained to approximate the reverse process q(zt1|zt). The training objective is expressed as:",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.6",
                                                    "matching_string": "in the forward diffusion process. The sequence of hyperparameters "
                                                },
                                                {
                                                    "pdf_id": "3.10",
                                                    "matching_string": "denoising UNet  is trained to approximate the reverse process "
                                                },
                                                {
                                                    "pdf_id": "3.8",
                                                    "matching_string": "t determines the noise strength at each step. The "
                                                },
                                                {
                                                    "pdf_id": "3.11",
                                                    "matching_string": "q(zt1|zt). The training objective is expressed as:"
                                                },
                                                {
                                                    "pdf_id": "2.100",
                                                    "matching_string": "For t=1,, T, where T represents the number of steps "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec3/sub3/equation3",
                                    "block type": "equation",
                                    "content": "=(x0),(0, ), c, t[(zt,c,t) 2^2]",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.14",
                                            "matching_string": "=(x0),(0, ), c, t[(zt,c,t) 2^2]"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/txl4",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 25,
                                            "key": "doc/body/sec3/sub3/txl4/txl0",
                                            "block type": "txl",
                                            "content": "Here, c denotes the conditioning texts. At the inference stage, Stable Diffusion effectively reconstructs an image from Gaussian noise step by step, predicting the noise added at each stage. The denoised results are then fed into a latent decoder () to regenerate colored images from the latent representations, denoted as x0 = (0).",
                                            "leftover": "0).",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.15",
                                                    "matching_string": "Here, c denotes the conditioning texts. At the inference "
                                                },
                                                {
                                                    "pdf_id": "3.17",
                                                    "matching_string": "from Gaussian noise step by step, predicting the noise added "
                                                },
                                                {
                                                    "pdf_id": "3.16",
                                                    "matching_string": "stage, Stable Diffusion effectively reconstructs an image "
                                                },
                                                {
                                                    "pdf_id": "3.18",
                                                    "matching_string": "at each stage. The denoised results are then fed into a latent "
                                                },
                                                {
                                                    "pdf_id": "3.19",
                                                    "matching_string": "decoder () to regenerate colored images from the latent "
                                                },
                                                {
                                                    "pdf_id": "3.20",
                                                    "matching_string": "representations, denoted as x0 = "
                                                },
                                                {
                                                    "pdf_id": "4.57",
                                                    "matching_string": "("
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/txl5",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 26,
                                            "key": "doc/body/sec3/sub3/txl5/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Stable Diffusion employs a UNet architecture that consists of convolution layers and transformer attention blocks. In these attention mechanisms, selfattention layers are used to aggregate the spatial features of the image itself and crossattention layers are designed to query information from text embedding. The main difference is that the crossattention layer uses text features as keys and values, while in selfattention layers, image features with spatial dimensions serve as query, key, and value by themselves, preserving more freedom to represent spatially varying visual elements. The selfattention layer takes a feature map of the image as input and computes the attention of the feature in location s with the entire feature map:",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.24",
                                                    "matching_string": "mechanisms, selfattention layers are used to aggregate "
                                                },
                                                {
                                                    "pdf_id": "3.26",
                                                    "matching_string": "layers are designed to query information from "
                                                },
                                                {
                                                    "pdf_id": "3.28",
                                                    "matching_string": "layer uses text features as keys and values, while "
                                                },
                                                {
                                                    "pdf_id": "3.30",
                                                    "matching_string": "serve as query, key, and value by themselves, preserving "
                                                },
                                                {
                                                    "pdf_id": "3.33",
                                                    "matching_string": "image as input and computes the attention of the feature in "
                                                },
                                                {
                                                    "pdf_id": "3.22",
                                                    "matching_string": "a UNet architecture that consists of convolution "
                                                },
                                                {
                                                    "pdf_id": "3.27",
                                                    "matching_string": "text embedding. The main difference is that the crossattention "
                                                },
                                                {
                                                    "pdf_id": "3.29",
                                                    "matching_string": "in selfattention layers, image features with spatial dimensions "
                                                },
                                                {
                                                    "pdf_id": "3.31",
                                                    "matching_string": "more freedom to represent spatially varying visual elements. "
                                                },
                                                {
                                                    "pdf_id": "3.34",
                                                    "matching_string": "location s with the entire feature map:"
                                                },
                                                {
                                                    "pdf_id": "3.25",
                                                    "matching_string": "the spatial features of the image itself and crossattention "
                                                },
                                                {
                                                    "pdf_id": "3.32",
                                                    "matching_string": "The selfattention layer takes a feature map of the "
                                                },
                                                {
                                                    "pdf_id": "3.23",
                                                    "matching_string": "layers and transformer attention blocks. In these attention "
                                                },
                                                {
                                                    "pdf_id": "3.21",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Stable Diffusion employs "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec3/sub3/equation6",
                                    "block type": "equation",
                                    "content": "s = SoftMax( Q(s)K()^ (d) )V()",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "4.56",
                                            "matching_string": "s = SoftMax( Q(s)K()^ (d) )V()"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub3/txl7",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 28,
                                            "key": "doc/body/sec3/sub3/txl7/txl0",
                                            "block type": "txl",
                                            "content": "where Q,K,V are linear projection layers, ^(hw) d is a flattened feature map obtained from the denoiser , where d is the feature dimension, and h,w are intermediate spatial dimensions. s, s is the input and output feature for location s respectively.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.12",
                                                    "matching_string": "where d is the feature dimension, and h,w are intermediate "
                                                },
                                                {
                                                    "pdf_id": "3.40",
                                                    "matching_string": "for location s respectively."
                                                },
                                                {
                                                    "pdf_id": "3.7",
                                                    "matching_string": "where Q,K,V are linear projection layers, ^(hw) d "
                                                },
                                                {
                                                    "pdf_id": "3.9",
                                                    "matching_string": "is a flattened feature map obtained from the denoiser , "
                                                },
                                                {
                                                    "pdf_id": "0.12",
                                                    "matching_string": "s "
                                                },
                                                {
                                                    "pdf_id": "3.13",
                                                    "matching_string": "spatial dimensions. s, is the input and output feature "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec3/sub3/txl8",
                                    "block type": "txl",
                                    "content": "Several works extend the selfattention layer to inject the reference image features, or generate stylealigned or subjectconsistent images, and demonstrate the effectiveness of this mechanism. Inspired by them, we extend the keys and values of the selfattention layer to multiple reference images and preserve the details of the referential appearance successfully.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.44",
                                            "matching_string": "the effectiveness of this mechanism. Inspired by them, we "
                                        },
                                        {
                                            "pdf_id": "3.46",
                                            "matching_string": "reference images and preserve the details of the referential "
                                        },
                                        {
                                            "pdf_id": "3.41",
                                            "matching_string": "Several works extend the selfattention layer to inject the "
                                        },
                                        {
                                            "pdf_id": "3.45",
                                            "matching_string": "extend the keys and values of the selfattention layer to multiple "
                                        },
                                        {
                                            "pdf_id": "3.42",
                                            "matching_string": "reference image features, or generate stylealigned "
                                        },
                                        {
                                            "pdf_id": "3.43",
                                            "matching_string": "or subjectconsistent images, and demonstrate "
                                        },
                                        {
                                            "pdf_id": "3.47",
                                            "matching_string": "appearance successfully."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 30,
                                    "key": "doc/body/sec3/sub4/tit",
                                    "block type": "title",
                                    "content": "Unified Reference Framework",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "3.48",
                                            "matching_string": "Unified Reference Framework"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 31,
                                            "key": "doc/body/sec3/sub4/txl0/txl0",
                                            "block type": "txl",
                                            "content": "As demonstrated in Fig.~, our Parts2Whole consists of two branches: the reference branch used to encode multiple parts of human appearance, and the denoising branch, to gradually denoise the randomly sampled noise to finally obtain the image. The two branches utilize the same network architecture UNet, initialized with the pretrained weights of Stable Diffusion. In detail, our framework mainly consists of three crucial components: 1) SemanticAware Appearance Encoder, encoding the multiscale features of various human parts from reference images; 2) Shared SelfAttention, which obtains detailed information and spatial information by sharing keys and values in selfattention layers between denoising UNet and appearance encoder, and supports pose control by utilizing a lightweight pose encoder; 3) Enhanced MaskGuided Subject Selection, achieving precisely subject selection by explicitly introducing subject masks into the selfattention mechanism.",
                                            "leftover": "utilizing ",
                                            "matches": [
                                                {
                                                    "pdf_id": "3.50",
                                                    "matching_string": "two branches: the reference branch used to encode multiple "
                                                },
                                                {
                                                    "pdf_id": "3.52",
                                                    "matching_string": "to gradually denoise the randomly sampled noise to finally "
                                                },
                                                {
                                                    "pdf_id": "3.54",
                                                    "matching_string": "same network architecture UNet, initialized with the pretrained "
                                                },
                                                {
                                                    "pdf_id": "3.56",
                                                    "matching_string": "framework mainly consists of three crucial components: 1) "
                                                },
                                                {
                                                    "pdf_id": "3.58",
                                                    "matching_string": "features of various human parts from reference images; "
                                                },
                                                {
                                                    "pdf_id": "3.60",
                                                    "matching_string": "and spatial information by sharing keys and values "
                                                },
                                                {
                                                    "pdf_id": "4.5",
                                                    "matching_string": "Subject Selection, achieving precisely subject selection by "
                                                },
                                                {
                                                    "pdf_id": "4.7",
                                                    "matching_string": "mechanism."
                                                },
                                                {
                                                    "pdf_id": "3.57",
                                                    "matching_string": "SemanticAware Appearance Encoder, encoding the multiscale "
                                                },
                                                {
                                                    "pdf_id": "3.59",
                                                    "matching_string": "2) Shared SelfAttention, which obtains detailed information "
                                                },
                                                {
                                                    "pdf_id": "3.61",
                                                    "matching_string": "in selfattention layers between denoising UNet and "
                                                },
                                                {
                                                    "pdf_id": "3.62",
                                                    "matching_string": "appearance encoder, and supports pose control by "
                                                },
                                                {
                                                    "pdf_id": "4.4",
                                                    "matching_string": "a lightweight pose encoder; 3) Enhanced MaskGuided "
                                                },
                                                {
                                                    "pdf_id": "4.6",
                                                    "matching_string": "explicitly introducing subject masks into the selfattention "
                                                },
                                                {
                                                    "pdf_id": "3.49",
                                                    "matching_string": "As demonstrated in Fig.~, our Parts2Whole consists of parts of "
                                                },
                                                {
                                                    "pdf_id": "3.51",
                                                    "matching_string": "human appearance, and the denoising branch, "
                                                },
                                                {
                                                    "pdf_id": "3.53",
                                                    "matching_string": "obtain the image. The two branches utilize the "
                                                },
                                                {
                                                    "pdf_id": "3.55",
                                                    "matching_string": "weights of Stable Diffusion. In detail, our "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/figure1",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "leaf id": 32,
                                            "key": "doc/body/sec3/sub4/figure1/cpt0",
                                            "block type": "cpt",
                                            "content": "Illustration of our MaskGuided Attention. For each patch s (red point) on the feature map ^0, given subject masks M^1:N on the N reference images, we only attend patch s to features in these masks along with the patches on itself.",
                                            "leftover": "M^",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.2",
                                                    "matching_string": "1:N on the N reference images, we only attend patch s to features "
                                                },
                                                {
                                                    "pdf_id": "4.0",
                                                    "matching_string": "Illustration of our MaskGuided Attention. For each "
                                                },
                                                {
                                                    "pdf_id": "4.1",
                                                    "matching_string": "patch s (red point) on the feature map ^0, given subject masks "
                                                },
                                                {
                                                    "pdf_id": "4.3",
                                                    "matching_string": "in these masks along with the patches on itself."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 33,
                                            "key": "doc/body/sec3/sub4/txl2/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In image conditioned generation tasks, previous work employs CLIP image encoder, or combined with some simple linear layers to encode reference images, thereby replacing the original text encoder in Stable Diffusion. However, such methods struggle to preserve appearance details due to the loss of spatial representations when encoding the reference images into semanticlevel features.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.11",
                                                    "matching_string": "linear layers to encode reference images, thereby replacing "
                                                },
                                                {
                                                    "pdf_id": "4.13",
                                                    "matching_string": "such methods struggle to preserve appearance details due to "
                                                },
                                                {
                                                    "pdf_id": "4.15",
                                                    "matching_string": "images into semanticlevel features."
                                                },
                                                {
                                                    "pdf_id": "4.14",
                                                    "matching_string": "the loss of spatial representations when encoding the reference "
                                                },
                                                {
                                                    "pdf_id": "4.10",
                                                    "matching_string": "CLIP image encoder, or combined with some simple "
                                                },
                                                {
                                                    "pdf_id": "4.12",
                                                    "matching_string": "the original text encoder in Stable Diffusion. However, "
                                                },
                                                {
                                                    "pdf_id": "4.8",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} In image conditioned generation tasks, previous work employs "
                                                },
                                                {
                                                    "pdf_id": "4.9",
                                                    "matching_string": ""
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl3",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 34,
                                            "key": "doc/body/sec3/sub4/txl3/txl0",
                                            "block type": "txl",
                                            "content": "Inspired by recent works on dense reference image conditioning, we propose a semanticaware appearance encoder with improved identity and details preservation. Specifically, we adopt a framework identical to the denoising UNet for the appearance encoder. Unlike the denoising branch, we do not add any noise to the reference images. Given N images capturing various parts of human appearance, we first compress them into latent features and then input them into the copied trainable UNet. Instead of simply piecing multiple reference images together, we pass the latent features of different parts through the appearance encoder one by one and provide a textual class label for each part. These text labels, such as face, hair, upper body clothes, etc., are converted into feature representations by CLIP text encoder and then injected into the appearance encoder through crossattention. This simple yet effective external condition provides a classifierlike guidance, which enables the encoder to have semantic awareness of different parts of the human appearance rather than simply performing operations such as image downsampling and upsampling. This helps produce results that are not only rich in detail, but also flexible and realistic.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.18",
                                                    "matching_string": "encoder with improved identity and details preservation. "
                                                },
                                                {
                                                    "pdf_id": "4.20",
                                                    "matching_string": "denoising UNet for the appearance encoder. Unlike the denoising "
                                                },
                                                {
                                                    "pdf_id": "4.22",
                                                    "matching_string": "images. Given N images capturing various parts of human "
                                                },
                                                {
                                                    "pdf_id": "4.24",
                                                    "matching_string": "then input them into the copied trainable UNet. Instead "
                                                },
                                                {
                                                    "pdf_id": "4.27",
                                                    "matching_string": "encoder one by one and provide a textual class label "
                                                },
                                                {
                                                    "pdf_id": "4.29",
                                                    "matching_string": "body clothes, etc., are converted into feature representations "
                                                },
                                                {
                                                    "pdf_id": "4.31",
                                                    "matching_string": "encoder through crossattention. This simple yet "
                                                },
                                                {
                                                    "pdf_id": "4.33",
                                                    "matching_string": "which enables the encoder to have semantic awareness "
                                                },
                                                {
                                                    "pdf_id": "4.36",
                                                    "matching_string": "and upsampling. This helps produce results that are not only "
                                                },
                                                {
                                                    "pdf_id": "4.21",
                                                    "matching_string": "branch, we do not add any noise to the reference "
                                                },
                                                {
                                                    "pdf_id": "4.25",
                                                    "matching_string": "of simply piecing multiple reference images together, we "
                                                },
                                                {
                                                    "pdf_id": "4.28",
                                                    "matching_string": "for each part. These text labels, such as face, hair, upper "
                                                },
                                                {
                                                    "pdf_id": "4.32",
                                                    "matching_string": "effective external condition provides a classifierlike guidance, "
                                                },
                                                {
                                                    "pdf_id": "4.34",
                                                    "matching_string": "of different parts of the human appearance rather than "
                                                },
                                                {
                                                    "pdf_id": "4.35",
                                                    "matching_string": "simply performing operations such as image downsampling "
                                                },
                                                {
                                                    "pdf_id": "4.37",
                                                    "matching_string": "rich in detail, but also flexible and realistic."
                                                },
                                                {
                                                    "pdf_id": "4.16",
                                                    "matching_string": "Inspired by recent works on dense reference "
                                                },
                                                {
                                                    "pdf_id": "4.30",
                                                    "matching_string": "by CLIP text encoder and then injected into the appearance "
                                                },
                                                {
                                                    "pdf_id": "4.19",
                                                    "matching_string": "Specifically, we adopt a framework identical to the "
                                                },
                                                {
                                                    "pdf_id": "4.23",
                                                    "matching_string": "appearance, we first compress them into latent features and "
                                                },
                                                {
                                                    "pdf_id": "4.26",
                                                    "matching_string": "pass the latent features of different parts through the appearance "
                                                },
                                                {
                                                    "pdf_id": "4.17",
                                                    "matching_string": "image conditioning, we propose a semanticaware appearance "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl4",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 35,
                                            "key": "doc/body/sec3/sub4/txl4/txl0",
                                            "block type": "txl",
                                            "content": "In the encoding process, we set the timestep to 0 and only perform one processing instead of iterating successively, so it will not cause time burden at the inference stage. We cache the features before each selfattention layer for the next multiimage conditioned generation.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.38",
                                                    "matching_string": "cache the features before each selfattention layer for the "
                                                },
                                                {
                                                    "pdf_id": "4.89",
                                                    "matching_string": "In the encoding process, we set the timestep to 0 and only "
                                                },
                                                {
                                                    "pdf_id": "4.91",
                                                    "matching_string": "perform one processing instead of iterating successively, so "
                                                },
                                                {
                                                    "pdf_id": "4.93",
                                                    "matching_string": "it will not cause time burden at the inference stage. We "
                                                },
                                                {
                                                    "pdf_id": "4.39",
                                                    "matching_string": "next multiimage conditioned generation."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl5",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 36,
                                            "key": "doc/body/sec3/sub4/txl5/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} After obtaining the multilayer feature maps of N reference images, we do not directly add them to the features in denoising UNet, but use shared keys and values in selfattention to achieve feature injection. This is because our reference and target images are not structurally aligned.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.41",
                                                    "matching_string": "maps of N reference images, we do not directly add "
                                                },
                                                {
                                                    "pdf_id": "4.43",
                                                    "matching_string": "keys and values in selfattention to achieve feature injection. "
                                                },
                                                {
                                                    "pdf_id": "4.45",
                                                    "matching_string": "not structurally aligned."
                                                },
                                                {
                                                    "pdf_id": "4.42",
                                                    "matching_string": "them to the features in denoising UNet, but use shared "
                                                },
                                                {
                                                    "pdf_id": "4.44",
                                                    "matching_string": "This is because our reference and target images are "
                                                },
                                                {
                                                    "pdf_id": "4.40",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} After obtaining the multilayer feature "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl6",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 37,
                                            "key": "doc/body/sec3/sub4/txl6/txl0",
                                            "block type": "txl",
                                            "content": "Take one certain selfattention layer as an example. Given the features of N reference images ^1:N and the feature maps ^0 in the denoising UNet, we concatenate the feature maps of them sidebyside as input to the selfattention layer, denoted as [^0|^1||^N]. This allows each location s on ^0 to attend to all locations on itself and reference feature maps, calculated as:",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.46",
                                                    "matching_string": "Take one certain selfattention layer as an example. "
                                                },
                                                {
                                                    "pdf_id": "4.49",
                                                    "matching_string": "the feature maps of them sidebyside as input to the selfattention "
                                                },
                                                {
                                                    "pdf_id": "4.51",
                                                    "matching_string": "each location s on ^0 to attend to all locations on itself and "
                                                },
                                                {
                                                    "pdf_id": "4.48",
                                                    "matching_string": "feature maps ^0 in the denoising UNet, we concatenate "
                                                },
                                                {
                                                    "pdf_id": "4.52",
                                                    "matching_string": "reference feature maps, calculated as:"
                                                },
                                                {
                                                    "pdf_id": "4.47",
                                                    "matching_string": "Given the features of N reference images ^1:N and the "
                                                },
                                                {
                                                    "pdf_id": "0.11",
                                                    "matching_string": ". "
                                                },
                                                {
                                                    "pdf_id": "4.50",
                                                    "matching_string": "layer, denoted as [^0|^1||^N]This allows "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 38,
                                    "key": "doc/body/sec3/sub4/equation7",
                                    "block type": "equation",
                                    "content": "s^0 = SoftMax( Q(s^0)K([^0|^1||^N])^ (d) ) V([^0|^1||^N])",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "4.53",
                                            "matching_string": "s^0 = SoftMax( Q(s^0)K([^0|^1||^N])^ (d) ) V([^0|^1||^N])"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl8",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 39,
                                            "key": "doc/body/sec3/sub4/txl8/txl0",
                                            "block type": "txl",
                                            "content": "We retain the crossattention layers in Stable Diffusion for injecting CLIP features of reference images and optional text input. We use the decoupled crossattention proposed by IPAdapter to support both images and text input. Specifically, feature maps ^0 obtained from shared selfattention serve as the origin of the query, and the reference image features and text features are each used as the key and value of the two crossattention. The final feature maps are the sum of the two crossattention outputs.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.59",
                                                    "matching_string": "We retain the crossattention layers in Stable Diffusion "
                                                },
                                                {
                                                    "pdf_id": "4.64",
                                                    "matching_string": "from shared selfattention serve as the origin of the query, "
                                                },
                                                {
                                                    "pdf_id": "4.66",
                                                    "matching_string": "used as the key and value of the two crossattention. The "
                                                },
                                                {
                                                    "pdf_id": "4.68",
                                                    "matching_string": "outputs."
                                                },
                                                {
                                                    "pdf_id": "4.62",
                                                    "matching_string": "proposed by IPAdapter to support both images "
                                                },
                                                {
                                                    "pdf_id": "4.65",
                                                    "matching_string": "and the reference image features and text features are each "
                                                },
                                                {
                                                    "pdf_id": "4.67",
                                                    "matching_string": "final feature maps are the sum of the two crossattention "
                                                },
                                                {
                                                    "pdf_id": "4.61",
                                                    "matching_string": "and optional text input. We use the decoupled crossattention "
                                                },
                                                {
                                                    "pdf_id": "4.60",
                                                    "matching_string": "for injecting CLIP features of reference images "
                                                },
                                                {
                                                    "pdf_id": "4.63",
                                                    "matching_string": "and text input. Specifically, feature maps ^0 obtained "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl9",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 40,
                                            "key": "doc/body/sec3/sub4/txl9/txl0",
                                            "block type": "txl",
                                            "content": "To further enhance the controllability of the human image generation, we add the pose map as an additional control. We construct a tiny convolution network, which is similar to the condition embedding network in ControlNet, to extract the features of the pose map. The features are then added to the initial feature maps in the denoising UNet.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.69",
                                                    "matching_string": "To further enhance the controllability of the human image "
                                                },
                                                {
                                                    "pdf_id": "4.71",
                                                    "matching_string": "We construct a tiny convolution network, which is similar "
                                                },
                                                {
                                                    "pdf_id": "4.74",
                                                    "matching_string": "added to the initial feature maps in the denoising UNet."
                                                },
                                                {
                                                    "pdf_id": "4.70",
                                                    "matching_string": "generation, we add the pose map as an additional control. "
                                                },
                                                {
                                                    "pdf_id": "4.73",
                                                    "matching_string": "to extract the features of the pose map. The features are then "
                                                },
                                                {
                                                    "pdf_id": "4.72",
                                                    "matching_string": "to the condition embedding network in ControlNet, "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub4/txl10",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 41,
                                            "key": "doc/body/sec3/sub4/txl10/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We find that the vanilla shared selfattention leads to interference from irrelevant subjects in the reference images (shown in the 6th column in Fig.~), resulting in an unnatural appearance and background. To synthesize human images conditioned on specified parts from each reference image, we enhance the vanilla selfattention mechanism by incorporating subject masks in the reference images. Fig.~ presents this mechanism. Starting with a patch s on a feature map ^0 in the denoising UNet, and subject masks M^1:N on the N reference images. When computing the attention map between the one in the denoising UNet and those from the appearance encoder, patches that do not lie in these masks are ignored. Hence, the target patch s only has access to features of the subjects specified by masks in the reference images, thereby avoiding interference from other elements such as the background. The final formulation of the maskguided attention is defined as follows:",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "4.76",
                                                    "matching_string": "the vanilla shared selfattention leads to interference from "
                                                },
                                                {
                                                    "pdf_id": "4.80",
                                                    "matching_string": "specified parts from each reference image, we enhance the "
                                                },
                                                {
                                                    "pdf_id": "4.85",
                                                    "matching_string": "images. When computing the attention map between "
                                                },
                                                {
                                                    "pdf_id": "4.87",
                                                    "matching_string": "encoder, patches that do not lie in these masks are ignored. "
                                                },
                                                {
                                                    "pdf_id": "4.90",
                                                    "matching_string": "of the subjects specified by masks in the reference images, "
                                                },
                                                {
                                                    "pdf_id": "4.94",
                                                    "matching_string": "the background. The final formulation of the maskguided "
                                                },
                                                {
                                                    "pdf_id": "5.0",
                                                    "matching_string": "attention is defined as follows:"
                                                },
                                                {
                                                    "pdf_id": "4.77",
                                                    "matching_string": "irrelevant subjects in the reference images (shown in the 6th "
                                                },
                                                {
                                                    "pdf_id": "4.79",
                                                    "matching_string": "background. To synthesize human images conditioned on "
                                                },
                                                {
                                                    "pdf_id": "4.82",
                                                    "matching_string": "masks in the reference images. Fig.~ presents this mechanism. "
                                                },
                                                {
                                                    "pdf_id": "4.84",
                                                    "matching_string": "denoising UNet, and subject masks M^1:N on the N reference "
                                                },
                                                {
                                                    "pdf_id": "4.86",
                                                    "matching_string": "the one in the denoising UNet and those from the appearance "
                                                },
                                                {
                                                    "pdf_id": "4.88",
                                                    "matching_string": "Hence, the target patch s only has access to features "
                                                },
                                                {
                                                    "pdf_id": "4.92",
                                                    "matching_string": "thereby avoiding interference from other elements such as "
                                                },
                                                {
                                                    "pdf_id": "4.83",
                                                    "matching_string": "Starting with a patch s on a feature map ^0 in the "
                                                },
                                                {
                                                    "pdf_id": "4.78",
                                                    "matching_string": "column in Fig.~), resulting in an unnatural appearance and "
                                                },
                                                {
                                                    "pdf_id": "4.81",
                                                    "matching_string": "vanilla selfattention mechanism by incorporating subject "
                                                },
                                                {
                                                    "pdf_id": "4.75",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} We find that "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 42,
                                    "key": "doc/body/sec3/sub4/equation11",
                                    "block type": "equation",
                                    "content": "s^0 = SoftMax( Q(s^0)K([^0||^NM^N])^ (d) ) V([^0||^NM^N])",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "5.1",
                                            "matching_string": "s^0 = SoftMax( Q(s^0)K([^0||^NM^N])^ (d) ) V([^0||^NM^N])"
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 43,
                                    "key": "doc/body/sec3/sub4/txl12",
                                    "block type": "txl",
                                    "content": "In practice, to avoid misalignment between masks and original images caused by downsampling, a fullones convolutional kernel is applied to the mask before each attention layer, ensuring that the mask preserves critical regions. Overall, the maskguided attention enhances the ability of Parts2Whole to precisely extract the appearance of specified subjects in reference images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "5.15",
                                            "matching_string": "In practice, to avoid misalignment between masks and "
                                        },
                                        {
                                            "pdf_id": "5.21",
                                            "matching_string": "layer, ensuring that the mask preserves critical regions. "
                                        },
                                        {
                                            "pdf_id": "5.25",
                                            "matching_string": "Parts2Whole to precisely extract the appearance of specified "
                                        },
                                        {
                                            "pdf_id": "5.17",
                                            "matching_string": "original images caused by downsampling, a fullones convolutional "
                                        },
                                        {
                                            "pdf_id": "5.23",
                                            "matching_string": "Overall, the maskguided attention enhances the ability of "
                                        },
                                        {
                                            "pdf_id": "5.19",
                                            "matching_string": "kernel is applied to the mask before each attention "
                                        },
                                        {
                                            "pdf_id": "5.27",
                                            "matching_string": "subjects in reference images."
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 44,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Experiments",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "5.28",
                                    "matching_string": "Experiments"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 45,
                                    "key": "doc/body/sec4/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Qualitative results generated by Parts2Whole and existing alternatives on our partitioned test set. We do not show the text condition in the figure, but notably, when we input the reference images to our proposed appearance encoder, we will pass in short labels such as face, hair or headwear, upper body clothes, lower body clothes, whole body clothes, shoes, etc.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "6.0",
                                            "matching_string": "Qualitative results generated by Parts2Whole and existing alternatives on our partitioned test set. We do not show the text "
                                        },
                                        {
                                            "pdf_id": "6.1",
                                            "matching_string": "condition in the figure, but notably, when we input the reference images to our proposed appearance encoder, we will pass in short labels "
                                        },
                                        {
                                            "pdf_id": "6.2",
                                            "matching_string": "such as face, hair or headwear, upper body clothes, lower body clothes, whole body clothes, shoes, etc."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 46,
                                    "key": "doc/body/sec4/sub1/tit",
                                    "block type": "title",
                                    "content": "Implementation Details",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "5.29",
                                            "matching_string": "Implementation Details"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub1/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 47,
                                            "key": "doc/body/sec4/sub1/txl0/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} To train the Parts2Whole model, we build a multimodal dataset comprising about 41,500 referencetarget pairs from the opensource DeepFashionMultiModal dataset. Each pair in this newly constructed dataset includes multiple reference images, which encompass human pose images (e.g., OpenPose, Human Parsing, DensePose), various aspects of human appearance (e.g., hair, face, clothes, shoes) with their short textual labels, and a target image featuring the same individual (ID) in the same outfit but in a different pose, along with textual captions.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.31",
                                                    "matching_string": "multimodal dataset comprising about 41,500 referencetarget "
                                                },
                                                {
                                                    "pdf_id": "5.44",
                                                    "matching_string": "pose images (e.g., OpenPose, Human Parsing, DensePose), "
                                                },
                                                {
                                                    "pdf_id": "5.48",
                                                    "matching_string": "clothes, shoes) with their short textual labels, and a target "
                                                },
                                                {
                                                    "pdf_id": "5.52",
                                                    "matching_string": "but in a different pose, along with textual captions."
                                                },
                                                {
                                                    "pdf_id": "5.30",
                                                    "matching_string": "To train the Parts2Whole model, we build a "
                                                },
                                                {
                                                    "pdf_id": "5.46",
                                                    "matching_string": "various aspects of human appearance (e.g., hair, face, "
                                                },
                                                {
                                                    "pdf_id": "5.50",
                                                    "matching_string": "image featuring the same individual (ID) in the same outfit "
                                                },
                                                {
                                                    "pdf_id": "5.42",
                                                    "matching_string": "includes multiple reference images, which encompass human "
                                                },
                                                {
                                                    "pdf_id": "5.32",
                                                    "matching_string": "pairs from the opensource DeepFashionMultiModal "
                                                },
                                                {
                                                    "pdf_id": "5.33",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} dataset. Each pair in this newly constructed dataset "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub1/txl1",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 48,
                                            "key": "doc/body/sec4/sub1/txl1/txl0",
                                            "block type": "txl",
                                            "content": "The DeepFashionMultiModal dataset exhibits noise in its ID data. For example, different images are tagged with the same ID but depict different individuals. To address this issue, we first cleanse the IDs by extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs to distinguish between different ID images within the same ID group. Subsequently, we utilize DWPose to generate pose images corresponding to each image. Guided by human parsing files, we crop human images into various parts. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images. Textual descriptions of the original dataset are used as captions. For constructing pairs, we select images with cleaned IDs that feature the same clothes and individual but in different poses. Specifically, a pair contains multiple parts from one human image as reference images, and an image of the person in another pose as the target. Finally, we build a total of about 41,500 pairs, of which the training set is about 40,000 and the test set is about 1,500 pairs.",
                                            "leftover": "InsightFace. RealESRGAN ",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.54",
                                                    "matching_string": "The DeepFashionMultiModal dataset exhibits noise in "
                                                },
                                                {
                                                    "pdf_id": "5.58",
                                                    "matching_string": "the same ID but depict different individuals. To address "
                                                },
                                                {
                                                    "pdf_id": "5.66",
                                                    "matching_string": "the similarity between image ID feature pairs to distinguish "
                                                },
                                                {
                                                    "pdf_id": "5.74",
                                                    "matching_string": "parsing files, we crop human images into various parts. "
                                                },
                                                {
                                                    "pdf_id": "5.80",
                                                    "matching_string": "the original dataset are used as captions. For constructing "
                                                },
                                                {
                                                    "pdf_id": "5.82",
                                                    "matching_string": "same clothes and individual but in different poses. Specifically, "
                                                },
                                                {
                                                    "pdf_id": "5.84",
                                                    "matching_string": "as reference images, and an image of the person in another "
                                                },
                                                {
                                                    "pdf_id": "5.86",
                                                    "matching_string": "pairs, of which the training set is about 40,000 and the test "
                                                },
                                                {
                                                    "pdf_id": "5.60",
                                                    "matching_string": "this issue, we first cleanse the IDs by extracting facial "
                                                },
                                                {
                                                    "pdf_id": "5.62",
                                                    "matching_string": "ID features from images tagged with the same ID using "
                                                },
                                                {
                                                    "pdf_id": "5.68",
                                                    "matching_string": "between different ID images within the same ID "
                                                },
                                                {
                                                    "pdf_id": "5.76",
                                                    "matching_string": "Due to the low resolution of the cropped parts, we apply "
                                                },
                                                {
                                                    "pdf_id": "5.79",
                                                    "matching_string": "obtaining clearer reference images. Textual descriptions of "
                                                },
                                                {
                                                    "pdf_id": "5.81",
                                                    "matching_string": "pairs, we select images with cleaned IDs that feature the "
                                                },
                                                {
                                                    "pdf_id": "5.83",
                                                    "matching_string": "a pair contains multiple parts from one human image "
                                                },
                                                {
                                                    "pdf_id": "5.85",
                                                    "matching_string": "pose as the target. Finally, we build a total of about 41,500 "
                                                },
                                                {
                                                    "pdf_id": "5.87",
                                                    "matching_string": "set is about 1,500 pairs."
                                                },
                                                {
                                                    "pdf_id": "5.56",
                                                    "matching_string": "its ID data. For example, different images are tagged with "
                                                },
                                                {
                                                    "pdf_id": "5.72",
                                                    "matching_string": "pose images corresponding to each image. Guided by human "
                                                },
                                                {
                                                    "pdf_id": "5.64",
                                                    "matching_string": "Cosine similarity is then used to evaluate "
                                                },
                                                {
                                                    "pdf_id": "5.70",
                                                    "matching_string": "group. Subsequently, we utilize DWPose to generate "
                                                },
                                                {
                                                    "pdf_id": "5.78",
                                                    "matching_string": "to enhance the image resolution, thus "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 49,
                                    "key": "doc/body/sec4/sub1/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In this work, the denoising UNet and the appearance encoder both leverage the pretrained weights from Stable Diffusion1.5. We use CLIP Vision Model with projection layers as our image encoder, initialized with Stable Diffusion Image Variations. During training, we set the initial learning rate 1e5 with a batch size of 64. The model is trained using 8 A800 GPUs, for a total of 30000 iterations. To maintain the capability of image generation, we randomly drop all of the reference image features and the pose condition with a probability of 0.2. At the same time, to improve the flexibility of generation, we randomly drop each appearance condition with a probability of 0.2, so that the human images can be generating from indefinite reference images. At the inference stage, we adopt DDIM sampler with 50 steps, and set the guidance scale to 7.5.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "5.11",
                                            "matching_string": "1e5 with a batch size of 64. The model is trained using 8 "
                                        },
                                        {
                                            "pdf_id": "5.16",
                                            "matching_string": "of 0.2. At the same time, to improve the flexibility "
                                        },
                                        {
                                            "pdf_id": "5.20",
                                            "matching_string": "with a probability of 0.2, so that the human images can be "
                                        },
                                        {
                                            "pdf_id": "5.26",
                                            "matching_string": "set the guidance scale to 7.5."
                                        },
                                        {
                                            "pdf_id": "5.100",
                                            "matching_string": "and the appearance encoder both leverage the pretrained "
                                        },
                                        {
                                            "pdf_id": "5.14",
                                            "matching_string": "reference image features and the pose condition with a probability "
                                        },
                                        {
                                            "pdf_id": "5.9",
                                            "matching_string": "encoder, initialized with Stable Diffusion Image "
                                        },
                                        {
                                            "pdf_id": "5.12",
                                            "matching_string": "A800 GPUs, for a total of 30000 iterations. To maintain the "
                                        },
                                        {
                                            "pdf_id": "5.13",
                                            "matching_string": "capability of image generation, we randomly drop all of the "
                                        },
                                        {
                                            "pdf_id": "5.18",
                                            "matching_string": "of generation, we randomly drop each appearance condition "
                                        },
                                        {
                                            "pdf_id": "5.22",
                                            "matching_string": "generating from indefinite reference images. At the inference "
                                        },
                                        {
                                            "pdf_id": "5.24",
                                            "matching_string": "stage, we adopt DDIM sampler with 50 steps, and "
                                        },
                                        {
                                            "pdf_id": "5.104",
                                            "matching_string": "CLIP Vision Model with projection layers as our image "
                                        },
                                        {
                                            "pdf_id": "5.10",
                                            "matching_string": "During training, we set the initial learning rate "
                                        },
                                        {
                                            "pdf_id": "5.102",
                                            "matching_string": "weights from Stable Diffusion1.5. We use "
                                        },
                                        {
                                            "pdf_id": "5.98",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} In this work, the denoising UNet Variations. "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 50,
                                    "key": "doc/body/sec4/sub2/tit",
                                    "block type": "title",
                                    "content": "Comparison with Existing Alternatives",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "5.34",
                                            "matching_string": "Comparison with Existing Alternatives"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/txl0",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 51,
                                            "key": "doc/body/sec4/sub2/txl0/txl0",
                                            "block type": "txl",
                                            "content": "Our Parts2Whole targets at controllable human image generation conditioned on multiple parts of human appearance. To evaluate the performance of our proposed framework, we compare our Parts2Whole with existing subjectdriven solutions. For fairness, we make some improvements to the methods, to make them more suitable for generating human images from multiple conditions.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.35",
                                                    "matching_string": "Our Parts2Whole targets at controllable human image generation "
                                                },
                                                {
                                                    "pdf_id": "5.37",
                                                    "matching_string": "To evaluate the performance of our proposed framework, "
                                                },
                                                {
                                                    "pdf_id": "5.39",
                                                    "matching_string": "solutions. For fairness, we make some improvements to the "
                                                },
                                                {
                                                    "pdf_id": "5.41",
                                                    "matching_string": "images from multiple conditions."
                                                },
                                                {
                                                    "pdf_id": "5.36",
                                                    "matching_string": "conditioned on multiple parts of human appearance. "
                                                },
                                                {
                                                    "pdf_id": "5.38",
                                                    "matching_string": "we compare our Parts2Whole with existing subjectdriven "
                                                },
                                                {
                                                    "pdf_id": "5.40",
                                                    "matching_string": "methods, to make them more suitable for generating human "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/txl1",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 52,
                                            "key": "doc/body/sec4/sub2/txl1/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningbased methods, we adopt DreamBooth LoRA and Custom Diffusion as baseline methods for comparison, as these methods are relatively robust and effective. DreamBooth LoRA inserts a smaller number of new weights into Stable Diffusion and only trains these parameters on just a few images of a subject or style, thereby associating a special word in the prompt with the example images. Custom Diffusion finetunes only key and value projection matrices in the crossattention layers to customize texttoimage models. Given as input several aspects of human appearance, we use these two methods to finetune Stable Diffusion, such that it learns to bind identifiers with specific human parts. As shown in, when it comes to multiaspect composition, the attributes of different parts in images generated by these tuningbase methods mix together, resulting in unrealistic human images. In contrast, Parts2Whole generates highfidelity results without the need for parameter tuning.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.49",
                                                    "matching_string": "methods are relatively robust and effective. DreamBooth "
                                                },
                                                {
                                                    "pdf_id": "5.55",
                                                    "matching_string": "images of a subject or style, thereby associating a special "
                                                },
                                                {
                                                    "pdf_id": "5.61",
                                                    "matching_string": "the crossattention layers to customize texttoimage models. "
                                                },
                                                {
                                                    "pdf_id": "5.65",
                                                    "matching_string": "use these two methods to finetune Stable Diffusion, such "
                                                },
                                                {
                                                    "pdf_id": "5.71",
                                                    "matching_string": "the attributes of different parts in images generated "
                                                },
                                                {
                                                    "pdf_id": "5.75",
                                                    "matching_string": "human images. In contrast, Parts2Whole generates "
                                                },
                                                {
                                                    "pdf_id": "5.51",
                                                    "matching_string": "LoRA inserts a smaller number of new weights into Stable "
                                                },
                                                {
                                                    "pdf_id": "5.57",
                                                    "matching_string": "word in the prompt with the example images. Custom Diffusion "
                                                },
                                                {
                                                    "pdf_id": "5.63",
                                                    "matching_string": "Given as input several aspects of human appearance, we "
                                                },
                                                {
                                                    "pdf_id": "5.67",
                                                    "matching_string": "that it learns to bind identifiers with specific human parts. "
                                                },
                                                {
                                                    "pdf_id": "5.73",
                                                    "matching_string": "by these tuningbase methods mix together, resulting in unrealistic "
                                                },
                                                {
                                                    "pdf_id": "5.77",
                                                    "matching_string": "highfidelity results without the need for parameter tuning."
                                                },
                                                {
                                                    "pdf_id": "5.45",
                                                    "matching_string": "methods, we adopt DreamBooth LoRA and Custom "
                                                },
                                                {
                                                    "pdf_id": "5.47",
                                                    "matching_string": "Diffusion as baseline methods for comparison, as these "
                                                },
                                                {
                                                    "pdf_id": "5.53",
                                                    "matching_string": "Diffusion and only trains these parameters on just a few "
                                                },
                                                {
                                                    "pdf_id": "5.59",
                                                    "matching_string": "finetunes only key and value projection matrices in "
                                                },
                                                {
                                                    "pdf_id": "5.69",
                                                    "matching_string": "As shown in, when it comes to multiaspect composition, "
                                                },
                                                {
                                                    "pdf_id": "5.43",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningbased "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 53,
                                            "key": "doc/body/sec4/sub2/txl2/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningfree methods, we adopt IPAdapter and SSREncoder for comparison. IPAdapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting, and can be combined with other adapters like ControlNet. We firstly use IPAdapter FaceID and ControlNet to generate human images from facial appearance and pose maps. Then we repaint hair, clothes, shoes and other areas using the specific image step by step, thereby achieving multiimage conditioned generation of portraits in a multistep way. SSREncoder is an effective encoder designed for selectively capturing any subject from single or multiple reference images by the text query or mask query. For fairness, we finetune it in our human dataset to enhance its ability for human images.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "5.92",
                                                    "matching_string": "prompting, and can be combined with other adapters "
                                                },
                                                {
                                                    "pdf_id": "5.94",
                                                    "matching_string": "and ControlNet to generate human images from facial appearance "
                                                },
                                                {
                                                    "pdf_id": "5.96",
                                                    "matching_string": "shoes and other areas using the specific image step by step, "
                                                },
                                                {
                                                    "pdf_id": "5.99",
                                                    "matching_string": "portraits in a multistep way. SSREncoder is an effective "
                                                },
                                                {
                                                    "pdf_id": "5.103",
                                                    "matching_string": "from single or multiple reference images by the text query "
                                                },
                                                {
                                                    "pdf_id": "6.3",
                                                    "matching_string": "dataset to enhance its ability for human images."
                                                },
                                                {
                                                    "pdf_id": "5.91",
                                                    "matching_string": "that can be plugged into diffusion models to enable image "
                                                },
                                                {
                                                    "pdf_id": "5.89",
                                                    "matching_string": "we adopt IPAdapter and SSREncoder "
                                                },
                                                {
                                                    "pdf_id": "5.90",
                                                    "matching_string": "for comparison. IPAdapter is an image prompt adapter "
                                                },
                                                {
                                                    "pdf_id": "5.105",
                                                    "matching_string": "or mask query. For fairness, we finetune it in our human "
                                                },
                                                {
                                                    "pdf_id": "5.93",
                                                    "matching_string": "like ControlNet. We firstly use IPAdapter FaceID "
                                                },
                                                {
                                                    "pdf_id": "5.95",
                                                    "matching_string": "and pose maps. Then we repaint hair, clothes, "
                                                },
                                                {
                                                    "pdf_id": "5.97",
                                                    "matching_string": "thereby achieving multiimage conditioned generation of "
                                                },
                                                {
                                                    "pdf_id": "5.101",
                                                    "matching_string": "encoder designed for selectively capturing any subject "
                                                },
                                                {
                                                    "pdf_id": "5.88",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Among the tuningfree methods, "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/txl3",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 54,
                                            "key": "doc/body/sec4/sub2/txl3/txl0",
                                            "block type": "txl",
                                            "content": "We compare our Parts2Whole with the above two referencebased alternatives in the test set. For quantitative comparison, we compute the commonly used CLIP score and DINO score to evaluate the similarity between the generated image and the specified human parts. For further alignment evaluation, we use DreamSim, a new metric for perceptual image similarity that bridges the gap between ''lowlevel'' metrics (e.g., LPIPS, PSNR, SSIM) and ''highlevel'' measures (e.g., CLIP). Since the generated image is conditioned on multiple parts of human appearance, it is difficult to evaluate the degree of alignment by calculating the average metric with these multiple images. Therefore, we calculate the above three indicators between the output image and the original reference portrait from which these different parts come. We present the quantitative results in and the qualitative results in ~. Both IPAdapter and SSREncoder fail to maintain alignment with the specified appearance images and often produce unrealistic results when multipart combinations are involved. In comparison, our method achieves the best results in terms of image quality and appearance alignment.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "6.4",
                                                    "matching_string": "We compare our Parts2Whole with the above two "
                                                },
                                                {
                                                    "pdf_id": "6.8",
                                                    "matching_string": "image and the specified human parts. For further "
                                                },
                                                {
                                                    "pdf_id": "6.10",
                                                    "matching_string": "for perceptual image similarity that bridges the gap between "
                                                },
                                                {
                                                    "pdf_id": "6.11",
                                                    "matching_string": "''lowlevel'' metrics (e.g., LPIPS, PSNR, SSIM) and ''highlevel'' "
                                                },
                                                {
                                                    "pdf_id": "6.13",
                                                    "matching_string": "conditioned on multiple parts of human appearance, it is "
                                                },
                                                {
                                                    "pdf_id": "6.14",
                                                    "matching_string": "difficult to evaluate the degree of alignment by calculating "
                                                },
                                                {
                                                    "pdf_id": "7.22",
                                                    "matching_string": "the average metric with these multiple images. Therefore, "
                                                },
                                                {
                                                    "pdf_id": "7.24",
                                                    "matching_string": "image and the original reference portrait from which these "
                                                },
                                                {
                                                    "pdf_id": "7.27",
                                                    "matching_string": "and SSREncoder fail to maintain alignment with the specified "
                                                },
                                                {
                                                    "pdf_id": "7.29",
                                                    "matching_string": "when multipart combinations are involved. In comparison, "
                                                },
                                                {
                                                    "pdf_id": "7.30",
                                                    "matching_string": "our method achieves the best results in terms of image quality "
                                                },
                                                {
                                                    "pdf_id": "6.5",
                                                    "matching_string": "referencebased alternatives in the test set. For quantitative "
                                                },
                                                {
                                                    "pdf_id": "6.6",
                                                    "matching_string": "comparison, we compute the commonly used CLIP score and DINO score "
                                                },
                                                {
                                                    "pdf_id": "7.23",
                                                    "matching_string": "we calculate the above three indicators between the output "
                                                },
                                                {
                                                    "pdf_id": "7.25",
                                                    "matching_string": "different parts come. We present the quantitative results in "
                                                },
                                                {
                                                    "pdf_id": "7.28",
                                                    "matching_string": "appearance images and often produce unrealistic results "
                                                },
                                                {
                                                    "pdf_id": "6.9",
                                                    "matching_string": "alignment evaluation, we use DreamSim, a new metric "
                                                },
                                                {
                                                    "pdf_id": "6.12",
                                                    "matching_string": "measures (e.g., CLIP). Since the generated image is "
                                                },
                                                {
                                                    "pdf_id": "7.26",
                                                    "matching_string": "and the qualitative results in ~. Both IPAdapter "
                                                },
                                                {
                                                    "pdf_id": "7.31",
                                                    "matching_string": "and appearance alignment."
                                                },
                                                {
                                                    "pdf_id": "6.7",
                                                    "matching_string": "to evaluate the similarity between the generated "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 55,
                                                    "key": "doc/body/sec4/sub2/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Quantitative comparison between our Parts2Whole and existing referencebased alternatives.",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.2",
                                                            "matching_string": "existing referencebased alternatives."
                                                        },
                                                        {
                                                            "pdf_id": "7.0",
                                                            "matching_string": "Quantitative comparison between our Parts2Whole and "
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec4/sub2/table4/tabular1",
                                            "block_type": "tabular",
                                            "children": [
                                                {
                                                    "leaf id": 56,
                                                    "key": "doc/body/sec4/sub2/table4/tabular1/txl0",
                                                    "block type": "txl",
                                                    "content": "{@{}lccc@{}} \\toprule Method & CLIP & DINO & DreamSim \\midrule IPAdapter + Inpaint & 80.1 & 69.8 & 0.445 SSREncoder & 86.9 & 75.1 & 0.346 Parts2Whole (Ours) & 91.2 & 93.7 & 0.221 \\bottomrule",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.6",
                                                            "matching_string": "IPAdapter + Inpaint & 80.1 & 69.8 & 0.445 "
                                                        },
                                                        {
                                                            "pdf_id": "7.9",
                                                            "matching_string": "Parts2Whole (Ours) & 91.2 & 93.7 & 0.221 "
                                                        },
                                                        {
                                                            "pdf_id": "0.14",
                                                            "matching_string": "1 "
                                                        },
                                                        {
                                                            "pdf_id": "7.4",
                                                            "matching_string": "Method & CLIP & DINO "
                                                        },
                                                        {
                                                            "pdf_id": "7.8",
                                                            "matching_string": "SSREncoder & 86.9 & 75.& 0.346 "
                                                        },
                                                        {
                                                            "pdf_id": "0.15",
                                                            "matching_string": "{@{}lccc@{}} \\toprule & DreamSim \\midrule \\bottomrule"
                                                        },
                                                        {
                                                            "pdf_id": "0.16",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "0.17",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "0.18",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "0.19",
                                                            "matching_string": ""
                                                        },
                                                        {
                                                            "pdf_id": "0.20",
                                                            "matching_string": ""
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/txl5",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 57,
                                            "key": "doc/body/sec4/sub2/txl5/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We conduct a user study to further evaluate the referencebased methods IPAdapter, SSREncoder and our Parts2Whole. We randomly select 20 pairs of referencetarget pairs from the test set. For each pair, we provide multiple referential appearance images, pose images, textual captions, and the generated human images. We evaluate the performance from two main aspects: first, the quality of the generated images, which primarily refers to the realism, rationality, and clarity of the images; and second, the similarity between the generated images and the reference images. The similarity assessment includes consistency in ID, pose, texture, and color between the generated images and the reference images. We involve 20 users in the user study, who are required to score the three methods based on these two evaluative aspects. The final experimental results are shown in, from which we observe that our model owns obvious superiorities for alignment with given appearance conditions.",
                                            "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} ",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.35",
                                                    "matching_string": "20 pairs of referencetarget pairs from the test set. For "
                                                },
                                                {
                                                    "pdf_id": "7.37",
                                                    "matching_string": "pose images, textual captions, and the generated human "
                                                },
                                                {
                                                    "pdf_id": "7.39",
                                                    "matching_string": "aspects: first, the quality of the generated images, which "
                                                },
                                                {
                                                    "pdf_id": "7.42",
                                                    "matching_string": "images and the reference images. The similarity assessment "
                                                },
                                                {
                                                    "pdf_id": "7.44",
                                                    "matching_string": "the generated images and the reference images. We involve "
                                                },
                                                {
                                                    "pdf_id": "7.46",
                                                    "matching_string": "three methods based on these two evaluative aspects. The "
                                                },
                                                {
                                                    "pdf_id": "7.48",
                                                    "matching_string": "we observe that our model owns obvious superiorities for "
                                                },
                                                {
                                                    "pdf_id": "7.41",
                                                    "matching_string": "images; and second, the similarity between the generated "
                                                },
                                                {
                                                    "pdf_id": "7.36",
                                                    "matching_string": "each pair, we provide multiple referential appearance images, "
                                                },
                                                {
                                                    "pdf_id": "7.40",
                                                    "matching_string": "primarily refers to the realism, rationality, and clarity of the "
                                                },
                                                {
                                                    "pdf_id": "7.43",
                                                    "matching_string": "includes consistency in ID, pose, texture, and color between "
                                                },
                                                {
                                                    "pdf_id": "7.45",
                                                    "matching_string": "20 users in the user study, who are required to score the "
                                                },
                                                {
                                                    "pdf_id": "7.49",
                                                    "matching_string": "alignment with given appearance conditions."
                                                },
                                                {
                                                    "pdf_id": "7.33",
                                                    "matching_string": "the referencebased methods IPAdapter, SSREncoder "
                                                },
                                                {
                                                    "pdf_id": "7.34",
                                                    "matching_string": "and our Parts2Whole. We randomly select "
                                                },
                                                {
                                                    "pdf_id": "7.38",
                                                    "matching_string": "images. We evaluate the performance from two main "
                                                },
                                                {
                                                    "pdf_id": "7.47",
                                                    "matching_string": "final experimental results are shown in, from which "
                                                },
                                                {
                                                    "pdf_id": "7.32",
                                                    "matching_string": "We conduct a user study to further evaluate "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table6",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table6/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 58,
                                                    "key": "doc/body/sec4/sub2/table6/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "User study on the comparison with existing referencebased alternatives. ''Quality'' and ''Similarity'' measures synthesis quality and appearance preservation. Each metric is rated from 1 (worst) to 5 (best).",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.13",
                                                            "matching_string": "alternatives. ''Quality'' and ''Similarity'' measures synthesis "
                                                        },
                                                        {
                                                            "pdf_id": "7.17",
                                                            "matching_string": "(worst) to 5 (best)."
                                                        },
                                                        {
                                                            "pdf_id": "7.12",
                                                            "matching_string": "User study on the comparison with existing referencebased "
                                                        },
                                                        {
                                                            "pdf_id": "7.15",
                                                            "matching_string": "quality and appearance preservation. Each metric is rated from 1 "
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec4/sub2/table6/tabular1",
                                            "block_type": "tabular",
                                            "children": [
                                                {
                                                    "leaf id": 59,
                                                    "key": "doc/body/sec4/sub2/table6/tabular1/txl0",
                                                    "block type": "txl",
                                                    "content": "{@{}lcccc@{}} \\toprule Method & Quality & Similarity \\midrule IPAdapter + Inpaint & 3.78 & 3.58 SSREncoder & 3.64 & 3.14 Parts2Whole (Ours) & 4.52 & 4.55 \\bottomrule",
                                                    "leftover": "{@{}lcccc@{}} \\toprule \\midrule \\bottomrule",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.21",
                                                            "matching_string": "Parts2Whole (Ours) & 4.52 & 4.55 "
                                                        },
                                                        {
                                                            "pdf_id": "7.18",
                                                            "matching_string": "Method & Quality & Similarity "
                                                        },
                                                        {
                                                            "pdf_id": "7.19",
                                                            "matching_string": "IPAdapter + Inpaint & 3.78 & 3.58 "
                                                        },
                                                        {
                                                            "pdf_id": "7.20",
                                                            "matching_string": "SSREncoder & 3.64 & 3.14 "
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 60,
                                    "key": "doc/body/sec4/sub3/tit",
                                    "block type": "title",
                                    "content": "Ablation Studies",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "7.50",
                                            "matching_string": "Ablation Studies"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/figure*0",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 61,
                                            "key": "doc/body/sec4/sub3/figure*0/cpt0",
                                            "block type": "cpt",
                                            "content": "Qualitative analysis of using different backbones for the appearance encoder, and our proposed methods.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "8.0",
                                                    "matching_string": "Qualitative analysis of using different backbones for the appearance encoder, and our proposed methods."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/txl1",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 62,
                                            "key": "doc/body/sec4/sub3/txl1/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} As described in Sec.~, to extract detailed features from multiple reference images, our Parts2Whole designs an appearance encoder by copying the network structure and pretrained weights from the denoising UNet. Here, we compare it to the baseline with other image encoders. Specifically, we leverage the CLIP image encoder, DINOv2, and ControlNet as feature extractors and apply the same training settings for fair comparison. The qualitative results of generated human images are presented in . From the second and third columns of the figure, we observe that these semanticlevel feature extractors cannot preserve the appearance details of multiple reference images and only extract color and rough texture. ControlNet directly adds different image features with misaligned structures to the feature maps, resulting in unstable image quality. In contrast, our proposed appearance encoder provides finegrained details of multiple aspects of human appearance.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.16",
                                                    "matching_string": "weights from the denoising UNet. Here, we compare it "
                                                },
                                                {
                                                    "pdf_id": "7.54",
                                                    "matching_string": "training settings for fair comparison. The qualitative results "
                                                },
                                                {
                                                    "pdf_id": "7.58",
                                                    "matching_string": "the appearance details of multiple reference images and "
                                                },
                                                {
                                                    "pdf_id": "7.59",
                                                    "matching_string": "only extract color and rough texture. ControlNet directly "
                                                },
                                                {
                                                    "pdf_id": "7.61",
                                                    "matching_string": "the feature maps, resulting in unstable image quality. In "
                                                },
                                                {
                                                    "pdf_id": "7.63",
                                                    "matching_string": "details of multiple aspects of human appearance."
                                                },
                                                {
                                                    "pdf_id": "7.14",
                                                    "matching_string": "by copying the network structure and pretrained "
                                                },
                                                {
                                                    "pdf_id": "7.51",
                                                    "matching_string": "to the baseline with other image encoders. Specifically, we "
                                                },
                                                {
                                                    "pdf_id": "7.53",
                                                    "matching_string": "ControlNet as feature extractors and apply the same "
                                                },
                                                {
                                                    "pdf_id": "7.56",
                                                    "matching_string": "From the second and third columns of the figure, we observe "
                                                },
                                                {
                                                    "pdf_id": "7.57",
                                                    "matching_string": "that these semanticlevel feature extractors cannot preserve "
                                                },
                                                {
                                                    "pdf_id": "7.60",
                                                    "matching_string": "adds different image features with misaligned structures to "
                                                },
                                                {
                                                    "pdf_id": "7.62",
                                                    "matching_string": "contrast, our proposed appearance encoder provides finegrained "
                                                },
                                                {
                                                    "pdf_id": "7.92",
                                                    "matching_string": "in Sec.~, to extract detailed features from multiple reference "
                                                },
                                                {
                                                    "pdf_id": "7.55",
                                                    "matching_string": "of generated human images are presented in "
                                                },
                                                {
                                                    "pdf_id": "7.94",
                                                    "matching_string": "images, our Parts2Whole designs an appearance "
                                                },
                                                {
                                                    "pdf_id": "0.21",
                                                    "matching_string": ". "
                                                },
                                                {
                                                    "pdf_id": "7.52",
                                                    "matching_string": "leverage the CLIP image encoder, DINOv2, and "
                                                },
                                                {
                                                    "pdf_id": "7.90",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} As described encoder "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/txl2",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 63,
                                            "key": "doc/body/sec4/sub3/txl2/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In the process of encoding multiple reference image features, we provide a textual class label for each aspect of human appearance, thus providing a classifierlike guidance. To assess the effectiveness of the additional external condition, we compare it with directly concatenating multiple reference images in the dimension of width as input to the appearance encoder. As shown in the 5th column in, simply piecing reference images produces images relatively aligned with the given images, but leads to stifflooking and unrealistic results. This is because modeling only the image itself makes the model lack awareness of different types of appearances. Conversely, after injecting different semantic labels for each reference image, the model has an awareness of various parts of the human appearance, producing realistic and flexible portraits.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.65",
                                                    "matching_string": "reference image features, we provide a textual class label "
                                                },
                                                {
                                                    "pdf_id": "7.67",
                                                    "matching_string": "classifierlike guidance. To assess the effectiveness of the "
                                                },
                                                {
                                                    "pdf_id": "7.68",
                                                    "matching_string": "additional external condition, we compare it with directly "
                                                },
                                                {
                                                    "pdf_id": "7.70",
                                                    "matching_string": "of width as input to the appearance encoder. As shown in "
                                                },
                                                {
                                                    "pdf_id": "7.72",
                                                    "matching_string": "produces images relatively aligned with the given images, "
                                                },
                                                {
                                                    "pdf_id": "7.74",
                                                    "matching_string": "modeling only the image itself makes the model lack "
                                                },
                                                {
                                                    "pdf_id": "7.76",
                                                    "matching_string": "injecting different semantic labels for each reference image, "
                                                },
                                                {
                                                    "pdf_id": "7.78",
                                                    "matching_string": "appearance, producing realistic and flexible portraits."
                                                },
                                                {
                                                    "pdf_id": "7.66",
                                                    "matching_string": "for each aspect of human appearance, thus providing a "
                                                },
                                                {
                                                    "pdf_id": "7.69",
                                                    "matching_string": "concatenating multiple reference images in the dimension "
                                                },
                                                {
                                                    "pdf_id": "7.73",
                                                    "matching_string": "but leads to stifflooking and unrealistic results. This is because "
                                                },
                                                {
                                                    "pdf_id": "7.75",
                                                    "matching_string": "awareness of different types of appearances. Conversely, after "
                                                },
                                                {
                                                    "pdf_id": "7.77",
                                                    "matching_string": "the model has an awareness of various parts of the human "
                                                },
                                                {
                                                    "pdf_id": "7.71",
                                                    "matching_string": "the 5th column in, simply piecing reference images "
                                                },
                                                {
                                                    "pdf_id": "7.64",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} In the process of encoding multiple "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/txl3",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 64,
                                            "key": "doc/body/sec4/sub3/txl3/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} To precisely select subjects from multiple reference images, we introduce the subject masks into the shared selfattention mechanism. To evaluate the effectiveness of our proposed maskguided attention, we compare it with that without masks. When not using subject masks, the image is generated with reference to all patches of the conditional images, including the unexpected background or other parts. As shown in, due to the generation being interfered with by irrelevant subjects, the model produces homogeneous colors or appears with unexpected backgrounds or subjects. On the contrary, with the support of maskguided attention, Parts2Whole accurately refers to the appearance of the specified parts to generate real human images.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "7.81",
                                                    "matching_string": "masks into the shared selfattention mechanism. To "
                                                },
                                                {
                                                    "pdf_id": "7.83",
                                                    "matching_string": "we compare it with that without masks. When not "
                                                },
                                                {
                                                    "pdf_id": "7.87",
                                                    "matching_string": "to the generation being interfered with by irrelevant subjects, "
                                                },
                                                {
                                                    "pdf_id": "7.89",
                                                    "matching_string": "with unexpected backgrounds or subjects. On the contrary, "
                                                },
                                                {
                                                    "pdf_id": "7.93",
                                                    "matching_string": "refers to the appearance of the specified parts to "
                                                },
                                                {
                                                    "pdf_id": "7.85",
                                                    "matching_string": "to all patches of the conditional images, including the unexpected "
                                                },
                                                {
                                                    "pdf_id": "7.84",
                                                    "matching_string": "using subject masks, the image is generated with reference "
                                                },
                                                {
                                                    "pdf_id": "7.88",
                                                    "matching_string": "the model produces homogeneous colors or appears "
                                                },
                                                {
                                                    "pdf_id": "7.91",
                                                    "matching_string": "with the support of maskguided attention, Parts2Whole accurately "
                                                },
                                                {
                                                    "pdf_id": "7.80",
                                                    "matching_string": "from multiple reference images, we introduce the subject "
                                                },
                                                {
                                                    "pdf_id": "7.82",
                                                    "matching_string": "evaluate the effectiveness of our proposed maskguided attention, "
                                                },
                                                {
                                                    "pdf_id": "7.86",
                                                    "matching_string": "background or other parts. As shown in, due "
                                                },
                                                {
                                                    "pdf_id": "7.95",
                                                    "matching_string": "generate real human images."
                                                },
                                                {
                                                    "pdf_id": "7.79",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} To precisely select subjects "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub3/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 65,
                                                    "key": "doc/body/sec4/sub3/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Quantitative analysis of using semanticaware encoder and maskguided subject selection.",
                                                    "leftover": "",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.3",
                                                            "matching_string": "and maskguided subject selection."
                                                        },
                                                        {
                                                            "pdf_id": "7.1",
                                                            "matching_string": "Quantitative analysis of using semanticaware encoder "
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "key": "doc/body/sec4/sub3/table4/tabular1",
                                            "block_type": "tabular",
                                            "children": [
                                                {
                                                    "leaf id": 66,
                                                    "key": "doc/body/sec4/sub3/table4/tabular1/txl0",
                                                    "block type": "txl",
                                                    "content": "{@{}lcccc@{}} \\toprule Method & CLIP & DINO & DreamSim & FID \\midrule w/o text labels & 90.1 & 91.9 & 0.248 & 23.95 w/o mask & 90.8 & 91.6 & 0.243 & 19.79 Parts2Whole & 91.2 & 93.7 & 0.221 & 17.29 \\bottomrule",
                                                    "leftover": "{@{}lcccc@{}} \\toprule \\midrule \\bottomrule",
                                                    "matches": [
                                                        {
                                                            "pdf_id": "7.7",
                                                            "matching_string": "w/o text labels & 90.1 & 91.9 & 0.248 & 23.95 "
                                                        },
                                                        {
                                                            "pdf_id": "7.10",
                                                            "matching_string": "w/o mask & 90.8 & 91.6 & 0.243 & 19.79 "
                                                        },
                                                        {
                                                            "pdf_id": "7.5",
                                                            "matching_string": "Method & CLIP & DINO & DreamSim & FID "
                                                        },
                                                        {
                                                            "pdf_id": "7.11",
                                                            "matching_string": "Parts2Whole & 91.2 & 93.7 & 0.221 & 17.29 "
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub3/figure*5",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 67,
                                            "key": "doc/body/sec4/sub3/figure*5/cpt0",
                                            "block type": "cpt",
                                            "content": "The generated results from combinations of a different number of conditions.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "8.1",
                                                    "matching_string": "The generated results from combinations of a different number of conditions."
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 68,
                                    "key": "doc/body/sec4/sub4/tit",
                                    "block type": "title",
                                    "content": "More Results",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "8.2",
                                            "matching_string": "More Results"
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub4/figure*0",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 69,
                                            "key": "doc/body/sec4/sub4/figure*0/cpt0",
                                            "block type": "cpt",
                                            "content": "Results of image generation using selected parts from different individuals as control conditions.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "9.0",
                                                    "matching_string": "Results of image generation using selected parts from different individuals as control conditions."
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub4/txl1",
                                    "block_type": "txl",
                                    "children": [
                                        {
                                            "leaf id": 70,
                                            "key": "doc/body/sec4/sub4/txl1/txl0",
                                            "block type": "txl",
                                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is able to generate human images from varying numbers of condition images, such as single hair or face input, or arbitrary combinations like ''Face + Hair'', ''Face + Clothes'', and ''Upper body clothes + Lower body clothes''. The experimental results are presented in . The generated results under different control condition combinations still maintain high quality and realism. This flexibility enables our method to have broader application.",
                                            "leftover": "",
                                            "matches": [
                                                {
                                                    "pdf_id": "8.4",
                                                    "matching_string": "generate human images from varying numbers of condition "
                                                },
                                                {
                                                    "pdf_id": "8.6",
                                                    "matching_string": "like ''Face + Hair'', ''Face + Clothes'', and ''Upper "
                                                },
                                                {
                                                    "pdf_id": "8.9",
                                                    "matching_string": "different control condition combinations still maintain high "
                                                },
                                                {
                                                    "pdf_id": "8.11",
                                                    "matching_string": "have broader application."
                                                },
                                                {
                                                    "pdf_id": "8.5",
                                                    "matching_string": "images, such as single hair or face input, or arbitrary combinations "
                                                },
                                                {
                                                    "pdf_id": "8.8",
                                                    "matching_string": "are presented in . The generated results under "
                                                },
                                                {
                                                    "pdf_id": "8.10",
                                                    "matching_string": "quality and realism. This flexibility enables our method to "
                                                },
                                                {
                                                    "pdf_id": "8.7",
                                                    "matching_string": "body clothes + Lower body clothes''. The experimental results "
                                                },
                                                {
                                                    "pdf_id": "8.3",
                                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is able to "
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 71,
                                    "key": "doc/body/sec4/sub4/txl2",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We select various parts from different human images to serve as conditional images. For example, the face from person A, the hair or headwear from person B, the upper clothes from person C, and the lower clothes from person D. These parts are collectively used as control conditions for generation. The experimental results, as shown in, demonstrate that our method not only accurately maps different parts of the reference image to the corresponding regions in the target image but also effectively preserves the details of the conditions, producing realistic images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "8.12",
                                            "matching_string": "C, and the lower clothes from person D. These parts are "
                                        },
                                        {
                                            "pdf_id": "8.15",
                                            "matching_string": "our method not only accurately maps different parts of the "
                                        },
                                        {
                                            "pdf_id": "8.16",
                                            "matching_string": "reference image to the corresponding regions in the target "
                                        },
                                        {
                                            "pdf_id": "8.18",
                                            "matching_string": "producing realistic images."
                                        },
                                        {
                                            "pdf_id": "8.24",
                                            "matching_string": "parts from different human images to serve as conditional "
                                        },
                                        {
                                            "pdf_id": "8.28",
                                            "matching_string": "or headwear from person B, the upper clothes from person "
                                        },
                                        {
                                            "pdf_id": "8.13",
                                            "matching_string": "collectively used as control conditions for generation. The "
                                        },
                                        {
                                            "pdf_id": "8.17",
                                            "matching_string": "image but also effectively preserves the details of the conditions, "
                                        },
                                        {
                                            "pdf_id": "8.26",
                                            "matching_string": "images. For example, the face from person A, the hair "
                                        },
                                        {
                                            "pdf_id": "8.14",
                                            "matching_string": "experimental results, as shown in, demonstrate that "
                                        },
                                        {
                                            "pdf_id": "8.22",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} We select various "
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 72,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Conclusion",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "8.19",
                                    "matching_string": "Conclusion"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 73,
                                    "key": "doc/body/sec5/txl0/txl0",
                                    "block type": "txl",
                                    "content": "In this work, we propose Parts2Whole, a novel framework for controllable human image generation conditioned on multiple reference images, including various aspects of human appearance (e.g., hair, face, clothes, shoes, etc.) and pose maps. Based on a dual UNet design, we develop a semanticaware appearance encoder to process each condition image with its label into multiscale feature maps and inject those detailrich reference features into the generation via a shared selfattention mechanism. This design retains details from multiple references and looks very good. We also enhance vanilla selfattention by incorporating subject masks, enabling Parts2Whole to synthesize human images from specified parts from condition images. Extensive experiments demonstrate that our Parts2Whole performs well in terms of image quality and condition alignment.",
                                    "leftover": "condition ",
                                    "matches": [
                                        {
                                            "pdf_id": "8.20",
                                            "matching_string": "In this work, we propose Parts2Whole, a novel framework "
                                        },
                                        {
                                            "pdf_id": "8.25",
                                            "matching_string": "appearance (e.g., hair, face, clothes, shoes, etc.) and "
                                        },
                                        {
                                            "pdf_id": "9.2",
                                            "matching_string": "inject those detailrich reference features into the generation "
                                        },
                                        {
                                            "pdf_id": "9.3",
                                            "matching_string": "via a shared selfattention mechanism. This design retains "
                                        },
                                        {
                                            "pdf_id": "9.5",
                                            "matching_string": "also enhance vanilla selfattention by incorporating subject "
                                        },
                                        {
                                            "pdf_id": "9.7",
                                            "matching_string": "from specified parts from condition images. Extensive experiments "
                                        },
                                        {
                                            "pdf_id": "9.9",
                                            "matching_string": "in terms of image quality and condition alignment."
                                        },
                                        {
                                            "pdf_id": "8.27",
                                            "matching_string": "pose maps. Based on a dual UNet design, we develop a "
                                        },
                                        {
                                            "pdf_id": "9.1",
                                            "matching_string": "image with its label into multiscale feature maps and "
                                        },
                                        {
                                            "pdf_id": "9.4",
                                            "matching_string": "details from multiple references and looks very good. We "
                                        },
                                        {
                                            "pdf_id": "9.6",
                                            "matching_string": "masks, enabling Parts2Whole to synthesize human images "
                                        },
                                        {
                                            "pdf_id": "8.21",
                                            "matching_string": "for controllable human image generation conditioned on "
                                        },
                                        {
                                            "pdf_id": "8.23",
                                            "matching_string": "multiple reference images, including various aspects of human "
                                        },
                                        {
                                            "pdf_id": "8.29",
                                            "matching_string": "semanticaware appearance encoder to process each "
                                        },
                                        {
                                            "pdf_id": "9.8",
                                            "matching_string": "demonstrate that our Parts2Whole performs well "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 74,
                                    "key": "doc/body/sec5/txl1/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Our Parts2Whole is currently trained at the resolution of 512, which may cause artifacts in some generated results. This could be improved by using higher resolutions and larger diffusion models like SDXL as our backbone. Furthermore, it will be valuable to achieve the tryon of layerwise clothing based on our Parts2Whole.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} ",
                                    "matches": [
                                        {
                                            "pdf_id": "9.11",
                                            "matching_string": "resolution of 512, which may cause artifacts in some generated "
                                        },
                                        {
                                            "pdf_id": "9.14",
                                            "matching_string": "backbone. Furthermore, it will be valuable to achieve the "
                                        },
                                        {
                                            "pdf_id": "9.12",
                                            "matching_string": "results. This could be improved by using higher resolutions "
                                        },
                                        {
                                            "pdf_id": "9.15",
                                            "matching_string": "tryon of layerwise clothing based on our Parts2Whole."
                                        },
                                        {
                                            "pdf_id": "9.10",
                                            "matching_string": "Our Parts2Whole is currently trained at the "
                                        },
                                        {
                                            "pdf_id": "9.13",
                                            "matching_string": "and larger diffusion models like SDXL as our "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/txl2",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 75,
                                    "key": "doc/body/sec5/txl2/txl0",
                                    "block type": "txl",
                                    "content": "{ \\small \\bibliographystyle{ieeenatfullname} \\bibliography{arxiv} } \\clearpage \\setcounter{page}{1}",
                                    "leftover": "{ \\small \\bibliographystyle{ieeenatfullname} \\bibliography{arxiv} } \\clearpage \\setcounter{page}{1}",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "leaf id": 76,
                            "key": "doc/body/sec5/txl3",
                            "block type": "txl",
                            "content": "supplementary",
                            "leftover": "supplementary",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 77,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Dataset",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.3",
                                    "matching_string": "Dataset"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 78,
                                    "key": "doc/body/sec6/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "A sample of referencetarget image pair in our dataset. Reference Human Part images are obtained from the reference image based on the human parsing image, while the pose and text description are descriptions of the target image.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "13.0",
                                            "matching_string": "A sample of referencetarget image pair in our dataset. Reference Human Part images are obtained from the reference image "
                                        },
                                        {
                                            "pdf_id": "13.1",
                                            "matching_string": "based on the human parsing image, while the pose and text description are descriptions of the target image."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/figure*1",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 79,
                                    "key": "doc/body/sec6/figure*1/cpt0",
                                    "block type": "cpt",
                                    "content": "Generated human images with OpenPose as pose condition by Parts2Whole. The upper row displays multiple reference human parts, while the lower row shows the results generated by our Parts2Whole method under the control of reference images and OpenPose image.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "13.2",
                                            "matching_string": "Generated human images with OpenPose as pose condition by Parts2Whole. The upper row displays multiple reference human "
                                        },
                                        {
                                            "pdf_id": "13.3",
                                            "matching_string": "parts, while the lower row shows the results generated by our Parts2Whole method under the control of reference images and OpenPose "
                                        },
                                        {
                                            "pdf_id": "13.4",
                                            "matching_string": "image."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl2",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 80,
                                    "key": "doc/body/sec6/txl2/txl0",
                                    "block type": "txl",
                                    "content": "Generating the human body from multiple conditional parts is a significant undertaking, but lacking a directly available dataset. The datasets related to this task, such as those for Virtual Tryon, primarily suffer from a lack of multiple controllable conditions and are often limited to single control conditions (clothing). Issues with these datasets include a limited variety of clothing types, absence of facial data, low resolution, and lack of textual captions. The DeepFashionMultiModal dataset aligns more closely with our task as it includes a vast array of human body images, the same person and same clothes in different poses, and precise human parsing labels. However, this dataset cannot be used directly and requires data cleansing and further postprocessing.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.4",
                                            "matching_string": "Generating the human body from multiple conditional parts "
                                        },
                                        {
                                            "pdf_id": "12.5",
                                            "matching_string": "is a significant undertaking, but lacking a directly available "
                                        },
                                        {
                                            "pdf_id": "12.6",
                                            "matching_string": "dataset. The datasets related to this task, such as those "
                                        },
                                        {
                                            "pdf_id": "12.8",
                                            "matching_string": "of multiple controllable conditions and are often limited "
                                        },
                                        {
                                            "pdf_id": "12.10",
                                            "matching_string": "datasets include a limited variety of clothing types, absence "
                                        },
                                        {
                                            "pdf_id": "12.13",
                                            "matching_string": "closely with our task as it includes a vast array of human "
                                        },
                                        {
                                            "pdf_id": "12.15",
                                            "matching_string": "poses, and precise human parsing labels. However, this "
                                        },
                                        {
                                            "pdf_id": "12.17",
                                            "matching_string": "and further postprocessing."
                                        },
                                        {
                                            "pdf_id": "12.9",
                                            "matching_string": "to single control conditions (clothing). Issues with these "
                                        },
                                        {
                                            "pdf_id": "12.11",
                                            "matching_string": "of facial data, low resolution, and lack of textual captions. "
                                        },
                                        {
                                            "pdf_id": "12.14",
                                            "matching_string": "body images, the same person and same clothes in different "
                                        },
                                        {
                                            "pdf_id": "12.16",
                                            "matching_string": "dataset cannot be used directly and requires data cleansing "
                                        },
                                        {
                                            "pdf_id": "12.7",
                                            "matching_string": "for Virtual Tryon, primarily suffer from a lack "
                                        },
                                        {
                                            "pdf_id": "12.12",
                                            "matching_string": "The DeepFashionMultiModal dataset aligns more "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl3",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 81,
                                    "key": "doc/body/sec6/txl3/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} In the DeepFashionMultiModal dataset, there is some confusion with IDs where images of different individuals are mistakenly labeled under the same ID. We start by cleansing these IDs, extracting facial ID features from images tagged with the same ID using InsightFace. Cosine similarity is then used to evaluate the similarity between image ID feature pairs, allowing us to reclassify IDs within the same ID group. After cleansing, images from the same ID and the same clothes are selected if there are two or more images available.",
                                    "leftover": "{\\vspace{+1mm}{TexttoImage Generation.}} InsightFace. ",
                                    "matches": [
                                        {
                                            "pdf_id": "12.19",
                                            "matching_string": "there is some confusion with IDs where images of different "
                                        },
                                        {
                                            "pdf_id": "12.21",
                                            "matching_string": "start by cleansing these IDs, extracting facial ID features "
                                        },
                                        {
                                            "pdf_id": "12.24",
                                            "matching_string": "between image ID feature pairs, allowing us to reclassify "
                                        },
                                        {
                                            "pdf_id": "12.26",
                                            "matching_string": "from the same ID and the same clothes are selected if there "
                                        },
                                        {
                                            "pdf_id": "12.20",
                                            "matching_string": "individuals are mistakenly labeled under the same ID. We "
                                        },
                                        {
                                            "pdf_id": "12.23",
                                            "matching_string": "Cosine similarity is then used to evaluate the similarity "
                                        },
                                        {
                                            "pdf_id": "12.25",
                                            "matching_string": "IDs within the same ID group. After cleansing, images "
                                        },
                                        {
                                            "pdf_id": "12.27",
                                            "matching_string": "are two or more images available."
                                        },
                                        {
                                            "pdf_id": "12.18",
                                            "matching_string": "In the DeepFashionMultiModal dataset, "
                                        },
                                        {
                                            "pdf_id": "12.22",
                                            "matching_string": "from images tagged with the same ID using "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl4",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 82,
                                    "key": "doc/body/sec6/txl4/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We use images with human parsing labels from the dataset as reference images. Target images are then selected from the same ID and clothing, creating pairs with the reference image.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.29",
                                            "matching_string": "parsing labels from the dataset as reference images. "
                                        },
                                        {
                                            "pdf_id": "12.31",
                                            "matching_string": "creating pairs with the reference image."
                                        },
                                        {
                                            "pdf_id": "12.30",
                                            "matching_string": "Target images are then selected from the same ID and clothing, "
                                        },
                                        {
                                            "pdf_id": "12.28",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} We use images with human "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl5",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 83,
                                    "key": "doc/body/sec6/txl5/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} We crop the images according to the provided human parsing labels. Specifically, we divide the human image into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each part is cropped according to the human parsing labels to obtain the crop image and corresponding mask image. Due to the low resolution of the cropped parts, we apply RealESRGAN to enhance the image resolution, thus obtaining clearer reference images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.33",
                                            "matching_string": "according to the provided human parsing labels. Specifically, "
                                        },
                                        {
                                            "pdf_id": "12.35",
                                            "matching_string": "clothes, lower body clothes, whole body clothes, hair or "
                                        },
                                        {
                                            "pdf_id": "12.37",
                                            "matching_string": "to the human parsing labels to obtain the crop image and "
                                        },
                                        {
                                            "pdf_id": "12.38",
                                            "matching_string": "corresponding mask image. Due to the low resolution of the "
                                        },
                                        {
                                            "pdf_id": "12.40",
                                            "matching_string": "image resolution, thus obtaining clearer reference images."
                                        },
                                        {
                                            "pdf_id": "12.34",
                                            "matching_string": "we divide the human image into six parts: upper body "
                                        },
                                        {
                                            "pdf_id": "12.36",
                                            "matching_string": "headwear, face, and footwear. Each part is cropped according "
                                        },
                                        {
                                            "pdf_id": "12.39",
                                            "matching_string": "cropped parts, we apply RealESRGAN to enhance the "
                                        },
                                        {
                                            "pdf_id": "12.32",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} We crop the images "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec6/txl6",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 84,
                                    "key": "doc/body/sec6/txl6/txl0",
                                    "block type": "txl",
                                    "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Based on the reference human parts, we need to generate images that resemble the target image, requiring a description of the target image. The description is divided into two parts: one for the human body's pose and another for the target image's textual description. For pose information, we utilize DWPose to generate pose images corresponding to each image, and for DensePose, we use the provided DensePose files from the dataset. The textual description for each image is taken directly from the dataset's accompanying text description.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.41",
                                            "matching_string": "the dataset. The textual description for each image is taken "
                                        },
                                        {
                                            "pdf_id": "12.81",
                                            "matching_string": "target image, requiring a description of the target image. "
                                        },
                                        {
                                            "pdf_id": "12.85",
                                            "matching_string": "body's pose and another for the target image's textual "
                                        },
                                        {
                                            "pdf_id": "12.91",
                                            "matching_string": "for DensePose, we use the provided DensePose files from directly from "
                                        },
                                        {
                                            "pdf_id": "12.79",
                                            "matching_string": "parts, we need to generate images that resemble the "
                                        },
                                        {
                                            "pdf_id": "12.83",
                                            "matching_string": "The description is divided into two parts: one for the human "
                                        },
                                        {
                                            "pdf_id": "12.89",
                                            "matching_string": "to generate pose images corresponding to each image, and "
                                        },
                                        {
                                            "pdf_id": "12.42",
                                            "matching_string": "the dataset's accompanying text description."
                                        },
                                        {
                                            "pdf_id": "12.87",
                                            "matching_string": "description. For pose information, we utilize "
                                        },
                                        {
                                            "pdf_id": "12.77",
                                            "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Based on the reference human DWPose "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 85,
                            "key": "doc/body/sec6/txl7",
                            "block type": "txl",
                            "content": "{\\vspace{+1mm}{TexttoImage Generation.}} Finally, we have constructed a multimodal dataset with approximately 41,500 referencetarget pairs derived from the opensource DeepFashionMultiModal dataset. The controllable conditions for each pair are categorized into two main types. The first type is the appearance reference image, which is subdivided into six parts: upper body clothes, lower body clothes, whole body clothes, hair or headwear, face, and footwear. Each image is accompanied by a corresponding mask and has undergone superresolution processing. These data elements are sourced from the original reference image. The second type is the target description, primarily consisting of pose and text description. The pose is further divided into OpenPose and DensePose, all of which are derived from the target image. A sample of referencetarget image pair in our dataset is shown in Fig.~.",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.44",
                                    "matching_string": "constructed a multimodal dataset with approximately "
                                },
                                {
                                    "pdf_id": "12.47",
                                    "matching_string": "conditions for each pair are categorized into two main "
                                },
                                {
                                    "pdf_id": "12.49",
                                    "matching_string": "which is subdivided into six parts: upper body clothes, "
                                },
                                {
                                    "pdf_id": "12.51",
                                    "matching_string": "face, and footwear. Each image is accompanied by a corresponding "
                                },
                                {
                                    "pdf_id": "12.53",
                                    "matching_string": "These data elements are sourced from the original "
                                },
                                {
                                    "pdf_id": "12.55",
                                    "matching_string": "primarily consisting of pose and text description. The "
                                },
                                {
                                    "pdf_id": "12.57",
                                    "matching_string": "of which are derived from the target image. A sample of "
                                },
                                {
                                    "pdf_id": "12.45",
                                    "matching_string": "41,500 referencetarget pairs derived from the opensource "
                                },
                                {
                                    "pdf_id": "12.48",
                                    "matching_string": "types. The first type is the appearance reference image, "
                                },
                                {
                                    "pdf_id": "12.50",
                                    "matching_string": "lower body clothes, whole body clothes, hair or headwear, "
                                },
                                {
                                    "pdf_id": "12.54",
                                    "matching_string": "reference image. The second type is the target description, "
                                },
                                {
                                    "pdf_id": "12.56",
                                    "matching_string": "pose is further divided into OpenPose and DensePose, all "
                                },
                                {
                                    "pdf_id": "12.58",
                                    "matching_string": "referencetarget image pair in our dataset is shown in "
                                },
                                {
                                    "pdf_id": "12.52",
                                    "matching_string": "mask and has undergone superresolution processing. "
                                },
                                {
                                    "pdf_id": "12.46",
                                    "matching_string": "DeepFashionMultiModal dataset. The controllable "
                                },
                                {
                                    "pdf_id": "12.43",
                                    "matching_string": "{\\vspace{+1mm}{TexttoImage Generation.}} Finally, we have Fig.~."
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec7",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 86,
                            "key": "doc/body/sec7/tit",
                            "block type": "title",
                            "content": "Different Types of Pose Maps",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.59",
                                    "matching_string": "Different Types of Pose Maps"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec7/figure*0",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "leaf id": 87,
                                    "key": "doc/body/sec7/figure*0/cpt0",
                                    "block type": "cpt",
                                    "content": "Results of singlecondition and multicondition generation. The top three rows labeled ''face'', ''upper body clothes'', ''lower body clothes'' represent separate conditions. The fourth row demonstrates joint control under multipart conditions.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "14.0",
                                            "matching_string": "Results of singlecondition and multicondition generation. The top three rows labeled ''face'', ''upper body clothes'', ''lower "
                                        },
                                        {
                                            "pdf_id": "14.1",
                                            "matching_string": "body clothes'' represent separate conditions. The fourth row demonstrates joint control under multipart conditions."
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 88,
                            "key": "doc/body/sec7/txl1",
                            "block type": "txl",
                            "content": "To show the ability of Parts2Whole to generate human image conditions on different types of pose maps, we train a new Parts2Whole model but with OpenPose as a condition. As shown in, the generated images strictly maintain consistency with the target pose, and each body part retains the appearance information from the reference images.",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.60",
                                    "matching_string": "To show the ability of Parts2Whole to generate human image "
                                },
                                {
                                    "pdf_id": "12.62",
                                    "matching_string": "new Parts2Whole model but with OpenPose as a condition. "
                                },
                                {
                                    "pdf_id": "12.64",
                                    "matching_string": "consistency with the target pose, and each body part retains "
                                },
                                {
                                    "pdf_id": "12.61",
                                    "matching_string": "conditions on different types of pose maps, we train a "
                                },
                                {
                                    "pdf_id": "12.65",
                                    "matching_string": "the appearance information from the reference images."
                                },
                                {
                                    "pdf_id": "12.63",
                                    "matching_string": "As shown in, the generated images strictly maintain "
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec8",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 89,
                            "key": "doc/body/sec8/tit",
                            "block type": "title",
                            "content": "Discussion about Existing Methods",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "12.66",
                                    "matching_string": "Discussion about Existing Methods"
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/txl0",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 90,
                                    "key": "doc/body/sec8/txl0/txl0",
                                    "block type": "txl",
                                    "content": "In our main text, we conduct experiments with both tuningbased methods DreamBooth LoRA and Custom Diffusion and tuningfree methods such as IPAdapter and SSREncoder. The results of these experiments are further illustrated in Fig~, which showcases the performance of these methods conditioned on both a single image and multiple images.",
                                    "leftover": "",
                                    "matches": [
                                        {
                                            "pdf_id": "12.67",
                                            "matching_string": "In our main text, we conduct experiments with both tuningbased "
                                        },
                                        {
                                            "pdf_id": "12.72",
                                            "matching_string": "the performance of these methods conditioned on both "
                                        },
                                        {
                                            "pdf_id": "12.69",
                                            "matching_string": "Diffusion and tuningfree methods such as IPAdapter "
                                        },
                                        {
                                            "pdf_id": "12.71",
                                            "matching_string": "experiments are further illustrated in Fig~, which showcases "
                                        },
                                        {
                                            "pdf_id": "12.73",
                                            "matching_string": "a single image and multiple images."
                                        },
                                        {
                                            "pdf_id": "12.68",
                                            "matching_string": "methods DreamBooth LoRA and Custom "
                                        },
                                        {
                                            "pdf_id": "12.70",
                                            "matching_string": "and SSREncoder. The results of these "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec8/txl1",
                            "block_type": "txl",
                            "children": [
                                {
                                    "leaf id": 91,
                                    "key": "doc/body/sec8/txl1/txl0",
                                    "block type": "txl",
                                    "content": "The results demonstrate that while these methods generally perform well under a single control condition, they exhibit significant issues when multiple conditions are applied. For example, DreamBooth LoRA encountered cases where pants were omitted, and both Custom Diffusion and SSREncoder show alterations in facial ID. The phenomenon observed is attributed to the spatial misalignment of the input multibody parts with the target image, and the lack of specific design in existing methods to address the variation in spatial positions during feature injection. For instance, methods like SSREncoder, Custom Diffusion, and IPAdapter incorporate features into the denoising UNet through crossattention mechanisms. They encode reference images into other modal features (e.g. semantic features) and utilize the crossattention keys (K) and values (V) from them rather than from image dimensional feature maps. In this process, the correlation between the reference images and the target image loses the spatial relationship of the original image dimensions. It is difficult for these methods to effectively model the attention from various conditional feature maps at different locations in the target image, resulting in a mixture of attributes from different subjects.",
                                    "leftover": "Diffusion, relationship ",
                                    "matches": [
                                        {
                                            "pdf_id": "12.74",
                                            "matching_string": "The results demonstrate that while these methods generally "
                                        },
                                        {
                                            "pdf_id": "12.76",
                                            "matching_string": "exhibit significant issues when multiple conditions are applied. "
                                        },
                                        {
                                            "pdf_id": "12.80",
                                            "matching_string": "where pants were omitted, and both Custom Diffusion "
                                        },
                                        {
                                            "pdf_id": "12.84",
                                            "matching_string": "observed is attributed to the spatial misalignment "
                                        },
                                        {
                                            "pdf_id": "12.88",
                                            "matching_string": "the lack of specific design in existing methods to address "
                                        },
                                        {
                                            "pdf_id": "13.8",
                                            "matching_string": "for these methods to effectively model the attention from "
                                        },
                                        {
                                            "pdf_id": "13.9",
                                            "matching_string": "reference images into other modal features (e.g. semantic "
                                        },
                                        {
                                            "pdf_id": "13.10",
                                            "matching_string": "various conditional feature maps at different locations in "
                                        },
                                        {
                                            "pdf_id": "13.12",
                                            "matching_string": "the target image, resulting in a mixture of attributes from "
                                        },
                                        {
                                            "pdf_id": "13.15",
                                            "matching_string": "feature maps. In this process, the correlation between the "
                                        },
                                        {
                                            "pdf_id": "12.75",
                                            "matching_string": "perform well under a single control condition, they "
                                        },
                                        {
                                            "pdf_id": "12.78",
                                            "matching_string": "For example, DreamBooth LoRA encountered cases "
                                        },
                                        {
                                            "pdf_id": "12.82",
                                            "matching_string": "and SSREncoder show alterations in facial ID. The phenomenon "
                                        },
                                        {
                                            "pdf_id": "12.90",
                                            "matching_string": "the variation in spatial positions during feature injection. "
                                        },
                                        {
                                            "pdf_id": "13.5",
                                            "matching_string": "and IPAdapter incorporate features into the denoising "
                                        },
                                        {
                                            "pdf_id": "13.6",
                                            "matching_string": "of the original image dimensions. It is difficult "
                                        },
                                        {
                                            "pdf_id": "13.11",
                                            "matching_string": "features) and utilize the crossattention keys (K) and values "
                                        },
                                        {
                                            "pdf_id": "13.14",
                                            "matching_string": "different subjects."
                                        },
                                        {
                                            "pdf_id": "13.16",
                                            "matching_string": "reference images and the target image loses the spatial "
                                        },
                                        {
                                            "pdf_id": "12.86",
                                            "matching_string": "of the input multibody parts with the target image, and "
                                        },
                                        {
                                            "pdf_id": "12.92",
                                            "matching_string": "For instance, methods like SSREncoder, Custom "
                                        },
                                        {
                                            "pdf_id": "13.7",
                                            "matching_string": "UNet through crossattention mechanisms. They encode "
                                        },
                                        {
                                            "pdf_id": "13.13",
                                            "matching_string": "(V) from them rather than from image dimensional "
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 92,
                            "key": "doc/body/sec8/txl2",
                            "block type": "txl",
                            "content": "Conversely, our Parts2Whole model employs shared selfattention between the reference features and the feature maps in the Denoising UNet, executed on the image dimension. This allows our model to establish a more precise correlation between different condition images and distinct positions within the feature maps, thereby generating results that are consistent with the detailed attributes of multicondition images.",
                            "leftover": "",
                            "matches": [
                                {
                                    "pdf_id": "13.17",
                                    "matching_string": "Conversely, our Parts2Whole model employs shared "
                                },
                                {
                                    "pdf_id": "14.2",
                                    "matching_string": "maps in the Denoising UNet, executed on the image dimension. "
                                },
                                {
                                    "pdf_id": "14.4",
                                    "matching_string": "correlation between different condition images and distinct "
                                },
                                {
                                    "pdf_id": "14.6",
                                    "matching_string": "that are consistent with the detailed attributes of multicondition "
                                },
                                {
                                    "pdf_id": "14.3",
                                    "matching_string": "This allows our model to establish a more precise "
                                },
                                {
                                    "pdf_id": "14.5",
                                    "matching_string": "positions within the feature maps, thereby generating results "
                                },
                                {
                                    "pdf_id": "13.18",
                                    "matching_string": "selfattention between the reference features and the feature "
                                },
                                {
                                    "pdf_id": "14.7",
                                    "matching_string": "images."
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 93,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuningfree mutual selfattention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.18",
                    "matching_string": "Qie, and Yinqiang Zheng. Masactrl: Tuningfree mutual "
                },
                {
                    "pdf_id": "9.20",
                    "matching_string": "editing. In Proceedings of the IEEE/CVF International Conference "
                },
                {
                    "pdf_id": "9.17",
                    "matching_string": "Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu "
                },
                {
                    "pdf_id": "9.19",
                    "matching_string": "selfattention control for consistent image synthesis and "
                },
                {
                    "pdf_id": "9.21",
                    "matching_string": "on Computer Vision, pages 2256022570, 2023."
                }
            ]
        },
        {
            "leaf id": 94,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al. Muse: Texttoimage generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.",
            "leftover": "William~",
            "matches": [
                {
                    "pdf_id": "9.22",
                    "matching_string": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, "
                },
                {
                    "pdf_id": "9.25",
                    "matching_string": "Texttoimage generation via masked generative transformers. "
                },
                {
                    "pdf_id": "9.23",
                    "matching_string": "Jose Lezama, Lu Jiang, MingHsuan Yang, Kevin Murphy, "
                },
                {
                    "pdf_id": "9.24",
                    "matching_string": "T Freeman, Michael Rubinstein, et~al. Muse: "
                },
                {
                    "pdf_id": "9.26",
                    "matching_string": "arXiv preprint arXiv:2301.00704, 2023."
                }
            ]
        },
        {
            "leaf id": 95,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot objectlevel image customization. arXiv preprint arXiv:2307.09481, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.27",
                    "matching_string": "Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, "
                },
                {
                    "pdf_id": "9.28",
                    "matching_string": "customization. arXiv preprint arXiv:2307.09481, 2023."
                },
                {
                    "pdf_id": "9.57",
                    "matching_string": "and Hengshuang Zhao. Anydoor: Zeroshot objectlevel "
                },
                {
                    "pdf_id": "9.29",
                    "matching_string": "image "
                }
            ]
        },
        {
            "leaf id": 96,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Vitonhd: Highresolution virtual tryon via misalignmentaware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.30",
                    "matching_string": "Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul "
                },
                {
                    "pdf_id": "9.32",
                    "matching_string": "misalignmentaware normalization. In Proceedings of the "
                },
                {
                    "pdf_id": "9.31",
                    "matching_string": "Choo. Vitonhd: Highresolution virtual tryon via "
                },
                {
                    "pdf_id": "9.33",
                    "matching_string": "IEEE/CVF conference on computer vision and pattern "
                },
                {
                    "pdf_id": "9.34",
                    "matching_string": "recognition, pages 1413114140, 2021."
                }
            ]
        },
        {
            "leaf id": 97,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "Jiankang Deng, Jia Guo, et~al. {InsightFace: 2D and 3D Face Analysis Project}. https://github.com/deepinsight/insightface, 2018. Accessed: 20240411.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.37",
                    "matching_string": "deepinsight/insightface, 2018. Accessed: 20240411."
                },
                {
                    "pdf_id": "9.35",
                    "matching_string": "Jiankang Deng, Jia Guo, et~al. {InsightFace: 2D and "
                },
                {
                    "pdf_id": "9.36",
                    "matching_string": "3D Face Analysis Project}. https://github.com/"
                },
                {
                    "pdf_id": "9.38",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 98,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.39",
                    "matching_string": "Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos "
                },
                {
                    "pdf_id": "9.40",
                    "matching_string": "Zafeiriou. Arcface: Additive angular margin loss for deep "
                },
                {
                    "pdf_id": "9.41",
                    "matching_string": "face recognition. In CVPR, 2019."
                }
            ]
        },
        {
            "leaf id": 99,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:\\penalty0 87808794, 2021.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.43",
                    "matching_string": "beat gans on image synthesis. Advances in neural information "
                },
                {
                    "pdf_id": "9.42",
                    "matching_string": "Prafulla Dhariwal and Alexander Nichol. Diffusion models "
                },
                {
                    "pdf_id": "9.44",
                    "matching_string": "processing systems, 34:\\penalty0 87808794, 2021."
                }
            ]
        },
        {
            "leaf id": 100,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.46",
                    "matching_string": "Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: "
                },
                {
                    "pdf_id": "9.45",
                    "matching_string": "Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy "
                },
                {
                    "pdf_id": "9.47",
                    "matching_string": "Learning new dimensions of human visual similarity "
                },
                {
                    "pdf_id": "9.48",
                    "matching_string": "using synthetic data, 2023."
                }
            ]
        },
        {
            "leaf id": 101,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit~H. Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing texttoimage generation using textual inversion, 2022.",
            "leftover": "Amit~",
            "matches": [
                {
                    "pdf_id": "9.51",
                    "matching_string": "image is worth one word: Personalizing texttoimage generation "
                },
                {
                    "pdf_id": "9.49",
                    "matching_string": "Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, "
                },
                {
                    "pdf_id": "9.50",
                    "matching_string": "H. Bermano, Gal Chechik, and Daniel CohenOr. An "
                },
                {
                    "pdf_id": "9.52",
                    "matching_string": "using textual inversion, 2022."
                }
            ]
        },
        {
            "leaf id": 102,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "Rinon Gal, Moab Arar, Yuval Atzmon, Amit~H Bermano, Gal Chechik, and Daniel CohenOr. Encoderbased domain tuning for fast personalization of texttoimage models. ACM Transactions on Graphics (TOG), 42\\penalty0 (4):\\penalty0 113, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "9.54",
                    "matching_string": "Gal Chechik, and Daniel CohenOr. Encoderbased domain "
                },
                {
                    "pdf_id": "9.53",
                    "matching_string": "Rinon Gal, Moab Arar, Yuval Atzmon, Amit~H Bermano, "
                },
                {
                    "pdf_id": "9.55",
                    "matching_string": "tuning for fast personalization of texttoimage models. ACM "
                },
                {
                    "pdf_id": "9.56",
                    "matching_string": "Transactions on Graphics (TOG), 42\\penalty0 (4):\\penalty0 113, 2023."
                }
            ]
        },
        {
            "leaf id": 103,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel CohenOr. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.0",
                    "matching_string": "CohenOr. Style aligned image generation via shared attention. "
                },
                {
                    "pdf_id": "9.58",
                    "matching_string": "Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel "
                },
                {
                    "pdf_id": "10.2",
                    "matching_string": "arXiv preprint arXiv:2312.02133, 2023."
                }
            ]
        },
        {
            "leaf id": 104,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:\\penalty0 68406851, 2020.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.4",
                    "matching_string": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion "
                },
                {
                    "pdf_id": "10.6",
                    "matching_string": "probabilistic models. Advances in neural information "
                },
                {
                    "pdf_id": "10.7",
                    "matching_string": "processing systems, 33:\\penalty0 68406851, 2020."
                }
            ]
        },
        {
            "leaf id": 105,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Lowrank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.",
            "leftover": "Edward~",
            "matches": [
                {
                    "pdf_id": "10.12",
                    "matching_string": "Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. "
                },
                {
                    "pdf_id": "10.10",
                    "matching_string": "J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, "
                },
                {
                    "pdf_id": "10.13",
                    "matching_string": "Lora: Lowrank adaptation of large language models. arXiv "
                },
                {
                    "pdf_id": "10.14",
                    "matching_string": "preprint arXiv:2106.09685, 2021."
                }
            ]
        },
        {
            "leaf id": 106,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable imagetovideo synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023{\\natexlab{a}}.",
            "leftover": "2023{\\natexlab{a}}.",
            "matches": [
                {
                    "pdf_id": "10.15",
                    "matching_string": "Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and "
                },
                {
                    "pdf_id": "10.17",
                    "matching_string": "imagetovideo synthesis for character animation. arXiv "
                },
                {
                    "pdf_id": "10.16",
                    "matching_string": "Liefeng Bo. Animate anyone: Consistent and controllable "
                },
                {
                    "pdf_id": "10.18",
                    "matching_string": "preprint arXiv:2311.17117, "
                }
            ]
        },
        {
            "leaf id": 107,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and TatJen Cham. Cocktail: Mixing multimodality control for textconditional image generation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023{\\natexlab{b}}.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.19",
                    "matching_string": "Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, "
                },
                {
                    "pdf_id": "10.21",
                    "matching_string": "Mixing multimodality control for textconditional image "
                },
                {
                    "pdf_id": "10.20",
                    "matching_string": "Chaoyue Wang, Dacheng Tao, and TatJen Cham. Cocktail: "
                },
                {
                    "pdf_id": "10.22",
                    "matching_string": "generation. In Thirtyseventh Conference on Neural Information "
                },
                {
                    "pdf_id": "10.23",
                    "matching_string": "Processing Systems, 2023{\\natexlab{b}}."
                }
            ]
        },
        {
            "leaf id": 108,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, YanPei Cao, Ding Liang, Yu Qiao, Bo Dai, et~al. Epidiff: Enhancing multiview synthesis via localized epipolarconstrained diffusion. arXiv preprint arXiv:2312.06725, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.24",
                    "matching_string": "Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, "
                },
                {
                    "pdf_id": "10.25",
                    "matching_string": "Yangguang Li, Xinyuan Chen, YanPei Cao, Ding Liang, Yu "
                },
                {
                    "pdf_id": "10.27",
                    "matching_string": "via localized epipolarconstrained diffusion. arXiv preprint "
                },
                {
                    "pdf_id": "10.26",
                    "matching_string": "Qiao, Bo Dai, et~al. Epidiff: Enhancing multiview synthesis "
                },
                {
                    "pdf_id": "10.28",
                    "matching_string": "arXiv:2312.06725, 2023."
                }
            ]
        },
        {
            "leaf id": 109,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. arXiv preprint arXiv:2402.12974, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.29",
                    "matching_string": "Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and "
                },
                {
                    "pdf_id": "10.30",
                    "matching_string": "Youngjung Uh. Visual style prompting with swapping selfattention. "
                },
                {
                    "pdf_id": "10.31",
                    "matching_string": "arXiv preprint arXiv:2402.12974, 2024."
                }
            ]
        },
        {
            "leaf id": 110,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen~Change Loy, and Ziwei Liu. Text2human: Textdriven controllable human image generation. ACM Transactions on Graphics (TOG), 41\\penalty0 (4):\\penalty0 111, 2022.",
            "leftover": "Chen~",
            "matches": [
                {
                    "pdf_id": "10.34",
                    "matching_string": "controllable human image generation. ACM Transactions on "
                },
                {
                    "pdf_id": "10.32",
                    "matching_string": "Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, "
                },
                {
                    "pdf_id": "10.33",
                    "matching_string": "Change Loy, and Ziwei Liu. Text2human: Textdriven "
                },
                {
                    "pdf_id": "5.8",
                    "matching_string": "(T"
                },
                {
                    "pdf_id": "10.35",
                    "matching_string": "Graphics OG), 41\\penalty0 (4):\\penalty0 111, 2022."
                }
            ]
        },
        {
            "leaf id": 111,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and Jingfeng Zhang. Scedit: Efficient and controllable image diffusion generation via skip connection editing. arXiv preprint arXiv:2312.11392, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.36",
                    "matching_string": "Zeyinzi Jiang, Chaojie Mao, Yulin Pan, Zhen Han, and "
                },
                {
                    "pdf_id": "10.38",
                    "matching_string": "generation via skip connection editing. arXiv preprint "
                },
                {
                    "pdf_id": "10.37",
                    "matching_string": "Jingfeng Zhang. Scedit: Efficient and controllable image diffusion "
                },
                {
                    "pdf_id": "10.66",
                    "matching_string": "arXiv:2312.11392, 2023."
                }
            ]
        },
        {
            "leaf id": 112,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "Diederik~P Kingma and Max Welling. Autoencoding variational bayes, 2022.",
            "leftover": "Diederik~P Kingma and Max Welling. Autoencoding variational ",
            "matches": [
                {
                    "pdf_id": "10.70",
                    "matching_string": "bayes, 2022."
                }
            ]
        },
        {
            "leaf id": 113,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and JunYan Zhu. Multiconcept customization of texttoimage diffusion, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.72",
                    "matching_string": "Shechtman, and JunYan Zhu. Multiconcept customization "
                },
                {
                    "pdf_id": "10.71",
                    "matching_string": "Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli "
                },
                {
                    "pdf_id": "10.73",
                    "matching_string": "of texttoimage diffusion, 2023."
                }
            ]
        },
        {
            "leaf id": 114,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "Lambda Labs. Stable diffusion image variations, 2022.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.78",
                    "matching_string": "Lambda Labs. Stable diffusion image variations, 2022."
                }
            ]
        },
        {
            "leaf id": 115,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.81",
                    "matching_string": "Cheng, and Ying Shan. Photomaker: Customizing "
                },
                {
                    "pdf_id": "10.83",
                    "matching_string": "Conference on Computer Vision and Pattern Recognition "
                },
                {
                    "pdf_id": "10.80",
                    "matching_string": "Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing "
                },
                {
                    "pdf_id": "10.82",
                    "matching_string": "realistic human photos via stacked id embedding. In IEEE "
                },
                {
                    "pdf_id": "10.84",
                    "matching_string": "(CVPR), 2024."
                }
            ]
        },
        {
            "leaf id": 116,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyperrealistic human generation with latent structural diffusion. arXiv preprint arXiv:2310.08579, 2023{\\natexlab{a}}.",
            "leftover": "2023{\\natexlab{a}}.",
            "matches": [
                {
                    "pdf_id": "10.85",
                    "matching_string": "Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, "
                },
                {
                    "pdf_id": "10.86",
                    "matching_string": "Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey "
                },
                {
                    "pdf_id": "10.88",
                    "matching_string": "with latent structural diffusion. arXiv preprint "
                },
                {
                    "pdf_id": "10.87",
                    "matching_string": "Tulyakov. Hyperhuman: Hyperrealistic human generation "
                },
                {
                    "pdf_id": "10.89",
                    "matching_string": "arXiv:2310.08579, "
                }
            ]
        },
        {
            "leaf id": 117,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.90",
                    "matching_string": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou "
                },
                {
                    "pdf_id": "10.92",
                    "matching_string": "and retrieval with rich annotations. In Proceedings of "
                },
                {
                    "pdf_id": "10.91",
                    "matching_string": "Tang. Deepfashion: Powering robust clothes recognition "
                },
                {
                    "pdf_id": "10.93",
                    "matching_string": "IEEE Conference on Computer Vision and Pattern Recognition "
                },
                {
                    "pdf_id": "10.94",
                    "matching_string": "(CVPR), 2016."
                }
            ]
        },
        {
            "leaf id": 118,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation, 2023{\\natexlab{b}}.",
            "leftover": "2023{\\natexlab{b}}.",
            "matches": [
                {
                    "pdf_id": "10.1",
                    "matching_string": "Cones: Concept neurons in diffusion models for customized "
                },
                {
                    "pdf_id": "10.95",
                    "matching_string": "Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng "
                },
                {
                    "pdf_id": "10.96",
                    "matching_string": "Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. "
                },
                {
                    "pdf_id": "10.3",
                    "matching_string": "generation, "
                }
            ]
        },
        {
            "leaf id": 119,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized texttoimage generation without testtime finetuning. arXiv preprint arXiv:2307.11410, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.5",
                    "matching_string": "Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. "
                },
                {
                    "pdf_id": "10.9",
                    "matching_string": "generation without testtime finetuning. arXiv preprint "
                },
                {
                    "pdf_id": "10.8",
                    "matching_string": "Subjectdiffusion: Open domain personalized texttoimage "
                },
                {
                    "pdf_id": "10.11",
                    "matching_string": "arXiv:2307.11410, 2023."
                }
            ]
        },
        {
            "leaf id": 120,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: highresolution multicategory virtual tryon. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22312235, 2022.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.40",
                    "matching_string": "Landi, Fabio Cesari, and Rita Cucchiara. Dress code: highresolution "
                },
                {
                    "pdf_id": "10.42",
                    "matching_string": "the IEEE/CVF Conference on Computer Vision and Pattern "
                },
                {
                    "pdf_id": "10.39",
                    "matching_string": "Davide Morelli, Matteo Fincato, Marcella Cornia, Federico "
                },
                {
                    "pdf_id": "10.41",
                    "matching_string": "multicategory virtual tryon. In Proceedings of "
                },
                {
                    "pdf_id": "10.43",
                    "matching_string": "Recognition, pages 22312235, 2022."
                }
            ]
        },
        {
            "leaf id": 121,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for texttoimage diffusion models. arXiv preprint arXiv:2302.08453, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.44",
                    "matching_string": "Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian "
                },
                {
                    "pdf_id": "10.45",
                    "matching_string": "Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: "
                },
                {
                    "pdf_id": "10.47",
                    "matching_string": "ability for texttoimage diffusion models. arXiv preprint "
                },
                {
                    "pdf_id": "10.46",
                    "matching_string": "Learning adapters to dig out more controllable "
                },
                {
                    "pdf_id": "10.48",
                    "matching_string": "arXiv:2302.08453, 2023."
                }
            ]
        },
        {
            "leaf id": 122,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with textguided diffusion models, 2022.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.50",
                    "matching_string": "Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and "
                },
                {
                    "pdf_id": "10.49",
                    "matching_string": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav "
                },
                {
                    "pdf_id": "10.51",
                    "matching_string": "Mark Chen. Glide: Towards photorealistic image generation "
                },
                {
                    "pdf_id": "10.52",
                    "matching_string": "and editing with textguided diffusion models, 2022."
                }
            ]
        },
        {
            "leaf id": 123,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, PoYao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.54",
                    "matching_string": "Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, "
                },
                {
                    "pdf_id": "10.56",
                    "matching_string": "Assran, Nicolas Ballas, Wojciech Galuba, Russell "
                },
                {
                    "pdf_id": "10.60",
                    "matching_string": "Bojanowski. Dinov2: Learning robust visual features without "
                },
                {
                    "pdf_id": "10.55",
                    "matching_string": "Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Mahmoud "
                },
                {
                    "pdf_id": "10.57",
                    "matching_string": "Howes, PoYao Huang, ShangWen Li, Ishan Misra, Michael "
                },
                {
                    "pdf_id": "10.59",
                    "matching_string": "Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr "
                },
                {
                    "pdf_id": "10.53",
                    "matching_string": "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy "
                },
                {
                    "pdf_id": "10.58",
                    "matching_string": "Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv Jegou, "
                },
                {
                    "pdf_id": "10.61",
                    "matching_string": "supervision, 2024."
                }
            ]
        },
        {
            "leaf id": 124,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for highresolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.64",
                    "matching_string": "Robin Rombach. Sdxl: Improving latent diffusion models "
                },
                {
                    "pdf_id": "10.62",
                    "matching_string": "Dustin Podell, Zion English, Kyle Lacey, Andreas "
                },
                {
                    "pdf_id": "10.65",
                    "matching_string": "for highresolution image synthesis. arXiv preprint "
                },
                {
                    "pdf_id": "10.63",
                    "matching_string": "Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and "
                },
                {
                    "pdf_id": "10.67",
                    "matching_string": "arXiv:2307.01952, 2023."
                }
            ]
        },
        {
            "leaf id": 125,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan~Carlos Niebles, Caiming Xiong, Silvio Savarese, et~al. Unicontrol: A unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.69",
                    "matching_string": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, "
                },
                {
                    "pdf_id": "10.76",
                    "matching_string": "model for controllable visual generation in the wild. arXiv "
                },
                {
                    "pdf_id": "10.74",
                    "matching_string": "Yingbo Zhou, Huan Wang, Juan~Carlos Niebles, Caiming "
                },
                {
                    "pdf_id": "10.75",
                    "matching_string": "Xiong, Silvio Savarese, et~al. Unicontrol: A unified diffusion "
                },
                {
                    "pdf_id": "10.77",
                    "matching_string": "preprint arXiv:2305.11147, 2023."
                }
            ]
        },
        {
            "leaf id": 126,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.97",
                    "matching_string": "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, "
                },
                {
                    "pdf_id": "10.99",
                    "matching_string": "Krueger, and Ilya Sutskever. Learning transferable visual "
                },
                {
                    "pdf_id": "10.79",
                    "matching_string": "Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya "
                },
                {
                    "pdf_id": "10.98",
                    "matching_string": "Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen "
                },
                {
                    "pdf_id": "10.100",
                    "matching_string": "models from natural language supervision, 2021."
                }
            ]
        },
        {
            "leaf id": 127,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1\\penalty0 (2):\\penalty0 3, 2022.",
            "leftover": "1\\penalty0 ",
            "matches": [
                {
                    "pdf_id": "10.101",
                    "matching_string": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, "
                },
                {
                    "pdf_id": "10.102",
                    "matching_string": "and Mark Chen. Hierarchical textconditional image generation "
                },
                {
                    "pdf_id": "10.103",
                    "matching_string": "with clip latents. arXiv preprint arXiv:2204.06125, "
                },
                {
                    "pdf_id": "10.104",
                    "matching_string": "(2):\\penalty0 3, 2022."
                }
            ]
        },
        {
            "leaf id": 128,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.107",
                    "matching_string": "synthesis with latent diffusion models. In Proceedings of "
                },
                {
                    "pdf_id": "10.105",
                    "matching_string": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, "
                },
                {
                    "pdf_id": "10.108",
                    "matching_string": "the IEEE/CVF conference on computer vision and pattern "
                },
                {
                    "pdf_id": "10.106",
                    "matching_string": "Patrick Esser, and Bj{\"o}rn Ommer. Highresolution image "
                },
                {
                    "pdf_id": "10.109",
                    "matching_string": "recognition, pages 1068410695, 2022."
                }
            ]
        },
        {
            "leaf id": 129,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computerassisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 59, 2015, proceedings, part III 18, pages 234241. Springer, 2015.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "10.111",
                    "matching_string": "Convolutional networks for biomedical image segmentation. "
                },
                {
                    "pdf_id": "11.0",
                    "matching_string": "Munich, Germany, October 59, 2015, proceedings, part III "
                },
                {
                    "pdf_id": "10.110",
                    "matching_string": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: "
                },
                {
                    "pdf_id": "10.112",
                    "matching_string": "In Medical image computing and computerassisted "
                },
                {
                    "pdf_id": "11.2",
                    "matching_string": "18, pages 234241. Springer, 2015."
                },
                {
                    "pdf_id": "10.113",
                    "matching_string": "interventionMICCAI 2015: 18th international conference, "
                }
            ]
        },
        {
            "leaf id": 130,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning texttoimage diffusion models for subjectdriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.4",
                    "matching_string": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, "
                },
                {
                    "pdf_id": "11.7",
                    "matching_string": "tuning texttoimage diffusion models for subjectdriven "
                },
                {
                    "pdf_id": "11.6",
                    "matching_string": "Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine "
                },
                {
                    "pdf_id": "11.8",
                    "matching_string": "generation. In Proceedings of the IEEE/CVF Conference "
                },
                {
                    "pdf_id": "11.9",
                    "matching_string": "on Computer Vision and Pattern Recognition, pages "
                },
                {
                    "pdf_id": "11.3",
                    "matching_string": "2023."
                },
                {
                    "pdf_id": "11.1",
                    "matching_string": "2250022510, "
                },
                {
                    "pdf_id": "11.10",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 131,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al. Photorealistic texttoimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:\\penalty0 3647936494, 2022.",
            "leftover": "et~",
            "matches": [
                {
                    "pdf_id": "11.21",
                    "matching_string": "language understanding. Advances in neural information "
                },
                {
                    "pdf_id": "11.16",
                    "matching_string": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala "
                },
                {
                    "pdf_id": "11.18",
                    "matching_string": "Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, "
                },
                {
                    "pdf_id": "11.20",
                    "matching_string": "al. Photorealistic texttoimage diffusion models with deep "
                },
                {
                    "pdf_id": "11.19",
                    "matching_string": "Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, "
                },
                {
                    "pdf_id": "11.22",
                    "matching_string": "processing systems, 35:\\penalty0 3647936494, 2022."
                }
            ]
        },
        {
            "leaf id": 132,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Jing Shi, Wei Xiong, Zhe Lin, and Hyun~Joon Jung. Instantbooth: Personalized texttoimage generation without testtime finetuning, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.30",
                    "matching_string": "Personalized texttoimage generation without testtime "
                },
                {
                    "pdf_id": "11.28",
                    "matching_string": "Jing Shi, Wei Xiong, Zhe Lin, and Hyun~Joon Jung. Instantbooth: "
                },
                {
                    "pdf_id": "11.32",
                    "matching_string": "finetuning, 2023."
                }
            ]
        },
        {
            "leaf id": 133,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.34",
                    "matching_string": "Denoising diffusion implicit models. arXiv preprint "
                },
                {
                    "pdf_id": "11.33",
                    "matching_string": "Jiaming Song, Chenlin Meng, and Stefano Ermon. "
                },
                {
                    "pdf_id": "11.38",
                    "matching_string": "arXiv:2010.02502, 2020."
                }
            ]
        },
        {
            "leaf id": 134,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Trainingfree consistent texttoimage generation. arXiv preprint arXiv:2402.03286, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.40",
                    "matching_string": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, "
                },
                {
                    "pdf_id": "11.43",
                    "matching_string": "consistent texttoimage generation. arXiv preprint "
                },
                {
                    "pdf_id": "11.42",
                    "matching_string": "Lior Wolf, Gal Chechik, and Yuval Atzmon. Trainingfree "
                },
                {
                    "pdf_id": "11.44",
                    "matching_string": "arXiv:2402.03286, 2024."
                }
            ]
        },
        {
            "leaf id": 135,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via nextscale prediction. 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.48",
                    "matching_string": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei "
                },
                {
                    "pdf_id": "11.50",
                    "matching_string": "Wang. Visual autoregressive modeling: Scalable image "
                },
                {
                    "pdf_id": "11.52",
                    "matching_string": "generation via nextscale prediction. 2024."
                }
            ]
        },
        {
            "leaf id": 136,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\\L}ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.58",
                    "matching_string": "Polosukhin. Attention is all you need. Advances in neural "
                },
                {
                    "pdf_id": "11.54",
                    "matching_string": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, "
                },
                {
                    "pdf_id": "11.56",
                    "matching_string": "Llion Jones, Aidan~N Gomez, {\\L}ukasz Kaiser, and Illia "
                },
                {
                    "pdf_id": "11.59",
                    "matching_string": "information processing systems, 30, 2017."
                }
            ]
        },
        {
            "leaf id": 137,
            "key": "doc/bib44",
            "block type": "bibliography",
            "content": "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zeroshot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.60",
                    "matching_string": "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony "
                },
                {
                    "pdf_id": "11.61",
                    "matching_string": "Chen. Instantid: Zeroshot identitypreserving generation "
                },
                {
                    "pdf_id": "11.62",
                    "matching_string": "in seconds. arXiv preprint arXiv:2401.07519, 2024."
                },
                {
                    "pdf_id": "11.63",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 138,
            "key": "doc/bib45",
            "block type": "bibliography",
            "content": "Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Realesrgan: Training realworld blind superresolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW).",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.64",
                    "matching_string": "Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. "
                },
                {
                    "pdf_id": "11.66",
                    "matching_string": "pure synthetic data. In International Conference on Computer "
                },
                {
                    "pdf_id": "11.65",
                    "matching_string": "Realesrgan: Training realworld blind superresolution with "
                },
                {
                    "pdf_id": "11.67",
                    "matching_string": "Vision Workshops (ICCVW)."
                }
            ]
        },
        {
            "leaf id": 139,
            "key": "doc/bib46",
            "block type": "bibliography",
            "content": "Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized texttoimage generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.68",
                    "matching_string": "Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei "
                },
                {
                    "pdf_id": "11.70",
                    "matching_string": "into textual embeddings for customized texttoimage "
                },
                {
                    "pdf_id": "11.69",
                    "matching_string": "Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts "
                },
                {
                    "pdf_id": "11.71",
                    "matching_string": "generation. In Proceedings of the IEEE/CVF International "
                },
                {
                    "pdf_id": "11.72",
                    "matching_string": "Conference on Computer Vision, pages 1594315953, 2023."
                },
                {
                    "pdf_id": "11.73",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 140,
            "key": "doc/bib47",
            "block type": "bibliography",
            "content": "Guangxuan Xiao, Tianwei Yin, William~T Freeman, Fr{\\'e}do Durand, and Song Han. Fastcomposer: Tuningfree multisubject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023.",
            "leftover": "Fr{\\'e}do ",
            "matches": [
                {
                    "pdf_id": "11.76",
                    "matching_string": "image generation with localized attention. arXiv "
                },
                {
                    "pdf_id": "11.75",
                    "matching_string": "Durand, and Song Han. Fastcomposer: Tuningfree multisubject "
                },
                {
                    "pdf_id": "11.74",
                    "matching_string": "Guangxuan Xiao, Tianwei Yin, William~T Freeman, "
                },
                {
                    "pdf_id": "11.77",
                    "matching_string": "preprint arXiv:2305.10431, 2023."
                }
            ]
        },
        {
            "leaf id": 141,
            "key": "doc/bib48",
            "block type": "bibliography",
            "content": "Zhongcong Xu, Jianfeng Zhang, Jun~Hao Liew, Hanshu Yan, JiaWei Liu, Chenxu Zhang, Jiashi Feng, and Mike~Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.80",
                    "matching_string": "Shou. Magicanimate: Temporally consistent human image "
                },
                {
                    "pdf_id": "11.78",
                    "matching_string": "Zhongcong Xu, Jianfeng Zhang, Jun~Hao Liew, Hanshu Yan, "
                },
                {
                    "pdf_id": "11.79",
                    "matching_string": "JiaWei Liu, Chenxu Zhang, Jiashi Feng, and Mike~Zheng "
                },
                {
                    "pdf_id": "11.81",
                    "matching_string": "animation using diffusion model. 2024."
                }
            ]
        },
        {
            "leaf id": 142,
            "key": "doc/bib49",
            "block type": "bibliography",
            "content": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplarbased image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023{\\natexlab{a}}.",
            "leftover": "1838118391, 2023{\\natexlab{a}}.",
            "matches": [
                {
                    "pdf_id": "11.83",
                    "matching_string": "Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by "
                },
                {
                    "pdf_id": "11.85",
                    "matching_string": "In Proceedings of the IEEE/CVF Conference on "
                },
                {
                    "pdf_id": "11.82",
                    "matching_string": "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin "
                },
                {
                    "pdf_id": "11.84",
                    "matching_string": "example: Exemplarbased image editing with diffusion models. "
                },
                {
                    "pdf_id": "11.13",
                    "matching_string": "Computer Vision and Pattern Recognition, pages "
                }
            ]
        },
        {
            "leaf id": 143,
            "key": "doc/bib50",
            "block type": "bibliography",
            "content": "Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective wholebody pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023{\\natexlab{b}}.",
            "leftover": "on Computer Vision, pages 42104220, 2023{\\natexlab{b}}.",
            "matches": [
                {
                    "pdf_id": "11.5",
                    "matching_string": "Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective "
                },
                {
                    "pdf_id": "11.12",
                    "matching_string": "In Proceedings of the IEEE/CVF International Conference "
                },
                {
                    "pdf_id": "11.11",
                    "matching_string": "wholebody pose estimation with twostages distillation. "
                }
            ]
        },
        {
            "leaf id": 144,
            "key": "doc/bib51",
            "block type": "bibliography",
            "content": "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for texttoimage diffusion models. 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.14",
                    "matching_string": "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: "
                },
                {
                    "pdf_id": "11.15",
                    "matching_string": "Text compatible image prompt adapter for texttoimage "
                },
                {
                    "pdf_id": "11.17",
                    "matching_string": "diffusion models. 2023."
                }
            ]
        },
        {
            "leaf id": 145,
            "key": "doc/bib52",
            "block type": "bibliography",
            "content": "Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for contentrich texttoimage generation, 2022.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.24",
                    "matching_string": "Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei "
                },
                {
                    "pdf_id": "11.26",
                    "matching_string": "Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and "
                },
                {
                    "pdf_id": "11.23",
                    "matching_string": "Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan "
                },
                {
                    "pdf_id": "11.27",
                    "matching_string": "Yonghui Wu. Scaling autoregressive models for contentrich "
                },
                {
                    "pdf_id": "11.25",
                    "matching_string": "Yang, Burcu~Karagol Ayan, Ben Hutchinson, Wei Han, "
                },
                {
                    "pdf_id": "11.29",
                    "matching_string": "texttoimage generation, 2022."
                }
            ]
        },
        {
            "leaf id": 146,
            "key": "doc/bib53",
            "block type": "bibliography",
            "content": "Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warpbased network for poseguided human video generation. arXiv preprint arXiv:1910.09139, 2019.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.35",
                    "matching_string": "Leonid Sigal. Dwnet: Dense warpbased network for "
                },
                {
                    "pdf_id": "11.31",
                    "matching_string": "Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and "
                },
                {
                    "pdf_id": "11.36",
                    "matching_string": "poseguided human video generation. arXiv preprint "
                },
                {
                    "pdf_id": "11.37",
                    "matching_string": "arXiv:1910.09139, 2019."
                }
            ]
        },
        {
            "leaf id": 147,
            "key": "doc/bib54",
            "block type": "bibliography",
            "content": "Lvmin Zhang. Referenceonly controlnet, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.39",
                    "matching_string": "Lvmin Zhang. Referenceonly controlnet, 2023."
                }
            ]
        },
        {
            "leaf id": 148,
            "key": "doc/bib55",
            "block type": "bibliography",
            "content": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to texttoimage diffusion models, 2023.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.41",
                    "matching_string": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding "
                },
                {
                    "pdf_id": "11.45",
                    "matching_string": "conditional control to texttoimage diffusion models, 2023."
                },
                {
                    "pdf_id": "11.46",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 149,
            "key": "doc/bib56",
            "block type": "bibliography",
            "content": "Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. Ssrencoder: Encoding selective subject representation for subjectdriven generation, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.47",
                    "matching_string": "Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng "
                },
                {
                    "pdf_id": "11.49",
                    "matching_string": "Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and "
                },
                {
                    "pdf_id": "11.51",
                    "matching_string": "Zhongliang Jing. Ssrencoder: Encoding selective subject "
                },
                {
                    "pdf_id": "11.53",
                    "matching_string": "representation for subjectdriven generation, 2024."
                },
                {
                    "pdf_id": "11.55",
                    "matching_string": ""
                }
            ]
        },
        {
            "leaf id": 150,
            "key": "doc/bib57",
            "block type": "bibliography",
            "content": "Shihao Zhao, Dongdong Chen, YenChun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee~K Wong. Unicontrolnet: Allinone control to texttoimage diffusion models. Advances in Neural Information Processing Systems, 36, 2024.",
            "leftover": "",
            "matches": [
                {
                    "pdf_id": "11.87",
                    "matching_string": "Unicontrolnet: Allinone control to texttoimage diffusion "
                },
                {
                    "pdf_id": "11.57",
                    "matching_string": "Shihao Zhao, Dongdong Chen, YenChun Chen, Jianmin "
                },
                {
                    "pdf_id": "11.86",
                    "matching_string": "Bao, Shaozhe Hao, Lu Yuan, and KwanYee~K Wong. "
                },
                {
                    "pdf_id": "11.88",
                    "matching_string": "models. Advances in Neural Information Processing Systems, "
                },
                {
                    "pdf_id": "11.89",
                    "matching_string": "36, 2024."
                }
            ]
        }
    ]
}