{
    "key": "doc",
    "block_type": "document",
    "children": [
        {
            "leaf id": 0,
            "key": "doc/tit",
            "block type": "title",
            "content": "PHLP: Sole Persistent Homology for Link Prediction  Interpretable Feature Extraction",
            "leftover": "PHLP: Sole Persistent Homology for Link Prediction  Interpretable Feature Extraction",
            "matches": []
        },
        {
            "leaf id": 1,
            "key": "doc/aut0",
            "block type": "author",
            "content": "{ Junwon~You, Eunwoo~Heo, JaeHun~Jung \\thanks{Junwon You and Eunwoo Heo are cofirst authors.} \\thanks{} \\thanks{Junwon You, Eunwoo Heo, and J.H. Jung are with the Department of Mathematics, Pohang University of Science and Technology (POSTECH), Pohang, South Korea. J.H. Jung is also with the Graduate School of Artificial Intelligence, POSTECH, Pohang, South Korea.} \\thanks{} \\thanks{Preprint. Under review.}}",
            "leftover": "{ Junwon~You, Eunwoo~Heo, JaeHun~Jung \\thanks{Junwon You and Eunwoo Heo are cofirst authors.} \\thanks{} \\thanks{Junwon You, Eunwoo Heo, and J.H. Jung are with the Department of Mathematics, Pohang University of Science and Technology (POSTECH), Pohang, South Korea. J.H. Jung is also with the Graduate School of Artificial Intelligence, POSTECH, Pohang, South Korea.} \\thanks{} \\thanks{Preprint. Under review.}}",
            "matches": []
        },
        {
            "leaf id": 2,
            "key": "doc/abs",
            "block type": "abstract",
            "content": "Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes. Although graph neural network (GNN)based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks. We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance. We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology. The PHLP utilizes the angle hop subgraph}and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL. Using only a classifier, PHLP performs similarly to stateoftheart (SOTA) models on most benchmark datasets. Incorporating the outputs calculated using PHLP into the existing GNNbased SOTA models improves performance across all benchmark datasets. To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs. The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance.",
            "leftover": "Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes. Although graph neural network (GNN)based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks. We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance. We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology. The PHLP utilizes the angle hop subgraph}and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL. Using only a classifier, PHLP performs similarly to stateoftheart (SOTA) models on most benchmark datasets. Incorporating the outputs calculated using PHLP into the existing GNNbased SOTA models improves performance across all benchmark datasets. To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs. The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance.",
            "matches": []
        },
        {
            "key": "doc/body",
            "block_type": "body",
            "children": [
                {
                    "key": "doc/body/IEEEkeywords0",
                    "block_type": "IEEEkeywords",
                    "children": [
                        {
                            "leaf id": 3,
                            "key": "doc/body/IEEEkeywords0/txl0",
                            "block type": "txl",
                            "content": "Graph analysis, link prediction, persistent homology, topological data analysis.",
                            "leftover": "Graph analysis, link prediction, persistent homology, topological data analysis.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec1",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 4,
                            "key": "doc/body/sec1/tit",
                            "block type": "title",
                            "content": "Introduction",
                            "leftover": "Introduction",
                            "matches": []
                        },
                        {
                            "leaf id": 5,
                            "key": "doc/body/sec1/txl0",
                            "block type": "txl",
                            "content": "Graph data pervade numerous domains such as social networks, biological systems, recommendation engines, and ecommerce networks. The graph is wellsuited for modeling complex realworld relationships.",
                            "leftover": "Graph data pervade numerous domains such as social networks, biological systems, recommendation engines, and ecommerce networks. The graph is wellsuited for modeling complex realworld relationships.",
                            "matches": []
                        },
                        {
                            "leaf id": 6,
                            "key": "doc/body/sec1/txl1",
                            "block type": "txl",
                            "content": "Predicting missing or potential connections within a graph is essential for many applications, unlocking valuable insight and facilitating intelligent decisionmaking. The ability to predict future network interactions can be applied to diverse domains, including friend recommendations on social networks, knowledge graph completion, identification of potential drugprotein interactions in bioinformatics, prediction protein interactions, and optimization of supply chain logistics.",
                            "leftover": "Predicting missing or potential connections within a graph is essential for many applications, unlocking valuable insight and facilitating intelligent decisionmaking. The ability to predict future network interactions can be applied to diverse domains, including friend recommendations on social networks, knowledge graph completion, identification of potential drugprotein interactions in bioinformatics, prediction protein interactions, and optimization of supply chain logistics.",
                            "matches": []
                        },
                        {
                            "leaf id": 7,
                            "key": "doc/body/sec1/txl2",
                            "block type": "txl",
                            "content": "The link prediction (LP) problem has been categorized into three major paradigms: heuristic methods, embedding methods, and graph neural network (GNN)based methods, which are explored in detail in Section. Recently, compared to heuristic and embedding methods, GNNbased models have achieved significant score improvements in capturing intricate relationships within graphs.",
                            "leftover": "The link prediction (LP) problem has been categorized into three major paradigms: heuristic methods, embedding methods, and graph neural network (GNN)based methods, which are explored in detail in Section. Recently, compared to heuristic and embedding methods, GNNbased models have achieved significant score improvements in capturing intricate relationships within graphs.",
                            "matches": []
                        },
                        {
                            "leaf id": 8,
                            "key": "doc/body/sec1/txl3",
                            "block type": "txl",
                            "content": "However, GNNbased methods are comprised of neural networks, making it challenging to understand the reasons for their performance. To explore these reasons, we employ persistent homology (PH), a mathematical tool in topological data analysis (TDA) that enables the inference of topological information regarding the manifold approximating the data by quantifying the persistence of topological features across multiple scales. Various research has had successful outcomes in applying PH to graph classification and node classification tasks. In contrast, relatively few studies have explored using PH for LP. The topological loopcounting (TLC) GNN is a notable example that uses PH. The TLCGNN injects topological information into a GNN, and experiments were conducted on benchmark data where node attributes are available.",
                            "leftover": "However, GNNbased methods are comprised of neural networks, making it challenging to understand the reasons for their performance. To explore these reasons, we employ persistent homology (PH), a mathematical tool in topological data analysis (TDA) that enables the inference of topological information regarding the manifold approximating the data by quantifying the persistence of topological features across multiple scales. Various research has had successful outcomes in applying PH to graph classification and node classification tasks. In contrast, relatively few studies have explored using PH for LP. The topological loopcounting (TLC) GNN is a notable example that uses PH. The TLCGNN injects topological information into a GNN, and experiments were conducted on benchmark data where node attributes are available.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec1/figure4",
                            "block_type": "figure",
                            "children": [
                                {
                                    "key": "doc/body/sec1/figure4/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 9,
                                            "key": "doc/body/sec1/figure4/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "(Left) The GNNbased method extracts feature vectors through optimization (dashed area), making it difficult to interpret what these vectors represent. (Right) The proposed method extracts feature vectors through the designed analysis process, resulting in interpretable vectors.",
                                            "leftover": "(Left) The GNNbased method extracts feature vectors through optimization (dashed area), making it difficult to interpret what these vectors represent. (Right) The proposed method extracts feature vectors through the designed analysis process, resulting in interpretable vectors.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec1/figure*5",
                            "block_type": "figure*",
                            "children": [
                                {
                                    "key": "doc/body/sec1/figure*5/cpt0",
                                    "block_type": "cpt",
                                    "children": [
                                        {
                                            "leaf id": 10,
                                            "key": "doc/body/sec1/figure*5/cpt0/txl0",
                                            "block type": "txl",
                                            "content": "Topological features in subgraphs with and without a target link (u,v). The diagram illustrates the topological information extraction process for the subgraph 𝒩, as described in Section. The presence (top) or absence (bottom) of the target link changes the topological structure of the graph. Top row: When the target link is connected, three features (C1, C2, and C3) are detected shown in the persistence image (PI) in the right column. The PI represents the topological features of the subgraph 𝒩 (Section). Bottom row: When the target link is absent, only two features (C2 and C3) are detected as depicted in the corresponding PI.",
                                            "leftover": "Topological features in subgraphs with and without a target link (u,v). The diagram illustrates the topological information extraction process for the subgraph 𝒩, as described in Section. The presence (top) or absence (bottom) of the target link changes the topological structure of the graph. Top row: When the target link is connected, three features (C1, C2, and C3) are detected shown in the persistence image (PI) in the right column. The PI represents the topological features of the subgraph 𝒩 (Section). Bottom row: When the target link is absent, only two features (C2 and C3) are detected as depicted in the corresponding PI.",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "leaf id": 11,
                            "key": "doc/body/sec1/txl6",
                            "block type": "txl",
                            "content": "In this context, as illustrated in Fig., we present a novel approach to LP, called PHLP, which calculates the topological information of a graph. To use the topological information of subgraphs for LP, we measure how the topological information changes depending on the existence of the target link, as illustrated in Fig.. To extract topological information from various perspectives, we utilize angle hop subgraphs}for each target node. Additionally, we propose new node labeling called degree double radius node labeling (Degree DRNL), which incorporates degree information for each node, using DRNL.",
                            "leftover": "In this context, as illustrated in Fig., we present a novel approach to LP, called PHLP, which calculates the topological information of a graph. To use the topological information of subgraphs for LP, we measure how the topological information changes depending on the existence of the target link, as illustrated in Fig.. To extract topological information from various perspectives, we utilize angle hop subgraphs}for each target node. Additionally, we propose new node labeling called degree double radius node labeling (Degree DRNL), which incorporates degree information for each node, using DRNL.",
                            "matches": []
                        },
                        {
                            "leaf id": 12,
                            "key": "doc/body/sec1/txl7",
                            "block type": "txl",
                            "content": "The contributions are summarized as follows:",
                            "leftover": "The contributions are summarized as follows:",
                            "matches": []
                        },
                        {
                            "leaf id": 13,
                            "key": "doc/body/sec1/itemize8",
                            "block type": "itemize",
                            "content": "We develop an explainable LP method, PHLP, that employs the topological information for LP through PH without relying on neural networks, as illustrated in Fig.. We demonstrate that the proposed method, even with a simple classifier such as a multilayer perceptron (MLP), can achieve LP performance close to that of stateoftheart (SOTA) models. This method surpassed the SOTA performance for the Power dataset. We reveal that merely incorporating vectors computed by PHLP into existing LP models, including SOTA models, can improve their performance. To the best of our knowledge, the proposed method using PH without a GNN is the first to achieve performance close to that of SOTA models.",
                            "leftover": "We develop an explainable LP method, PHLP, that employs the topological information for LP through PH without relying on neural networks, as illustrated in Fig.. We demonstrate that the proposed method, even with a simple classifier such as a multilayer perceptron (MLP), can achieve LP performance close to that of stateoftheart (SOTA) models. This method surpassed the SOTA performance for the Power dataset. We reveal that merely incorporating vectors computed by PHLP into existing LP models, including SOTA models, can improve their performance. To the best of our knowledge, the proposed method using PH without a GNN is the first to achieve performance close to that of SOTA models.",
                            "matches": []
                        }
                    ]
                },
                {
                    "key": "doc/body/sec2",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 14,
                            "key": "doc/body/sec2/tit",
                            "block type": "title",
                            "content": "Related Work",
                            "leftover": "Related Work",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec2/sub0",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 15,
                                    "key": "doc/body/sec2/sub0/tit",
                                    "block type": "title",
                                    "content": "Link Prediction",
                                    "leftover": "Link Prediction",
                                    "matches": []
                                },
                                {
                                    "leaf id": 16,
                                    "key": "doc/body/sec2/sub0/txl0",
                                    "block type": "txl",
                                    "content": "Heuristic Methods. Heuristicbased approaches to LP compute the predefined structural features within the observed nodes and edges of the graph. Classic methods, such as common neighbors, AdamicAdar, Jaccard coefficient, and preferential attachment, rely on simple heuristics that capture certain aspects of node relationships. Zhou et al. proposed a local random walk method, whereas Jeh and Widom developed SimRank to quantify similarity based on the structural context. Although heuristic methods provide a preliminary understanding of LP, they are limited by their inability to capture complex relationships within graphs. Furthermore, heuristic methods are effective only when the defined heuristics align with the graph structure; therefore, applying heuristic methods across all graph datasets can be challenging.",
                                    "leftover": "Heuristic Methods. Heuristicbased approaches to LP compute the predefined structural features within the observed nodes and edges of the graph. Classic methods, such as common neighbors, AdamicAdar, Jaccard coefficient, and preferential attachment, rely on simple heuristics that capture certain aspects of node relationships. Zhou et al. proposed a local random walk method, whereas Jeh and Widom developed SimRank to quantify similarity based on the structural context. Although heuristic methods provide a preliminary understanding of LP, they are limited by their inability to capture complex relationships within graphs. Furthermore, heuristic methods are effective only when the defined heuristics align with the graph structure; therefore, applying heuristic methods across all graph datasets can be challenging.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 17,
                                    "key": "doc/body/sec2/sub0/txl1",
                                    "block type": "txl",
                                    "content": "Embedding Methods. Embedding methods map nodes from the graph into a lowdimensional vector space where geometric relationships mirror the graph structure. Koren et al. demonstrated the power of matrix factorization for collaborative filtering. Perozzi et al. introduced DeepWalk, using random walks to generate node sequences and employing the skipgram model to produce embeddings. Tang et al. developed largescale information network embedding (LINE), which preserves local and global structures. Grover and Leskovec further advanced this approach with Node2Vec (N2V), proposing a flexible notion of the neighborhood to capture diverse node relationships.",
                                    "leftover": "Embedding Methods. Embedding methods map nodes from the graph into a lowdimensional vector space where geometric relationships mirror the graph structure. Koren et al. demonstrated the power of matrix factorization for collaborative filtering. Perozzi et al. introduced DeepWalk, using random walks to generate node sequences and employing the skipgram model to produce embeddings. Tang et al. developed largescale information network embedding (LINE), which preserves local and global structures. Grover and Leskovec further advanced this approach with Node2Vec (N2V), proposing a flexible notion of the neighborhood to capture diverse node relationships.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 18,
                                    "key": "doc/body/sec2/sub0/txl2",
                                    "block type": "txl",
                                    "content": "Embedding methods are advantageous due to their applicability regardless of the data characteristics using optimization. Node representations capture global properties and longrange effects through the learning process. However, these methods often require significantly large dimensions to express basic heuristics, resulting in lower performance than heuristic methods. Moreover, in embedding methods, Ribeiro et al. explained that two nodes with similar neighborhood structures may have vastly different embedded vectors, especially when they are far apart in the graph, leading to incorrect predictions.",
                                    "leftover": "Embedding methods are advantageous due to their applicability regardless of the data characteristics using optimization. Node representations capture global properties and longrange effects through the learning process. However, these methods often require significantly large dimensions to express basic heuristics, resulting in lower performance than heuristic methods. Moreover, in embedding methods, Ribeiro et al. explained that two nodes with similar neighborhood structures may have vastly different embedded vectors, especially when they are far apart in the graph, leading to incorrect predictions.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 19,
                                    "key": "doc/body/sec2/sub0/txl3",
                                    "block type": "txl",
                                    "content": "GNNBased Methods. The GNN has become a pivotal approach to LP due to its ability to grasp graphstructured data. By effectively incorporating local and global information through message passing and graph aggregation layers, GNNs enhance LP performance. The model by Zhang et el. uses subgraphs as the primary structural units to learn and predict connections, resulting in significant improvement. This paradigm shift led to research focusing on refining and advancing subgraph methods in the context of GNNs. Following this trend, Pan et al. proposed WalkPool (WP), a new pooling mechanism that uses attention to jointly encode node representations and graph topology into learned topological features. However, despite their superior performance, GNNbased methods pose a challenge in comprehending the underlying mechanisms driving their predictions. Within this context, we develop the PHLP, based on PH, with performance comparable to GNNbased models.",
                                    "leftover": "GNNBased Methods. The GNN has become a pivotal approach to LP due to its ability to grasp graphstructured data. By effectively incorporating local and global information through message passing and graph aggregation layers, GNNs enhance LP performance. The model by Zhang et el. uses subgraphs as the primary structural units to learn and predict connections, resulting in significant improvement. This paradigm shift led to research focusing on refining and advancing subgraph methods in the context of GNNs. Following this trend, Pan et al. proposed WalkPool (WP), a new pooling mechanism that uses attention to jointly encode node representations and graph topology into learned topological features. However, despite their superior performance, GNNbased methods pose a challenge in comprehending the underlying mechanisms driving their predictions. Within this context, we develop the PHLP, based on PH, with performance comparable to GNNbased models.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec2/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 20,
                                    "key": "doc/body/sec2/sub1/tit",
                                    "block type": "title",
                                    "content": "Persistent Homology on Graph Data",
                                    "leftover": "Persistent Homology on Graph Data",
                                    "matches": []
                                },
                                {
                                    "leaf id": 21,
                                    "key": "doc/body/sec2/sub1/txl0",
                                    "block type": "txl",
                                    "content": "In recent years, PH, a method of analyzing the topological features of data, has been widely used to analyze graph data. It has demonstrated its effectiveness in graph classification tasks by analyzing the topology of graphs and has been applied to node classification tasks. However, its suitability for LP tasks has been limited, and research on applying PH for LP has progressed slowly. Yan et al. proposed an intriguing approach by integrating PH with GNNs. While their model demonstrates the potential of PH for capturing topological features of graph data, it relies on GNN structures. Additionally, the TLCGNN requires further research on datasets without node attributes.",
                                    "leftover": "In recent years, PH, a method of analyzing the topological features of data, has been widely used to analyze graph data. It has demonstrated its effectiveness in graph classification tasks by analyzing the topology of graphs and has been applied to node classification tasks. However, its suitability for LP tasks has been limited, and research on applying PH for LP has progressed slowly. Yan et al. proposed an intriguing approach by integrating PH with GNNs. While their model demonstrates the potential of PH for capturing topological features of graph data, it relies on GNN structures. Additionally, the TLCGNN requires further research on datasets without node attributes.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 22,
                                    "key": "doc/body/sec2/sub1/txl1",
                                    "block type": "txl",
                                    "content": "Although PH has demonstrated success in graph and node classification tasks, its filtration technique, tailored to analyzing the entire graph structure, might not be optimal for LP as the role of each node in LP differs from that in graph or node classification tasks. To address this challenge and advance research in LP, we develop a filtration method tailored explicitly to LP tasks.",
                                    "leftover": "Although PH has demonstrated success in graph and node classification tasks, its filtration technique, tailored to analyzing the entire graph structure, might not be optimal for LP as the role of each node in LP differs from that in graph or node classification tasks. To address this challenge and advance research in LP, we develop a filtration method tailored explicitly to LP tasks.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec3",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 23,
                            "key": "doc/body/sec3/tit",
                            "block type": "title",
                            "content": "Method",
                            "leftover": "Method",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec3/sub0",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 24,
                                    "key": "doc/body/sec3/sub0/tit",
                                    "block type": "title",
                                    "content": "Outline of the Proposed Methods",
                                    "leftover": "Outline of the Proposed Methods",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub0/figure*0",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 25,
                                            "key": "doc/body/sec3/sub0/figure*0/cpt0",
                                            "block type": "cpt",
                                            "content": "Overall structure of persistent homology for link prediction (PHLP) and multiangle PHLP (MAPHLP). (a) PHLP calculates the topological information based on the existence of target links in angle hop subgraphs for each target node. (b) With a classifier, MAPHLP integrates topological information across various angles to perform LP.",
                                            "leftover": "Overall structure of persistent homology for link prediction (PHLP) and multiangle PHLP (MAPHLP). (a) PHLP calculates the topological information based on the existence of target links in angle hop subgraphs for each target node. (b) With a classifier, MAPHLP integrates topological information across various angles to perform LP.",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 26,
                                    "key": "doc/body/sec3/sub0/txl1",
                                    "block type": "txl",
                                    "content": "We propose (a) PHLP and (b) multiangle PHLP (MAPHLP) as described in Fig.. The PHLP method analyzes the topological structure of the graph, focusing on target links. First, PHLP samples a (k,l)angle hop subgraph for the given target nodes (Section). Then, PHLP computes persistence images (PIs; Section) for cases with and without the target link. To calculate PIs, we introduce the node labeling and define the edgeweight function (Section). Through PHLP, each target node is transformed into a vector comprising PIs. In addition, LP is performed using the calculated vectors with a classifier (Section). To reflect diverse topological information, we also propose MAPHLP, which analyzes data from various angles (Section).",
                                    "leftover": "We propose (a) PHLP and (b) multiangle PHLP (MAPHLP) as described in Fig.. The PHLP method analyzes the topological structure of the graph, focusing on target links. First, PHLP samples a (k,l)angle hop subgraph for the given target nodes (Section). Then, PHLP computes persistence images (PIs; Section) for cases with and without the target link. To calculate PIs, we introduce the node labeling and define the edgeweight function (Section). Through PHLP, each target node is transformed into a vector comprising PIs. In addition, LP is performed using the calculated vectors with a classifier (Section). To reflect diverse topological information, we also propose MAPHLP, which analyzes data from various angles (Section).",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 27,
                                    "key": "doc/body/sec3/sub1/tit",
                                    "block type": "title",
                                    "content": "Extracting Angle Hop Subgraph",
                                    "leftover": "Extracting Angle Hop Subgraph",
                                    "matches": []
                                },
                                {
                                    "leaf id": 28,
                                    "key": "doc/body/sec3/sub1/txl0",
                                    "block type": "txl",
                                    "content": "Given a graph G = (V, E) and two nodes u,v ∈ V, a khop enclosing subgraph for (u,v) is defined as 𝒩^ku,v = (V', E') such that",
                                    "leftover": "Given a graph G = (V, E) and two nodes u,v ∈ V, a khop enclosing subgraph for (u,v) is defined as 𝒩^ku,v = (V', E') such that",
                                    "matches": []
                                },
                                {
                                    "leaf id": 29,
                                    "key": "doc/body/sec3/sub1/align*1",
                                    "block type": "align*",
                                    "content": "V' = z ∈V |d(u, z) ≤k or d(z, v) ≤k, E' = (z, w) ∈E |z ∈V' and w ∈V',",
                                    "leftover": "V' = z ∈V |d(u, z) ≤k or d(z, v) ≤k, E' = (z, w) ∈E |z ∈V' and w ∈V',",
                                    "matches": []
                                },
                                {
                                    "leaf id": 30,
                                    "key": "doc/body/sec3/sub1/txl2",
                                    "block type": "txl",
                                    "content": "where d(z, w) is the minimum number of edges in any path from z to w in G. We define a (k,l)angle hop enclosing subgraph, where the term ''angle'' signifies viewing the subgraph from multiple perspectives. The (k,l)angle hop subgraph is a generalization of the khop subgraph. Given a graph G = (V, E) and two nodes u, v ∈ V, a (k,l)angle hop enclosing subgraph for (u, v) is defined as 𝒩^(k,l)u,v = (V', E') such that",
                                    "leftover": "where d(z, w) is the minimum number of edges in any path from z to w in G. We define a (k,l)angle hop enclosing subgraph, where the term ''angle'' signifies viewing the subgraph from multiple perspectives. The (k,l)angle hop subgraph is a generalization of the khop subgraph. Given a graph G = (V, E) and two nodes u, v ∈ V, a (k,l)angle hop enclosing subgraph for (u, v) is defined as 𝒩^(k,l)u,v = (V', E') such that",
                                    "matches": []
                                },
                                {
                                    "leaf id": 31,
                                    "key": "doc/body/sec3/sub1/align*3",
                                    "block type": "align*",
                                    "content": "V' = z ∈V |d(u, z) ≤k or d(z, v) ≤l, E' = (z, w) ∈E |z ∈V' and w ∈V' .",
                                    "leftover": "V' = z ∈V |d(u, z) ≤k or d(z, v) ≤l, E' = (z, w) ∈E |z ∈V' and w ∈V' .",
                                    "matches": []
                                },
                                {
                                    "leaf id": 32,
                                    "key": "doc/body/sec3/sub1/txl4",
                                    "block type": "txl",
                                    "content": "Thus, the angle hop can generate subgraphs in various forms, providing flexibility to adapt to various graph characteristics.",
                                    "leftover": "Thus, the angle hop can generate subgraphs in various forms, providing flexibility to adapt to various graph characteristics.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 33,
                                    "key": "doc/body/sec3/sub2/tit",
                                    "block type": "title",
                                    "content": "Filtration of the Subgraph",
                                    "leftover": "Filtration of the Subgraph",
                                    "matches": []
                                },
                                {
                                    "leaf id": 34,
                                    "key": "doc/body/sec3/sub2/txl0",
                                    "block type": "txl",
                                    "content": "For a given subgraph, the Rips filtration is employed to calculate the topology using PH. To apply the Rips filtration, we define an edgeweight function using node labeling that reflects the topology of the given graph.",
                                    "leftover": "For a given subgraph, the Rips filtration is employed to calculate the topology using PH. To apply the Rips filtration, we define an edgeweight function using node labeling that reflects the topology of the given graph.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec3/sub2/figure1",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub2/figure1/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 35,
                                                    "key": "doc/body/sec3/sub2/figure1/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Node labeling on graphs. (a) Node label values without considering the graph structure cannot distinguish between G1 and G2 using DRNL. (b) Applying Degree DRNL allows G1 and G2 to be distinguished solely by node label values.",
                                                    "leftover": "Node labeling on graphs. (a) Node label values without considering the graph structure cannot distinguish between G1 and G2 using DRNL. (b) Applying Degree DRNL allows G1 and G2 to be distinguished solely by node label values.",
                                                    "matches": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec3/sub2/figure2",
                                    "block_type": "figure",
                                    "children": [
                                        {
                                            "key": "doc/body/sec3/sub2/figure2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 36,
                                                    "key": "doc/body/sec3/sub2/figure2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Persistence images (PIs) for two node labeling methods for the graphs in Fig.. (a) DRNL exhibits identical zerodimensional PIs for G1 and G2, (b) Degree DRNL produces distinct outcomes, effectively distinguishing between the two.",
                                                    "leftover": "Persistence images (PIs) for two node labeling methods for the graphs in Fig.. (a) DRNL exhibits identical zerodimensional PIs for G1 and G2, (b) Degree DRNL produces distinct outcomes, effectively distinguishing between the two.",
                                                    "matches": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 37,
                                    "key": "doc/body/sec3/sub2/txl3",
                                    "block type": "txl",
                                    "content": "Degree DRNL. Zhang et al. introduced DRNL, which computes the distance from any node to two fixed nodes. For any subgraph 𝒩=(V',E') of G and two nodes a,b ∈ V', the DRNL f^(a,b)drnl: V' →ℕ based on (a,b) of G for any vertex w in V', is defined as",
                                    "leftover": "Degree DRNL. Zhang et al. introduced DRNL, which computes the distance from any node to two fixed nodes. For any subgraph 𝒩=(V',E') of G and two nodes a,b ∈ V', the DRNL f^(a,b)drnl: V' →ℕ based on (a,b) of G for any vertex w in V', is defined as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 38,
                                    "key": "doc/body/sec3/sub2/frm4",
                                    "block type": "frm",
                                    "content": "f^(a,b)drnl(w) = 1 + min(d(w,a), d(w,b)) + qw(qw + rw  1),",
                                    "leftover": "f^(a,b)drnl(w) = 1 + min(d(w,a), d(w,b)) + qw(qw + rw  1),",
                                    "matches": []
                                },
                                {
                                    "leaf id": 39,
                                    "key": "doc/body/sec3/sub2/txl5",
                                    "block type": "txl",
                                    "content": "where qw ∈ℤ and rw ∈ 0, 1 are integers representing the quotient and remainder, respectively, such that d(w,a)+d(w,b) = 2qw + rw. We call these two nodes, a and b, center nodes. These center nodes do not need to be the target nodes used when extracting the subgraph.",
                                    "leftover": "where qw ∈ℤ and rw ∈ 0, 1 are integers representing the quotient and remainder, respectively, such that d(w,a)+d(w,b) = 2qw + rw. We call these two nodes, a and b, center nodes. These center nodes do not need to be the target nodes used when extracting the subgraph.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 40,
                                    "key": "doc/body/sec3/sub2/txl6",
                                    "block type": "txl",
                                    "content": "However, DRNL encounters limitations when the graph is transformed into nodelabel information. As depicted in Fig., DRNL assigns the same node labels to different graphs, resulting in identical zerodimensional PIs (Fig., Section). To incorporate the local topology of each node with the effects of DRNL, we introduced Degree DRNL. For a given subgraph 𝒩 = (V', E') of G and center nodes a,b ∈ V', the Degree DRNL f^(a,b)degdrnl : V' →ℝ based on (a,b), for all vertex w in V', is defined as",
                                    "leftover": "However, DRNL encounters limitations when the graph is transformed into nodelabel information. As depicted in Fig., DRNL assigns the same node labels to different graphs, resulting in identical zerodimensional PIs (Fig., Section). To incorporate the local topology of each node with the effects of DRNL, we introduced Degree DRNL. For a given subgraph 𝒩 = (V', E') of G and center nodes a,b ∈ V', the Degree DRNL f^(a,b)degdrnl : V' →ℝ based on (a,b), for all vertex w in V', is defined as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 41,
                                    "key": "doc/body/sec3/sub2/frm7",
                                    "block type": "frm",
                                    "content": "f^(a,b)degdrnl(w) = f^(a,b)drnl(w) + {M  (w)M,",
                                    "leftover": "f^(a,b)degdrnl(w) = f^(a,b)drnl(w) + {M  (w)M,",
                                    "matches": []
                                },
                                {
                                    "leaf id": 42,
                                    "key": "doc/body/sec3/sub2/txl8",
                                    "block type": "txl",
                                    "content": "where M denotes the maximum degree of nodes in 𝒩. The (M(w))/M term above assigns larger values for lower degrees of w. When M = (w), the value of Degree DRNL matches the original DRNL, ensuring that the edges connected to nodes with higher degrees are assigned smaller values, promoting their earlier emergence in the filtration. Fig. demonstrates various node labels obtained using Degree DRNL, resulting in PIs that can be distinguished from each other (Fig.).",
                                    "leftover": "where M denotes the maximum degree of nodes in 𝒩. The (M(w))/M term above assigns larger values for lower degrees of w. When M = (w), the value of Degree DRNL matches the original DRNL, ensuring that the edges connected to nodes with higher degrees are assigned smaller values, promoting their earlier emergence in the filtration. Fig. demonstrates various node labels obtained using Degree DRNL, resulting in PIs that can be distinguished from each other (Fig.).",
                                    "matches": []
                                },
                                {
                                    "leaf id": 43,
                                    "key": "doc/body/sec3/sub2/txl9",
                                    "block type": "txl",
                                    "content": "Edgeweight function.} For a given subgraph 𝒩 = (V', E'), f : V' →ℕ denotes any node labeling function. The edgeweight function W:E' →ℝ, for any edge (w, z) in E', is defined as",
                                    "leftover": "Edgeweight function.} For a given subgraph 𝒩 = (V', E'), f : V' →ℕ denotes any node labeling function. The edgeweight function W:E' →ℝ, for any edge (w, z) in E', is defined as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 44,
                                    "key": "doc/body/sec3/sub2/frm10",
                                    "block type": "frm",
                                    "content": "W(w,z) =max(f(w),f(z)) + {min(f(w),f(z))max(f(w),f(z)).",
                                    "leftover": "W(w,z) =max(f(w),f(z)) + {min(f(w),f(z))max(f(w),f(z)).",
                                    "matches": []
                                },
                                {
                                    "leaf id": 45,
                                    "key": "doc/body/sec3/sub2/txl11",
                                    "block type": "txl",
                                    "content": "The min/max term in the definition of W refines values further, enhancing the discriminative power by reducing the occurrence of identical edge weights.",
                                    "leftover": "The min/max term in the definition of W refines values further, enhancing the discriminative power by reducing the occurrence of identical edge weights.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 46,
                                    "key": "doc/body/sec3/sub3/tit",
                                    "block type": "title",
                                    "content": "Persistent Homology",
                                    "leftover": "Persistent Homology",
                                    "matches": []
                                },
                                {
                                    "leaf id": 47,
                                    "key": "doc/body/sec3/sub3/txl0",
                                    "block type": "txl",
                                    "content": "Given an edgeweighted subgraph 𝒩 = (V', E', W), we construct a Rips filtration and compute its PH. First, we create a sequence of subgraphs 𝒩ϵϵ∈ℝ, where each 𝒩ϵ = (V', E'ϵ) and E'ϵ= e ∈ E | W(e) ≤ϵ. Second, we convert each subgraph 𝒩ϵ into the Rips complex Kϵ = τ∈𝕏| (w,z) ∈ E'ϵ for any two vertices w,z ∈τ, where 𝕏 is the power set of V'. In Kϵ, a simplex τ is formed when the vertices in τ are pairwise connected by edges in 𝒩ϵ. Then, the Rips filtration is obtained as Kϵ1↪ Kϵ2↪⋯↪ Kϵm = 𝕏 for ϵ1 ≤ϵ2 ≤⋯≤ϵm. Third, we compute the pdimensional homology group Hp(Kϵ) for each complex Kϵ and track how these groups change as ϵ increases. The persistence diagram D comprises persistence pairs (b,d) representing the ϵ values at which a homological feature appears b and disappears d, respectively, in the filtration.",
                                    "leftover": "Given an edgeweighted subgraph 𝒩 = (V', E', W), we construct a Rips filtration and compute its PH. First, we create a sequence of subgraphs 𝒩ϵϵ∈ℝ, where each 𝒩ϵ = (V', E'ϵ) and E'ϵ= e ∈ E | W(e) ≤ϵ. Second, we convert each subgraph 𝒩ϵ into the Rips complex Kϵ = τ∈𝕏| (w,z) ∈ E'ϵ for any two vertices w,z ∈τ, where 𝕏 is the power set of V'. In Kϵ, a simplex τ is formed when the vertices in τ are pairwise connected by edges in 𝒩ϵ. Then, the Rips filtration is obtained as Kϵ1↪ Kϵ2↪⋯↪ Kϵm = 𝕏 for ϵ1 ≤ϵ2 ≤⋯≤ϵm. Third, we compute the pdimensional homology group Hp(Kϵ) for each complex Kϵ and track how these groups change as ϵ increases. The persistence diagram D comprises persistence pairs (b,d) representing the ϵ values at which a homological feature appears b and disappears d, respectively, in the filtration.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 48,
                                    "key": "doc/body/sec3/sub4/tit",
                                    "block type": "title",
                                    "content": "Persistence Image",
                                    "leftover": "Persistence Image",
                                    "matches": []
                                },
                                {
                                    "leaf id": 49,
                                    "key": "doc/body/sec3/sub4/txl0",
                                    "block type": "txl",
                                    "content": "We convert the persistence diagram into a PI. For a given persistence diagram D, consider a linear transform L: ℝ^2 →ℝ^2 defined by L(x, y) = (x, yx). The image set of D under this transformation is denoted as L(D). For each point (b,d') in L(D), a weight function ϕ(b,d'): ℝ^2 →ℝ is defined that assigns a weight to each point in the persistence diagram. A common choice for ϕ(b,d') is the Gaussian function centered at (b,d'). The nonnegative function is defined as h:ℝ^2 →ℝ, as h(x,y)=1/log(1+ | y |). The function h is zero along the horizontal xaxis, and is continuous and piecewise differentiable, satisfying the conditions presented in. The persistence surface ρD:ℝ^2 →ℝ is defined as",
                                    "leftover": "We convert the persistence diagram into a PI. For a given persistence diagram D, consider a linear transform L: ℝ^2 →ℝ^2 defined by L(x, y) = (x, yx). The image set of D under this transformation is denoted as L(D). For each point (b,d') in L(D), a weight function ϕ(b,d'): ℝ^2 →ℝ is defined that assigns a weight to each point in the persistence diagram. A common choice for ϕ(b,d') is the Gaussian function centered at (b,d'). The nonnegative function is defined as h:ℝ^2 →ℝ, as h(x,y)=1/log(1+ | y |). The function h is zero along the horizontal xaxis, and is continuous and piecewise differentiable, satisfying the conditions presented in. The persistence surface ρD:ℝ^2 →ℝ is defined as",
                                    "matches": []
                                },
                                {
                                    "leaf id": 50,
                                    "key": "doc/body/sec3/sub4/frm1",
                                    "block type": "frm",
                                    "content": "ρD(z) = ∑(b,d') ∈L(D) h(b,d')ϕ(b,d')(z).",
                                    "leftover": "ρD(z) = ∑(b,d') ∈L(D) h(b,d')ϕ(b,d')(z).",
                                    "matches": []
                                },
                                {
                                    "leaf id": 51,
                                    "key": "doc/body/sec3/sub4/txl2",
                                    "block type": "txl",
                                    "content": "The continuous surface ρD is discretized into a finitedimensional representation over a predefined grid. This grid consists of n cells, each corresponding to a specific region in the plane. The PI is defined as an array of values I(ρD)p for each cell p. Each I(ρD)p in this array is computed by integrating the persistence surface ρD over the area of cell p:",
                                    "leftover": "The continuous surface ρD is discretized into a finitedimensional representation over a predefined grid. This grid consists of n cells, each corresponding to a specific region in the plane. The PI is defined as an array of values I(ρD)p for each cell p. Each I(ρD)p in this array is computed by integrating the persistence surface ρD over the area of cell p:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 52,
                                    "key": "doc/body/sec3/sub4/frm3",
                                    "block type": "frm",
                                    "content": "I(ρD)p = ∬p ρD dy dx.",
                                    "leftover": "I(ρD)p = ∬p ρD dy dx.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub5",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 53,
                                    "key": "doc/body/sec3/sub5/tit",
                                    "block type": "title",
                                    "content": "Predicting the Existence of the Target Link",
                                    "leftover": "Predicting the Existence of the Target Link",
                                    "matches": []
                                },
                                {
                                    "leaf id": 54,
                                    "key": "doc/body/sec3/sub5/txl0",
                                    "block type": "txl",
                                    "content": "For the given target nodes (u,v), we sample the (k,l)angle hop subgraph 𝒩^(k,l)u,v, denoted as 𝒩^ (Section), assuming that the target link does not exist during this process. On this subgraph, we extract topological features by calculating PH and its vectorization (i.e., the PI, as described in Sections and). The vectorization is calculated for each dimension and concatenated. If k ≠ l, for symmetry, we repeat the same process with the (l,k)angle hop subgraph once and consider the average of the two vectors, denoting this vector as x^. To observe the difference in topological features, we consider a subgraph 𝒩+ obtained by connecting the target link to 𝒩^. For this graph, x^+ denotes the vector obtained using this method.",
                                    "leftover": "For the given target nodes (u,v), we sample the (k,l)angle hop subgraph 𝒩^(k,l)u,v, denoted as 𝒩^ (Section), assuming that the target link does not exist during this process. On this subgraph, we extract topological features by calculating PH and its vectorization (i.e., the PI, as described in Sections and). The vectorization is calculated for each dimension and concatenated. If k ≠ l, for symmetry, we repeat the same process with the (l,k)angle hop subgraph once and consider the average of the two vectors, denoting this vector as x^. To observe the difference in topological features, we consider a subgraph 𝒩+ obtained by connecting the target link to 𝒩^. For this graph, x^+ denotes the vector obtained using this method.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 55,
                                    "key": "doc/body/sec3/sub5/txl1",
                                    "block type": "txl",
                                    "content": "To predict the existence of the target link with the vectors x^ and x^+, we employ an MLP classifier Φ: ℝ^2(d+1)n^2→ℝ where n represents the resolution of the PI, and d denotes the maximal dimension of PH. The model predicts the existence of a link between two target nodes with the following probability:",
                                    "leftover": "To predict the existence of the target link with the vectors x^ and x^+, we employ an MLP classifier Φ: ℝ^2(d+1)n^2→ℝ where n represents the resolution of the PI, and d denotes the maximal dimension of PH. The model predicts the existence of a link between two target nodes with the following probability:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 56,
                                    "key": "doc/body/sec3/sub5/frm2",
                                    "block type": "frm",
                                    "content": "zuv = σ(Φ(x)),",
                                    "leftover": "zuv = σ(Φ(x)),",
                                    "matches": []
                                },
                                {
                                    "leaf id": 57,
                                    "key": "doc/body/sec3/sub5/txl3",
                                    "block type": "txl",
                                    "content": "where x is the concatenation of x^ and x^+, and σ is the activation function. For the training dataset 𝒳⊆ V × V, comprising positive and negative links corresponding to the elements of E and (V× V)∖ E, respectively, we define the loss function as follows:",
                                    "leftover": "where x is the concatenation of x^ and x^+, and σ is the activation function. For the training dataset 𝒳⊆ V × V, comprising positive and negative links corresponding to the elements of E and (V× V)∖ E, respectively, we define the loss function as follows:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 58,
                                    "key": "doc/body/sec3/sub5/frm4",
                                    "block type": "frm",
                                    "content": "∑(u,v) ∈𝒳 BCE(zuv, yuv),",
                                    "leftover": "∑(u,v) ∈𝒳 BCE(zuv, yuv),",
                                    "matches": []
                                },
                                {
                                    "leaf id": 59,
                                    "key": "doc/body/sec3/sub5/txl5",
                                    "block type": "txl",
                                    "content": "where BCE(·,·) represents the binary crossentropy loss and yuv denotes the label of the target link (u,v), which is 0 for negative links or 1 for positive links.",
                                    "leftover": "where BCE(·,·) represents the binary crossentropy loss and yuv denotes the label of the target link (u,v), which is 0 for negative links or 1 for positive links.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub6",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 60,
                                    "key": "doc/body/sec3/sub6/tit",
                                    "block type": "title",
                                    "content": "Multiangle PHLP",
                                    "leftover": "Multiangle PHLP",
                                    "matches": []
                                },
                                {
                                    "leaf id": 61,
                                    "key": "doc/body/sec3/sub6/txl0",
                                    "block type": "txl",
                                    "content": "The MAPHLP maximizes the advantages of PHLP by examining data from various angles through the extraction of subgraphs based on a hyperparameter, the maximum hop (max hop, denoted as H). The types of angles are elements of all combinations of k and l within the set (k,l) ∈ℤ^2 | 0 ≤ l ≤ k ≤ H, k > 0. If we define the prediction probability of a PHLP for each type of angle hop as zi for i=1, 2, ..., N, then MAPHLP predicts the likelihood of the link existence with the following probability:",
                                    "leftover": "The MAPHLP maximizes the advantages of PHLP by examining data from various angles through the extraction of subgraphs based on a hyperparameter, the maximum hop (max hop, denoted as H). The types of angles are elements of all combinations of k and l within the set (k,l) ∈ℤ^2 | 0 ≤ l ≤ k ≤ H, k > 0. If we define the prediction probability of a PHLP for each type of angle hop as zi for i=1, 2, ..., N, then MAPHLP predicts the likelihood of the link existence with the following probability:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 62,
                                    "key": "doc/body/sec3/sub6/frm1",
                                    "block type": "frm",
                                    "content": "p = ∑i=1^N αizi,",
                                    "leftover": "p = ∑i=1^N αizi,",
                                    "matches": []
                                },
                                {
                                    "leaf id": 63,
                                    "key": "doc/body/sec3/sub6/txl2",
                                    "block type": "txl",
                                    "content": "where α = (α1,...,αN) ∈ℝ^N is a trainable parameter. We apply the softmax function to the parameter α to ensure that the sum of all elements equals 1. Moreover, MAPHLP is trained using the binary crossentropy loss.",
                                    "leftover": "where α = (α1,...,αN) ∈ℝ^N is a trainable parameter. We apply the softmax function to the parameter α to ensure that the sum of all elements equals 1. Moreover, MAPHLP is trained using the binary crossentropy loss.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec3/sub7",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 64,
                                    "key": "doc/body/sec3/sub7/tit",
                                    "block type": "title",
                                    "content": "Hybrid Method",
                                    "leftover": "Hybrid Method",
                                    "matches": []
                                },
                                {
                                    "leaf id": 65,
                                    "key": "doc/body/sec3/sub7/txl0",
                                    "block type": "txl",
                                    "content": "The proposed approach easily integrates with existing subgraph methods. Subgraph methods treat the LP task as a binary classification problem comprising two components: a feature extractor F and classifier P. Vectors with PH information calculated using the proposed methods are incorporated through concatenation before the classifier. The detailed process of the hybrid method is outlined as follows:",
                                    "leftover": "The proposed approach easily integrates with existing subgraph methods. Subgraph methods treat the LP task as a binary classification problem comprising two components: a feature extractor F and classifier P. Vectors with PH information calculated using the proposed methods are incorporated through concatenation before the classifier. The detailed process of the hybrid method is outlined as follows:",
                                    "matches": []
                                },
                                {
                                    "leaf id": 66,
                                    "key": "doc/body/sec3/sub7/enumerate1",
                                    "block type": "enumerate",
                                    "content": "Subgraph Extraction:}For the given graph G and target nodes (u,v), khop subgraph 𝒩^ku,v is extracted. Feature Extraction:}Existing methods extract features Z = F(𝒩^ku,v) from the subgraph. Persistent Image Calculation:}The methods described in Sections,, and are applied to 𝒩^ku,v, where I denotes the PI vector. An MLP Φ:ℝ^m →ℝ^n transforms the PI into a format similar to Z. For the hybrid method of MAPHLP, 𝒩^ku,v is replaced with multiangle subgraphs, concatenating their PI vectors. Classification:}Next, α1 Z and α2 Φ(I) are concatenated, where α1 and α2 are trainable parameters. The softmax function is applied to the parameter α=(α1,α2), ensuring that the sum of elements equals 1, denoted by J. This concatenated vector is classified using the existing method's classifier, P(J).",
                                    "leftover": "Subgraph Extraction:}For the given graph G and target nodes (u,v), khop subgraph 𝒩^ku,v is extracted. Feature Extraction:}Existing methods extract features Z = F(𝒩^ku,v) from the subgraph. Persistent Image Calculation:}The methods described in Sections,, and are applied to 𝒩^ku,v, where I denotes the PI vector. An MLP Φ:ℝ^m →ℝ^n transforms the PI into a format similar to Z. For the hybrid method of MAPHLP, 𝒩^ku,v is replaced with multiangle subgraphs, concatenating their PI vectors. Classification:}Next, α1 Z and α2 Φ(I) are concatenated, where α1 and α2 are trainable parameters. The softmax function is applied to the parameter α=(α1,α2), ensuring that the sum of elements equals 1, denoted by J. This concatenated vector is classified using the existing method's classifier, P(J).",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec4",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 67,
                            "key": "doc/body/sec4/tit",
                            "block type": "title",
                            "content": "Experiments",
                            "leftover": "Experiments",
                            "matches": []
                        },
                        {
                            "leaf id": 68,
                            "key": "doc/body/sec4/txl0",
                            "block type": "txl",
                            "content": "This section evaluates the performance of MAPHLP. The experiments were also conducted using only zerodimensional homology (MAPHLP (dim0)). We used the area under the curve (AUC) as an evaluation metric. We repeated all experiments 10 times and reported the mean and standard deviation of the AUC values.",
                            "leftover": "This section evaluates the performance of MAPHLP. The experiments were also conducted using only zerodimensional homology (MAPHLP (dim0)). We used the area under the curve (AUC) as an evaluation metric. We repeated all experiments 10 times and reported the mean and standard deviation of the AUC values.",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec4/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 69,
                                    "key": "doc/body/sec4/sub1/tit",
                                    "block type": "title",
                                    "content": "Experimental Settings",
                                    "leftover": "Experimental Settings",
                                    "matches": []
                                },
                                {
                                    "leaf id": 70,
                                    "key": "doc/body/sec4/sub1/txl0",
                                    "block type": "txl",
                                    "content": "Baselines.}To evaluate the effectiveness of PHLP, we compared the proposed model with five heuristic methods, four embeddingbased methods, and two GNNbased models. The heuristic methods include the AdamicAdar (AA), Katz index (Katz), PageRank (PR), WeisfeilerLehman graph kernel (WLK), and WeisfeilerLehman neural machine (WLNM). For the embeddingbased methods, we applied N2V, spectral clustering (SPC), matrix factorization (MF), and LINE. Moreover, SEAL and WP represent the GNNbased methods.",
                                    "leftover": "Baselines.}To evaluate the effectiveness of PHLP, we compared the proposed model with five heuristic methods, four embeddingbased methods, and two GNNbased models. The heuristic methods include the AdamicAdar (AA), Katz index (Katz), PageRank (PR), WeisfeilerLehman graph kernel (WLK), and WeisfeilerLehman neural machine (WLNM). For the embeddingbased methods, we applied N2V, spectral clustering (SPC), matrix factorization (MF), and LINE. Moreover, SEAL and WP represent the GNNbased methods.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 71,
                                    "key": "doc/body/sec4/sub1/txl1",
                                    "block type": "txl",
                                    "content": "Datasets.",
                                    "leftover": "Datasets.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub1/table2",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub1/table2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 72,
                                                    "key": "doc/body/sec4/sub1/table2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Statistics of the datasets",
                                                    "leftover": "Statistics of the datasets",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 73,
                                            "key": "doc/body/sec4/sub1/table2/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|ccccc} Dataset & #Nodes & #Edges & Avg. node deg. & Density USAir & 332 & 2126 & 12.81 & 3.86e2 NS & 1589 & 2742 & 3.45 & 2.17e3 PB & 1222 & 16714 & 27.36 & 2.24e2 Yeast & 2375 & 11693 & 9.85 & 4.15e3 {C.ele} & 297 & 2148 & 14.46 & 4.87e2 Power & 4941 & 6594 & 2.67 & 5.40e4 Router & 5022 & 6258 & 2.49 & 4.96e4 {E.coli} & 1805 & 15660 & 16.24 & 9.61e3",
                                            "leftover": "{l|ccccc} Dataset & #Nodes & #Edges & Avg. node deg. & Density USAir & 332 & 2126 & 12.81 & 3.86e2 NS & 1589 & 2742 & 3.45 & 2.17e3 PB & 1222 & 16714 & 27.36 & 2.24e2 Yeast & 2375 & 11693 & 9.85 & 4.15e3 {C.ele} & 297 & 2148 & 14.46 & 4.87e2 Power & 4941 & 6594 & 2.67 & 5.40e4 Router & 5022 & 6258 & 2.49 & 4.96e4 {E.coli} & 1805 & 15660 & 16.24 & 9.61e3",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 74,
                                    "key": "doc/body/sec4/sub1/txl3",
                                    "block type": "txl",
                                    "content": "In line with previous studies and, we evaluate the performance of our MAPHLP on the eight datasets in Table without node attributes: USAir, NS, PB, Yeast, C.~elegans (C.~ele), Power, Router, and E.~coli. The detailed statistics for each dataset are summarized in Table.",
                                    "leftover": "In line with previous studies and, we evaluate the performance of our MAPHLP on the eight datasets in Table without node attributes: USAir, NS, PB, Yeast, C.~elegans (C.~ele), Power, Router, and E.~coli. The detailed statistics for each dataset are summarized in Table.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 75,
                                    "key": "doc/body/sec4/sub1/txl4",
                                    "block type": "txl",
                                    "content": "Implementation Details.} All edges in the datasets were split into training, validation, and testing datasets with proportions of 0.85, 0.05, and 0.1, respectively, ensuring a fair comparison with previous studies. The max hop M was set to 3 for most datasets (Table). However, for the E.~coli dataset, it was reduced to 2 when employing onedimensional homology due to memory constraints. Conversely, for the Power dataset, the max hop was set to 7 because it does not demand heavy memory and computation time. The sigmoid function was employed for the activation function of the PHLP classifier. Tables and present the results of the hybrid methods using SEAL and WP, respectively. For these experiments, a twolayer MLP was used for the MLP Φ in Step 3 of Section. We set the khops following the original methods, SEAL and WP, and the max hops M of MAPHLP were set as the k, except for the Power dataset. For the Power dataset, we set the khop to 1hop and max hop M to 7, respectively, which is discussed in detail in Section.",
                                    "leftover": "Implementation Details.} All edges in the datasets were split into training, validation, and testing datasets with proportions of 0.85, 0.05, and 0.1, respectively, ensuring a fair comparison with previous studies. The max hop M was set to 3 for most datasets (Table). However, for the E.~coli dataset, it was reduced to 2 when employing onedimensional homology due to memory constraints. Conversely, for the Power dataset, the max hop was set to 7 because it does not demand heavy memory and computation time. The sigmoid function was employed for the activation function of the PHLP classifier. Tables and present the results of the hybrid methods using SEAL and WP, respectively. For these experiments, a twolayer MLP was used for the MLP Φ in Step 3 of Section. We set the khops following the original methods, SEAL and WP, and the max hops M of MAPHLP were set as the k, except for the Power dataset. For the Power dataset, we set the khop to 1hop and max hop M to 7, respectively, which is discussed in detail in Section.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub1/table*5",
                                    "block_type": "table*",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub1/table*5/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 76,
                                                    "key": "doc/body/sec4/sub1/table*5/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Link prediction performance measured by the AUC on Benchmark datasets (90% observed links)",
                                                    "leftover": "Link prediction performance measured by the AUC on Benchmark datasets (90% observed links)",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 77,
                                            "key": "doc/body/sec4/sub1/table*5/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cccccccccc} Dataset & USAir & NS & PB & Yeast & C.~ele & Power & Router & E.~coli AA & 95.06 ± 1.03 & 94.45 ± 0.93 & 92.36 ± 0.34 & 89.43 ± 0.62 & 86.95 ± 1.40 & 58.79 ± 0.88 & 56.43 ± 0.51 & 95.36 ± 0.34 Katz & 92.88 ± 1.42 & 94.85 ± 1.10 & 92.92 ± 0.35 & 92.24 ± 0.61 & 86.34 ± 1.89 & 65.39 ± 1.59 & 38.62 ± 1.35 & 93.50 ± 0.44 PR & 94.67 ± 1.08 & 94.89 ± 1.08 & 93.54 ± 0.41 & 92.76 ± 0.55 & 90.32 ± 1.49 & 66.00 ± 1.59 & 38.76 ± 1.39 & 95.57 ± 0.44 WLK & 96.63 ± 0.73 & 98.57 ± 0.51 & 93.83 ± 0.59 & 95.86 ± 0.54 & 89.72 ± 1.67 & 82.41 ± 3.43 & 87.42 ± 2.08 & 96.94 ± 0.29 WLNM & 95.95 ± 1.10 & 98.61 ± 0.49 & 93.49 ± 0.47 & 95.62 ± 0.52 & 86.18 ± 1.72 & 84.76 ± 0.98 & 94.41 ± 0.88 & 97.21 ± 0.27 N2V & 91.44 ± 1.78 & 91.52 ± 1.28 & 85.79 ± 0.78 & 93.67 ± 0.46 & 84.11 ± 1.27 & 76.22 ± 0.92 & 65.46 ± 0.86 & 90.82 ± 1.49 SPC & 74.22 ± 3.11 & 89.94 ± 2.39 & 83.96 ± 0.86 & 93.25 ± 0.40 & 51.90 ± 2.57 & 91.78 ± 0.61 & 68.79 ± 2.42 & 94.92 ± 0.32 MF & 94.08 ± 0.80 & 74.55 ± 4.34 & 94.30 ± 0.53 & 90.28 ± 0.69 & 85.90 ± 1.74 & 50.63 ± 1.10 & 78.03 ± 1.63 & 93.76 ± 0.56 LINE & 81.47 ± 10.71 & 80.63 ± 1.90 & 76.95 ± 2.76 & 87.45 ± 3.33 & 69.21 ± 3.14 & 55.63 ± 1.47 & 67.15 ± 2.10 & 82.38 ± 2.19 SEAL & 97.10 ± 0.87 & 98.25 ± 0.61 & 95.07 ± 0.39 & 97.60 ± 0.33 & 89.54 ± 1.23 & 86.21 ± 2.89 & 95.07 ± 1.63 & 97.57 ± 0.30 WP & 98.20 ± 0.57 & 99.12 ± 0.45 & 95.42 ± 0.25 & 98.21 ± 0.17 & 93.30 ± 0.91 & 92.11 ± 0.76 & 97.15 ± 0.29 & 98.54 ± 0.19 MAPHLP & 97.10 ± 0.69 & 98.88±0.45 & 95.10±0.26 & 97.98±0.22 & 90.33±1.16 & 93.05±0.45 & 96.30±0.43 & 97.64±0.20 MAPHLP (dim0) & 97.10±0.73 & 98.78±0.65 & 95.06±0.28 & 97.98±0.23 & 89.88±1.22 & 93.37 ±0.41 & 96.37±0.43 & 97.72±0.17",
                                            "leftover": "{l|cccccccccc} Dataset & USAir & NS & PB & Yeast & C.~ele & Power & Router & E.~coli AA & 95.06 ± 1.03 & 94.45 ± 0.93 & 92.36 ± 0.34 & 89.43 ± 0.62 & 86.95 ± 1.40 & 58.79 ± 0.88 & 56.43 ± 0.51 & 95.36 ± 0.34 Katz & 92.88 ± 1.42 & 94.85 ± 1.10 & 92.92 ± 0.35 & 92.24 ± 0.61 & 86.34 ± 1.89 & 65.39 ± 1.59 & 38.62 ± 1.35 & 93.50 ± 0.44 PR & 94.67 ± 1.08 & 94.89 ± 1.08 & 93.54 ± 0.41 & 92.76 ± 0.55 & 90.32 ± 1.49 & 66.00 ± 1.59 & 38.76 ± 1.39 & 95.57 ± 0.44 WLK & 96.63 ± 0.73 & 98.57 ± 0.51 & 93.83 ± 0.59 & 95.86 ± 0.54 & 89.72 ± 1.67 & 82.41 ± 3.43 & 87.42 ± 2.08 & 96.94 ± 0.29 WLNM & 95.95 ± 1.10 & 98.61 ± 0.49 & 93.49 ± 0.47 & 95.62 ± 0.52 & 86.18 ± 1.72 & 84.76 ± 0.98 & 94.41 ± 0.88 & 97.21 ± 0.27 N2V & 91.44 ± 1.78 & 91.52 ± 1.28 & 85.79 ± 0.78 & 93.67 ± 0.46 & 84.11 ± 1.27 & 76.22 ± 0.92 & 65.46 ± 0.86 & 90.82 ± 1.49 SPC & 74.22 ± 3.11 & 89.94 ± 2.39 & 83.96 ± 0.86 & 93.25 ± 0.40 & 51.90 ± 2.57 & 91.78 ± 0.61 & 68.79 ± 2.42 & 94.92 ± 0.32 MF & 94.08 ± 0.80 & 74.55 ± 4.34 & 94.30 ± 0.53 & 90.28 ± 0.69 & 85.90 ± 1.74 & 50.63 ± 1.10 & 78.03 ± 1.63 & 93.76 ± 0.56 LINE & 81.47 ± 10.71 & 80.63 ± 1.90 & 76.95 ± 2.76 & 87.45 ± 3.33 & 69.21 ± 3.14 & 55.63 ± 1.47 & 67.15 ± 2.10 & 82.38 ± 2.19 SEAL & 97.10 ± 0.87 & 98.25 ± 0.61 & 95.07 ± 0.39 & 97.60 ± 0.33 & 89.54 ± 1.23 & 86.21 ± 2.89 & 95.07 ± 1.63 & 97.57 ± 0.30 WP & 98.20 ± 0.57 & 99.12 ± 0.45 & 95.42 ± 0.25 & 98.21 ± 0.17 & 93.30 ± 0.91 & 92.11 ± 0.76 & 97.15 ± 0.29 & 98.54 ± 0.19 MAPHLP & 97.10 ± 0.69 & 98.88±0.45 & 95.10±0.26 & 97.98±0.22 & 90.33±1.16 & 93.05±0.45 & 96.30±0.43 & 97.64±0.20 MAPHLP (dim0) & 97.10±0.73 & 98.78±0.65 & 95.06±0.28 & 97.98±0.23 & 89.88±1.22 & 93.37 ±0.41 & 96.37±0.43 & 97.72±0.17",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 78,
                                            "key": "doc/body/sec4/sub1/table*5/txl2",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub2",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 79,
                                    "key": "doc/body/sec4/sub2/tit",
                                    "block type": "title",
                                    "content": "Results",
                                    "leftover": "Results",
                                    "matches": []
                                },
                                {
                                    "leaf id": 80,
                                    "key": "doc/body/sec4/sub2/txl0",
                                    "block type": "txl",
                                    "content": "Results of MAPHLP. Table presents the AUC scores for each model on the benchmark datasets. Bold marks the best results, and underline indicates the secondbest results. The results of AA, Katz, WLK, WLNM, N2V, SPC, MF, LINE, and SEAL are copied from SEAL for comparison. The MAPHLP demonstrates high performance across most datasets, achieving competitive scores. The proposed model outperforms several baselines, falling between the SEAL and WP models in terms of the AUC score. Notably, for the Power dataset, MAPHLP achieves the highest AUC score, indicating its effectiveness in capturing link patterns.",
                                    "leftover": "Results of MAPHLP. Table presents the AUC scores for each model on the benchmark datasets. Bold marks the best results, and underline indicates the secondbest results. The results of AA, Katz, WLK, WLNM, N2V, SPC, MF, LINE, and SEAL are copied from SEAL for comparison. The MAPHLP demonstrates high performance across most datasets, achieving competitive scores. The proposed model outperforms several baselines, falling between the SEAL and WP models in terms of the AUC score. Notably, for the Power dataset, MAPHLP achieves the highest AUC score, indicating its effectiveness in capturing link patterns.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 81,
                                    "key": "doc/body/sec4/sub2/txl1",
                                    "block type": "txl",
                                    "content": "Results of Hybrid Methods.",
                                    "leftover": "Results of Hybrid Methods.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table2",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table2/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 82,
                                                    "key": "doc/body/sec4/sub2/table2/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "AUC scores for SEAL with and without TDA features",
                                                    "leftover": "AUC scores for SEAL with and without TDA features",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 83,
                                            "key": "doc/body/sec4/sub2/table2/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cccccccccc} Dataset & SEAL & MAPHLP + SEAL USAir & 97.10 ± 0.87 & 97.41 ± 0.62 NS & 98.25 ± 0.61 & 98.97 ± 0.30 PB & 95.07±0.39 & 95.14 ± 0.39 Yeast & 97.60±0.33 & 97.93 ± 0.18 C.ele & 89.54 ± 1.23 & 89.61 ± 1.12 Power & 86.21 ± 2.89 & 95.53 ± 0.33 Router & 95.07 ± 1.63 & 96.15 ± 1.26 E.coli & 97.57±0.30 & 97.93±0.34",
                                            "leftover": "{l|cccccccccc} Dataset & SEAL & MAPHLP + SEAL USAir & 97.10 ± 0.87 & 97.41 ± 0.62 NS & 98.25 ± 0.61 & 98.97 ± 0.30 PB & 95.07±0.39 & 95.14 ± 0.39 Yeast & 97.60±0.33 & 97.93 ± 0.18 C.ele & 89.54 ± 1.23 & 89.61 ± 1.12 Power & 86.21 ± 2.89 & 95.53 ± 0.33 Router & 95.07 ± 1.63 & 96.15 ± 1.26 E.coli & 97.57±0.30 & 97.93±0.34",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 84,
                                    "key": "doc/body/sec4/sub2/txl3",
                                    "block type": "txl",
                                    "content": "Simply concatenating the PI vector calculated using PHLP with the final output of the SEAL model increases AUC scores for all datasets, as listed in Table. This outcome suggests that when the SEAL model lacks topological information for inference, the vectors calculated using PHLP can serve as additional inputs.",
                                    "leftover": "Simply concatenating the PI vector calculated using PHLP with the final output of the SEAL model increases AUC scores for all datasets, as listed in Table. This outcome suggests that when the SEAL model lacks topological information for inference, the vectors calculated using PHLP can serve as additional inputs.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub2/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub2/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 85,
                                                    "key": "doc/body/sec4/sub2/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "AUC scores for WALKPOOL (WP) with and without TDA features",
                                                    "leftover": "AUC scores for WALKPOOL (WP) with and without TDA features",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 86,
                                            "key": "doc/body/sec4/sub2/table4/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cccccccccc} Dataset & WP & MAPHLP + WP USAir & 98.20±0.57 & 98.27 ± 0.53 NS & 99.12±0.45 & 99.24 ± 0.32 PB & 95.42±0.25 & 95.58 ± 0.32 Yeast & 98.21±0.17 & 98.25 ± 0.18 C.ele & 93.30±0.91 & 93.32 ± 0.71 Power & 92.11±0.76 & 96.09 ± 0.38 Router & 97.15±0.29 & 97.18 ± 0.24 E.coli & 98.54±0.19 & 98.57 ± 0.20",
                                            "leftover": "{l|cccccccccc} Dataset & WP & MAPHLP + WP USAir & 98.20±0.57 & 98.27 ± 0.53 NS & 99.12±0.45 & 99.24 ± 0.32 PB & 95.42±0.25 & 95.58 ± 0.32 Yeast & 98.21±0.17 & 98.25 ± 0.18 C.ele & 93.30±0.91 & 93.32 ± 0.71 Power & 92.11±0.76 & 96.09 ± 0.38 Router & 97.15±0.29 & 97.18 ± 0.24 E.coli & 98.54±0.19 & 98.57 ± 0.20",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 87,
                                    "key": "doc/body/sec4/sub2/txl5",
                                    "block type": "txl",
                                    "content": "Similarly, we attempted to hybridize PHLP with the current SOTA model, WP. As presented in Table, a slight increase in AUC scores is observed for all datasets. The Power dataset demonstrates significant improvement.",
                                    "leftover": "Similarly, we attempted to hybridize PHLP with the current SOTA model, WP. As presented in Table, a slight increase in AUC scores is observed for all datasets. The Power dataset demonstrates significant improvement.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub3",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 88,
                                    "key": "doc/body/sec4/sub3/tit",
                                    "block type": "title",
                                    "content": "Ablation Study",
                                    "leftover": "Ablation Study",
                                    "matches": []
                                },
                                {
                                    "leaf id": 89,
                                    "key": "doc/body/sec4/sub3/txl0",
                                    "block type": "txl",
                                    "content": "Effects of Degree DRNL.}To assess the proposed Degree DRNL regarding the influence of incorporating degree information on model performance, we conducted experiments using DRNL and Degree DRNL and compared the results.",
                                    "leftover": "Effects of Degree DRNL.}To assess the proposed Degree DRNL regarding the influence of incorporating degree information on model performance, we conducted experiments using DRNL and Degree DRNL and compared the results.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub3/table1",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub3/table1/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 90,
                                                    "key": "doc/body/sec4/sub3/table1/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "AUC scores for MAPHLP (dim0) by node labeling",
                                                    "leftover": "AUC scores for MAPHLP (dim0) by node labeling",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 91,
                                            "key": "doc/body/sec4/sub3/table1/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cccccccccc} Dataset & DRNL & Degree DRNL USAir & 96.73±0.64 & 97.10 ± 0.73 NS & 98.35±0.58 & 98.78±0.65 PB & 94.49±0.27 & 95.06 ± 0.28 Yeast & 97.42±0.27 & 97.98 ± 0.23 C.ele& 88.97±1.37 & 89.88 ± 1.22 Power & 88.51±0.81 & 92.77 ± 0.47 Router & 96.21±0.53 & 96.37 ± 0.43 E.coli & 97.15±0.18 & 97.72 ± 0.17",
                                            "leftover": "{l|cccccccccc} Dataset & DRNL & Degree DRNL USAir & 96.73±0.64 & 97.10 ± 0.73 NS & 98.35±0.58 & 98.78±0.65 PB & 94.49±0.27 & 95.06 ± 0.28 Yeast & 97.42±0.27 & 97.98 ± 0.23 C.ele& 88.97±1.37 & 89.88 ± 1.22 Power & 88.51±0.81 & 92.77 ± 0.47 Router & 96.21±0.53 & 96.37 ± 0.43 E.coli & 97.15±0.18 & 97.72 ± 0.17",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 92,
                                    "key": "doc/body/sec4/sub3/txl2",
                                    "block type": "txl",
                                    "content": "We used MAPHLP (dim0) for the experiments. Table presents the AUC scores of MAPHLP (dim0) with DRNL and Degree DRNL. Across all datasets, MAPHLP (dim0) yields higher AUC scores when used with Degree DRNL than with DRNL. The substantial improvement observed in the Power dataset is noteworthy, where Degree DRNL yields an increase of over 4 points in the AUC score. These experiments demonstrate the importance of incorporating degree information into node labeling, revealing its efficacy in enhancing the performance of MAPHLP.",
                                    "leftover": "We used MAPHLP (dim0) for the experiments. Table presents the AUC scores of MAPHLP (dim0) with DRNL and Degree DRNL. Across all datasets, MAPHLP (dim0) yields higher AUC scores when used with Degree DRNL than with DRNL. The substantial improvement observed in the Power dataset is noteworthy, where Degree DRNL yields an increase of over 4 points in the AUC score. These experiments demonstrate the importance of incorporating degree information into node labeling, revealing its efficacy in enhancing the performance of MAPHLP.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub3/table3",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub3/table3/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 93,
                                                    "key": "doc/body/sec4/sub3/table3/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "AUC scores for MAPHLP (dim0) with various (k,l)angle hops",
                                                    "leftover": "AUC scores for MAPHLP (dim0) with various (k,l)angle hops",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 94,
                                            "key": "doc/body/sec4/sub3/table3/tabularx1",
                                            "block type": "tabularx",
                                            "content": "{0.44}{l|YY} Dataset & (1,0) & (1,1) USAir & 96.15±0.83 & 95.87±0.83 NS & 98.28±0.55 & 98.66±0.66 PB & 93.95±0.34 & 94.46±0.36 Yeast & 95.52±0.32 & 97.31±0.20 C.ele & 86.18±2.12 & 87.57±1.20 Power & 73.39±0.99 & 77.83±1.44 Router & 92.09±0.57 & 93.25±0.47 E.coli & 96.94±0.24 & 96.95±0.28",
                                            "leftover": "{0.44}{l|YY} Dataset & (1,0) & (1,1) USAir & 96.15±0.83 & 95.87±0.83 NS & 98.28±0.55 & 98.66±0.66 PB & 93.95±0.34 & 94.46±0.36 Yeast & 95.52±0.32 & 97.31±0.20 C.ele & 86.18±2.12 & 87.57±1.20 Power & 73.39±0.99 & 77.83±1.44 Router & 92.09±0.57 & 93.25±0.47 E.coli & 96.94±0.24 & 96.95±0.28",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 95,
                                            "key": "doc/body/sec4/sub3/table3/tabularx2",
                                            "block type": "tabularx",
                                            "content": "{0.44}{l|YYY} Dataset & (2,0) & (2,1) & (2,2) USAir & 96.69±0.92 & 96.74±0.84 & 96.85±0.83 NS & 98.72±0.51 & 98.59±0.65 & 98.56±0.47 PB & 94.78±0.30 & 94.73±0.30 & 94.82±0.24 Yeast & 97.71±0.18 & 97.66±0.27 & 97.58±0.28 C.ele & 88.86±1.48 & 89.16±1.31 & 89.08±1.07 Power & 80.27±1.07 & 83.90±1.29 & 86.12±0.86 Router & 95.65±0.44 & 95.71±0.39 & 94.51±0.69 E.coli & 97.26±0.16 & 97.29±0.24 & 97.41±0.21",
                                            "leftover": "{0.44}{l|YYY} Dataset & (2,0) & (2,1) & (2,2) USAir & 96.69±0.92 & 96.74±0.84 & 96.85±0.83 NS & 98.72±0.51 & 98.59±0.65 & 98.56±0.47 PB & 94.78±0.30 & 94.73±0.30 & 94.82±0.24 Yeast & 97.71±0.18 & 97.66±0.27 & 97.58±0.28 C.ele & 88.86±1.48 & 89.16±1.31 & 89.08±1.07 Power & 80.27±1.07 & 83.90±1.29 & 86.12±0.86 Router & 95.65±0.44 & 95.71±0.39 & 94.51±0.69 E.coli & 97.26±0.16 & 97.29±0.24 & 97.41±0.21",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 96,
                                    "key": "doc/body/sec4/sub3/txl4",
                                    "block type": "txl",
                                    "content": "Angles of PHLP.}Table presents the performance of PHLP (dim 0) concerning various (k,l)angle hop subgraphs. Section proposed angle hop subgraphs as an alternative to traditional khop subgraphs to capture information from various perspectives. Moreover, MAPHLP is proposed to aggregate information from multiple angles. To investigate performance when extracting information from specific angles, we conducted experiments using PHLP at different angles. We used only zerodimensional PIs for the experiments. Overall, the results demonstrate that the performance is favorable for cases corresponding to the khop subgraph (where k and l are the same). However, some datasets perform better when k and l differ, highlighting the importance of varying angles to achieve the best performance. Therefore, using MAPHLP is recommended to maximize performance consistently across datasets.",
                                    "leftover": "Angles of PHLP.}Table presents the performance of PHLP (dim 0) concerning various (k,l)angle hop subgraphs. Section proposed angle hop subgraphs as an alternative to traditional khop subgraphs to capture information from various perspectives. Moreover, MAPHLP is proposed to aggregate information from multiple angles. To investigate performance when extracting information from specific angles, we conducted experiments using PHLP at different angles. We used only zerodimensional PIs for the experiments. Overall, the results demonstrate that the performance is favorable for cases corresponding to the khop subgraph (where k and l are the same). However, some datasets perform better when k and l differ, highlighting the importance of varying angles to achieve the best performance. Therefore, using MAPHLP is recommended to maximize performance consistently across datasets.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 97,
                                    "key": "doc/body/sec4/sub3/txl5",
                                    "block type": "txl",
                                    "content": "Comparison with TLCGNN. To demonstrate that the proposed method extracts superior topological information compared to the conventional TLCGNN approach, we conducted the same experiments. The TLCGNN was constructed by augmenting the graph convolutional network (GCN) model with PI information. We replaced the PI component of the TLCGNN model with the PI vector produced by MAPHLP, resulting in the MAPHLPGNN. The zerodimensional PH was employed in this study for fair comparison because TLCGNN used only zerodimensional PH. Additionally, we conducted experiments where the PH vectors were replaced with zero vectors, denoted as GCN. Table presents the experimental results.",
                                    "leftover": "Comparison with TLCGNN. To demonstrate that the proposed method extracts superior topological information compared to the conventional TLCGNN approach, we conducted the same experiments. The TLCGNN was constructed by augmenting the graph convolutional network (GCN) model with PI information. We replaced the PI component of the TLCGNN model with the PI vector produced by MAPHLP, resulting in the MAPHLPGNN. The zerodimensional PH was employed in this study for fair comparison because TLCGNN used only zerodimensional PH. Additionally, we conducted experiments where the PH vectors were replaced with zero vectors, denoted as GCN. Table presents the experimental results.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub3/table6",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub3/table6/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 98,
                                                    "key": "doc/body/sec4/sub3/table6/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "comparison of AUC scores with TLCGNN",
                                                    "leftover": "comparison of AUC scores with TLCGNN",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 99,
                                            "key": "doc/body/sec4/sub3/table6/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cccccccccc} Dataset & GCN & TLCGNN & MAPHLPGNN Cora & 92.20±0.83 & 93.16±0.56 & 93.14±0.93 CiteSeer & 86.52±1.29 & 87.38±0.97 & 92.08±0.53 PubMed & 96.63±0.15 & 96.30±0.25 & 98.07±0.07",
                                            "leftover": "{l|cccccccccc} Dataset & GCN & TLCGNN & MAPHLPGNN Cora & 92.20±0.83 & 93.16±0.56 & 93.14±0.93 CiteSeer & 86.52±1.29 & 87.38±0.97 & 92.08±0.53 PubMed & 96.63±0.15 & 96.30±0.25 & 98.07±0.07",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 100,
                                            "key": "doc/body/sec4/sub3/table6/txl2",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 101,
                                    "key": "doc/body/sec4/sub3/txl7",
                                    "block type": "txl",
                                    "content": "The TLCGNN is employed when the given data includes node attributes. Hence, we conducted experiments using the following widely used benchmark datasets with node attributes: Cora, CiteSeer, and PubMed. The MAPHLPGNN outperformed the TLCGNN significantly on the CiteSeer and PubMed datasets while achieving similar performance on the Cora dataset. The TLCGNN does not exhibit performance improvement for the PubMed dataset despite adding topological information. However, the proposed MAPHLPGNN demonstrates substantial performance enhancement. Although the proposed model is developed for datasets without node attributes, it exhibits effective performance on datasets with node attributes through hybridization with the existing methods: SEAL+PHLP, WP+PHLP, and MAPHLPGNN. These experiments verify the versatility and effectiveness of this approach across diverse datasets.",
                                    "leftover": "The TLCGNN is employed when the given data includes node attributes. Hence, we conducted experiments using the following widely used benchmark datasets with node attributes: Cora, CiteSeer, and PubMed. The MAPHLPGNN outperformed the TLCGNN significantly on the CiteSeer and PubMed datasets while achieving similar performance on the Cora dataset. The TLCGNN does not exhibit performance improvement for the PubMed dataset despite adding topological information. However, the proposed MAPHLPGNN demonstrates substantial performance enhancement. Although the proposed model is developed for datasets without node attributes, it exhibits effective performance on datasets with node attributes through hybridization with the existing methods: SEAL+PHLP, WP+PHLP, and MAPHLPGNN. These experiments verify the versatility and effectiveness of this approach across diverse datasets.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec4/sub4",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 102,
                                    "key": "doc/body/sec4/sub4/tit",
                                    "block type": "title",
                                    "content": "The hops and max hops of the hybrid methods",
                                    "leftover": "The hops and max hops of the hybrid methods",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub4/table*0",
                                    "block_type": "table*",
                                    "children": [
                                        {
                                            "key": "doc/body/sec4/sub4/table*0/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 103,
                                                    "key": "doc/body/sec4/sub4/table*0/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "AUC scores on the power dataset varying khop and max hop M of the hybrid methods",
                                                    "leftover": "AUC scores on the power dataset varying khop and max hop M of the hybrid methods",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 104,
                                            "key": "doc/body/sec4/sub4/table*0/tabular1",
                                            "block type": "tabular",
                                            "content": "{c|ccccccccc} \\multicolumn{2}{c|}{} & \\multicolumn{7}c{MAPHLP (with max hop M)} \\multicolumn{2}{c|}M& 1 & 2 & 3 & 4 & 5 & 6 & 7 \\multirow{8}{*}{\\rotatebox{90}{SEAL (with khop)}}& \\multicolumn{1}{c|}k&\\multicolumn{3}c{not robust to k} & \\multicolumn{4}{|c}{robust to k} & \\multicolumn{1}{c|}{1} & 86.66 ± 0.56 & 90.22 ± 0.79 & 92.63 ± 0.54 & \\multicolumn{1}{|c}{94.50 ± 0.41} & 95.12 ± 0.40 & 95.46 ± 0.38 & 95.53\\pm0.33 & \\multicolumn{1}{c|}{2} & 91.40 ± 0.88 & 90.20 ± 0.80 & 92.50 ± 0.59 & \\multicolumn{1}{|c}{94.39 ± 0.39} & 95.00 ± 0.46 & 95.31 ± 0.40 & 95.39\\pm0.36 & \\multicolumn{1}{c|}{3} & 93.21 ± 0.64 & 92.79 ± 0.60 & 92.57 ± 0.58 & \\multicolumn{1}{|c}{94.22 ± 0.43} & 94.86 ± 0.42 & 95.21\\pm0.45 & 95.19 ± 0.44 & \\multicolumn{1}{c|}{4} & 94.51 ± 0.58 & 94.23 ± 0.34 & 94.21 ± 0.41 & \\multicolumn{1}{|c}{94.31 ± 0.40} & 94.80 ± 0.37 & 95.10 ± 0.33 & 95.27\\pm0.36 & \\multicolumn{1}{c|}{5} & 94.73 ± 0.56 & 94.45 ± 0.44 & 94.61 ± 0.51 & \\multicolumn{1}{|c}{94.80 ± 0.53} & 94.91 ± 0.54 & 95.13 ± 0.51 & 95.19\\pm0.46 & \\multicolumn{1}{c|}{6} & 94.58 ± 0.94 & 94.81 ± 0.32 & 94.87 ± 0.42 & \\multicolumn{1}{|c}{95.06 ± 0.50} & 95.11 ± 0.46 & 95.25\\pm0.45 & 95.25 ± 0.46 & \\multicolumn{1}{c|}{7} & 93.97 ± 0.73 & 94.22 ± 0.35 & 94.43 ± 0.44 & \\multicolumn{1}{|c}{94.78 ± 0.45} & 94.92 ± 0.39 & 94.99\\pm0.52 & 94.98 ± 0.39 \\multirow{8}{*}{\\rotatebox{90}{WP (with khop)}}& \\multicolumn{1}{c|}k& \\multicolumn{2}c{not robust to k} & \\multicolumn{5}{|c}{robust to k} & \\multicolumn{1}{c|}{1} & 87.53 ± 0.73 & 91.48 ± 0.64 & \\multicolumn{1}{|c}{93.55 ± 0.48} & 94.84 ± 0.43 & 95.53 ± 0.46 & 95.88 ± 0.31 & 96.09\\pm0.38 & \\multicolumn{1}{c|}{2} & 92.51 ± 0.58 & 91.59 ± 0.77 & \\multicolumn{1}{|c}{93.49 ± 0.58} & 94.83 ± 0.53 & 95.56 ± 0.59 & 95.88 ± 0.38 & 96.06\\pm0.45 & \\multicolumn{1}{c|}{3} & 94.04 ± 0.46 & 93.07 ± 0.67 & \\multicolumn{1}{|c}{93.61 ± 0.52} & 94.86 ± 0.54 & 95.61 ± 0.60 & 95.86 ± 0.40 & 96.00\\pm0.52 & \\multicolumn{1}{c|}{4} & 93.55 ± 0.71 & 92.61 ± 0.76 & \\multicolumn{1}{|c}{93.68 ± 0.55} & 94.85 ± 0.55 & 95.59 ± 0.58 & 95.87 ± 0.38 & 96.03\\pm0.45 & \\multicolumn{1}{c|}{5} & 93.40 ± 0.70 & 92.64 ± 0.69 & \\multicolumn{1}{|c}{93.66 ± 0.53} & 94.84 ± 0.54 & 95.55 ± 0.59 & 95.85 ± 0.39 & 96.04\\pm0.52 & \\multicolumn{1}{c|}{6} & 93.34 ± 0.75 & 92.66 ± 0.72 & \\multicolumn{1}{|c}{93.64 ± 0.55} & 94.91 ± 0.57 & 95.55 ± 0.58 & 95.85 ± 0.44 & 95.98\\pm0.55 & \\multicolumn{1}{c|}{7} & 93.30 ± 0.73 & 92.61 ± 0.69 & \\multicolumn{1}{|c}{93.65 ± 0.56} & 94.87 ± 0.56 & 95.56 ± 0.58 & 95.90 ± 0.39 & 96.01\\pm0.52",
                                            "leftover": "{c|ccccccccc} \\multicolumn{2}{c|}{} & \\multicolumn{7}c{MAPHLP (with max hop M)} \\multicolumn{2}{c|}M& 1 & 2 & 3 & 4 & 5 & 6 & 7 \\multirow{8}{*}{\\rotatebox{90}{SEAL (with khop)}}& \\multicolumn{1}{c|}k&\\multicolumn{3}c{not robust to k} & \\multicolumn{4}{|c}{robust to k} & \\multicolumn{1}{c|}{1} & 86.66 ± 0.56 & 90.22 ± 0.79 & 92.63 ± 0.54 & \\multicolumn{1}{|c}{94.50 ± 0.41} & 95.12 ± 0.40 & 95.46 ± 0.38 & 95.53\\pm0.33 & \\multicolumn{1}{c|}{2} & 91.40 ± 0.88 & 90.20 ± 0.80 & 92.50 ± 0.59 & \\multicolumn{1}{|c}{94.39 ± 0.39} & 95.00 ± 0.46 & 95.31 ± 0.40 & 95.39\\pm0.36 & \\multicolumn{1}{c|}{3} & 93.21 ± 0.64 & 92.79 ± 0.60 & 92.57 ± 0.58 & \\multicolumn{1}{|c}{94.22 ± 0.43} & 94.86 ± 0.42 & 95.21\\pm0.45 & 95.19 ± 0.44 & \\multicolumn{1}{c|}{4} & 94.51 ± 0.58 & 94.23 ± 0.34 & 94.21 ± 0.41 & \\multicolumn{1}{|c}{94.31 ± 0.40} & 94.80 ± 0.37 & 95.10 ± 0.33 & 95.27\\pm0.36 & \\multicolumn{1}{c|}{5} & 94.73 ± 0.56 & 94.45 ± 0.44 & 94.61 ± 0.51 & \\multicolumn{1}{|c}{94.80 ± 0.53} & 94.91 ± 0.54 & 95.13 ± 0.51 & 95.19\\pm0.46 & \\multicolumn{1}{c|}{6} & 94.58 ± 0.94 & 94.81 ± 0.32 & 94.87 ± 0.42 & \\multicolumn{1}{|c}{95.06 ± 0.50} & 95.11 ± 0.46 & 95.25\\pm0.45 & 95.25 ± 0.46 & \\multicolumn{1}{c|}{7} & 93.97 ± 0.73 & 94.22 ± 0.35 & 94.43 ± 0.44 & \\multicolumn{1}{|c}{94.78 ± 0.45} & 94.92 ± 0.39 & 94.99\\pm0.52 & 94.98 ± 0.39 \\multirow{8}{*}{\\rotatebox{90}{WP (with khop)}}& \\multicolumn{1}{c|}k& \\multicolumn{2}c{not robust to k} & \\multicolumn{5}{|c}{robust to k} & \\multicolumn{1}{c|}{1} & 87.53 ± 0.73 & 91.48 ± 0.64 & \\multicolumn{1}{|c}{93.55 ± 0.48} & 94.84 ± 0.43 & 95.53 ± 0.46 & 95.88 ± 0.31 & 96.09\\pm0.38 & \\multicolumn{1}{c|}{2} & 92.51 ± 0.58 & 91.59 ± 0.77 & \\multicolumn{1}{|c}{93.49 ± 0.58} & 94.83 ± 0.53 & 95.56 ± 0.59 & 95.88 ± 0.38 & 96.06\\pm0.45 & \\multicolumn{1}{c|}{3} & 94.04 ± 0.46 & 93.07 ± 0.67 & \\multicolumn{1}{|c}{93.61 ± 0.52} & 94.86 ± 0.54 & 95.61 ± 0.60 & 95.86 ± 0.40 & 96.00\\pm0.52 & \\multicolumn{1}{c|}{4} & 93.55 ± 0.71 & 92.61 ± 0.76 & \\multicolumn{1}{|c}{93.68 ± 0.55} & 94.85 ± 0.55 & 95.59 ± 0.58 & 95.87 ± 0.38 & 96.03\\pm0.45 & \\multicolumn{1}{c|}{5} & 93.40 ± 0.70 & 92.64 ± 0.69 & \\multicolumn{1}{|c}{93.66 ± 0.53} & 94.84 ± 0.54 & 95.55 ± 0.59 & 95.85 ± 0.39 & 96.04\\pm0.52 & \\multicolumn{1}{c|}{6} & 93.34 ± 0.75 & 92.66 ± 0.72 & \\multicolumn{1}{|c}{93.64 ± 0.55} & 94.91 ± 0.57 & 95.55 ± 0.58 & 95.85 ± 0.44 & 95.98\\pm0.55 & \\multicolumn{1}{c|}{7} & 93.30 ± 0.73 & 92.61 ± 0.69 & \\multicolumn{1}{|c}{93.65 ± 0.56} & 94.87 ± 0.56 & 95.56 ± 0.58 & 95.90 ± 0.39 & 96.01\\pm0.52",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 105,
                                            "key": "doc/body/sec4/sub4/table*0/txl2",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 106,
                                    "key": "doc/body/sec4/sub4/txl1",
                                    "block type": "txl",
                                    "content": "Determining the hyperparameters such as ''hop'' and ''max hop'' is crucial for the performance of the hybrid method. We conducted experiments to explore the effects of different combinations of these parameters. Given that the hybrid methods (e.g., MAPHLP + SEAL and MAPHLP + WP) exhibited the highest performance improvement on the Power dataset, we conducted experiments on the Power dataset. Table presents the AUC scores for varying hop (SEAL or WP) and max hop (MAPHLP). For each target node, while the SEAL and WP extract a khop subgraph, the MAPHLP calculates the PIs based on a subgraph with max hop M. When the parameter M is 1 or 2, the AUC scores are not robust to k, showing large variations; however, when M is 3, although MAPHLP + SEAL still exhibits variations up to 2, MAPHLP + WP shows only minor variations. As M exceeds 3, the AUC scores of MAPHLP + SEAL and MAPHLP + WP are robust to k, exhibiting little sensitivity (maximum 0.84) to variations. This suggests that setting both the hop and the max hop to identical values may be permissible without further searching for optimal hyperparameters.",
                                    "leftover": "Determining the hyperparameters such as ''hop'' and ''max hop'' is crucial for the performance of the hybrid method. We conducted experiments to explore the effects of different combinations of these parameters. Given that the hybrid methods (e.g., MAPHLP + SEAL and MAPHLP + WP) exhibited the highest performance improvement on the Power dataset, we conducted experiments on the Power dataset. Table presents the AUC scores for varying hop (SEAL or WP) and max hop (MAPHLP). For each target node, while the SEAL and WP extract a khop subgraph, the MAPHLP calculates the PIs based on a subgraph with max hop M. When the parameter M is 1 or 2, the AUC scores are not robust to k, showing large variations; however, when M is 3, although MAPHLP + SEAL still exhibits variations up to 2, MAPHLP + WP shows only minor variations. As M exceeds 3, the AUC scores of MAPHLP + SEAL and MAPHLP + WP are robust to k, exhibiting little sensitivity (maximum 0.84) to variations. This suggests that setting both the hop and the max hop to identical values may be permissible without further searching for optimal hyperparameters.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec4/sub4/figure*2",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 107,
                                            "key": "doc/body/sec4/sub4/figure*2/cpt0",
                                            "block type": "cpt",
                                            "content": "Visualization of vectors calculated using MAPHLP (dim0). For each dataset, the first and second columns depict the projections of persistence images (PIs) when double radius node labeling (DRNL) is applied for node labeling, and the third and fourth columns represent the values obtained when Degree DRNL is applied. The first and third columns plot the values produced from positive edges (i.e., target nodes labeled 1), and the second and fourth columns plot the values produced from negative edges (i.e., target nodes labeled 0).",
                                            "leftover": "Visualization of vectors calculated using MAPHLP (dim0). For each dataset, the first and second columns depict the projections of persistence images (PIs) when double radius node labeling (DRNL) is applied for node labeling, and the third and fourth columns represent the values obtained when Degree DRNL is applied. The first and third columns plot the values produced from positive edges (i.e., target nodes labeled 1), and the second and fourth columns plot the values produced from negative edges (i.e., target nodes labeled 0).",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "key": "doc/body/sec4/sub4/figure*3",
                                    "block_type": "figure*",
                                    "children": [
                                        {
                                            "leaf id": 108,
                                            "key": "doc/body/sec4/sub4/figure*3/cpt0",
                                            "block type": "cpt",
                                            "content": "Visualization of vectors calculated using MAPHLP (dim0).",
                                            "leftover": "Visualization of vectors calculated using MAPHLP (dim0).",
                                            "matches": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec5",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 109,
                            "key": "doc/body/sec5/tit",
                            "block type": "title",
                            "content": "Analysis",
                            "leftover": "Analysis",
                            "matches": []
                        },
                        {
                            "key": "doc/body/sec5/sub0",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 110,
                                    "key": "doc/body/sec5/sub0/tit",
                                    "block type": "title",
                                    "content": "Analysis of the PHLP",
                                    "leftover": "Analysis of the PHLP",
                                    "matches": []
                                },
                                {
                                    "leaf id": 111,
                                    "key": "doc/body/sec5/sub0/txl0",
                                    "block type": "txl",
                                    "content": "Figs. and visualize concatenated PIs to illustrate how MAPHLP (dim0) extracts topological features for LP. We let 𝒵⊆ℝ^2 × k × r^2 be a set of vectors calculated by MAPHLP, where k is the number of angles, and r denotes the PI resolution. For (z1, z2) ∈𝒵, z1 ∈ℝ^k × r^2 is the concatenation of PIs for all angles with a target link, and z2 ∈ℝ^k × r^2 is the concatenation for cases without a target link. We consider a function h:ℝ^k × r^2→ℝ defined as h(v⃗1, ..., v⃗k) = 1/k∑i=1^k ‖v⃗i ‖1, where v⃗i ∈ℝ^r^2 are PIs, and ‖·‖1 denotes the L1norm. For visualization, we transform 𝒵 into points in ℝ^2 using the function G, defined as G(z1,z2) = (h(z1),h(z2)) for each (z1, z2) ∈𝒵.",
                                    "leftover": "Figs. and visualize concatenated PIs to illustrate how MAPHLP (dim0) extracts topological features for LP. We let 𝒵⊆ℝ^2 × k × r^2 be a set of vectors calculated by MAPHLP, where k is the number of angles, and r denotes the PI resolution. For (z1, z2) ∈𝒵, z1 ∈ℝ^k × r^2 is the concatenation of PIs for all angles with a target link, and z2 ∈ℝ^k × r^2 is the concatenation for cases without a target link. We consider a function h:ℝ^k × r^2→ℝ defined as h(v⃗1, ..., v⃗k) = 1/k∑i=1^k ‖v⃗i ‖1, where v⃗i ∈ℝ^r^2 are PIs, and ‖·‖1 denotes the L1norm. For visualization, we transform 𝒵 into points in ℝ^2 using the function G, defined as G(z1,z2) = (h(z1),h(z2)) for each (z1, z2) ∈𝒵.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 112,
                                    "key": "doc/body/sec5/sub0/txl1",
                                    "block type": "txl",
                                    "content": "We plot distributions of points separately for positive and negative links, considering both DRNL and Degree DRNL. The distributions of the NS and Yeast datasets between positive and negative links display significant differences, supporting the highest performance in Table. In contrast, the distributions for the C.~ele and Power datasets are the most similar when using Degree DRNL, correlating with the lowest scores in Table.",
                                    "leftover": "We plot distributions of points separately for positive and negative links, considering both DRNL and Degree DRNL. The distributions of the NS and Yeast datasets between positive and negative links display significant differences, supporting the highest performance in Table. In contrast, the distributions for the C.~ele and Power datasets are the most similar when using Degree DRNL, correlating with the lowest scores in Table.",
                                    "matches": []
                                }
                            ]
                        },
                        {
                            "key": "doc/body/sec5/sub1",
                            "block_type": "sub",
                            "children": [
                                {
                                    "leaf id": 113,
                                    "key": "doc/body/sec5/sub1/tit",
                                    "block type": "title",
                                    "content": "Analysis of the Power Dataset",
                                    "leftover": "Analysis of the Power Dataset",
                                    "matches": []
                                },
                                {
                                    "leaf id": 114,
                                    "key": "doc/body/sec5/sub1/txl0",
                                    "block type": "txl",
                                    "content": "In most LP models, including the SOTA models SEAL and WP, the Power dataset tends to have the lowest AUC scores among the datasets. In Table, the Power dataset is at the bottom in terms of scores across models (e.g., WLK, WLNM, MF, LINE, SEAL, and WP). However, the proposed model achieves the highest AUC scores on the Power dataset among baseline models, prompting an analysis of the reasons for this performance.",
                                    "leftover": "In most LP models, including the SOTA models SEAL and WP, the Power dataset tends to have the lowest AUC scores among the datasets. In Table, the Power dataset is at the bottom in terms of scores across models (e.g., WLK, WLNM, MF, LINE, SEAL, and WP). However, the proposed model achieves the highest AUC scores on the Power dataset among baseline models, prompting an analysis of the reasons for this performance.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 115,
                                    "key": "doc/body/sec5/sub1/txl1",
                                    "block type": "txl",
                                    "content": "In Fig., for DRNL, the Power dataset exhibits horizontal lines, indicating that the values h(z2) have a limited range of outcomes for vectors z2 in cases without the target link; thus, the set of values h(z2) with the same value should be spread out. This observation implies that, for numerous subgraphs the calculation of PIs yields similar outcomes despite the differences in their topological structures, posing a challenge in distinguishing between them.",
                                    "leftover": "In Fig., for DRNL, the Power dataset exhibits horizontal lines, indicating that the values h(z2) have a limited range of outcomes for vectors z2 in cases without the target link; thus, the set of values h(z2) with the same value should be spread out. This observation implies that, for numerous subgraphs the calculation of PIs yields similar outcomes despite the differences in their topological structures, posing a challenge in distinguishing between them.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 116,
                                    "key": "doc/body/sec5/sub1/txl2",
                                    "block type": "txl",
                                    "content": "To address this problem, we applied Degree DRNL, which incorporates degree information. The points in Fig. are distributed without horizontal lines, leading to the highest score increase, as listed in Table.",
                                    "leftover": "To address this problem, we applied Degree DRNL, which incorporates degree information. The points in Fig. are distributed without horizontal lines, leading to the highest score increase, as listed in Table.",
                                    "matches": []
                                },
                                {
                                    "leaf id": 117,
                                    "key": "doc/body/sec5/sub1/txl3",
                                    "block type": "txl",
                                    "content": "The performance of heuristic methods, such as AA, Katz, and PR, tend to be similar to random guessing on datasets with low density, particularly in the cases of the Power and Router datasets. Embedding methods also display low performance. In contrast, the GNNbased methods demonstrate improved performance using subgraphs and the network learning ability. However, the performance for the Power dataset is significantly lower than that for the Router dataset.",
                                    "leftover": "The performance of heuristic methods, such as AA, Katz, and PR, tend to be similar to random guessing on datasets with low density, particularly in the cases of the Power and Router datasets. Embedding methods also display low performance. In contrast, the GNNbased methods demonstrate improved performance using subgraphs and the network learning ability. However, the performance for the Power dataset is significantly lower than that for the Router dataset.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec5/sub1/table4",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec5/sub1/table4/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 118,
                                                    "key": "doc/body/sec5/sub1/table4/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Average number of nodes in subgraphs for the Power and Router datasets",
                                                    "leftover": "Average number of nodes in subgraphs for the Power and Router datasets",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 119,
                                            "key": "doc/body/sec5/sub1/table4/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|cc|cc} & \\multicolumn{2}{c|}Power & \\multicolumn{2}cRouter & positive & negative & positive & negative 1hop & 8.03 & 9.12 & 5.11 & 6.72 2hop & 22.26 & 24.85 & 29.21 & 13.94 3hop & 43.11 & 49.50 & 120.35 & 55.22 4hop & 71.72 & 82.16 & 411.87 & 176.34 5hop & 99.28 & 116.75 & 740.80 & 411.35 6hop & 136.23 & 158.27 & 1272.42 & 852.13 7hop & 182.22 & 210.35 & 1835.46 & 1498.58",
                                            "leftover": "{l|cc|cc} & \\multicolumn{2}{c|}Power & \\multicolumn{2}cRouter & positive & negative & positive & negative 1hop & 8.03 & 9.12 & 5.11 & 6.72 2hop & 22.26 & 24.85 & 29.21 & 13.94 3hop & 43.11 & 49.50 & 120.35 & 55.22 4hop & 71.72 & 82.16 & 411.87 & 176.34 5hop & 99.28 & 116.75 & 740.80 & 411.35 6hop & 136.23 & 158.27 & 1272.42 & 852.13 7hop & 182.22 & 210.35 & 1835.46 & 1498.58",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 120,
                                    "key": "doc/body/sec5/sub1/txl5",
                                    "block type": "txl",
                                    "content": "To bridge this gap, we analyzed subgraphs with node labeling. The number of nodes within the selected subgraphs between positive and negative links was significantly different on the Router dataset but not the Power dataset (Table). This difference is attributed to the presence of the hub nodes in the Router dataset, which are connected to numerous nodes. Thus, the subgraphs corresponding to positive links tend to have more nodes than those corresponding to negative links.",
                                    "leftover": "To bridge this gap, we analyzed subgraphs with node labeling. The number of nodes within the selected subgraphs between positive and negative links was significantly different on the Router dataset but not the Power dataset (Table). This difference is attributed to the presence of the hub nodes in the Router dataset, which are connected to numerous nodes. Thus, the subgraphs corresponding to positive links tend to have more nodes than those corresponding to negative links.",
                                    "matches": []
                                },
                                {
                                    "key": "doc/body/sec5/sub1/table6",
                                    "block_type": "table",
                                    "children": [
                                        {
                                            "key": "doc/body/sec5/sub1/table6/cpt0",
                                            "block_type": "cpt",
                                            "children": [
                                                {
                                                    "leaf id": 121,
                                                    "key": "doc/body/sec5/sub1/table6/cpt0/txl0",
                                                    "block type": "txl",
                                                    "content": "Comparison of models by Max hop settings on the Power and Router datasets",
                                                    "leftover": "Comparison of models by Max hop settings on the Power and Router datasets",
                                                    "matches": []
                                                }
                                            ]
                                        },
                                        {
                                            "leaf id": 122,
                                            "key": "doc/body/sec5/sub1/table6/tabular1",
                                            "block type": "tabular",
                                            "content": "{l|l|cccc} \\multirow{2}{*}{\\rotatebox{90}{}}& Model & MAPHLP & MAPHLP & WP & MAPHLP + WP \\cmidrule{26} & Center & target & random &  & random \\multirow{7}{*}{\\rotatebox{90}Power} & 1hop & 78.05±1.20 & 85.66±0.86 & 80.24±0.95 & 87.53±0.73 & 2hop & 86.34±1.04 & 90.52±0.73 & 89.40±1.00 & 91.59±0.77 & 3hop & 89.65±0.64 & 91.90±0.58 & 92.11±0.77 & 93.61±0.52 & 4hop & 91.38±0.53 & 92.67±0.55 & 91.67±0.80 & 94.85±0.55 & 5hop & 92.27±0.40 & 93.06±0.44 & 91.39±0.78 & 95.55±0.59 & 6hop & 92.77±0.47 & 93.16±0.49 & 91.55±0.83 & 95.85±0.44 & 7hop &93.06 ±0.43 & 93.37 ±0.41 & 91.50 ± 0.89 & 96.01 ±0.52 \\multirow{7}{*}{\\rotatebox{90}Router} & 1hop & 93.12 ± 0.45 & 93.40 ± 0.46 & 94.48 ± 0.36 & 94.83 ± 0.41 & 2hop & 95.96 ± 0.40 & 95.70 ± 0.45 & 97.15 ± 0.27 & 97.22 ± 0.23 & 3hop & 96.38 ± 0.41 & 96.11 ± 0.43 & 97.28 ± 0.24 & 97.42 ± 0.27 & 4hop & 96.45 ± 0.40 & 96.22 ± 0.43 & OOM\\footnotemark & OOM & 5hop & 96.46 ± 0.42 & 96.24 ± 0.48 & OOM & OOM & 6hop & 96.44 ± 0.45 & 96.23 ± 0.47 & OOM & OOM & 7hop & 96.43 ± 0.45 & 96.19 ± 0.49 & OOM & OOM",
                                            "leftover": "{l|l|cccc} \\multirow{2}{*}{\\rotatebox{90}{}}& Model & MAPHLP & MAPHLP & WP & MAPHLP + WP \\cmidrule{26} & Center & target & random &  & random \\multirow{7}{*}{\\rotatebox{90}Power} & 1hop & 78.05±1.20 & 85.66±0.86 & 80.24±0.95 & 87.53±0.73 & 2hop & 86.34±1.04 & 90.52±0.73 & 89.40±1.00 & 91.59±0.77 & 3hop & 89.65±0.64 & 91.90±0.58 & 92.11±0.77 & 93.61±0.52 & 4hop & 91.38±0.53 & 92.67±0.55 & 91.67±0.80 & 94.85±0.55 & 5hop & 92.27±0.40 & 93.06±0.44 & 91.39±0.78 & 95.55±0.59 & 6hop & 92.77±0.47 & 93.16±0.49 & 91.55±0.83 & 95.85±0.44 & 7hop &93.06 ±0.43 & 93.37 ±0.41 & 91.50 ± 0.89 & 96.01 ±0.52 \\multirow{7}{*}{\\rotatebox{90}Router} & 1hop & 93.12 ± 0.45 & 93.40 ± 0.46 & 94.48 ± 0.36 & 94.83 ± 0.41 & 2hop & 95.96 ± 0.40 & 95.70 ± 0.45 & 97.15 ± 0.27 & 97.22 ± 0.23 & 3hop & 96.38 ± 0.41 & 96.11 ± 0.43 & 97.28 ± 0.24 & 97.42 ± 0.27 & 4hop & 96.45 ± 0.40 & 96.22 ± 0.43 & OOM\\footnotemark & OOM & 5hop & 96.46 ± 0.42 & 96.24 ± 0.48 & OOM & OOM & 6hop & 96.44 ± 0.45 & 96.23 ± 0.47 & OOM & OOM & 7hop & 96.43 ± 0.45 & 96.19 ± 0.49 & OOM & OOM",
                                            "matches": []
                                        },
                                        {
                                            "leaf id": 123,
                                            "key": "doc/body/sec5/sub1/table6/txl2",
                                            "block type": "txl",
                                            "content": "}",
                                            "leftover": "}",
                                            "matches": []
                                        }
                                    ]
                                },
                                {
                                    "leaf id": 124,
                                    "key": "doc/body/sec5/sub1/txl7",
                                    "block type": "txl",
                                    "content": "However, the Power dataset does not have hub nodes, and the number of nodes in the subgraph of positive links remains small. We randomly changed the center nodes (a,b) for node labeling f^(a,b)degdrnl increasing the performance, as listed in Table. This outcome highlights that setting target nodes as the center nodes may not effectively analyze the topological structure in the case of small graphs. Furthermore, the performance for the Power dataset continues to increase with increasing hops (Table), achieving an AUC score of 95.87, which is significantly better than that of 92.11 for WP.",
                                    "leftover": "However, the Power dataset does not have hub nodes, and the number of nodes in the subgraph of positive links remains small. We randomly changed the center nodes (a,b) for node labeling f^(a,b)degdrnl increasing the performance, as listed in Table. This outcome highlights that setting target nodes as the center nodes may not effectively analyze the topological structure in the case of small graphs. Furthermore, the performance for the Power dataset continues to increase with increasing hops (Table), achieving an AUC score of 95.87, which is significantly better than that of 92.11 for WP.",
                                    "matches": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "key": "doc/body/sec6",
                    "block_type": "sec",
                    "children": [
                        {
                            "leaf id": 125,
                            "key": "doc/body/sec6/tit",
                            "block type": "title",
                            "content": "Conclusion",
                            "leftover": "Conclusion",
                            "matches": []
                        },
                        {
                            "leaf id": 126,
                            "key": "doc/body/sec6/txl0",
                            "block type": "txl",
                            "content": "This paper proposes PHLP, an explainable method that applies PH to analyze the topological structure of graphs to overcome the limitations of GNNbased methods for LP. By employing the proposed methods, such as angle hop subgraphs and Degree DRNL, PHLP improves the analysis of the topological structure of graphs. The experimental results demonstrate that the proposed PHLP method achieves competitive performance across benchmark datasets, even SOTA performance, especially on the Power dataset. Additionally, when integrated with existing GNNbased methods, PHLP improves performance across all datasets. By analyzing the topological information of the given graphs, PHLP addresses the limitations of GNNbased methods and enhances overall performance. As demonstrated, PHLP provides explainable algorithms without relying on complex deep learning techniques, providing insight into the factors that significantly influence performance for the LP problem of graph data. \\bibliographystyleIEEEtran \\bibliographyreference",
                            "leftover": "This paper proposes PHLP, an explainable method that applies PH to analyze the topological structure of graphs to overcome the limitations of GNNbased methods for LP. By employing the proposed methods, such as angle hop subgraphs and Degree DRNL, PHLP improves the analysis of the topological structure of graphs. The experimental results demonstrate that the proposed PHLP method achieves competitive performance across benchmark datasets, even SOTA performance, especially on the Power dataset. Additionally, when integrated with existing GNNbased methods, PHLP improves performance across all datasets. By analyzing the topological information of the given graphs, PHLP addresses the limitations of GNNbased methods and enhances overall performance. As demonstrated, PHLP provides explainable algorithms without relying on complex deep learning techniques, providing insight into the factors that significantly influence performance for the LP problem of graph data. \\bibliographystyleIEEEtran \\bibliographyreference",
                            "matches": []
                        }
                    ]
                }
            ]
        },
        {
            "leaf id": 127,
            "key": "doc/bib0",
            "block type": "bibliography",
            "content": "Z.~Zhang, P.~Cui, and W.~Zhu, ''Deep learning on graphs: A survey,'' IEEE Transactions on Knowledge and Data Engineering, vol.~34, no.~1, pp. 249270, 2020.",
            "leftover": "Z.~Zhang, P.~Cui, and W.~Zhu, ''Deep learning on graphs: A survey,'' IEEE Transactions on Knowledge and Data Engineering, vol.~34, no.~1, pp. 249270, 2020.",
            "matches": []
        },
        {
            "leaf id": 128,
            "key": "doc/bib1",
            "block type": "bibliography",
            "content": "Z.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and S.~Y. Philip, ''A comprehensive survey on graph neural networks,'' IEEE transactions on neural networks and learning systems, vol.~32, no.~1, pp. 424, 2020.",
            "leftover": "Z.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and S.~Y. Philip, ''A comprehensive survey on graph neural networks,'' IEEE transactions on neural networks and learning systems, vol.~32, no.~1, pp. 424, 2020.",
            "matches": []
        },
        {
            "leaf id": 129,
            "key": "doc/bib2",
            "block type": "bibliography",
            "content": "L.~A. Adamic and E.~Adar, ''Friends and neighbors on the web,'' Social networks, vol.~25, no.~3, pp. 211230, 2003.",
            "leftover": "L.~A. Adamic and E.~Adar, ''Friends and neighbors on the web,'' Social networks, vol.~25, no.~3, pp. 211230, 2003.",
            "matches": []
        },
        {
            "leaf id": 130,
            "key": "doc/bib3",
            "block type": "bibliography",
            "content": "L.~Yao, L.~Wang, L.~Pan, and K.~Yao, ''Link prediction based on commonneighbors for dynamic social network,'' Procedia Computer Science, vol.~83, pp. 8289, 2016.",
            "leftover": "L.~Yao, L.~Wang, L.~Pan, and K.~Yao, ''Link prediction based on commonneighbors for dynamic social network,'' Procedia Computer Science, vol.~83, pp. 8289, 2016.",
            "matches": []
        },
        {
            "leaf id": 131,
            "key": "doc/bib4",
            "block type": "bibliography",
            "content": "M.~Fire, L.~Tenenboim, O.~Lesser, R.~Puzis, L.~Rokach, and Y.~Elovici, ''Link prediction in social networks using computationally efficient topological features,'' in 2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing. 1em plus 0.5em minus 0.4em\\relax IEEE, 2011, pp. 7380.",
            "leftover": "M.~Fire, L.~Tenenboim, O.~Lesser, R.~Puzis, L.~Rokach, and Y.~Elovici, ''Link prediction in social networks using computationally efficient topological features,'' in 2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing. 1em plus 0.5em minus 0.4em\\relax IEEE, 2011, pp. 7380.",
            "matches": []
        },
        {
            "leaf id": 132,
            "key": "doc/bib5",
            "block type": "bibliography",
            "content": "S.~M. Kazemi and D.~Poole, ''Simple embedding for link prediction in knowledge graphs,'' Advances in neural information processing systems, vol.~31, 2018.",
            "leftover": "S.~M. Kazemi and D.~Poole, ''Simple embedding for link prediction in knowledge graphs,'' Advances in neural information processing systems, vol.~31, 2018.",
            "matches": []
        },
        {
            "leaf id": 133,
            "key": "doc/bib6",
            "block type": "bibliography",
            "content": "M.~Nayyeri, G.~M. Cil, S.~Vahdati, F.~Osborne, A.~Kravchenko, S.~Angioni, A.~Salatino, D.~R. Recupero, E.~Motta, and J.~Lehmann, ''Link prediction of weighted triples for knowledge graph completion within the scholarly domain,'' Ieee Access, vol.~9, pp. 116\\,002116\\,014, 2021.",
            "leftover": "M.~Nayyeri, G.~M. Cil, S.~Vahdati, F.~Osborne, A.~Kravchenko, S.~Angioni, A.~Salatino, D.~R. Recupero, E.~Motta, and J.~Lehmann, ''Link prediction of weighted triples for knowledge graph completion within the scholarly domain,'' Ieee Access, vol.~9, pp. 116\\,002116\\,014, 2021.",
            "matches": []
        },
        {
            "leaf id": 134,
            "key": "doc/bib7",
            "block type": "bibliography",
            "content": "Z.~Stanfield, M.~Co{\\cs}kun, and M.~Koyut{''u}rk, ''Drug response prediction as a link prediction problem,'' Scientific reports, vol.~7, no.~1, p. 40321, 2017.",
            "leftover": "Z.~Stanfield, M.~Co{\\cs}kun, and M.~Koyut{''u}rk, ''Drug response prediction as a link prediction problem,'' Scientific reports, vol.~7, no.~1, p. 40321, 2017.",
            "matches": []
        },
        {
            "leaf id": 135,
            "key": "doc/bib8",
            "block type": "bibliography",
            "content": "E.~Nasiri, K.~Berahmand, M.~Rostami, and M.~Dabiri, ''A novel link prediction algorithm for proteinprotein interaction networks by attributed graph embedding,'' Computers in Biology and Medicine, vol. 137, p. 104772, 2021.",
            "leftover": "E.~Nasiri, K.~Berahmand, M.~Rostami, and M.~Dabiri, ''A novel link prediction algorithm for proteinprotein interaction networks by attributed graph embedding,'' Computers in Biology and Medicine, vol. 137, p. 104772, 2021.",
            "matches": []
        },
        {
            "leaf id": 136,
            "key": "doc/bib9",
            "block type": "bibliography",
            "content": "C.~Lei and J.~Ruan, ''A novel link prediction algorithm for reconstructing proteinprotein interaction networks by topological similarity,'' Bioinformatics, vol.~29, no.~3, pp. 355364, 2013.",
            "leftover": "C.~Lei and J.~Ruan, ''A novel link prediction algorithm for reconstructing proteinprotein interaction networks by topological similarity,'' Bioinformatics, vol.~29, no.~3, pp. 355364, 2013.",
            "matches": []
        },
        {
            "leaf id": 137,
            "key": "doc/bib10",
            "block type": "bibliography",
            "content": "I.~A. Kov{\\'a}cs, K.~Luck, K.~Spirohn, Y.~Wang, C.~Pollis, S.~Schlabach, W.~Bian, D.K. Kim, N.~Kishore, T.~Hao et~al., ''Networkbased prediction of protein interactions,'' Nature communications, vol.~10, no.~1, p. 1240, 2019.",
            "leftover": "I.~A. Kov{\\'a}cs, K.~Luck, K.~Spirohn, Y.~Wang, C.~Pollis, S.~Schlabach, W.~Bian, D.K. Kim, N.~Kishore, T.~Hao et~al., ''Networkbased prediction of protein interactions,'' Nature communications, vol.~10, no.~1, p. 1240, 2019.",
            "matches": []
        },
        {
            "leaf id": 138,
            "key": "doc/bib11",
            "block type": "bibliography",
            "content": "N.~Brockmann, E.~Elson~Kosasih, and A.~Brintrup, ''Supply chain link prediction on uncertain knowledge graph,'' ACM SIGKDD Explorations Newsletter, vol.~24, no.~2, pp. 124130, 2022.",
            "leftover": "N.~Brockmann, E.~Elson~Kosasih, and A.~Brintrup, ''Supply chain link prediction on uncertain knowledge graph,'' ACM SIGKDD Explorations Newsletter, vol.~24, no.~2, pp. 124130, 2022.",
            "matches": []
        },
        {
            "leaf id": 139,
            "key": "doc/bib12",
            "block type": "bibliography",
            "content": "A.~Brintrup, P.~Wichmann, P.~Woodall, D.~McFarlane, E.~Nicks, and W.~Krechel, ''Predicting hidden links in supply networks,'' Complexity, vol. 2018, pp. 112, 2018.",
            "leftover": "A.~Brintrup, P.~Wichmann, P.~Woodall, D.~McFarlane, E.~Nicks, and W.~Krechel, ''Predicting hidden links in supply networks,'' Complexity, vol. 2018, pp. 112, 2018.",
            "matches": []
        },
        {
            "leaf id": 140,
            "key": "doc/bib13",
            "block type": "bibliography",
            "content": "L.~L{''u} and T.~Zhou, ''Link prediction in complex networks: A survey,'' Physica A: statistical mechanics and its applications, vol. 390, no.~6, pp. 11501170, 2011.",
            "leftover": "L.~L{''u} and T.~Zhou, ''Link prediction in complex networks: A survey,'' Physica A: statistical mechanics and its applications, vol. 390, no.~6, pp. 11501170, 2011.",
            "matches": []
        },
        {
            "leaf id": 141,
            "key": "doc/bib14",
            "block type": "bibliography",
            "content": "A.L. Barab{\\'a}si and R.~Albert, ''Emergence of scaling in random networks,'' science, vol. 286, no. 5439, pp. 509512, 1999.",
            "leftover": "A.L. Barab{\\'a}si and R.~Albert, ''Emergence of scaling in random networks,'' science, vol. 286, no. 5439, pp. 509512, 1999.",
            "matches": []
        },
        {
            "leaf id": 142,
            "key": "doc/bib15",
            "block type": "bibliography",
            "content": "T.~Zhou, L.~L{''u}, and Y.C. Zhang, ''Predicting missing links via local information,'' The European Physical Journal B, vol.~71, pp. 623630, 2009.",
            "leftover": "T.~Zhou, L.~L{''u}, and Y.C. Zhang, ''Predicting missing links via local information,'' The European Physical Journal B, vol.~71, pp. 623630, 2009.",
            "matches": []
        },
        {
            "leaf id": 143,
            "key": "doc/bib16",
            "block type": "bibliography",
            "content": "S.~Brin and L.~Page, ''Reprint of: The anatomy of a largescale hypertextual web search engine,'' Computer networks, vol.~56, no.~18, pp. 38253833, 2012.",
            "leftover": "S.~Brin and L.~Page, ''Reprint of: The anatomy of a largescale hypertextual web search engine,'' Computer networks, vol.~56, no.~18, pp. 38253833, 2012.",
            "matches": []
        },
        {
            "leaf id": 144,
            "key": "doc/bib17",
            "block type": "bibliography",
            "content": "G.~Jeh and J.~Widom, ''Simrank: a measure of structuralcontext similarity,'' in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, 2002, pp. 538543.",
            "leftover": "G.~Jeh and J.~Widom, ''Simrank: a measure of structuralcontext similarity,'' in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, 2002, pp. 538543.",
            "matches": []
        },
        {
            "leaf id": 145,
            "key": "doc/bib18",
            "block type": "bibliography",
            "content": "Y.~Koren, R.~Bell, and C.~Volinsky, ''Matrix factorization techniques for recommender systems,'' Computer, vol.~42, no.~8, pp. 3037, 2009.",
            "leftover": "Y.~Koren, R.~Bell, and C.~Volinsky, ''Matrix factorization techniques for recommender systems,'' Computer, vol.~42, no.~8, pp. 3037, 2009.",
            "matches": []
        },
        {
            "leaf id": 146,
            "key": "doc/bib19",
            "block type": "bibliography",
            "content": "B.~Perozzi, R.~AlRfou, and S.~Skiena, ''Deepwalk: Online learning of social representations,'' in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701710.",
            "leftover": "B.~Perozzi, R.~AlRfou, and S.~Skiena, ''Deepwalk: Online learning of social representations,'' in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701710.",
            "matches": []
        },
        {
            "leaf id": 147,
            "key": "doc/bib20",
            "block type": "bibliography",
            "content": "A.~Grover and J.~Leskovec, ''node2vec: Scalable feature learning for networks,'' in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855864.",
            "leftover": "A.~Grover and J.~Leskovec, ''node2vec: Scalable feature learning for networks,'' in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855864.",
            "matches": []
        },
        {
            "leaf id": 148,
            "key": "doc/bib21",
            "block type": "bibliography",
            "content": "J.~Tang, M.~Qu, M.~Wang, M.~Zhang, J.~Yan, and Q.~Mei, ''Line: Largescale information network embedding,'' in Proceedings of the 24th international conference on world wide web, 2015, pp. 10671077.",
            "leftover": "J.~Tang, M.~Qu, M.~Wang, M.~Zhang, J.~Yan, and Q.~Mei, ''Line: Largescale information network embedding,'' in Proceedings of the 24th international conference on world wide web, 2015, pp. 10671077.",
            "matches": []
        },
        {
            "leaf id": 149,
            "key": "doc/bib22",
            "block type": "bibliography",
            "content": "T.~N. Kipf and M.~Welling, ''Variational graph autoencoders,'' arXiv preprint arXiv:1611.07308, 2016.",
            "leftover": "T.~N. Kipf and M.~Welling, ''Variational graph autoencoders,'' arXiv preprint arXiv:1611.07308, 2016.",
            "matches": []
        },
        {
            "leaf id": 150,
            "key": "doc/bib23",
            "block type": "bibliography",
            "content": "M.~Zhang and Y.~Chen, ''Link prediction based on graph neural networks,'' Advances in neural information processing systems, vol.~31, 2018.",
            "leftover": "M.~Zhang and Y.~Chen, ''Link prediction based on graph neural networks,'' Advances in neural information processing systems, vol.~31, 2018.",
            "matches": []
        },
        {
            "leaf id": 151,
            "key": "doc/bib24",
            "block type": "bibliography",
            "content": "S.~Yun, S.~Kim, J.~Lee, J.~Kang, and H.~J. Kim, ''Neognns: Neighborhood overlapaware graph neural networks for link prediction,'' Advances in Neural Information Processing Systems, vol.~34, pp. 13\\,68313\\,694, 2021.",
            "leftover": "S.~Yun, S.~Kim, J.~Lee, J.~Kang, and H.~J. Kim, ''Neognns: Neighborhood overlapaware graph neural networks for link prediction,'' Advances in Neural Information Processing Systems, vol.~34, pp. 13\\,68313\\,694, 2021.",
            "matches": []
        },
        {
            "leaf id": 152,
            "key": "doc/bib25",
            "block type": "bibliography",
            "content": "C.~Mavromatis and G.~Karypis, ''Graph infoclust: Leveraging clusterlevel node information for unsupervised graph representation learning,'' arXiv preprint arXiv:2009.06946, 2020.",
            "leftover": "C.~Mavromatis and G.~Karypis, ''Graph infoclust: Leveraging clusterlevel node information for unsupervised graph representation learning,'' arXiv preprint arXiv:2009.06946, 2020.",
            "matches": []
        },
        {
            "leaf id": 153,
            "key": "doc/bib26",
            "block type": "bibliography",
            "content": "Z.~Yan, T.~Ma, L.~Gao, Z.~Tang, and C.~Chen, ''Link prediction with persistent homology: An interactive view,'' in International conference on machine learning. 1em plus 0.5em minus 0.4em\\relax PMLR, 2021, pp. 11\\,65911\\,669.",
            "leftover": "Z.~Yan, T.~Ma, L.~Gao, Z.~Tang, and C.~Chen, ''Link prediction with persistent homology: An interactive view,'' in International conference on machine learning. 1em plus 0.5em minus 0.4em\\relax PMLR, 2021, pp. 11\\,65911\\,669.",
            "matches": []
        },
        {
            "leaf id": 154,
            "key": "doc/bib27",
            "block type": "bibliography",
            "content": "L.~Pan, C.~Shi, and I.~Dokmani{\\'c}, ''Neural link prediction with walk pooling,'' arXiv preprint arXiv:2110.04375, 2021.",
            "leftover": "L.~Pan, C.~Shi, and I.~Dokmani{\\'c}, ''Neural link prediction with walk pooling,'' arXiv preprint arXiv:2110.04375, 2021.",
            "matches": []
        },
        {
            "leaf id": 155,
            "key": "doc/bib28",
            "block type": "bibliography",
            "content": "S.~Huber, ''Persistent homology in data science,'' in Data ScienceAnalytics and Applications: Proceedings of the 3rd International Data Science ConferenceiDSC2020. 1em plus 0.5em minus 0.4em\\relax Springer, 2021, pp. 8188.",
            "leftover": "S.~Huber, ''Persistent homology in data science,'' in Data ScienceAnalytics and Applications: Proceedings of the 3rd International Data Science ConferenceiDSC2020. 1em plus 0.5em minus 0.4em\\relax Springer, 2021, pp. 8188.",
            "matches": []
        },
        {
            "leaf id": 156,
            "key": "doc/bib29",
            "block type": "bibliography",
            "content": "T.~K. Dey and Y.~Wang, Computational topology for data analysis. 1em plus 0.5em minus 0.4em\\relax Cambridge University Press, 2022.",
            "leftover": "T.~K. Dey and Y.~Wang, Computational topology for data analysis. 1em plus 0.5em minus 0.4em\\relax Cambridge University Press, 2022.",
            "matches": []
        },
        {
            "leaf id": 157,
            "key": "doc/bib30",
            "block type": "bibliography",
            "content": "M.~Horn, E.~De~Brouwer, M.~Moor, Y.~Moreau, B.~Rieck, and K.~Borgwardt, ''Topological graph neural networks,'' arXiv preprint arXiv:2102.07835, 2021.",
            "leftover": "M.~Horn, E.~De~Brouwer, M.~Moor, Y.~Moreau, B.~Rieck, and K.~Borgwardt, ''Topological graph neural networks,'' arXiv preprint arXiv:2102.07835, 2021.",
            "matches": []
        },
        {
            "leaf id": 158,
            "key": "doc/bib31",
            "block type": "bibliography",
            "content": "X.~Ye, F.~Sun, and S.~Xiang, ''Treph: A plugin topological layer for graph neural networks,'' Entropy, vol.~25, no.~2, p. 331, 2023.",
            "leftover": "X.~Ye, F.~Sun, and S.~Xiang, ''Treph: A plugin topological layer for graph neural networks,'' Entropy, vol.~25, no.~2, p. 331, 2023.",
            "matches": []
        },
        {
            "leaf id": 159,
            "key": "doc/bib32",
            "block type": "bibliography",
            "content": "M.~Carri{\\'e}re, F.~Chazal, Y.~Ike, T.~Lacombe, M.~Royer, and Y.~Umeda, ''Perslay: A neural network layer for persistence diagrams and new graph topological signatures,'' in International Conference on Artificial Intelligence and Statistics. 1em plus 0.5em minus 0.4em\\relax PMLR, 2020, pp. 27862796.",
            "leftover": "M.~Carri{\\'e}re, F.~Chazal, Y.~Ike, T.~Lacombe, M.~Royer, and Y.~Umeda, ''Perslay: A neural network layer for persistence diagrams and new graph topological signatures,'' in International Conference on Artificial Intelligence and Statistics. 1em plus 0.5em minus 0.4em\\relax PMLR, 2020, pp. 27862796.",
            "matches": []
        },
        {
            "leaf id": 160,
            "key": "doc/bib33",
            "block type": "bibliography",
            "content": "F.~M. Taiwo, U.~Islambekov, and C.~G. Akcora, ''Explaining the power of topological data analysis in graph machine learning,'' arXiv preprint arXiv:2401.04250, 2024.",
            "leftover": "F.~M. Taiwo, U.~Islambekov, and C.~G. Akcora, ''Explaining the power of topological data analysis in graph machine learning,'' arXiv preprint arXiv:2401.04250, 2024.",
            "matches": []
        },
        {
            "leaf id": 161,
            "key": "doc/bib34",
            "block type": "bibliography",
            "content": "T.~Wen, E.~Chen, and Y.~Chen, ''Tensorview topological graph neural network,'' arXiv preprint arXiv:2401.12007, 2024.",
            "leftover": "T.~Wen, E.~Chen, and Y.~Chen, ''Tensorview topological graph neural network,'' arXiv preprint arXiv:2401.12007, 2024.",
            "matches": []
        },
        {
            "leaf id": 162,
            "key": "doc/bib35",
            "block type": "bibliography",
            "content": "J.~Immonen, A.~Souza, and V.~Garg, ''Going beyond persistent homology using persistent homology,'' Advances in Neural Information Processing Systems, vol.~36, 2024.",
            "leftover": "J.~Immonen, A.~Souza, and V.~Garg, ''Going beyond persistent homology using persistent homology,'' Advances in Neural Information Processing Systems, vol.~36, 2024.",
            "matches": []
        },
        {
            "leaf id": 163,
            "key": "doc/bib36",
            "block type": "bibliography",
            "content": "C.~Ying, X.~Zhao, and T.~Yu, ''Boosting graph pooling with persistent homology,'' arXiv preprint arXiv:2402.16346, 2024.",
            "leftover": "C.~Ying, X.~Zhao, and T.~Yu, ''Boosting graph pooling with persistent homology,'' arXiv preprint arXiv:2402.16346, 2024.",
            "matches": []
        },
        {
            "leaf id": 164,
            "key": "doc/bib37",
            "block type": "bibliography",
            "content": "Q.~Zhao and Y.~Wang, ''Learning metrics for persistencebased summaries and applications for graph classification,'' Advances in Neural Information Processing Systems, vol.~32, 2019.",
            "leftover": "Q.~Zhao and Y.~Wang, ''Learning metrics for persistencebased summaries and applications for graph classification,'' Advances in Neural Information Processing Systems, vol.~32, 2019.",
            "matches": []
        },
        {
            "leaf id": 165,
            "key": "doc/bib38",
            "block type": "bibliography",
            "content": "Y.~Chen, B.~Coskunuzer, and Y.~Gel, ''Topological relational learning on graphs,'' Advances in neural information processing systems, vol.~34, pp. 27\\,02927\\,042, 2021.",
            "leftover": "Y.~Chen, B.~Coskunuzer, and Y.~Gel, ''Topological relational learning on graphs,'' Advances in neural information processing systems, vol.~34, pp. 27\\,02927\\,042, 2021.",
            "matches": []
        },
        {
            "leaf id": 166,
            "key": "doc/bib39",
            "block type": "bibliography",
            "content": "Q.~Zhao, Z.~Ye, C.~Chen, and Y.~Wang, ''Persistence enhanced graph neural network,'' in International Conference on Artificial Intelligence and Statistics. 1em plus 0.5em minus 0.4em\\relax PMLR, 2020, pp. 28962906.",
            "leftover": "Q.~Zhao, Z.~Ye, C.~Chen, and Y.~Wang, ''Persistence enhanced graph neural network,'' in International Conference on Artificial Intelligence and Statistics. 1em plus 0.5em minus 0.4em\\relax PMLR, 2020, pp. 28962906.",
            "matches": []
        },
        {
            "leaf id": 167,
            "key": "doc/bib40",
            "block type": "bibliography",
            "content": "M.~Nickel, X.~Jiang, and V.~Tresp, ''Reducing the rank in relational factorization models by including observable patterns,'' Advances in Neural Information Processing Systems, vol.~27, 2014.",
            "leftover": "M.~Nickel, X.~Jiang, and V.~Tresp, ''Reducing the rank in relational factorization models by including observable patterns,'' Advances in Neural Information Processing Systems, vol.~27, 2014.",
            "matches": []
        },
        {
            "leaf id": 168,
            "key": "doc/bib41",
            "block type": "bibliography",
            "content": "L.~F. Ribeiro, P.~H. Saverese, and D.~R. Figueiredo, ''struc2vec: Learning node representations from structural identity,'' in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 385394.",
            "leftover": "L.~F. Ribeiro, P.~H. Saverese, and D.~R. Figueiredo, ''struc2vec: Learning node representations from structural identity,'' in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 385394.",
            "matches": []
        },
        {
            "leaf id": 169,
            "key": "doc/bib42",
            "block type": "bibliography",
            "content": "L.~Vietoris, ''{''U}ber den h{''o}heren zusammenhang kompakter r{''a}ume und eine klasse von zusammenhangstreuen abbildungen,'' Mathematische Annalen, vol.~97, no.~1, pp. 454472, 1927.",
            "leftover": "L.~Vietoris, ''{''U}ber den h{''o}heren zusammenhang kompakter r{''a}ume und eine klasse von zusammenhangstreuen abbildungen,'' Mathematische Annalen, vol.~97, no.~1, pp. 454472, 1927.",
            "matches": []
        },
        {
            "leaf id": 170,
            "key": "doc/bib43",
            "block type": "bibliography",
            "content": "M.~Gromov, ''Hyperbolic groups,'' in Essays in group theory. 1em plus 0.5em minus 0.4em\\relax Springer, 1987, pp. 75263.",
            "leftover": "M.~Gromov, ''Hyperbolic groups,'' in Essays in group theory. 1em plus 0.5em minus 0.4em\\relax Springer, 1987, pp. 75263.",
            "matches": []
        },
        {
            "leaf id": 171,
            "key": "doc/bib44",
            "block type": "bibliography",
            "content": "Edelsbrunner, Letscher, and Zomorodian, ''Topological persistence and simplification,'' Discrete & computational geometry, vol.~28, pp. 511533, 2002.",
            "leftover": "Edelsbrunner, Letscher, and Zomorodian, ''Topological persistence and simplification,'' Discrete & computational geometry, vol.~28, pp. 511533, 2002.",
            "matches": []
        },
        {
            "leaf id": 172,
            "key": "doc/bib45",
            "block type": "bibliography",
            "content": "H.~Adams, T.~Emerson, M.~Kirby, R.~Neville, C.~Peterson, P.~Shipman, S.~Chepushtanova, E.~Hanson, F.~Motta, and L.~Ziegelmeier, ''Persistence images: A stable vector representation of persistent homology,'' Journal of Machine Learning Research, vol.~18, 2017.",
            "leftover": "H.~Adams, T.~Emerson, M.~Kirby, R.~Neville, C.~Peterson, P.~Shipman, S.~Chepushtanova, E.~Hanson, F.~Motta, and L.~Ziegelmeier, ''Persistence images: A stable vector representation of persistent homology,'' Journal of Machine Learning Research, vol.~18, 2017.",
            "matches": []
        },
        {
            "leaf id": 173,
            "key": "doc/bib46",
            "block type": "bibliography",
            "content": "A.~P. Bradley, ''The use of the area under the roc curve in the evaluation of machine learning algorithms,'' Pattern recognition, vol.~30, no.~7, pp. 11451159, 1997.",
            "leftover": "A.~P. Bradley, ''The use of the area under the roc curve in the evaluation of machine learning algorithms,'' Pattern recognition, vol.~30, no.~7, pp. 11451159, 1997.",
            "matches": []
        },
        {
            "leaf id": 174,
            "key": "doc/bib47",
            "block type": "bibliography",
            "content": "L.~Katz, ''A new status index derived from sociometric analysis,'' Psychometrika, vol.~18, no.~1, pp. 3943, 1953.",
            "leftover": "L.~Katz, ''A new status index derived from sociometric analysis,'' Psychometrika, vol.~18, no.~1, pp. 3943, 1953.",
            "matches": []
        },
        {
            "leaf id": 175,
            "key": "doc/bib48",
            "block type": "bibliography",
            "content": "S.~Brin and L.~Page, ''The anatomy of a largescale hypertextual web search engine,'' Computer networks and ISDN systems, vol.~30, no. 17, pp. 107117, 1998.",
            "leftover": "S.~Brin and L.~Page, ''The anatomy of a largescale hypertextual web search engine,'' Computer networks and ISDN systems, vol.~30, no. 17, pp. 107117, 1998.",
            "matches": []
        },
        {
            "leaf id": 176,
            "key": "doc/bib49",
            "block type": "bibliography",
            "content": "N.~Shervashidze, P.~Schweitzer, E.~J. Van~Leeuwen, K.~Mehlhorn, and K.~M. Borgwardt, ''Weisfeilerlehman graph kernels.'' Journal of Machine Learning Research, vol.~12, no.~9, 2011.",
            "leftover": "N.~Shervashidze, P.~Schweitzer, E.~J. Van~Leeuwen, K.~Mehlhorn, and K.~M. Borgwardt, ''Weisfeilerlehman graph kernels.'' Journal of Machine Learning Research, vol.~12, no.~9, 2011.",
            "matches": []
        },
        {
            "leaf id": 177,
            "key": "doc/bib50",
            "block type": "bibliography",
            "content": "M.~Zhang and Y.~Chen, ''Weisfeilerlehman neural machine for link prediction,'' in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 575583.",
            "leftover": "M.~Zhang and Y.~Chen, ''Weisfeilerlehman neural machine for link prediction,'' in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 575583.",
            "matches": []
        },
        {
            "leaf id": 178,
            "key": "doc/bib51",
            "block type": "bibliography",
            "content": "L.~Tang and H.~Liu, ''Leveraging social media networks for classification,'' Data Mining and Knowledge Discovery, vol.~23, pp. 447478, 2011.",
            "leftover": "L.~Tang and H.~Liu, ''Leveraging social media networks for classification,'' Data Mining and Knowledge Discovery, vol.~23, pp. 447478, 2011.",
            "matches": []
        },
        {
            "leaf id": 179,
            "key": "doc/bib52",
            "block type": "bibliography",
            "content": "V.~Batagelj and A.~Mrvar, ''Pajek datasets,'' http://vlado.fmf.unilj.si/pub/networks/data/, 2006.",
            "leftover": "V.~Batagelj and A.~Mrvar, ''Pajek datasets,'' http://vlado.fmf.unilj.si/pub/networks/data/, 2006.",
            "matches": []
        },
        {
            "leaf id": 180,
            "key": "doc/bib53",
            "block type": "bibliography",
            "content": "M.~E. Newman, ''Finding community structure in networks using the eigenvectors of matrices,'' Physical review E, vol.~74, no.~3, p. 036104, 2006.",
            "leftover": "M.~E. Newman, ''Finding community structure in networks using the eigenvectors of matrices,'' Physical review E, vol.~74, no.~3, p. 036104, 2006.",
            "matches": []
        },
        {
            "leaf id": 181,
            "key": "doc/bib54",
            "block type": "bibliography",
            "content": "R.~Ackland et~al., ''Mapping the us political blogosphere: Are conservative bloggers more prominent?'' in BlogTalk Downunder 2005 Conference, Sydney. 1em plus 0.5em minus 0.4em\\relax BlogTalk Downunder 2005 Conference, Sydney, 2005.",
            "leftover": "R.~Ackland et~al., ''Mapping the us political blogosphere: Are conservative bloggers more prominent?'' in BlogTalk Downunder 2005 Conference, Sydney. 1em plus 0.5em minus 0.4em\\relax BlogTalk Downunder 2005 Conference, Sydney, 2005.",
            "matches": []
        },
        {
            "leaf id": 182,
            "key": "doc/bib55",
            "block type": "bibliography",
            "content": "C.~Von~Mering, R.~Krause, B.~Snel, M.~Cornell, S.~G. Oliver, S.~Fields, and P.~Bork, ''Comparative assessment of largescale data sets of proteinprotein interactions,'' Nature, vol. 417, no. 6887, pp. 399403, 2002.",
            "leftover": "C.~Von~Mering, R.~Krause, B.~Snel, M.~Cornell, S.~G. Oliver, S.~Fields, and P.~Bork, ''Comparative assessment of largescale data sets of proteinprotein interactions,'' Nature, vol. 417, no. 6887, pp. 399403, 2002.",
            "matches": []
        },
        {
            "leaf id": 183,
            "key": "doc/bib56",
            "block type": "bibliography",
            "content": "D.~J. Watts and S.~H. Strogatz, ''Collective dynamics of 'smallworld'networks,'' nature, vol. 393, no. 6684, pp. 440442, 1998.",
            "leftover": "D.~J. Watts and S.~H. Strogatz, ''Collective dynamics of 'smallworld'networks,'' nature, vol. 393, no. 6684, pp. 440442, 1998.",
            "matches": []
        },
        {
            "leaf id": 184,
            "key": "doc/bib57",
            "block type": "bibliography",
            "content": "N.~Spring, R.~Mahajan, and D.~Wetherall, ''Measuring isp topologies with rocketfuel,'' ACM SIGCOMM Computer Communication Review, vol.~32, no.~4, pp. 133145, 2002.",
            "leftover": "N.~Spring, R.~Mahajan, and D.~Wetherall, ''Measuring isp topologies with rocketfuel,'' ACM SIGCOMM Computer Communication Review, vol.~32, no.~4, pp. 133145, 2002.",
            "matches": []
        },
        {
            "leaf id": 185,
            "key": "doc/bib58",
            "block type": "bibliography",
            "content": "M.~Zhang, Z.~Cui, S.~Jiang, and Y.~Chen, ''Beyond link prediction: Predicting hyperlinks in adjacency space,'' in Proceedings of the AAAI Conference on Artificial Intelligence, vol.~32, no.~1, 2018.",
            "leftover": "M.~Zhang, Z.~Cui, S.~Jiang, and Y.~Chen, ''Beyond link prediction: Predicting hyperlinks in adjacency space,'' in Proceedings of the AAAI Conference on Artificial Intelligence, vol.~32, no.~1, 2018.",
            "matches": []
        },
        {
            "leaf id": 186,
            "key": "doc/bib59",
            "block type": "bibliography",
            "content": "A.~K. McCallum, K.~Nigam, J.~Rennie, and K.~Seymore, ''Automating the construction of internet portals with machine learning,'' Information Retrieval, vol.~3, pp. 127163, 2000.",
            "leftover": "A.~K. McCallum, K.~Nigam, J.~Rennie, and K.~Seymore, ''Automating the construction of internet portals with machine learning,'' Information Retrieval, vol.~3, pp. 127163, 2000.",
            "matches": []
        },
        {
            "leaf id": 187,
            "key": "doc/bib60",
            "block type": "bibliography",
            "content": "C.~L. Giles, K.~D. Bollacker, and S.~Lawrence, ''Citeseer: An automatic citation indexing system,'' in Proceedings of the third ACM conference on Digital libraries, 1998, pp. 8998.",
            "leftover": "C.~L. Giles, K.~D. Bollacker, and S.~Lawrence, ''Citeseer: An automatic citation indexing system,'' in Proceedings of the third ACM conference on Digital libraries, 1998, pp. 8998.",
            "matches": []
        },
        {
            "leaf id": 188,
            "key": "doc/bib61",
            "block type": "bibliography",
            "content": "G.~Namata, B.~London, L.~Getoor, B.~Huang, and U.~Edu, ''Querydriven active surveying for collective classification,'' in 10th international workshop on mining and learning with graphs, vol.~8, 2012, p.~1.",
            "leftover": "G.~Namata, B.~London, L.~Getoor, B.~Huang, and U.~Edu, ''Querydriven active surveying for collective classification,'' in 10th international workshop on mining and learning with graphs, vol.~8, 2012, p.~1.",
            "matches": []
        }
    ]
}