\section{Experiments}
This section evaluates the performance of MA-PHLP. 
The experiments were also conducted using only zero-dimensional homology (MA-PHLP (dim$0$)).
We used the area under the curve (AUC)~\cite{bradley1997use} as an evaluation metric. We repeated all experiments 10 times and reported the mean and standard deviation of the AUC values. 

\subsection{Experimental Settings}

\noindent\textbf{Baselines.} To evaluate the effectiveness of PHLP, we compared the proposed model with five heuristic methods, four embedding-based methods, and two GNN-based models. The heuristic methods include the Adamic-Adar (AA)~\cite{adamic2003friends}, Katz index (Katz)~\cite{katz1953new}, PageRank (PR)~\cite{brin1998anatomy}, Weisfeiler-Lehman graph kernel (WLK)~\cite{shervashidze2011weisfeiler}, and Weisfeiler-Lehman neural machine (WLNM)~\cite{zhang2017weisfeiler}. 
For the embedding-based methods, we applied N2V~\cite{grover2016node2vec}, spectral clustering (SPC)~\cite{tang2011leveraging}, matrix factorization (MF)~\cite{koren2009matrix}, and LINE~\cite{tang2015line}. Moreover, SEAL~\cite{zhang2018link} and WP~\cite{pan2021neural} represent the GNN-based methods. 

\noindent\textbf{Datasets.}
\begin{table}[ht!]
\centering
\caption{Statistics of the datasets}
\begin{tabular}{l|ccccc}
\toprule
Dataset & \#Nodes & \#Edges & Avg. node deg. & Density \\
\midrule
{USAir} & 332 & 2126 & 12.81 & 3.86e-2 \\ 
{NS} & 1589 & 2742 & 3.45 & 2.17e-3 \\ 
{PB} & 1222 & 16714 & 27.36 & 2.24e-2 \\ 
{Yeast} & 2375 & 11693 & 9.85 & 4.15e-3 \\ 
{C.ele} & 297 & 2148 & 14.46 & 4.87e-2 \\ 
{Power} & 4941 & 6594 & 2.67 & 5.40e-4 \\ 
{Router} & 5022 & 6258 & 2.49 & 4.96e-4 \\ 
{E.coli} & 1805 & 15660 & 16.24 & 9.61e-3 \\ 
\bottomrule
\end{tabular}
\label{tbl:dataset}
\end{table}
In line with previous studies~\cite{zhang2018link} and~\cite{pan2021neural}, we evaluate the performance of our MA-PHLP on the eight datasets in Table~\ref{tbl:dataset} without node attributes: USAir~\cite{batagelj2006pajek}, NS~\cite{newman2006finding}, PB~\cite{ackland2005mapping}, Yeast~\cite{von2002comparative}, C.~elegans (C.~ele)~\cite{watts1998collective}, Power~\cite{watts1998collective}, Router~\cite{spring2002measuring}, and E.~coli~\cite{zhang2018beyond}. 
The detailed statistics for each dataset are summarized in Table~\ref{tbl:dataset}.

\noindent\textbf{Implementation Details.} 
All edges in the datasets were split into training, validation, and testing datasets with proportions of $0.85$, $0.05$, and $0.1$, respectively, ensuring a fair comparison with previous studies.
The max hop $M$ was set to $3$ for most datasets (Table~\ref{tbl:MA-PHLP}). 
However, for the E.~coli dataset, it was reduced to $2$ when employing one-dimensional homology due to memory constraints. 
Conversely, for the Power dataset, the max hop was set to $7$ because it does not demand heavy memory and computation time.
The sigmoid function was employed for the activation function of the PHLP classifier.
Tables~\ref{tbl:PHLP+SEAL} and~\ref{tbl:PHLP+WP} present the results of the hybrid methods using SEAL~\cite{zhang2018link} and WP~\cite{pan2021neural}, respectively.
For these experiments, a two-layer MLP was used for the MLP $\Phi$ in Step $3$ of Section~\ref{subsec:hybrid}. 
We set the $k$-hops following the original methods, SEAL and WP, and the max hops $M$ of MA-PHLP were set as the $k$, except for the Power dataset.
For the Power dataset, we set the $k$-hop to $1$-hop and max hop $M$ to $7$, respectively, which is discussed in detail in Section~\ref{hop_analysis}.

\begin{table*}
\centering
\caption{Link prediction performance measured by the AUC on Benchmark datasets (90\% observed links)}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccccccc}
\toprule
Dataset & USAir & NS & PB & Yeast & C.~ele & Power & Router & E.~coli \\
\midrule

AA & $95.06 \pm 1.03$ & $94.45 \pm 0.93$ & $92.36 \pm 0.34$ & $89.43 \pm 0.62$ & $86.95 \pm 1.40$ & $58.79 \pm 0.88$ & $56.43 \pm 0.51$ & $95.36 \pm 0.34$ \\
Katz & $92.88 \pm 1.42$ & $94.85 \pm 1.10$ & $92.92 \pm 0.35$ & $92.24 \pm 0.61$ & $86.34 \pm 1.89$ & $65.39 \pm 1.59$ & $38.62 \pm 1.35$ & $93.50 \pm 0.44$ \\
PR & $94.67 \pm 1.08$ & $94.89 \pm 1.08$ & $93.54 \pm 0.41$ & $92.76 \pm 0.55$ & $90.32 \pm 1.49$ & $66.00 \pm 1.59$ & $38.76 \pm 1.39$ & $95.57 \pm 0.44$ \\
WLK & $96.63 \pm 0.73$ & $98.57 \pm 0.51$ & $93.83 \pm 0.59$ & $95.86 \pm 0.54$ & $89.72 \pm 1.67$ & $82.41 \pm 3.43$ & $87.42 \pm 2.08$ & $96.94 \pm 0.29$ \\
WLNM & $95.95 \pm 1.10$ & $98.61 \pm 0.49$ & $93.49 \pm 0.47$ & $95.62 \pm 0.52$ & $86.18 \pm 1.72$ & $84.76 \pm 0.98$ & $94.41 \pm 0.88$ & $97.21 \pm 0.27$ \\
N2V & $91.44 \pm 1.78$ & $91.52 \pm 1.28$ & $85.79 \pm 0.78$ & $93.67 \pm 0.46$ & $84.11 \pm 1.27$ & $76.22 \pm 0.92$ & $65.46 \pm 0.86$ & $90.82 \pm 1.49$ \\
SPC & $74.22 \pm 3.11$ & $89.94 \pm 2.39$ & $83.96 \pm 0.86$ & $93.25 \pm 0.40$ & $51.90 \pm 2.57$ & $91.78 \pm 0.61$ & $68.79 \pm 2.42$ & $94.92 \pm 0.32$ \\
MF & $94.08 \pm 0.80$ & $74.55 \pm 4.34$ & $94.30 \pm 0.53$ & $90.28 \pm 0.69$ & $85.90 \pm 1.74$ & $50.63 \pm 1.10$ & $78.03 \pm 1.63$ & $93.76 \pm 0.56$ \\
LINE & $81.47 \pm 10.71$ & $80.63 \pm 1.90$ & $76.95 \pm 2.76$ & $87.45 \pm 3.33$ & $69.21 \pm 3.14$ & $55.63 \pm 1.47$ & $67.15 \pm 2.10$ & $82.38 \pm 2.19$ \\
SEAL & $97.10 \pm 0.87$ & $98.25 \pm 0.61$ & $95.07 \pm 0.39$ & $97.60 \pm 0.33$ & $89.54 \pm 1.23$ & $86.21 \pm 2.89$ & ${95.07 \pm 1.63}$ & $97.57 \pm 0.30$ \\
WP & $\mathbf{98.20} \pm \mathbf{0.57}$ & $\mathbf{99.12} \pm \mathbf{0.45}$ & $\mathbf{95.42} \pm \mathbf{0.25}$ & $\mathbf{98.21} \pm \mathbf{0.17}$ & $\mathbf{93.30} \pm \mathbf{0.91}$ & $92.11 \pm 0.76$ & $\mathbf{97.15} \pm \mathbf{0.29}$ & $\mathbf{98.54} \pm \mathbf{0.19}$ \\
\midrule
MA-PHLP & $\underline{ 97.10 \pm 0.69 }$ & $\underline{98.88\pm0.45}$ & $\underline{95.10\pm0.26}$ & $\underline{97.98\pm0.22}$ & $\underline{90.33\pm1.16}$ & $\underline{93.05\pm0.45}$ & $96.30\pm0.43$ & $97.64\pm0.20$  \\
MA-PHLP (dim$0$) & $97.10\pm0.73$ & $98.78\pm0.65$ & $95.06\pm0.28$ & $97.98\pm0.23$ & $89.88\pm1.22$ & $\mathbf{93.37} \pm\mathbf{0.41}$ & $\underline{96.37\pm0.43}$ & $\underline{97.72\pm0.17}$  \\
\bottomrule
\end{tabular}}
\label{tbl:MA-PHLP}
\end{table*}

\subsection{Results}
\noindent\textbf{Results of MA-PHLP.}
Table~\ref{tbl:MA-PHLP} presents the AUC scores for each model on the benchmark datasets. 
Bold marks the best results, and underline indicates the second-best results.
The results of AA, Katz, WLK, WLNM, N2V, SPC, MF, LINE, and SEAL are copied from SEAL~\cite{zhang2018link} for comparison. 
The MA-PHLP demonstrates high performance across most datasets, achieving competitive scores.
The proposed model outperforms several baselines, falling between the SEAL and WP models in terms of the AUC score. 
Notably, for the Power dataset, MA-PHLP achieves the highest AUC score, indicating its effectiveness in capturing link patterns.

\noindent\textbf{Results of Hybrid Methods.}
\begin{table}[ht!]
\centering
\caption{AUC scores for SEAL with and without TDA features}
\begin{tabular}{l|cccccccccc}
\toprule
Dataset & SEAL & MA-PHLP + SEAL \\
\midrule
USAir & $97.10 \pm 0.87$ & $\mathbf{97.41} \pm \mathbf{0.62}$ \\
NS & $98.25 \pm 0.61$ & $\mathbf{98.97} \pm \mathbf{0.30}$ \\
PB & $95.07\pm0.39$ & $\mathbf{95.14} \pm \mathbf{0.39}$ \\
Yeast & $97.60\pm0.33$ & $\mathbf{97.93} \pm \mathbf{0.18}$ \\
C.ele & $89.54 \pm 1.23$ & $\mathbf{89.61} \pm \mathbf{1.12}$ \\
Power & $86.21 \pm 2.89$ & $\mathbf{95.53} \pm \mathbf{0.33}$ \\
Router & $95.07 \pm 1.63$ & $\mathbf{96.15} \pm \mathbf{1.26}$ \\
E.coli & $97.57\pm0.30$ & $\mathbf{97.93}\pm\mathbf{0.34}$ \\
\bottomrule
\end{tabular}\label{tbl:PHLP+SEAL}
\end{table}
Simply concatenating the PI vector calculated using PHLP with the final output of the SEAL model increases AUC scores for all datasets, as listed in Table~\ref{tbl:PHLP+SEAL}. This outcome suggests that when the SEAL model lacks topological information for inference, 
the vectors calculated using PHLP can serve as additional inputs.

\begin{table}[ht!]
\centering
\caption{AUC scores for WALKPOOL (WP) \\ with and without TDA features}
\begin{tabular}{l|cccccccccc}
\toprule
Dataset & WP & MA-PHLP + WP \\
\midrule
USAir & $ 98.20\pm0.57 $ & $\mathbf{98.27} \pm \mathbf{0.53}$ \\
NS & $ 99.12\pm0.45 $ & $\mathbf{99.24} \pm \mathbf{0.32}$ \\
PB & $ 95.42\pm0.25 $ & $\mathbf{95.58} \pm \mathbf{0.32}$ \\
Yeast & $ 98.21\pm0.17 $ & $\mathbf{98.25} \pm \mathbf{0.18}$ \\
C.ele & $ 93.30\pm0.91 $ & $\mathbf{93.32} \pm \mathbf{0.71}$ \\ 
Power & $ 92.11\pm0.76 $ & $\mathbf{96.09} \pm \mathbf{0.38}$ \\
Router & $ 97.15\pm0.29 $ & $\mathbf{97.18} \pm \mathbf{0.24}$ \\
E.coli & $ 98.54\pm0.19 $ & $\mathbf{98.57} \pm \mathbf{0.20}$ \\
\bottomrule
\end{tabular}\label{tbl:PHLP+WP}
\end{table}
Similarly, we attempted to hybridize PHLP with the current SOTA model, WP. 
As presented in Table~\ref{tbl:PHLP+WP}, a slight increase in AUC scores is observed for all datasets. The Power dataset demonstrates significant improvement.


\subsection{Ablation Study}
\noindent\textbf{Effects of Degree DRNL.} To assess the proposed Degree DRNL regarding the influence of incorporating degree information on model performance, we conducted experiments using DRNL and Degree DRNL and compared the results. 
\begin{table}[ht!]
\centering
\caption{AUC scores for MA-PHLP (dim$0$) by node labeling}
\begin{tabular}{l|cccccccccc}
\toprule
Dataset & DRNL & Degree DRNL \\
\midrule
USAir & $96.73\pm0.64$ & $\mathbf{97.10} \pm \mathbf{0.73}$ \\
NS & ${98.35} \pm {0.58}$ & $\mathbf{98.78}\pm\mathbf{0.65}$ \\ 
PB & $94.49\pm0.27$ & $\mathbf{95.06} \pm \mathbf{0.28}$ \\ 
Yeast & $97.42\pm0.27$ & $\mathbf{97.98} \pm \mathbf{0.23}$ \\
C.ele& $88.97\pm1.37$ & $\mathbf{89.88} \pm \mathbf{1.22}$ \\
Power & $88.51\pm0.81$ & $\mathbf{92.77} \pm \mathbf{0.47}$ \\
Router & $96.21\pm0.53$ & $\mathbf{96.37} \pm \mathbf{0.43}$ \\ 
E.coli & $97.15\pm0.18$ & $\mathbf{97.72} \pm \mathbf{0.17}$ \\
\bottomrule
\end{tabular}
\label{tbl:nodelabel}
\end{table}
We used MA-PHLP (dim$0$) for the experiments.
Table~\ref{tbl:nodelabel} presents the AUC scores of MA-PHLP (dim$0$) with DRNL and Degree DRNL.
Across all datasets, MA-PHLP (dim$0$) yields higher AUC scores when used with Degree DRNL than with DRNL.
The substantial improvement observed in the Power dataset is noteworthy, where Degree DRNL yields an increase of over $4$ points in the AUC score.
These experiments demonstrate the importance of incorporating degree information into node labeling, revealing its efficacy in enhancing the performance of MA-PHLP. 


\begin{table}[ht!]
\centering
\caption{AUC scores for MA-PHLP (dim$0$) with various $(k,l)$-angle hops}
\begin{tabularx}{0.44\textwidth}{l|YY}
\toprule
Dataset & (1,0) & (1,1) \\
\midrule
USAir & $\mathbf{96.15}\pm\mathbf{0.83}$ & $95.87\pm0.83$ \\
NS & $98.28\pm0.55$ & $\mathbf{98.66}\pm\mathbf{0.66}$ \\
PB & $93.95\pm0.34$ & $\mathbf{94.46}\pm\mathbf{0.36}$ \\
Yeast & $95.52\pm0.32$ & $\mathbf{97.31}\pm\mathbf{0.20}$ \\
C.ele & $86.18\pm2.12$ & $\mathbf{87.57}\pm\mathbf{1.20}$ \\
Power & $73.39\pm0.99$ & $\mathbf{77.83}\pm\mathbf{1.44}$ \\
Router & $92.09\pm0.57$ & $\mathbf{93.25}\pm\mathbf{0.47}$ \\
E.coli & $96.94\pm0.24$ & $\mathbf{96.95}\pm\mathbf{0.28}$ 
\end{tabularx}

\begin{tabularx}{0.44\textwidth}{l|YYY}
\toprule
Dataset & (2,0) & (2,1) & (2,2) \\
\midrule
USAir & $96.69\pm0.92$ & $96.74\pm0.84$ & $\mathbf{96.85}\pm\mathbf{0.83}$ \\
NS & $\mathbf{98.72}\pm\mathbf{0.51}$ & $98.59\pm0.65$ & $98.56\pm0.47$ \\
PB & $94.78\pm0.30$ & $94.73\pm0.30$ & $\mathbf{94.82}\pm\mathbf{0.24}$ \\
Yeast & $\mathbf{97.71}\pm\mathbf{0.18}$ & $97.66\pm0.27$ & $97.58\pm0.28$ \\
C.ele & $88.86\pm1.48$ & $\mathbf{89.16}\pm\mathbf{1.31}$ & $89.08\pm1.07$ \\
Power & $80.27\pm1.07$ & $83.90\pm1.29$ & $\mathbf{86.12}\pm\mathbf{0.86}$ \\
Router & $95.65\pm0.44$ & $\mathbf{95.71}\pm\mathbf{0.39}$ & $94.51\pm0.69$ \\
E.coli & $97.26\pm0.16$ & $97.29\pm0.24$ & $\mathbf{97.41}\pm\mathbf{0.21}$ \\
\bottomrule
\end{tabularx}
\label{tbl:angle}
\end{table}

\noindent\textbf{Angles of PHLP.} Table~\ref{tbl:angle} presents the performance of PHLP (dim $0$) concerning various \((k,l)\)-angle hop subgraphs.
Section~\ref{subsec:anglehop} proposed angle hop subgraphs as an alternative to traditional $k$-hop subgraphs to capture information from various perspectives. 
Moreover, MA-PHLP is proposed to aggregate information from multiple angles. 
To investigate performance when extracting information from specific angles, we conducted experiments using PHLP at different angles.
We used only zero-dimensional PIs for the experiments.
Overall, the results demonstrate that the performance is favorable for cases corresponding to the $k$-hop subgraph (where $k$ and $l$ are the same). 
However, some datasets perform better when $k$ and $l$ differ, highlighting the importance of varying angles to achieve the best performance. Therefore, using MA-PHLP is recommended to maximize performance consistently across datasets.


\noindent\textbf{Comparison with TLC-GNN.}
To demonstrate that the proposed method extracts superior topological information compared to the conventional TLC-GNN approach, we conducted the same experiments. 
The TLC-GNN was constructed by augmenting the graph convolutional network (GCN) model with PI information. 
We replaced the PI component of the TLC-GNN model with the PI vector produced by MA-PHLP, resulting in the MA-PHLP-GNN. 
The zero-dimensional PH was employed in this study for fair comparison because TLC-GNN used only zero-dimensional PH.
Additionally, we conducted experiments where the PH vectors were replaced with zero vectors, denoted as GCN. 
Table~\ref{tbl:tlcgnn} presents the experimental results.
\begin{table}[ht!]
\centering
\caption{comparison of AUC scores with TLC-GNN}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l|cccccccccc}
\toprule
Dataset & GCN & TLC-GNN & MA-PHLP-GNN \\ 
\midrule
Cora & $92.20\pm0.83$ & $\mathbf{93.16}\pm\mathbf{0.56}$ & $93.14\pm0.93$ \\ 
CiteSeer & $86.52\pm1.29$ & $87.38\pm0.97$ & $\mathbf{92.08}\pm\mathbf{0.53}$ \\ 
PubMed & $96.63\pm0.15$ & $96.30\pm0.25$ & $\mathbf{98.07}\pm\mathbf{0.07}$ \\ 
\bottomrule
\end{tabular}}
\label{tbl:tlcgnn}
\end{table}

The TLC-GNN is employed when the given data includes node attributes.
Hence, we conducted experiments using the following widely used benchmark datasets with node attributes: Cora~\cite{mccallum2000automating}, CiteSeer~\cite{giles1998citeseer}, and PubMed~\cite{namata2012query}. 
The MA-PHLP-GNN outperformed the TLC-GNN significantly on the CiteSeer and PubMed datasets while achieving similar performance on the Cora dataset. 
The TLC-GNN does not exhibit performance improvement for the PubMed dataset despite adding topological information. 
However, the proposed MA-PHLP-GNN demonstrates substantial performance enhancement.
Although the proposed model is developed for datasets without node attributes, it exhibits effective performance on datasets with node attributes through hybridization with the existing methods: SEAL+PHLP, WP+PHLP, and MA-PHLP-GNN. 
These experiments verify the versatility and effectiveness of this approach across diverse datasets.

\subsection{The hops and max hops of the hybrid methods}
\label{hop_analysis}

\begin{table*}[h!]
\centering
\caption{AUC scores on the power dataset varying $k$-hop and max hop $M$ of the hybrid methods}
\label{tab:hybrid_grid}
\renewcommand{\arraystretch}{1.2} 
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|ccccccccc}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{7}{c}{MA-PHLP (with max hop $M$)} \\
\midrule
\multicolumn{2}{c|}{$M$}& $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$\\
\midrule
\multirow{8}{*}{\rotatebox{90}{SEAL (with $k$-hop)}}& \multicolumn{1}{c|}{$k$}&\multicolumn{3}{c}{not robust to $k$} & \multicolumn{4}{|c}{robust to $k$} \\
& \multicolumn{1}{c|}{$1$} & $86.66 \pm 0.56$ & $90.22 \pm 0.79$ & $92.63 \pm 0.54$ & \multicolumn{1}{|c}{$94.50 \pm 0.41$} & $95.12 \pm 0.40$ & $95.46 \pm 0.38$ & $\textbf{95.53 $\pm$ 0.33}$ \\
& \multicolumn{1}{c|}{$2$} & $91.40 \pm 0.88$ & $90.20 \pm 0.80$ & $92.50 \pm 0.59$ & \multicolumn{1}{|c}{$94.39 \pm 0.39$} & $95.00 \pm 0.46$ & $95.31 \pm 0.40$ & $\textbf{95.39 $\pm$ 0.36}$ \\
& \multicolumn{1}{c|}{$3$} & $93.21 \pm 0.64$ & $92.79 \pm 0.60$ & $92.57 \pm 0.58$ & \multicolumn{1}{|c}{$94.22 \pm 0.43$} & $94.86 \pm 0.42$ & $\textbf{95.21 $\pm$ 0.45}$ & $95.19 \pm 0.44$ \\
& \multicolumn{1}{c|}{$4$} & $94.51 \pm 0.58$ & $94.23 \pm 0.34$ & $94.21 \pm 0.41$ & \multicolumn{1}{|c}{$94.31 \pm 0.40$} & $94.80 \pm 0.37$ & $95.10 \pm 0.33$ & $\textbf{95.27 $\pm$ 0.36}$ \\
& \multicolumn{1}{c|}{$5$} & $94.73 \pm 0.56$ & $94.45 \pm 0.44$ & $94.61 \pm 0.51$ & \multicolumn{1}{|c}{$94.80 \pm 0.53$} & $94.91 \pm 0.54$ & $95.13 \pm 0.51$ & $\textbf{95.19 $\pm$ 0.46}$ \\
& \multicolumn{1}{c|}{$6$} & $94.58 \pm 0.94$ & $94.81 \pm 0.32$ & $94.87 \pm 0.42$ & \multicolumn{1}{|c}{$95.06 \pm 0.50$} & $95.11 \pm 0.46$ & $\textbf{95.25 $\pm$ 0.45}$ & $95.25 \pm 0.46$ \\
& \multicolumn{1}{c|}{$7$} & $93.97 \pm 0.73$ & $94.22 \pm 0.35$ & $94.43 \pm 0.44$ & \multicolumn{1}{|c}{$94.78 \pm 0.45$} & $94.92 \pm 0.39$ & $\textbf{94.99 $\pm$ 0.52}$ & $94.98 \pm 0.39$ \\
\midrule
\multirow{8}{*}{\rotatebox{90}{WP (with $k$-hop)}}& \multicolumn{1}{c|}{$k$}& \multicolumn{2}{c}{not 
robust to $k$} & \multicolumn{5}{|c}{robust to $k$} \\
& \multicolumn{1}{c|}{$1$} & $87.53 \pm 0.73$ & $91.48 \pm 0.64$ & \multicolumn{1}{|c}{$93.55 \pm 0.48$} & $94.84 \pm 0.43$ & $95.53 \pm 0.46$ & $95.88 \pm 0.31$ & $\textbf{96.09 $\pm$ 0.38}$ \\
& \multicolumn{1}{c|}{$2$} & $92.51 \pm 0.58$ & $91.59 \pm 0.77$ & \multicolumn{1}{|c}{$93.49 \pm 0.58$} & $94.83 \pm 0.53$ & $95.56 \pm 0.59$ & $95.88 \pm 0.38$ & $\textbf{96.06 $\pm$ 0.45}$ \\
& \multicolumn{1}{c|}{$3$} & $94.04 \pm 0.46$ & $93.07 \pm 0.67$ & \multicolumn{1}{|c}{$93.61 \pm 0.52$} & $94.86 \pm 0.54$ & $95.61 \pm 0.60$ & $95.86 \pm 0.40$ & $\textbf{96.00 $\pm$ 0.52}$ \\
& \multicolumn{1}{c|}{$4$} & $93.55 \pm 0.71$ & $92.61 \pm 0.76$ & \multicolumn{1}{|c}{$93.68 \pm 0.55$} & $94.85 \pm 0.55$ & $95.59 \pm 0.58$ & $95.87 \pm 0.38$ & $\textbf{96.03 $\pm$ 0.45}$ \\
& \multicolumn{1}{c|}{$5$} & $93.40 \pm 0.70$ & $92.64 \pm 0.69$ & \multicolumn{1}{|c}{$93.66 \pm 0.53$} & $94.84 \pm 0.54$ & $95.55 \pm 0.59$ & $95.85 \pm 0.39$ & $\textbf{96.04 $\pm$ 0.52}$ \\
& \multicolumn{1}{c|}{$6$} & $93.34 \pm 0.75$ & $92.66 \pm 0.72$ & \multicolumn{1}{|c}{$93.64 \pm 0.55$} & $94.91 \pm 0.57$ & $95.55 \pm 0.58$ & $95.85 \pm 0.44$ & $\textbf{95.98 $\pm$ 0.55}$ \\
& \multicolumn{1}{c|}{$7$} & $93.30 \pm 0.73$ & $92.61 \pm 0.69$ & \multicolumn{1}{|c}{$93.65 \pm 0.56$} & $94.87 \pm 0.56$ & $95.56 \pm 0.58$ & $95.90 \pm 0.39$ & $\textbf{96.01 $\pm$ 0.52}$ \\
\bottomrule   
\end{tabular}}
\end{table*}

Determining the hyperparameters such as ``hop" and ``max hop" is crucial for the performance of the hybrid method. We conducted experiments to explore the effects of different combinations of these parameters. 
Given that the hybrid methods (e.g., MA-PHLP + SEAL and MA-PHLP + WP) exhibited the highest performance improvement on the Power dataset, 
we conducted experiments on the Power dataset.
Table~\ref{tab:hybrid_grid} presents the AUC scores for varying hop (SEAL or WP) and max hop (MA-PHLP).
For each target node, while the SEAL and WP extract a $k$-hop subgraph, the MA-PHLP calculates the PIs based on a subgraph with max hop $M$.
When the parameter \( M \) is \( 1 \) or \( 2 \), the AUC scores are not robust to \( k \), showing large variations; however, when \( M \) is \( 3 \), although MA-PHLP + SEAL still exhibits variations up to \(2\), MA-PHLP + WP shows only minor variations.
As $M$ exceeds $3$, the AUC scores of MA-PHLP + SEAL and MA-PHLP + WP are robust to $k$, exhibiting little sensitivity (maximum $0.84$) to variations.
This suggests that setting both the hop and the max hop to identical values may be permissible without further searching for optimal hyperparameters.
